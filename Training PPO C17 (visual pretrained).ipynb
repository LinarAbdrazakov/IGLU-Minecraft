{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "from models import VisualEncoder\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        features_dim = 512\n",
    "        self.encoder = VisualEncoder(features_dim)\n",
    "        self.encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AtariCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.action_head = nn.Linear(features_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(features_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.encoder.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        if self.use_cuda:\n",
    "            obs.cuda()\n",
    "            \n",
    "        features = self.encoder(obs)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "    env.update_taskset(TaskSet(preset=['C17']))\n",
    "    env = PovOnlyWrapper(env)\n",
    "    env = IgluActionWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-18 10:02:20,060\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-09-18 10:02:20,070\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[2m\u001b[36m(pid=35494)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=35494)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO C17 pretrained</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/82e40_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/82e40_00000</a><br/>\n",
       "                Run data is saved locally in <code>/IGLU-Minecraft/wandb/run-20210918_100220-82e40_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35494)\u001b[0m 2021-09-18 10:02:23,575\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=35494)\u001b[0m 2021-09-18 10:02:23,575\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=35494)\u001b[0m 2021-09-18 10:02:29,425\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-03-29\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0\n",
      "  episode_reward_mean: -2.0\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3492794672648112\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012402718118078582\n",
      "          policy_loss: -0.025099800527095796\n",
      "          total_loss: 0.01665419919623269\n",
      "          vf_explained_var: 0.426866352558136\n",
      "          vf_loss: 0.05276625651038355\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.38850574712643\n",
      "    ram_util_percent: 80.34137931034485\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03996095457277098\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 57.856870578838276\n",
      "    mean_inference_ms: 1.4974994735641556\n",
      "    mean_raw_obs_processing_ms: 0.1514219975733495\n",
      "  time_since_restore: 60.33184623718262\n",
      "  time_this_iter_s: 60.33184623718262\n",
      "  time_total_s: 60.33184623718262\n",
      "  timers:\n",
      "    learn_throughput: 1538.805\n",
      "    learn_time_ms: 649.855\n",
      "    load_throughput: 58270.408\n",
      "    load_time_ms: 17.161\n",
      "    sample_throughput: 16.762\n",
      "    sample_time_ms: 59659.269\n",
      "    update_time_ms: 2.192\n",
      "  timestamp: 1631959409\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         60.3318</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">      -2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-03-39\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -1.0\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5049749784999424\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009203416569786791\n",
      "          policy_loss: -0.04313008727298843\n",
      "          total_loss: -0.04535575947827763\n",
      "          vf_explained_var: 0.14363862574100494\n",
      "          vf_loss: 0.010983393232648572\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.300000000000004\n",
      "    ram_util_percent: 87.62142857142858\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03956392439339839\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 45.3881735265704\n",
      "    mean_inference_ms: 1.464721563872073\n",
      "    mean_raw_obs_processing_ms: 0.1414445261613236\n",
      "  time_since_restore: 70.44574737548828\n",
      "  time_this_iter_s: 10.113901138305664\n",
      "  time_total_s: 70.44574737548828\n",
      "  timers:\n",
      "    learn_throughput: 1618.057\n",
      "    learn_time_ms: 618.025\n",
      "    load_throughput: 97821.769\n",
      "    load_time_ms: 10.223\n",
      "    sample_throughput: 28.911\n",
      "    sample_time_ms: 34589.061\n",
      "    update_time_ms: 2.437\n",
      "  timestamp: 1631959419\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         70.4457</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">      -1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-03-49\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6666666666666666\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 3\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.11498308579127\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0170997831035636\n",
      "          policy_loss: -0.01669652871787548\n",
      "          total_loss: -0.020949005087216695\n",
      "          vf_explained_var: -0.019618937745690346\n",
      "          vf_loss: 0.013477399971129166\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.83571428571428\n",
      "    ram_util_percent: 87.60714285714286\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039364690829532235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.38179052019549\n",
      "    mean_inference_ms: 1.4482539708923878\n",
      "    mean_raw_obs_processing_ms: 0.14721359671725912\n",
      "  time_since_restore: 79.95124983787537\n",
      "  time_this_iter_s: 9.505502462387085\n",
      "  time_total_s: 79.95124983787537\n",
      "  timers:\n",
      "    learn_throughput: 1656.968\n",
      "    learn_time_ms: 603.512\n",
      "    load_throughput: 125510.324\n",
      "    load_time_ms: 7.967\n",
      "    sample_throughput: 38.412\n",
      "    sample_time_ms: 26033.695\n",
      "    update_time_ms: 2.278\n",
      "  timestamp: 1631959429\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         79.9512</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">-0.666667</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-04-02\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 4\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9144049260351392\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010913096190881235\n",
      "          policy_loss: 0.09790919232699606\n",
      "          total_loss: 0.08772630592187246\n",
      "          vf_explained_var: 0.19994424283504486\n",
      "          vf_loss: 0.006778542905683733\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.599999999999994\n",
      "    ram_util_percent: 93.95\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03920729898776956\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 34.03534031672191\n",
      "    mean_inference_ms: 1.43493062539071\n",
      "    mean_raw_obs_processing_ms: 0.1470359631241357\n",
      "  time_since_restore: 93.02604532241821\n",
      "  time_this_iter_s: 13.074795484542847\n",
      "  time_total_s: 93.02604532241821\n",
      "  timers:\n",
      "    learn_throughput: 1654.271\n",
      "    learn_time_ms: 604.496\n",
      "    load_throughput: 141822.836\n",
      "    load_time_ms: 7.051\n",
      "    sample_throughput: 44.17\n",
      "    sample_time_ms: 22639.775\n",
      "    update_time_ms: 2.273\n",
      "  timestamp: 1631959442\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">          93.026</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">    -0.5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-04-10\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.4\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 5\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1709410429000853\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009136174667206376\n",
      "          policy_loss: 0.18994184368186526\n",
      "          total_loss: 0.17185809537768365\n",
      "          vf_explained_var: -0.13642852008342743\n",
      "          vf_loss: 0.0017984241939201537\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 5000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.833333333333336\n",
      "    ram_util_percent: 94.72500000000001\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03905221390257786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.829267580892832\n",
      "    mean_inference_ms: 1.4239456660462182\n",
      "    mean_raw_obs_processing_ms: 0.14589811052778723\n",
      "  time_since_restore: 101.15279722213745\n",
      "  time_this_iter_s: 8.126751899719238\n",
      "  time_total_s: 101.15279722213745\n",
      "  timers:\n",
      "    learn_throughput: 1669.448\n",
      "    learn_time_ms: 599.001\n",
      "    load_throughput: 159338.682\n",
      "    load_time_ms: 6.276\n",
      "    sample_throughput: 50.968\n",
      "    sample_time_ms: 19620.163\n",
      "    update_time_ms: 2.244\n",
      "  timestamp: 1631959450\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         101.153</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">    -0.4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-04-19\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3333333333333333\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 6\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3826948483784993\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010733825029628758\n",
      "          policy_loss: -0.04424335988652375\n",
      "          total_loss: -0.05943734370585945\n",
      "          vf_explained_var: -0.3178164064884186\n",
      "          vf_loss: 0.006486203524077104\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.04615384615385\n",
      "    ram_util_percent: 94.63846153846154\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0389028103644272\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.379318824126745\n",
      "    mean_inference_ms: 1.4149952985762166\n",
      "    mean_raw_obs_processing_ms: 0.14466051963528134\n",
      "  time_since_restore: 110.0023581981659\n",
      "  time_this_iter_s: 8.849560976028442\n",
      "  time_total_s: 110.0023581981659\n",
      "  timers:\n",
      "    learn_throughput: 1677.084\n",
      "    learn_time_ms: 596.273\n",
      "    load_throughput: 173539.455\n",
      "    load_time_ms: 5.762\n",
      "    sample_throughput: 56.412\n",
      "    sample_time_ms: 17726.65\n",
      "    update_time_ms: 2.211\n",
      "  timestamp: 1631959459\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         110.002</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">-0.333333</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 7000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-04-28\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.2857142857142857\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 7\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3844860752423602\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013298341637684816\n",
      "          policy_loss: 0.021755423479610018\n",
      "          total_loss: 0.0026361430684725446\n",
      "          vf_explained_var: -0.23994502425193787\n",
      "          vf_loss: 0.0020659138716938386\n",
      "    num_agent_steps_sampled: 7000\n",
      "    num_agent_steps_trained: 7000\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 7000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.43333333333334\n",
      "    ram_util_percent: 94.53333333333335\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03877632712591329\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.43564954710652\n",
      "    mean_inference_ms: 1.4076258745016808\n",
      "    mean_raw_obs_processing_ms: 0.14351887608953753\n",
      "  time_since_restore: 118.72496747970581\n",
      "  time_this_iter_s: 8.722609281539917\n",
      "  time_total_s: 118.72496747970581\n",
      "  timers:\n",
      "    learn_throughput: 1689.084\n",
      "    learn_time_ms: 592.037\n",
      "    load_throughput: 185877.711\n",
      "    load_time_ms: 5.38\n",
      "    sample_throughput: 61.131\n",
      "    sample_time_ms: 16358.286\n",
      "    update_time_ms: 2.18\n",
      "  timestamp: 1631959468\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         118.725</td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\">-0.285714</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-04-38\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.25\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 8\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.284607442220052\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01539579000363403\n",
      "          policy_loss: -0.019971351077159246\n",
      "          total_loss: -0.03671116564008925\n",
      "          vf_explained_var: 0.036355022341012955\n",
      "          vf_loss: 0.0030271016953823467\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.05\n",
      "    ram_util_percent: 94.39999999999999\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866572117026724\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.868361309340138\n",
      "    mean_inference_ms: 1.4016504112790849\n",
      "    mean_raw_obs_processing_ms: 0.14282631820377006\n",
      "  time_since_restore: 128.63887691497803\n",
      "  time_this_iter_s: 9.913909435272217\n",
      "  time_total_s: 128.63887691497803\n",
      "  timers:\n",
      "    learn_throughput: 1686.66\n",
      "    learn_time_ms: 592.888\n",
      "    load_throughput: 195241.69\n",
      "    load_time_ms: 5.122\n",
      "    sample_throughput: 64.612\n",
      "    sample_time_ms: 15476.883\n",
      "    update_time_ms: 2.166\n",
      "  timestamp: 1631959478\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         128.639</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">   -0.25</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-04-48\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.2222222222222222\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 9\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0337217887242636\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010666946317417105\n",
      "          policy_loss: 0.021765688558419545\n",
      "          total_loss: 0.006169976045687993\n",
      "          vf_explained_var: -0.07091762125492096\n",
      "          vf_loss: 0.0026081173059840997\n",
      "    num_agent_steps_sampled: 9000\n",
      "    num_agent_steps_trained: 9000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.54\n",
      "    ram_util_percent: 94.23333333333333\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038565433807080066\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.57348156021745\n",
      "    mean_inference_ms: 1.3965837779704475\n",
      "    mean_raw_obs_processing_ms: 0.14256552009114584\n",
      "  time_since_restore: 138.54640412330627\n",
      "  time_this_iter_s: 9.907527208328247\n",
      "  time_total_s: 138.54640412330627\n",
      "  timers:\n",
      "    learn_throughput: 1685.769\n",
      "    learn_time_ms: 593.201\n",
      "    load_throughput: 202352.926\n",
      "    load_time_ms: 4.942\n",
      "    sample_throughput: 67.609\n",
      "    sample_time_ms: 14790.979\n",
      "    update_time_ms: 2.149\n",
      "  timestamp: 1631959488\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         138.546</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">-0.222222</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-04-58\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.2\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 10\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.500775835249159\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017261451856591916\n",
      "          policy_loss: -0.012421180307865144\n",
      "          total_loss: -0.031928955101304585\n",
      "          vf_explained_var: -0.9992537498474121\n",
      "          vf_loss: 0.002047693588408745\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.87142857142857\n",
      "    ram_util_percent: 94.20000000000002\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03847377749509927\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.484043307732023\n",
      "    mean_inference_ms: 1.3922017871427075\n",
      "    mean_raw_obs_processing_ms: 0.1420623253550991\n",
      "  time_since_restore: 148.47923517227173\n",
      "  time_this_iter_s: 9.932831048965454\n",
      "  time_total_s: 148.47923517227173\n",
      "  timers:\n",
      "    learn_throughput: 1692.992\n",
      "    learn_time_ms: 590.67\n",
      "    load_throughput: 209696.327\n",
      "    load_time_ms: 4.769\n",
      "    sample_throughput: 70.187\n",
      "    sample_time_ms: 14247.592\n",
      "    update_time_ms: 2.123\n",
      "  timestamp: 1631959498\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         148.479</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">    -0.2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 11000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-05-07\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.18181818181818182\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 11\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.28419560458925\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009038869010951661\n",
      "          policy_loss: -0.11170045302973854\n",
      "          total_loss: -0.1308594412687752\n",
      "          vf_explained_var: -0.3595516085624695\n",
      "          vf_loss: 0.0018751916039036586\n",
      "    num_agent_steps_sampled: 11000\n",
      "    num_agent_steps_trained: 11000\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 11000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.41428571428571\n",
      "    ram_util_percent: 94.35714285714288\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03839199695730306\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.550376760031984\n",
      "    mean_inference_ms: 1.3884113135870555\n",
      "    mean_raw_obs_processing_ms: 0.1417263803216315\n",
      "  time_since_restore: 158.1425542831421\n",
      "  time_this_iter_s: 9.663319110870361\n",
      "  time_total_s: 158.1425542831421\n",
      "  timers:\n",
      "    learn_throughput: 1719.275\n",
      "    learn_time_ms: 581.64\n",
      "    load_throughput: 295542.105\n",
      "    load_time_ms: 3.384\n",
      "    sample_throughput: 108.799\n",
      "    sample_time_ms: 9191.26\n",
      "    update_time_ms: 2.092\n",
      "  timestamp: 1631959507\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 11\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         158.143</td><td style=\"text-align: right;\">11000</td><td style=\"text-align: right;\">-0.181818</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-05-18\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.16666666666666666\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 12\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.078777007261912\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008066301129373739\n",
      "          policy_loss: -0.03558609129654037\n",
      "          total_loss: -0.052876170145140754\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0018844302801880985\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.707142857142856\n",
      "    ram_util_percent: 94.40714285714284\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03831601481006016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.744155194221317\n",
      "    mean_inference_ms: 1.3850242590215573\n",
      "    mean_raw_obs_processing_ms: 0.14123883553461242\n",
      "  time_since_restore: 168.36737418174744\n",
      "  time_this_iter_s: 10.224819898605347\n",
      "  time_total_s: 168.36737418174744\n",
      "  timers:\n",
      "    learn_throughput: 1723.51\n",
      "    learn_time_ms: 580.211\n",
      "    load_throughput: 296410.959\n",
      "    load_time_ms: 3.374\n",
      "    sample_throughput: 108.65\n",
      "    sample_time_ms: 9203.896\n",
      "    update_time_ms: 2.011\n",
      "  timestamp: 1631959518\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         168.367</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-0.166667</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 13000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-05-27\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.15384615384615385\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 13\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1653157856729295\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014326294104446507\n",
      "          policy_loss: -0.06463646673493915\n",
      "          total_loss: -0.07067967785729302\n",
      "          vf_explained_var: 0.046967729926109314\n",
      "          vf_loss: 0.012744685273436416\n",
      "    num_agent_steps_sampled: 13000\n",
      "    num_agent_steps_trained: 13000\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 13000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.96923076923077\n",
      "    ram_util_percent: 94.54615384615384\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038244549903719276\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.032560394922953\n",
      "    mean_inference_ms: 1.3819221210609185\n",
      "    mean_raw_obs_processing_ms: 0.14121298481540043\n",
      "  time_since_restore: 177.4257457256317\n",
      "  time_this_iter_s: 9.058371543884277\n",
      "  time_total_s: 177.4257457256317\n",
      "  timers:\n",
      "    learn_throughput: 1723.818\n",
      "    learn_time_ms: 580.108\n",
      "    load_throughput: 298986.627\n",
      "    load_time_ms: 3.345\n",
      "    sample_throughput: 109.179\n",
      "    sample_time_ms: 9159.306\n",
      "    update_time_ms: 2.007\n",
      "  timestamp: 1631959527\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 13\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         177.426</td><td style=\"text-align: right;\">13000</td><td style=\"text-align: right;\">-0.153846</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 14000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-05-37\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.14285714285714285\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 14\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0490508476893106\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009795819833096549\n",
      "          policy_loss: 0.013467979513936572\n",
      "          total_loss: -0.0033467012974951003\n",
      "          vf_explained_var: -0.581901490688324\n",
      "          vf_loss: 0.0017166610710167636\n",
      "    num_agent_steps_sampled: 14000\n",
      "    num_agent_steps_trained: 14000\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.940000000000005\n",
      "    ram_util_percent: 94.53333333333332\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03817707962523749\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.404542505926504\n",
      "    mean_inference_ms: 1.3791402049493156\n",
      "    mean_raw_obs_processing_ms: 0.14125187357074487\n",
      "  time_since_restore: 187.49068021774292\n",
      "  time_this_iter_s: 10.064934492111206\n",
      "  time_total_s: 187.49068021774292\n",
      "  timers:\n",
      "    learn_throughput: 1734.244\n",
      "    learn_time_ms: 576.62\n",
      "    load_throughput: 309129.797\n",
      "    load_time_ms: 3.235\n",
      "    sample_throughput: 112.842\n",
      "    sample_time_ms: 8861.959\n",
      "    update_time_ms: 1.971\n",
      "  timestamp: 1631959537\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 14\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         187.491</td><td style=\"text-align: right;\">14000</td><td style=\"text-align: right;\">-0.142857</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-05-47\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.13333333333333333\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 15\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1211488551563686\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01362459821398226\n",
      "          policy_loss: 0.06957134703795115\n",
      "          total_loss: 0.052586839637822576\n",
      "          vf_explained_var: -0.7435972690582275\n",
      "          vf_loss: 0.0015020610984518297\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_agent_steps_trained: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.278571428571425\n",
      "    ram_util_percent: 94.52142857142859\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03811616689884879\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.846540460126903\n",
      "    mean_inference_ms: 1.3766896460430371\n",
      "    mean_raw_obs_processing_ms: 0.1412638128533401\n",
      "  time_since_restore: 197.7816309928894\n",
      "  time_this_iter_s: 10.290950775146484\n",
      "  time_total_s: 197.7816309928894\n",
      "  timers:\n",
      "    learn_throughput: 1728.331\n",
      "    learn_time_ms: 578.593\n",
      "    load_throughput: 308150.935\n",
      "    load_time_ms: 3.245\n",
      "    sample_throughput: 110.176\n",
      "    sample_time_ms: 9076.412\n",
      "    update_time_ms: 1.963\n",
      "  timestamp: 1631959547\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         197.782</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">-0.133333</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-05-57\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.125\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 16\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2260145505269366\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010386593992773177\n",
      "          policy_loss: -0.03330863792863157\n",
      "          total_loss: -0.049439006133211984\n",
      "          vf_explained_var: -0.7347405552864075\n",
      "          vf_loss: 0.004052457574936044\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.67333333333333\n",
      "    ram_util_percent: 94.52\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03806048610725027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.3458762672008\n",
      "    mean_inference_ms: 1.3744801845561863\n",
      "    mean_raw_obs_processing_ms: 0.14133120425744716\n",
      "  time_since_restore: 207.75699400901794\n",
      "  time_this_iter_s: 9.97536301612854\n",
      "  time_total_s: 207.75699400901794\n",
      "  timers:\n",
      "    learn_throughput: 1733.388\n",
      "    learn_time_ms: 576.905\n",
      "    load_throughput: 309198.163\n",
      "    load_time_ms: 3.234\n",
      "    sample_throughput: 108.806\n",
      "    sample_time_ms: 9190.682\n",
      "    update_time_ms: 1.943\n",
      "  timestamp: 1631959557\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 16\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         207.757</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">  -0.125</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 17000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-06-07\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.11764705882352941\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 17\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.115039261182149\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014789994586797306\n",
      "          policy_loss: 0.016896752868261602\n",
      "          total_loss: 0.0018976710529790984\n",
      "          vf_explained_var: -0.4274230897426605\n",
      "          vf_loss: 0.0031933141658858706\n",
      "    num_agent_steps_sampled: 17000\n",
      "    num_agent_steps_trained: 17000\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 17000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.307142857142864\n",
      "    ram_util_percent: 94.4857142857143\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03800783675260409\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.89359362100829\n",
      "    mean_inference_ms: 1.372488017934703\n",
      "    mean_raw_obs_processing_ms: 0.14152307670902475\n",
      "  time_since_restore: 217.74602031707764\n",
      "  time_this_iter_s: 9.989026308059692\n",
      "  time_total_s: 217.74602031707764\n",
      "  timers:\n",
      "    learn_throughput: 1731.081\n",
      "    learn_time_ms: 577.674\n",
      "    load_throughput: 309590.711\n",
      "    load_time_ms: 3.23\n",
      "    sample_throughput: 107.336\n",
      "    sample_time_ms: 9316.576\n",
      "    update_time_ms: 1.936\n",
      "  timestamp: 1631959567\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 17\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         217.746</td><td style=\"text-align: right;\">17000</td><td style=\"text-align: right;\">-0.117647</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-06-17\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.1111111111111111\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 18\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.257981130811903\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00978872158945647\n",
      "          policy_loss: -0.09547640590204133\n",
      "          total_loss: -0.1068228580057621\n",
      "          vf_explained_var: 0.4405584931373596\n",
      "          vf_loss: 0.009275616151798102\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.04285714285714\n",
      "    ram_util_percent: 94.54999999999998\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03796140289335512\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.482159871025882\n",
      "    mean_inference_ms: 1.3707177970066144\n",
      "    mean_raw_obs_processing_ms: 0.14160946839174243\n",
      "  time_since_restore: 227.45827460289001\n",
      "  time_this_iter_s: 9.712254285812378\n",
      "  time_total_s: 227.45827460289001\n",
      "  timers:\n",
      "    learn_throughput: 1738.737\n",
      "    learn_time_ms: 575.13\n",
      "    load_throughput: 310760.545\n",
      "    load_time_ms: 3.218\n",
      "    sample_throughput: 107.539\n",
      "    sample_time_ms: 9298.959\n",
      "    update_time_ms: 1.939\n",
      "  timestamp: 1631959577\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         227.458</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">-0.111111</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 19000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-06-27\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.10526315789473684\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 19\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.139578527874417\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01137547968255323\n",
      "          policy_loss: -0.0439227856695652\n",
      "          total_loss: -0.06073622860842281\n",
      "          vf_explained_var: -0.7091367840766907\n",
      "          vf_loss: 0.002307245146625468\n",
      "    num_agent_steps_sampled: 19000\n",
      "    num_agent_steps_trained: 19000\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 19000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.72857142857142\n",
      "    ram_util_percent: 94.47142857142858\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03791655228803331\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.107225512913725\n",
      "    mean_inference_ms: 1.3690522793651503\n",
      "    mean_raw_obs_processing_ms: 0.14160678319508158\n",
      "  time_since_restore: 237.55257034301758\n",
      "  time_this_iter_s: 10.094295740127563\n",
      "  time_total_s: 237.55257034301758\n",
      "  timers:\n",
      "    learn_throughput: 1746.736\n",
      "    learn_time_ms: 572.496\n",
      "    load_throughput: 315015.397\n",
      "    load_time_ms: 3.174\n",
      "    sample_throughput: 107.293\n",
      "    sample_time_ms: 9320.262\n",
      "    update_time_ms: 1.955\n",
      "  timestamp: 1631959587\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 19\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         237.553</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\">-0.105263</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-06-35\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 20\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2890706830554537\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009474833195367162\n",
      "          policy_loss: -0.08529599778768089\n",
      "          total_loss: -0.103316220579048\n",
      "          vf_explained_var: -0.22962182760238647\n",
      "          vf_loss: 0.0029755147736674798\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.1\n",
      "    ram_util_percent: 94.54166666666667\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03787399433200237\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.75940696380279\n",
      "    mean_inference_ms: 1.3674725811972468\n",
      "    mean_raw_obs_processing_ms: 0.14151366817856828\n",
      "  time_since_restore: 245.82247734069824\n",
      "  time_this_iter_s: 8.269906997680664\n",
      "  time_total_s: 245.82247734069824\n",
      "  timers:\n",
      "    learn_throughput: 1742.478\n",
      "    learn_time_ms: 573.895\n",
      "    load_throughput: 315332.752\n",
      "    load_time_ms: 3.171\n",
      "    sample_throughput: 109.259\n",
      "    sample_time_ms: 9152.588\n",
      "    update_time_ms: 1.95\n",
      "  timestamp: 1631959595\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 20\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         245.822</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-06-44\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.09523809523809523\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 21\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2092301302485997\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021165039701802505\n",
      "          policy_loss: -0.08358113231758277\n",
      "          total_loss: -0.08915461831622654\n",
      "          vf_explained_var: 0.49158337712287903\n",
      "          vf_loss: 0.012285808041633572\n",
      "    num_agent_steps_sampled: 21000\n",
      "    num_agent_steps_trained: 21000\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 21000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.72307692307693\n",
      "    ram_util_percent: 94.5\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03783696292167173\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.437289774195442\n",
      "    mean_inference_ms: 1.366052976092589\n",
      "    mean_raw_obs_processing_ms: 0.14135613842198338\n",
      "  time_since_restore: 254.82271003723145\n",
      "  time_this_iter_s: 9.000232696533203\n",
      "  time_total_s: 254.82271003723145\n",
      "  timers:\n",
      "    learn_throughput: 1729.335\n",
      "    learn_time_ms: 578.257\n",
      "    load_throughput: 316603.813\n",
      "    load_time_ms: 3.159\n",
      "    sample_throughput: 110.109\n",
      "    sample_time_ms: 9081.896\n",
      "    update_time_ms: 1.951\n",
      "  timestamp: 1631959604\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 21\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         254.823</td><td style=\"text-align: right;\">21000</td><td style=\"text-align: right;\">-0.0952381</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 22000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-06-53\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.09090909090909091\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 22\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.500310116344028\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009462584053949074\n",
      "          policy_loss: 0.023176496517327096\n",
      "          total_loss: 0.003612618034498559\n",
      "          vf_explained_var: -0.07403869926929474\n",
      "          vf_loss: 0.002600449018549019\n",
      "    num_agent_steps_sampled: 22000\n",
      "    num_agent_steps_trained: 22000\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.974999999999994\n",
      "    ram_util_percent: 94.39166666666667\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03780264124646577\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.137279201371967\n",
      "    mean_inference_ms: 1.364738934778974\n",
      "    mean_raw_obs_processing_ms: 0.14114502075065283\n",
      "  time_since_restore: 263.4441890716553\n",
      "  time_this_iter_s: 8.621479034423828\n",
      "  time_total_s: 263.4441890716553\n",
      "  timers:\n",
      "    learn_throughput: 1721.918\n",
      "    learn_time_ms: 580.748\n",
      "    load_throughput: 313052.149\n",
      "    load_time_ms: 3.194\n",
      "    sample_throughput: 112.121\n",
      "    sample_time_ms: 8918.967\n",
      "    update_time_ms: 2.017\n",
      "  timestamp: 1631959613\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 22\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         263.444</td><td style=\"text-align: right;\">22000</td><td style=\"text-align: right;\">-0.0909091</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 23000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-07-03\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08695652173913043\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 23\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.606561716397603\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.001634640364166076\n",
      "          policy_loss: -0.022896224829471772\n",
      "          total_loss: -0.0483690562347571\n",
      "          vf_explained_var: 0.7077088952064514\n",
      "          vf_loss: 0.0001023934458417999\n",
      "    num_agent_steps_sampled: 23000\n",
      "    num_agent_steps_trained: 23000\n",
      "    num_steps_sampled: 23000\n",
      "    num_steps_trained: 23000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.0\n",
      "    ram_util_percent: 94.4357142857143\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037774701402954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.859179851123427\n",
      "    mean_inference_ms: 1.3636163216482597\n",
      "    mean_raw_obs_processing_ms: 0.1410959528778647\n",
      "  time_since_restore: 273.3564896583557\n",
      "  time_this_iter_s: 9.91230058670044\n",
      "  time_total_s: 273.3564896583557\n",
      "  timers:\n",
      "    learn_throughput: 1709.083\n",
      "    learn_time_ms: 585.109\n",
      "    load_throughput: 311561.558\n",
      "    load_time_ms: 3.21\n",
      "    sample_throughput: 111.111\n",
      "    sample_time_ms: 8999.974\n",
      "    update_time_ms: 2.019\n",
      "  timestamp: 1631959623\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 23\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         273.356</td><td style=\"text-align: right;\">23000</td><td style=\"text-align: right;\">-0.0869565</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-07-13\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08333333333333333\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 24\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0317751513587106\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016389916724409165\n",
      "          policy_loss: -0.10435375591946973\n",
      "          total_loss: -0.12068382038010492\n",
      "          vf_explained_var: -0.6981037855148315\n",
      "          vf_loss: 0.001529201044791585\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.333333333333336\n",
      "    ram_util_percent: 94.37999999999998\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037750473791901\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.60143525096674\n",
      "    mean_inference_ms: 1.362640716700896\n",
      "    mean_raw_obs_processing_ms: 0.14100955272870544\n",
      "  time_since_restore: 283.640588760376\n",
      "  time_this_iter_s: 10.284099102020264\n",
      "  time_total_s: 283.640588760376\n",
      "  timers:\n",
      "    learn_throughput: 1698.848\n",
      "    learn_time_ms: 588.634\n",
      "    load_throughput: 307734.93\n",
      "    load_time_ms: 3.25\n",
      "    sample_throughput: 110.886\n",
      "    sample_time_ms: 9018.243\n",
      "    update_time_ms: 2.077\n",
      "  timestamp: 1631959633\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 24\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         283.641</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">-0.0833333</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 25000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-07-23\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 25\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.108827859825558\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012782489632911023\n",
      "          policy_loss: 0.04386012309955226\n",
      "          total_loss: 0.029232029451264276\n",
      "          vf_explained_var: -0.1897844672203064\n",
      "          vf_loss: 0.004542810978212704\n",
      "    num_agent_steps_sampled: 25000\n",
      "    num_agent_steps_trained: 25000\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 25000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.138461538461534\n",
      "    ram_util_percent: 94.33076923076922\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037729138502717335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.36030968208643\n",
      "    mean_inference_ms: 1.3617441714679848\n",
      "    mean_raw_obs_processing_ms: 0.14101729140493088\n",
      "  time_since_restore: 292.99344205856323\n",
      "  time_this_iter_s: 9.352853298187256\n",
      "  time_total_s: 292.99344205856323\n",
      "  timers:\n",
      "    learn_throughput: 1706.076\n",
      "    learn_time_ms: 586.14\n",
      "    load_throughput: 305348.971\n",
      "    load_time_ms: 3.275\n",
      "    sample_throughput: 112.021\n",
      "    sample_time_ms: 8926.886\n",
      "    update_time_ms: 2.067\n",
      "  timestamp: 1631959643\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 25\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         292.993</td><td style=\"text-align: right;\">25000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 26000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-07-32\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.07692307692307693\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 26\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.196173922220866\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015051619816734125\n",
      "          policy_loss: -0.17696228068735864\n",
      "          total_loss: -0.194289730137421\n",
      "          vf_explained_var: -0.043487828224897385\n",
      "          vf_loss: 0.0023765413384858724\n",
      "    num_agent_steps_sampled: 26000\n",
      "    num_agent_steps_trained: 26000\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 26000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.5\n",
      "    ram_util_percent: 94.16428571428571\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03770759431990769\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.134794680169701\n",
      "    mean_inference_ms: 1.3608663944948989\n",
      "    mean_raw_obs_processing_ms: 0.14107181676755984\n",
      "  time_since_restore: 302.7030053138733\n",
      "  time_this_iter_s: 9.709563255310059\n",
      "  time_total_s: 302.7030053138733\n",
      "  timers:\n",
      "    learn_throughput: 1704.358\n",
      "    learn_time_ms: 586.731\n",
      "    load_throughput: 304283.455\n",
      "    load_time_ms: 3.286\n",
      "    sample_throughput: 112.363\n",
      "    sample_time_ms: 8899.721\n",
      "    update_time_ms: 2.076\n",
      "  timestamp: 1631959652\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 26\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         302.703</td><td style=\"text-align: right;\">26000</td><td style=\"text-align: right;\">-0.0769231</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 27000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-07-42\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.07407407407407407\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 27\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2832618872324626\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013483498383594038\n",
      "          policy_loss: -0.041976318839523526\n",
      "          total_loss: -0.05888974852859974\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.003896665058305694\n",
      "    num_agent_steps_sampled: 27000\n",
      "    num_agent_steps_trained: 27000\n",
      "    num_steps_sampled: 27000\n",
      "    num_steps_trained: 27000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.78\n",
      "    ram_util_percent: 94.1866666666667\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03768637708626218\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.924075224544126\n",
      "    mean_inference_ms: 1.3600255505655\n",
      "    mean_raw_obs_processing_ms: 0.14107919962002444\n",
      "  time_since_restore: 312.93423342704773\n",
      "  time_this_iter_s: 10.231228113174438\n",
      "  time_total_s: 312.93423342704773\n",
      "  timers:\n",
      "    learn_throughput: 1692.186\n",
      "    learn_time_ms: 590.952\n",
      "    load_throughput: 291901.537\n",
      "    load_time_ms: 3.426\n",
      "    sample_throughput: 112.113\n",
      "    sample_time_ms: 8919.554\n",
      "    update_time_ms: 2.077\n",
      "  timestamp: 1631959662\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 27\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         312.934</td><td style=\"text-align: right;\">27000</td><td style=\"text-align: right;\">-0.0740741</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-07-53\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.07142857142857142\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 28\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.340794046719869\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0132446583163913\n",
      "          policy_loss: -0.06778586361971166\n",
      "          total_loss: -0.0855235359734959\n",
      "          vf_explained_var: -0.7176200151443481\n",
      "          vf_loss: 0.003683569923871093\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.292857142857144\n",
      "    ram_util_percent: 94.22857142857143\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03766564286444847\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.726608306146996\n",
      "    mean_inference_ms: 1.3592141908334952\n",
      "    mean_raw_obs_processing_ms: 0.14104676911101138\n",
      "  time_since_restore: 323.0331594944\n",
      "  time_this_iter_s: 10.098926067352295\n",
      "  time_total_s: 323.0331594944\n",
      "  timers:\n",
      "    learn_throughput: 1693.593\n",
      "    learn_time_ms: 590.461\n",
      "    load_throughput: 290721.97\n",
      "    load_time_ms: 3.44\n",
      "    sample_throughput: 111.624\n",
      "    sample_time_ms: 8958.675\n",
      "    update_time_ms: 2.078\n",
      "  timestamp: 1631959673\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 28\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         323.033</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">-0.0714286</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 29000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-08-03\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.06896551724137931\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 29\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.28643192715115\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012511591386425132\n",
      "          policy_loss: -0.05006254739645455\n",
      "          total_loss: -0.0692457476630807\n",
      "          vf_explained_var: -0.878873884677887\n",
      "          vf_loss: 0.0018043770648849507\n",
      "    num_agent_steps_sampled: 29000\n",
      "    num_agent_steps_trained: 29000\n",
      "    num_steps_sampled: 29000\n",
      "    num_steps_trained: 29000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.54\n",
      "    ram_util_percent: 94.27999999999997\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03764605224578589\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.541027249016725\n",
      "    mean_inference_ms: 1.3584552138784114\n",
      "    mean_raw_obs_processing_ms: 0.14105016032082537\n",
      "  time_since_restore: 333.1395118236542\n",
      "  time_this_iter_s: 10.10635232925415\n",
      "  time_total_s: 333.1395118236542\n",
      "  timers:\n",
      "    learn_throughput: 1686.446\n",
      "    learn_time_ms: 592.963\n",
      "    load_throughput: 289950.227\n",
      "    load_time_ms: 3.449\n",
      "    sample_throughput: 111.641\n",
      "    sample_time_ms: 8957.292\n",
      "    update_time_ms: 2.138\n",
      "  timestamp: 1631959683\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29000\n",
      "  training_iteration: 29\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">          333.14</td><td style=\"text-align: right;\">29000</td><td style=\"text-align: right;\">-0.0689655</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-08-30\n",
      "  done: false\n",
      "  episode_len_mean: 996.0666666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.06666666666666667\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 30\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3590837717056274\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008589659497914601\n",
      "          policy_loss: 0.03383567391170396\n",
      "          total_loss: 0.01238038121826119\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.000847095013240404\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.08205128205129\n",
      "    ram_util_percent: 92.76923076923077\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03763008071368877\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.367153925469877\n",
      "    mean_inference_ms: 1.357814710762777\n",
      "    mean_raw_obs_processing_ms: 0.15912477228431002\n",
      "  time_since_restore: 360.38003754615784\n",
      "  time_this_iter_s: 27.240525722503662\n",
      "  time_total_s: 360.38003754615784\n",
      "  timers:\n",
      "    learn_throughput: 1680.397\n",
      "    learn_time_ms: 595.098\n",
      "    load_throughput: 222219.491\n",
      "    load_time_ms: 4.5\n",
      "    sample_throughput: 92.156\n",
      "    sample_time_ms: 10851.114\n",
      "    update_time_ms: 2.14\n",
      "  timestamp: 1631959710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 30\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">          360.38</td><td style=\"text-align: right;\">30000</td><td style=\"text-align: right;\">-0.0666667</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           996.067</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 31000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-08-42\n",
      "  done: false\n",
      "  episode_len_mean: 996.1935483870968\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.06451612903225806\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 31\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3378247923321194\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008482138581811455\n",
      "          policy_loss: -0.04345305444051822\n",
      "          total_loss: -0.06497596142192681\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0005830192055630808\n",
      "    num_agent_steps_sampled: 31000\n",
      "    num_agent_steps_trained: 31000\n",
      "    num_steps_sampled: 31000\n",
      "    num_steps_trained: 31000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.70625\n",
      "    ram_util_percent: 93.93125\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0376162958576978\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.204653587941234\n",
      "    mean_inference_ms: 1.3572341983749798\n",
      "    mean_raw_obs_processing_ms: 0.1755105992906144\n",
      "  time_since_restore: 372.09170746803284\n",
      "  time_this_iter_s: 11.711669921875\n",
      "  time_total_s: 372.09170746803284\n",
      "  timers:\n",
      "    learn_throughput: 1674.378\n",
      "    learn_time_ms: 597.237\n",
      "    load_throughput: 219031.714\n",
      "    load_time_ms: 4.566\n",
      "    sample_throughput: 89.927\n",
      "    sample_time_ms: 11120.07\n",
      "    update_time_ms: 2.145\n",
      "  timestamp: 1631959722\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31000\n",
      "  training_iteration: 31\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         372.092</td><td style=\"text-align: right;\">31000</td><td style=\"text-align: right;\">-0.0645161</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           996.194</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-08-52\n",
      "  done: false\n",
      "  episode_len_mean: 996.3125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.0625\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 32\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.308473587036133\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010744570184025942\n",
      "          policy_loss: -0.07316086250874732\n",
      "          total_loss: -0.09289086138208708\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.001743050627475087\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.88\n",
      "    ram_util_percent: 93.66666666666667\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03760321059305469\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.051141133781893\n",
      "    mean_inference_ms: 1.356679548359141\n",
      "    mean_raw_obs_processing_ms: 0.19036066133795487\n",
      "  time_since_restore: 382.3436613082886\n",
      "  time_this_iter_s: 10.251953840255737\n",
      "  time_total_s: 382.3436613082886\n",
      "  timers:\n",
      "    learn_throughput: 1680.289\n",
      "    learn_time_ms: 595.136\n",
      "    load_throughput: 220643.682\n",
      "    load_time_ms: 4.532\n",
      "    sample_throughput: 88.611\n",
      "    sample_time_ms: 11285.298\n",
      "    update_time_ms: 2.087\n",
      "  timestamp: 1631959732\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 32\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         382.344</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\"> -0.0625</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           996.312</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 33000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-09-02\n",
      "  done: false\n",
      "  episode_len_mean: 996.4242424242424\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.06060606060606061\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 33\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2830215374628704\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012350879362846139\n",
      "          policy_loss: -0.11028787520610624\n",
      "          total_loss: -0.12972198190788428\n",
      "          vf_explained_var: -0.5865018367767334\n",
      "          vf_loss: 0.001543475619594877\n",
      "    num_agent_steps_sampled: 33000\n",
      "    num_agent_steps_trained: 33000\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 33000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.46666666666667\n",
      "    ram_util_percent: 93.56666666666663\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03759115133358339\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.906059675444176\n",
      "    mean_inference_ms: 1.3561549909708122\n",
      "    mean_raw_obs_processing_ms: 0.203839324901498\n",
      "  time_since_restore: 392.79410338401794\n",
      "  time_this_iter_s: 10.45044207572937\n",
      "  time_total_s: 392.79410338401794\n",
      "  timers:\n",
      "    learn_throughput: 1695.508\n",
      "    learn_time_ms: 589.794\n",
      "    load_throughput: 221453.334\n",
      "    load_time_ms: 4.516\n",
      "    sample_throughput: 88.149\n",
      "    sample_time_ms: 11344.448\n",
      "    update_time_ms: 2.084\n",
      "  timestamp: 1631959742\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 33\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         392.794</td><td style=\"text-align: right;\">33000</td><td style=\"text-align: right;\">-0.0606061</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           996.424</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 34000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-09-13\n",
      "  done: false\n",
      "  episode_len_mean: 996.5294117647059\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.058823529411764705\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 34\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3102658298280505\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009285722359608888\n",
      "          policy_loss: -0.08411404656039344\n",
      "          total_loss: -0.10522564806871944\n",
      "          vf_explained_var: -0.8286908268928528\n",
      "          vf_loss: 0.0005981981516621697\n",
      "    num_agent_steps_sampled: 34000\n",
      "    num_agent_steps_trained: 34000\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 34000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.26\n",
      "    ram_util_percent: 93.56000000000002\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03757938584104627\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.768432167208232\n",
      "    mean_inference_ms: 1.355639193642689\n",
      "    mean_raw_obs_processing_ms: 0.21610620033781802\n",
      "  time_since_restore: 402.9209957122803\n",
      "  time_this_iter_s: 10.126892328262329\n",
      "  time_total_s: 402.9209957122803\n",
      "  timers:\n",
      "    learn_throughput: 1704.469\n",
      "    learn_time_ms: 586.693\n",
      "    load_throughput: 223075.172\n",
      "    load_time_ms: 4.483\n",
      "    sample_throughput: 88.246\n",
      "    sample_time_ms: 11331.921\n",
      "    update_time_ms: 2.046\n",
      "  timestamp: 1631959753\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 34\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         402.921</td><td style=\"text-align: right;\">34000</td><td style=\"text-align: right;\">-0.0588235</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           996.529</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 35000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-09-23\n",
      "  done: false\n",
      "  episode_len_mean: 996.6285714285714\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05714285714285714\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 35\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2585052437252466\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012728553627258351\n",
      "          policy_loss: -0.037526778142071435\n",
      "          total_loss: -0.05784857821547323\n",
      "          vf_explained_var: -0.8716360330581665\n",
      "          vf_loss: 0.00035396884122747\n",
      "    num_agent_steps_sampled: 35000\n",
      "    num_agent_steps_trained: 35000\n",
      "    num_steps_sampled: 35000\n",
      "    num_steps_trained: 35000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.59285714285715\n",
      "    ram_util_percent: 93.2142857142857\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03756769655848364\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.637795023369282\n",
      "    mean_inference_ms: 1.3551348376945818\n",
      "    mean_raw_obs_processing_ms: 0.22727754811285625\n",
      "  time_since_restore: 413.1682951450348\n",
      "  time_this_iter_s: 10.247299432754517\n",
      "  time_total_s: 413.1682951450348\n",
      "  timers:\n",
      "    learn_throughput: 1705.919\n",
      "    learn_time_ms: 586.194\n",
      "    load_throughput: 224671.588\n",
      "    load_time_ms: 4.451\n",
      "    sample_throughput: 87.551\n",
      "    sample_time_ms: 11421.907\n",
      "    update_time_ms: 2.05\n",
      "  timestamp: 1631959763\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 35\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         413.168</td><td style=\"text-align: right;\">35000</td><td style=\"text-align: right;\">-0.0571429</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           996.629</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-09-33\n",
      "  done: false\n",
      "  episode_len_mean: 996.7222222222222\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05555555555555555\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 36\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.27299751440684\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009408744033687563\n",
      "          policy_loss: -0.03726676627993584\n",
      "          total_loss: -0.05702322791847918\n",
      "          vf_explained_var: -0.6034050583839417\n",
      "          vf_loss: 0.0015622004997567275\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.11333333333334\n",
      "    ram_util_percent: 93.1\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03755628170151195\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.513562154397588\n",
      "    mean_inference_ms: 1.3546334388109293\n",
      "    mean_raw_obs_processing_ms: 0.23747309600759042\n",
      "  time_since_restore: 423.3628900051117\n",
      "  time_this_iter_s: 10.194594860076904\n",
      "  time_total_s: 423.3628900051117\n",
      "  timers:\n",
      "    learn_throughput: 1703.096\n",
      "    learn_time_ms: 587.166\n",
      "    load_throughput: 223784.534\n",
      "    load_time_ms: 4.469\n",
      "    sample_throughput: 87.189\n",
      "    sample_time_ms: 11469.351\n",
      "    update_time_ms: 2.104\n",
      "  timestamp: 1631959773\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 36\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         423.363</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">-0.0555556</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           996.722</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 37000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-09-43\n",
      "  done: false\n",
      "  episode_len_mean: 996.8108108108108\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05405405405405406\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 37\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.278931146197849\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01164173757782585\n",
      "          policy_loss: -0.013244051569037968\n",
      "          total_loss: -0.03353134344021479\n",
      "          vf_explained_var: -0.8039679527282715\n",
      "          vf_loss: 0.0007557584891199237\n",
      "    num_agent_steps_sampled: 37000\n",
      "    num_agent_steps_trained: 37000\n",
      "    num_steps_sampled: 37000\n",
      "    num_steps_trained: 37000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.25714285714285\n",
      "    ram_util_percent: 92.94285714285715\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037544680898336555\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.39517127614609\n",
      "    mean_inference_ms: 1.3541323822835367\n",
      "    mean_raw_obs_processing_ms: 0.24678506159383562\n",
      "  time_since_restore: 433.4135766029358\n",
      "  time_this_iter_s: 10.050686597824097\n",
      "  time_total_s: 433.4135766029358\n",
      "  timers:\n",
      "    learn_throughput: 1717.449\n",
      "    learn_time_ms: 582.259\n",
      "    load_throughput: 229954.659\n",
      "    load_time_ms: 4.349\n",
      "    sample_throughput: 87.288\n",
      "    sample_time_ms: 11456.35\n",
      "    update_time_ms: 2.102\n",
      "  timestamp: 1631959783\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37000\n",
      "  training_iteration: 37\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         433.414</td><td style=\"text-align: right;\">37000</td><td style=\"text-align: right;\">-0.0540541</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           996.811</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 38000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-09-53\n",
      "  done: false\n",
      "  episode_len_mean: 996.8947368421053\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05263157894736842\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 38\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2992008288701373\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012328957178665532\n",
      "          policy_loss: -0.024265974366830454\n",
      "          total_loss: -0.04488927427058419\n",
      "          vf_explained_var: -0.9647973775863647\n",
      "          vf_loss: 0.0005193639386561699\n",
      "    num_agent_steps_sampled: 38000\n",
      "    num_agent_steps_trained: 38000\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 38000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.04666666666667\n",
      "    ram_util_percent: 92.88666666666667\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03753306885298116\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.28229779606968\n",
      "    mean_inference_ms: 1.3536355025409583\n",
      "    mean_raw_obs_processing_ms: 0.25529547350083515\n",
      "  time_since_restore: 443.593544960022\n",
      "  time_this_iter_s: 10.179968357086182\n",
      "  time_total_s: 443.593544960022\n",
      "  timers:\n",
      "    learn_throughput: 1716.791\n",
      "    learn_time_ms: 582.482\n",
      "    load_throughput: 230055.563\n",
      "    load_time_ms: 4.347\n",
      "    sample_throughput: 87.228\n",
      "    sample_time_ms: 11464.259\n",
      "    update_time_ms: 2.083\n",
      "  timestamp: 1631959793\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 38\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         443.594</td><td style=\"text-align: right;\">38000</td><td style=\"text-align: right;\">-0.0526316</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           996.895</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 39000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-10-03\n",
      "  done: false\n",
      "  episode_len_mean: 996.974358974359\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05128205128205128\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 39\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.289631321695116\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00927927369254916\n",
      "          policy_loss: -0.03176774010062218\n",
      "          total_loss: -0.053092758357524875\n",
      "          vf_explained_var: -0.9371511936187744\n",
      "          vf_loss: 0.00017940239324363777\n",
      "    num_agent_steps_sampled: 39000\n",
      "    num_agent_steps_trained: 39000\n",
      "    num_steps_sampled: 39000\n",
      "    num_steps_trained: 39000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.41428571428571\n",
      "    ram_util_percent: 92.89285714285714\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03752159772461942\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.174481412800572\n",
      "    mean_inference_ms: 1.3531440212975328\n",
      "    mean_raw_obs_processing_ms: 0.2630885124068434\n",
      "  time_since_restore: 453.6694691181183\n",
      "  time_this_iter_s: 10.075924158096313\n",
      "  time_total_s: 453.6694691181183\n",
      "  timers:\n",
      "    learn_throughput: 1726.019\n",
      "    learn_time_ms: 579.368\n",
      "    load_throughput: 230494.257\n",
      "    load_time_ms: 4.339\n",
      "    sample_throughput: 87.226\n",
      "    sample_time_ms: 11464.47\n",
      "    update_time_ms: 1.996\n",
      "  timestamp: 1631959803\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 39\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         453.669</td><td style=\"text-align: right;\">39000</td><td style=\"text-align: right;\">-0.0512821</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           996.974</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-10-14\n",
      "  done: false\n",
      "  episode_len_mean: 997.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 40\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.287764321433173\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011653843056801507\n",
      "          policy_loss: 0.013590118371778065\n",
      "          total_loss: -0.007083112125595411\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00045633607062174836\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.29333333333334\n",
      "    ram_util_percent: 92.90000000000002\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0375101561728262\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.071413667921552\n",
      "    mean_inference_ms: 1.352655277404634\n",
      "    mean_raw_obs_processing_ms: 0.2702414413353945\n",
      "  time_since_restore: 463.8162226676941\n",
      "  time_this_iter_s: 10.146753549575806\n",
      "  time_total_s: 463.8162226676941\n",
      "  timers:\n",
      "    learn_throughput: 1736.282\n",
      "    learn_time_ms: 575.943\n",
      "    load_throughput: 304586.181\n",
      "    load_time_ms: 3.283\n",
      "    sample_throughput: 102.463\n",
      "    sample_time_ms: 9759.614\n",
      "    update_time_ms: 2.009\n",
      "  timestamp: 1631959814\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 40\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         463.816</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            997.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 41000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-10-24\n",
      "  done: false\n",
      "  episode_len_mean: 997.1219512195122\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.04878048780487805\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 41\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2976620700624255\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009117265284183404\n",
      "          policy_loss: 0.026681849929607575\n",
      "          total_loss: 0.005243268867747651\n",
      "          vf_explained_var: -0.6900829672813416\n",
      "          vf_loss: 0.00017044857175076483\n",
      "    num_agent_steps_sampled: 41000\n",
      "    num_agent_steps_trained: 41000\n",
      "    num_steps_sampled: 41000\n",
      "    num_steps_trained: 41000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.23571428571429\n",
      "    ram_util_percent: 92.95000000000002\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037498630348123445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.972867049379548\n",
      "    mean_inference_ms: 1.3521723553295466\n",
      "    mean_raw_obs_processing_ms: 0.27680226538054775\n",
      "  time_since_restore: 474.08564043045044\n",
      "  time_this_iter_s: 10.269417762756348\n",
      "  time_total_s: 474.08564043045044\n",
      "  timers:\n",
      "    learn_throughput: 1757.394\n",
      "    learn_time_ms: 569.024\n",
      "    load_throughput: 310772.058\n",
      "    load_time_ms: 3.218\n",
      "    sample_throughput: 103.924\n",
      "    sample_time_ms: 9622.371\n",
      "    update_time_ms: 2.007\n",
      "  timestamp: 1631959824\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 41000\n",
      "  training_iteration: 41\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         474.086</td><td style=\"text-align: right;\">41000</td><td style=\"text-align: right;\">-0.0487805</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           997.122</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 42000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-10-34\n",
      "  done: false\n",
      "  episode_len_mean: 997.1904761904761\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.047619047619047616\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 42\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.299887839953105\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007687775520375527\n",
      "          policy_loss: -0.016077745147049426\n",
      "          total_loss: -0.03758685404641761\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0003366022991637389\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.29333333333333\n",
      "    ram_util_percent: 92.98\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03748727116244214\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.878536922715934\n",
      "    mean_inference_ms: 1.3516948717256867\n",
      "    mean_raw_obs_processing_ms: 0.28282545358339306\n",
      "  time_since_restore: 484.36933994293213\n",
      "  time_this_iter_s: 10.28369951248169\n",
      "  time_total_s: 484.36933994293213\n",
      "  timers:\n",
      "    learn_throughput: 1756.336\n",
      "    learn_time_ms: 569.367\n",
      "    load_throughput: 311071.688\n",
      "    load_time_ms: 3.215\n",
      "    sample_throughput: 103.894\n",
      "    sample_time_ms: 9625.202\n",
      "    update_time_ms: 2.007\n",
      "  timestamp: 1631959834\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 42\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         484.369</td><td style=\"text-align: right;\">42000</td><td style=\"text-align: right;\">-0.047619</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            997.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 43000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-10-44\n",
      "  done: false\n",
      "  episode_len_mean: 997.2558139534884\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.046511627906976744\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 43\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.379678983158535\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016250515255546875\n",
      "          policy_loss: 0.08174675413303906\n",
      "          total_loss: 0.06111873338619868\n",
      "          vf_explained_var: -0.9726847410202026\n",
      "          vf_loss: 0.0007311899407391643\n",
      "    num_agent_steps_sampled: 43000\n",
      "    num_agent_steps_trained: 43000\n",
      "    num_steps_sampled: 43000\n",
      "    num_steps_trained: 43000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.09285714285715\n",
      "    ram_util_percent: 93.06428571428572\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03747618131726558\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.787880109394862\n",
      "    mean_inference_ms: 1.3512232463445886\n",
      "    mean_raw_obs_processing_ms: 0.28835378259880634\n",
      "  time_since_restore: 494.14378023147583\n",
      "  time_this_iter_s: 9.774440288543701\n",
      "  time_total_s: 494.14378023147583\n",
      "  timers:\n",
      "    learn_throughput: 1751.599\n",
      "    learn_time_ms: 570.907\n",
      "    load_throughput: 309993.422\n",
      "    load_time_ms: 3.226\n",
      "    sample_throughput: 104.646\n",
      "    sample_time_ms: 9556.071\n",
      "    update_time_ms: 2.011\n",
      "  timestamp: 1631959844\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 43000\n",
      "  training_iteration: 43\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         494.144</td><td style=\"text-align: right;\">43000</td><td style=\"text-align: right;\">-0.0465116</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           997.256</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-10-54\n",
      "  done: false\n",
      "  episode_len_mean: 997.3181818181819\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.045454545454545456\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 44\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.425692325168186\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013473731710013818\n",
      "          policy_loss: -0.025582917407155036\n",
      "          total_loss: -0.047344146317078006\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0004746318287086777\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.07333333333333\n",
      "    ram_util_percent: 93.13333333333331\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037465321442207454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.701029467151109\n",
      "    mean_inference_ms: 1.3507634170110114\n",
      "    mean_raw_obs_processing_ms: 0.2934275986190816\n",
      "  time_since_restore: 504.6036560535431\n",
      "  time_this_iter_s: 10.45987582206726\n",
      "  time_total_s: 504.6036560535431\n",
      "  timers:\n",
      "    learn_throughput: 1751.144\n",
      "    learn_time_ms: 571.055\n",
      "    load_throughput: 310656.969\n",
      "    load_time_ms: 3.219\n",
      "    sample_throughput: 104.284\n",
      "    sample_time_ms: 9589.172\n",
      "    update_time_ms: 1.999\n",
      "  timestamp: 1631959854\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 44\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         504.604</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">-0.0454545</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           997.318</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 45000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-11-05\n",
      "  done: false\n",
      "  episode_len_mean: 997.3777777777777\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.044444444444444446\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 45\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.354156960381402\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012020968897928617\n",
      "          policy_loss: 0.0065367976617481975\n",
      "          total_loss: -0.0010093010341127714\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.014192326460357032\n",
      "    num_agent_steps_sampled: 45000\n",
      "    num_agent_steps_trained: 45000\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 45000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.940000000000005\n",
      "    ram_util_percent: 93.18666666666668\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03745470189275524\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.617868976528742\n",
      "    mean_inference_ms: 1.3503123269979962\n",
      "    mean_raw_obs_processing_ms: 0.2980799545897391\n",
      "  time_since_restore: 515.2825841903687\n",
      "  time_this_iter_s: 10.678928136825562\n",
      "  time_total_s: 515.2825841903687\n",
      "  timers:\n",
      "    learn_throughput: 1751.945\n",
      "    learn_time_ms: 570.794\n",
      "    load_throughput: 310629.36\n",
      "    load_time_ms: 3.219\n",
      "    sample_throughput: 103.814\n",
      "    sample_time_ms: 9632.595\n",
      "    update_time_ms: 1.994\n",
      "  timestamp: 1631959865\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 45\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         515.283</td><td style=\"text-align: right;\">45000</td><td style=\"text-align: right;\">-0.0444444</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           997.378</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 46000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-11-16\n",
      "  done: false\n",
      "  episode_len_mean: 997.4347826086956\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.043478260869565216\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 46\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4274791320165\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010063812323440195\n",
      "          policy_loss: -0.05756257801420159\n",
      "          total_loss: -0.07982082015110387\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.000506976311670668\n",
      "    num_agent_steps_sampled: 46000\n",
      "    num_agent_steps_trained: 46000\n",
      "    num_steps_sampled: 46000\n",
      "    num_steps_trained: 46000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.75\n",
      "    ram_util_percent: 93.2\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03744432507690152\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.538200881132482\n",
      "    mean_inference_ms: 1.3498720332545673\n",
      "    mean_raw_obs_processing_ms: 0.30235388306903493\n",
      "  time_since_restore: 526.0620036125183\n",
      "  time_this_iter_s: 10.779419422149658\n",
      "  time_total_s: 526.0620036125183\n",
      "  timers:\n",
      "    learn_throughput: 1755.853\n",
      "    learn_time_ms: 569.524\n",
      "    load_throughput: 312760.354\n",
      "    load_time_ms: 3.197\n",
      "    sample_throughput: 103.174\n",
      "    sample_time_ms: 9692.345\n",
      "    update_time_ms: 2.03\n",
      "  timestamp: 1631959876\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 46000\n",
      "  training_iteration: 46\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         526.062</td><td style=\"text-align: right;\">46000</td><td style=\"text-align: right;\">-0.0434783</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           997.435</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 47000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-11-27\n",
      "  done: false\n",
      "  episode_len_mean: 997.4893617021277\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.0425531914893617\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 47\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.414933268229167\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010678499376569799\n",
      "          policy_loss: 0.002512597499622239\n",
      "          total_loss: -0.01951062451634142\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0005243349920621969\n",
      "    num_agent_steps_sampled: 47000\n",
      "    num_agent_steps_trained: 47000\n",
      "    num_steps_sampled: 47000\n",
      "    num_steps_trained: 47000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.053333333333335\n",
      "    ram_util_percent: 93.23333333333333\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03743417005078813\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.461744263000543\n",
      "    mean_inference_ms: 1.3494430483317492\n",
      "    mean_raw_obs_processing_ms: 0.30627676060685505\n",
      "  time_since_restore: 536.691793680191\n",
      "  time_this_iter_s: 10.62979006767273\n",
      "  time_total_s: 536.691793680191\n",
      "  timers:\n",
      "    learn_throughput: 1756.076\n",
      "    learn_time_ms: 569.451\n",
      "    load_throughput: 312292.286\n",
      "    load_time_ms: 3.202\n",
      "    sample_throughput: 102.561\n",
      "    sample_time_ms: 9750.316\n",
      "    update_time_ms: 2.028\n",
      "  timestamp: 1631959887\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47000\n",
      "  training_iteration: 47\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         536.692</td><td style=\"text-align: right;\">47000</td><td style=\"text-align: right;\">-0.0425532</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           997.489</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-11-37\n",
      "  done: false\n",
      "  episode_len_mean: 997.5416666666666\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.041666666666666664\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 48\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3959426482518515\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01308219739717676\n",
      "          policy_loss: -0.05078372359275818\n",
      "          total_loss: -0.06946339685883787\n",
      "          vf_explained_var: -0.9998812079429626\n",
      "          vf_loss: 0.003317420218243367\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.84\n",
      "    ram_util_percent: 93.29999999999998\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03742422188158281\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.388253406834266\n",
      "    mean_inference_ms: 1.3490220537681317\n",
      "    mean_raw_obs_processing_ms: 0.3098802262019236\n",
      "  time_since_restore: 547.2104029655457\n",
      "  time_this_iter_s: 10.518609285354614\n",
      "  time_total_s: 547.2104029655457\n",
      "  timers:\n",
      "    learn_throughput: 1754.428\n",
      "    learn_time_ms: 569.986\n",
      "    load_throughput: 314118.898\n",
      "    load_time_ms: 3.184\n",
      "    sample_throughput: 102.211\n",
      "    sample_time_ms: 9783.669\n",
      "    update_time_ms: 2.03\n",
      "  timestamp: 1631959897\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 48\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">          547.21</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">-0.0416667</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           997.542</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 49000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-11-48\n",
      "  done: false\n",
      "  episode_len_mean: 997.5918367346939\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.061224489795918366\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 49\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3756461064020793\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015832247600380375\n",
      "          policy_loss: -0.03956616794069608\n",
      "          total_loss: -0.009259256720542907\n",
      "          vf_explained_var: -0.5134047269821167\n",
      "          vf_loss: 0.051688535281250045\n",
      "    num_agent_steps_sampled: 49000\n",
      "    num_agent_steps_trained: 49000\n",
      "    num_steps_sampled: 49000\n",
      "    num_steps_trained: 49000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.82666666666667\n",
      "    ram_util_percent: 93.29999999999998\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037414355805593956\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.317584414438436\n",
      "    mean_inference_ms: 1.348607613835661\n",
      "    mean_raw_obs_processing_ms: 0.31318632788747947\n",
      "  time_since_restore: 557.7692699432373\n",
      "  time_this_iter_s: 10.55886697769165\n",
      "  time_total_s: 557.7692699432373\n",
      "  timers:\n",
      "    learn_throughput: 1755.431\n",
      "    learn_time_ms: 569.661\n",
      "    load_throughput: 312865.337\n",
      "    load_time_ms: 3.196\n",
      "    sample_throughput: 101.706\n",
      "    sample_time_ms: 9832.264\n",
      "    update_time_ms: 2.029\n",
      "  timestamp: 1631959908\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49000\n",
      "  training_iteration: 49\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         557.769</td><td style=\"text-align: right;\">49000</td><td style=\"text-align: right;\">-0.0612245</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           997.592</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 50000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-11-58\n",
      "  done: false\n",
      "  episode_len_mean: 997.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 50\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2876119348737927\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015275066666130993\n",
      "          policy_loss: -0.024288906157016753\n",
      "          total_loss: 0.007941013491815991\n",
      "          vf_explained_var: -0.4320981800556183\n",
      "          vf_loss: 0.052814781148400575\n",
      "    num_agent_steps_sampled: 50000\n",
      "    num_agent_steps_trained: 50000\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.05333333333333\n",
      "    ram_util_percent: 93.31999999999998\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03740469269950309\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.249545152637955\n",
      "    mean_inference_ms: 1.3482024695292438\n",
      "    mean_raw_obs_processing_ms: 0.3162198447557415\n",
      "  time_since_restore: 568.2771050930023\n",
      "  time_this_iter_s: 10.507835149765015\n",
      "  time_total_s: 568.2771050930023\n",
      "  timers:\n",
      "    learn_throughput: 1754.061\n",
      "    learn_time_ms: 570.105\n",
      "    load_throughput: 311918.375\n",
      "    load_time_ms: 3.206\n",
      "    sample_throughput: 101.339\n",
      "    sample_time_ms: 9867.894\n",
      "    update_time_ms: 2.038\n",
      "  timestamp: 1631959918\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 50\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         568.277</td><td style=\"text-align: right;\">50000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            997.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 51000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-12-09\n",
      "  done: false\n",
      "  episode_len_mean: 997.6862745098039\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 51\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.381451718012492\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010531038597114585\n",
      "          policy_loss: -0.03148355678551727\n",
      "          total_loss: -0.05050406774712934\n",
      "          vf_explained_var: -0.7867043614387512\n",
      "          vf_loss: 0.0032143491954128777\n",
      "    num_agent_steps_sampled: 51000\n",
      "    num_agent_steps_trained: 51000\n",
      "    num_steps_sampled: 51000\n",
      "    num_steps_trained: 51000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.526666666666664\n",
      "    ram_util_percent: 93.4066666666667\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037395437166568055\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.183888229728252\n",
      "    mean_inference_ms: 1.3478102514952741\n",
      "    mean_raw_obs_processing_ms: 0.318999155764824\n",
      "  time_since_restore: 578.5252285003662\n",
      "  time_this_iter_s: 10.248123407363892\n",
      "  time_total_s: 578.5252285003662\n",
      "  timers:\n",
      "    learn_throughput: 1749.021\n",
      "    learn_time_ms: 571.748\n",
      "    load_throughput: 312117.992\n",
      "    load_time_ms: 3.204\n",
      "    sample_throughput: 101.377\n",
      "    sample_time_ms: 9864.127\n",
      "    update_time_ms: 2.039\n",
      "  timestamp: 1631959929\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 51000\n",
      "  training_iteration: 51\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         578.525</td><td style=\"text-align: right;\">51000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           997.686</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-12-19\n",
      "  done: false\n",
      "  episode_len_mean: 997.7307692307693\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.019230769230769232\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 52\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4193272829055785\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007318911992729099\n",
      "          policy_loss: -0.033296646508905625\n",
      "          total_loss: -0.05421226227449046\n",
      "          vf_explained_var: -0.9658640623092651\n",
      "          vf_loss: 0.0021798186301667656\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.62142857142857\n",
      "    ram_util_percent: 93.40000000000002\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03738648314320735\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.120445728370393\n",
      "    mean_inference_ms: 1.3474291449234395\n",
      "    mean_raw_obs_processing_ms: 0.3215474093024302\n",
      "  time_since_restore: 588.7488086223602\n",
      "  time_this_iter_s: 10.223580121994019\n",
      "  time_total_s: 588.7488086223602\n",
      "  timers:\n",
      "    learn_throughput: 1746.321\n",
      "    learn_time_ms: 572.632\n",
      "    load_throughput: 311827.935\n",
      "    load_time_ms: 3.207\n",
      "    sample_throughput: 101.514\n",
      "    sample_time_ms: 9850.813\n",
      "    update_time_ms: 8.425\n",
      "  timestamp: 1631959939\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 52\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         588.749</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">-0.0192308</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           997.731</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 53000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-12-29\n",
      "  done: false\n",
      "  episode_len_mean: 997.7735849056604\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.018867924528301886\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 53\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3643340190251667\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008690289520602345\n",
      "          policy_loss: -0.0753146403365665\n",
      "          total_loss: -0.09591665259665913\n",
      "          vf_explained_var: -0.9879668354988098\n",
      "          vf_loss: 0.0017377842204748756\n",
      "    num_agent_steps_sampled: 53000\n",
      "    num_agent_steps_trained: 53000\n",
      "    num_steps_sampled: 53000\n",
      "    num_steps_trained: 53000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.01333333333332\n",
      "    ram_util_percent: 93.69333333333334\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03737757529825126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.05910376529408\n",
      "    mean_inference_ms: 1.3470511491592816\n",
      "    mean_raw_obs_processing_ms: 0.3238786738154498\n",
      "  time_since_restore: 598.8674256801605\n",
      "  time_this_iter_s: 10.118617057800293\n",
      "  time_total_s: 598.8674256801605\n",
      "  timers:\n",
      "    learn_throughput: 1745.907\n",
      "    learn_time_ms: 572.768\n",
      "    load_throughput: 313428.785\n",
      "    load_time_ms: 3.191\n",
      "    sample_throughput: 101.162\n",
      "    sample_time_ms: 9885.11\n",
      "    update_time_ms: 8.431\n",
      "  timestamp: 1631959949\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 53000\n",
      "  training_iteration: 53\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         598.867</td><td style=\"text-align: right;\">53000</td><td style=\"text-align: right;\">-0.0188679</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           997.774</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 54000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-12-39\n",
      "  done: false\n",
      "  episode_len_mean: 997.8148148148148\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.018518518518518517\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 54\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3707481384277345\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010769437253775724\n",
      "          policy_loss: -0.05385129816002316\n",
      "          total_loss: -0.07310596406459809\n",
      "          vf_explained_var: -0.9533731937408447\n",
      "          vf_loss: 0.002837399693412913\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_steps_sampled: 54000\n",
      "    num_steps_trained: 54000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.34999999999999\n",
      "    ram_util_percent: 93.70000000000002\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037368769663531996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.99972284374719\n",
      "    mean_inference_ms: 1.346679644442395\n",
      "    mean_raw_obs_processing_ms: 0.3260099719391922\n",
      "  time_since_restore: 608.9011209011078\n",
      "  time_this_iter_s: 10.033695220947266\n",
      "  time_total_s: 608.9011209011078\n",
      "  timers:\n",
      "    learn_throughput: 1745.557\n",
      "    learn_time_ms: 572.883\n",
      "    load_throughput: 314630.22\n",
      "    load_time_ms: 3.178\n",
      "    sample_throughput: 101.601\n",
      "    sample_time_ms: 9842.447\n",
      "    update_time_ms: 8.432\n",
      "  timestamp: 1631959959\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 54000\n",
      "  training_iteration: 54\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         608.901</td><td style=\"text-align: right;\">54000</td><td style=\"text-align: right;\">-0.0185185</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           997.815</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 55000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-12-49\n",
      "  done: false\n",
      "  episode_len_mean: 997.8545454545455\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.01818181818181818\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 55\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.374098367161221\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010605229774074385\n",
      "          policy_loss: -0.09519330544604196\n",
      "          total_loss: -0.11536521017551422\n",
      "          vf_explained_var: -0.7838355302810669\n",
      "          vf_loss: 0.001978295959997922\n",
      "    num_agent_steps_sampled: 55000\n",
      "    num_agent_steps_trained: 55000\n",
      "    num_steps_sampled: 55000\n",
      "    num_steps_trained: 55000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.35333333333333\n",
      "    ram_util_percent: 93.54666666666665\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03736021129045177\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.942243746037683\n",
      "    mean_inference_ms: 1.3463195633147342\n",
      "    mean_raw_obs_processing_ms: 0.3279592287915121\n",
      "  time_since_restore: 619.068311214447\n",
      "  time_this_iter_s: 10.167190313339233\n",
      "  time_total_s: 619.068311214447\n",
      "  timers:\n",
      "    learn_throughput: 1741.267\n",
      "    learn_time_ms: 574.294\n",
      "    load_throughput: 314474.527\n",
      "    load_time_ms: 3.18\n",
      "    sample_throughput: 102.147\n",
      "    sample_time_ms: 9789.842\n",
      "    update_time_ms: 8.439\n",
      "  timestamp: 1631959969\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55000\n",
      "  training_iteration: 55\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         619.068</td><td style=\"text-align: right;\">55000</td><td style=\"text-align: right;\">-0.0181818</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           997.855</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-12-59\n",
      "  done: false\n",
      "  episode_len_mean: 997.8928571428571\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.017857142857142856\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 56\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.293845600552029\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012454466318778293\n",
      "          policy_loss: -0.07945406859119733\n",
      "          total_loss: -0.09862838971118132\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0018959650899180109\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.75714285714287\n",
      "    ram_util_percent: 93.57857142857141\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03735172626578075\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.886576360037806\n",
      "    mean_inference_ms: 1.3459643878949925\n",
      "    mean_raw_obs_processing_ms: 0.3297360565270249\n",
      "  time_since_restore: 629.1942687034607\n",
      "  time_this_iter_s: 10.125957489013672\n",
      "  time_total_s: 629.1942687034607\n",
      "  timers:\n",
      "    learn_throughput: 1743.345\n",
      "    learn_time_ms: 573.61\n",
      "    load_throughput: 313646.758\n",
      "    load_time_ms: 3.188\n",
      "    sample_throughput: 102.825\n",
      "    sample_time_ms: 9725.261\n",
      "    update_time_ms: 8.351\n",
      "  timestamp: 1631959979\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 56\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         629.194</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">-0.0178571</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           997.893</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 57000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-13-09\n",
      "  done: false\n",
      "  episode_len_mean: 997.9298245614035\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.017543859649122806\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 57\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.316038070784675\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010175945680196355\n",
      "          policy_loss: -0.059258715622127055\n",
      "          total_loss: -0.07914431442817052\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0017483914076971512\n",
      "    num_agent_steps_sampled: 57000\n",
      "    num_agent_steps_trained: 57000\n",
      "    num_steps_sampled: 57000\n",
      "    num_steps_trained: 57000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.964285714285715\n",
      "    ram_util_percent: 93.59999999999998\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03734332035146194\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.832612812246154\n",
      "    mean_inference_ms: 1.3456131367201503\n",
      "    mean_raw_obs_processing_ms: 0.3313532615258258\n",
      "  time_since_restore: 639.2653267383575\n",
      "  time_this_iter_s: 10.07105803489685\n",
      "  time_total_s: 639.2653267383575\n",
      "  timers:\n",
      "    learn_throughput: 1740.48\n",
      "    learn_time_ms: 574.554\n",
      "    load_throughput: 315676.88\n",
      "    load_time_ms: 3.168\n",
      "    sample_throughput: 103.429\n",
      "    sample_time_ms: 9668.467\n",
      "    update_time_ms: 8.353\n",
      "  timestamp: 1631959989\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 57000\n",
      "  training_iteration: 57\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         639.265</td><td style=\"text-align: right;\">57000</td><td style=\"text-align: right;\">-0.0175439</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            997.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 58000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-13-20\n",
      "  done: false\n",
      "  episode_len_mean: 997.9655172413793\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.13793103448275862\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 58\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2889952659606934\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007750952367877571\n",
      "          policy_loss: -0.2509398250116242\n",
      "          total_loss: -0.2700495956672562\n",
      "          vf_explained_var: -0.6447222232818604\n",
      "          vf_loss: 0.0026175398353694215\n",
      "    num_agent_steps_sampled: 58000\n",
      "    num_agent_steps_trained: 58000\n",
      "    num_steps_sampled: 58000\n",
      "    num_steps_trained: 58000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.81333333333334\n",
      "    ram_util_percent: 93.65333333333335\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03733503591439774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.780306281553779\n",
      "    mean_inference_ms: 1.345266980847225\n",
      "    mean_raw_obs_processing_ms: 0.3328285385210562\n",
      "  time_since_restore: 649.477835893631\n",
      "  time_this_iter_s: 10.212509155273438\n",
      "  time_total_s: 649.477835893631\n",
      "  timers:\n",
      "    learn_throughput: 1738.419\n",
      "    learn_time_ms: 575.235\n",
      "    load_throughput: 314793.155\n",
      "    load_time_ms: 3.177\n",
      "    sample_throughput: 103.765\n",
      "    sample_time_ms: 9637.152\n",
      "    update_time_ms: 8.355\n",
      "  timestamp: 1631960000\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 58000\n",
      "  training_iteration: 58\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         649.478</td><td style=\"text-align: right;\">58000</td><td style=\"text-align: right;\">-0.137931</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.966</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 59000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-13-30\n",
      "  done: false\n",
      "  episode_len_mean: 998.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.13559322033898305\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 59\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.328983391655816\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01722646997004778\n",
      "          policy_loss: -0.04321821936302715\n",
      "          total_loss: -0.060315344027347034\n",
      "          vf_explained_var: -0.6799799799919128\n",
      "          vf_loss: 0.0036087414165700063\n",
      "    num_agent_steps_sampled: 59000\n",
      "    num_agent_steps_trained: 59000\n",
      "    num_steps_sampled: 59000\n",
      "    num_steps_trained: 59000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.785714285714285\n",
      "    ram_util_percent: 93.60714285714286\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03732686382751931\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.729512715954822\n",
      "    mean_inference_ms: 1.3449254775674642\n",
      "    mean_raw_obs_processing_ms: 0.33416640063087616\n",
      "  time_since_restore: 659.4179463386536\n",
      "  time_this_iter_s: 9.940110445022583\n",
      "  time_total_s: 659.4179463386536\n",
      "  timers:\n",
      "    learn_throughput: 1737.688\n",
      "    learn_time_ms: 575.477\n",
      "    load_throughput: 313447.523\n",
      "    load_time_ms: 3.19\n",
      "    sample_throughput: 104.439\n",
      "    sample_time_ms: 9574.989\n",
      "    update_time_ms: 8.354\n",
      "  timestamp: 1631960010\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 59000\n",
      "  training_iteration: 59\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         659.418</td><td style=\"text-align: right;\">59000</td><td style=\"text-align: right;\">-0.135593</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">               998</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-13-56\n",
      "  done: false\n",
      "  episode_len_mean: 995.9666666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.13333333333333333\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 60\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.156905542479621\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015450866181032384\n",
      "          policy_loss: -0.04976804123984443\n",
      "          total_loss: -0.0660186661200391\n",
      "          vf_explained_var: -0.7155047655105591\n",
      "          vf_loss: 0.00300079708168697\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.11794871794872\n",
      "    ram_util_percent: 93.54615384615384\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037318774061287724\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.680147020371475\n",
      "    mean_inference_ms: 1.3445869570637166\n",
      "    mean_raw_obs_processing_ms: 0.340060784603384\n",
      "  time_since_restore: 686.1764607429504\n",
      "  time_this_iter_s: 26.758514404296875\n",
      "  time_total_s: 686.1764607429504\n",
      "  timers:\n",
      "    learn_throughput: 1737.366\n",
      "    learn_time_ms: 575.584\n",
      "    load_throughput: 217152.679\n",
      "    load_time_ms: 4.605\n",
      "    sample_throughput: 89.298\n",
      "    sample_time_ms: 11198.432\n",
      "    update_time_ms: 8.437\n",
      "  timestamp: 1631960036\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 60\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         686.176</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">-0.133333</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           995.967</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 61000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-14-07\n",
      "  done: false\n",
      "  episode_len_mean: 996.0327868852459\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.18032786885245902\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 61\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.24154335392846\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021789675442439316\n",
      "          policy_loss: 0.0019959689842330083\n",
      "          total_loss: 0.01678810119628906\n",
      "          vf_explained_var: -0.43423575162887573\n",
      "          vf_loss: 0.033939114104335505\n",
      "    num_agent_steps_sampled: 61000\n",
      "    num_agent_steps_trained: 61000\n",
      "    num_steps_sampled: 61000\n",
      "    num_steps_trained: 61000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.29333333333333\n",
      "    ram_util_percent: 93.73999999999998\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037310725832419706\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.632398455085196\n",
      "    mean_inference_ms: 1.3442500757756988\n",
      "    mean_raw_obs_processing_ms: 0.34560786487733897\n",
      "  time_since_restore: 696.9884021282196\n",
      "  time_this_iter_s: 10.811941385269165\n",
      "  time_total_s: 696.9884021282196\n",
      "  timers:\n",
      "    learn_throughput: 1736.144\n",
      "    learn_time_ms: 575.989\n",
      "    load_throughput: 216163.353\n",
      "    load_time_ms: 4.626\n",
      "    sample_throughput: 88.855\n",
      "    sample_time_ms: 11254.354\n",
      "    update_time_ms: 8.448\n",
      "  timestamp: 1631960047\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 61000\n",
      "  training_iteration: 61\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         696.988</td><td style=\"text-align: right;\">61000</td><td style=\"text-align: right;\">-0.180328</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.033</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 62000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-14-17\n",
      "  done: false\n",
      "  episode_len_mean: 996.0967741935484\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1774193548387097\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 62\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.302064220110575\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011728631873496553\n",
      "          policy_loss: -0.12853300505214268\n",
      "          total_loss: -0.13969859840969245\n",
      "          vf_explained_var: 0.21008874475955963\n",
      "          vf_loss: 0.009216108845753803\n",
      "    num_agent_steps_sampled: 62000\n",
      "    num_agent_steps_trained: 62000\n",
      "    num_steps_sampled: 62000\n",
      "    num_steps_trained: 62000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.330769230769235\n",
      "    ram_util_percent: 94.06153846153848\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0373028346928813\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.585821624710585\n",
      "    mean_inference_ms: 1.3439183295306023\n",
      "    mean_raw_obs_processing_ms: 0.35082969436798866\n",
      "  time_since_restore: 706.4034571647644\n",
      "  time_this_iter_s: 9.4150550365448\n",
      "  time_total_s: 706.4034571647644\n",
      "  timers:\n",
      "    learn_throughput: 1739.675\n",
      "    learn_time_ms: 574.82\n",
      "    load_throughput: 216724.056\n",
      "    load_time_ms: 4.614\n",
      "    sample_throughput: 89.437\n",
      "    sample_time_ms: 11181.08\n",
      "    update_time_ms: 2.066\n",
      "  timestamp: 1631960057\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 62000\n",
      "  training_iteration: 62\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         706.403</td><td style=\"text-align: right;\">62000</td><td style=\"text-align: right;\">-0.177419</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.097</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 63000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-14-26\n",
      "  done: false\n",
      "  episode_len_mean: 996.1587301587301\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1746031746031746\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 63\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.993754670355055\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017320552444402646\n",
      "          policy_loss: -0.04478741897684005\n",
      "          total_loss: -0.058543177342249286\n",
      "          vf_explained_var: -0.8688874840736389\n",
      "          vf_loss: 0.0022846651257067505\n",
      "    num_agent_steps_sampled: 63000\n",
      "    num_agent_steps_trained: 63000\n",
      "    num_steps_sampled: 63000\n",
      "    num_steps_trained: 63000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.07857142857143\n",
      "    ram_util_percent: 93.7857142857143\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03729526149579024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.540399118453717\n",
      "    mean_inference_ms: 1.3435959734824654\n",
      "    mean_raw_obs_processing_ms: 0.35574461926112333\n",
      "  time_since_restore: 715.9354956150055\n",
      "  time_this_iter_s: 9.532038450241089\n",
      "  time_total_s: 715.9354956150055\n",
      "  timers:\n",
      "    learn_throughput: 1741.714\n",
      "    learn_time_ms: 574.147\n",
      "    load_throughput: 215882.977\n",
      "    load_time_ms: 4.632\n",
      "    sample_throughput: 89.903\n",
      "    sample_time_ms: 11123.07\n",
      "    update_time_ms: 2.068\n",
      "  timestamp: 1631960066\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 63000\n",
      "  training_iteration: 63\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         715.935</td><td style=\"text-align: right;\">63000</td><td style=\"text-align: right;\">-0.174603</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.159</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-14-36\n",
      "  done: false\n",
      "  episode_len_mean: 996.21875\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.171875\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 64\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.231626926528083\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020322032428893008\n",
      "          policy_loss: -0.08782083456818428\n",
      "          total_loss: -0.10167825584713784\n",
      "          vf_explained_var: -0.5363778471946716\n",
      "          vf_loss: 0.003886389741415365\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.0\n",
      "    ram_util_percent: 93.91428571428574\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037287924944901185\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.496074965609683\n",
      "    mean_inference_ms: 1.3432788315367963\n",
      "    mean_raw_obs_processing_ms: 0.36037275307292527\n",
      "  time_since_restore: 725.4088525772095\n",
      "  time_this_iter_s: 9.47335696220398\n",
      "  time_total_s: 725.4088525772095\n",
      "  timers:\n",
      "    learn_throughput: 1743.004\n",
      "    learn_time_ms: 573.722\n",
      "    load_throughput: 215664.3\n",
      "    load_time_ms: 4.637\n",
      "    sample_throughput: 90.355\n",
      "    sample_time_ms: 11067.464\n",
      "    update_time_ms: 2.06\n",
      "  timestamp: 1631960076\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 64\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         725.409</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">-0.171875</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.219</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 65000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-14-45\n",
      "  done: false\n",
      "  episode_len_mean: 996.276923076923\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.16923076923076924\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 65\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3119264629152085\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014481094065915013\n",
      "          policy_loss: -0.07502325707011753\n",
      "          total_loss: -0.0685908564676841\n",
      "          vf_explained_var: -0.15499289333820343\n",
      "          vf_loss: 0.02466429522157543\n",
      "    num_agent_steps_sampled: 65000\n",
      "    num_agent_steps_trained: 65000\n",
      "    num_steps_sampled: 65000\n",
      "    num_steps_trained: 65000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.22307692307691\n",
      "    ram_util_percent: 93.67692307692307\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037280650190687055\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.452756780422847\n",
      "    mean_inference_ms: 1.3429669004947713\n",
      "    mean_raw_obs_processing_ms: 0.36473120688347\n",
      "  time_since_restore: 734.6570467948914\n",
      "  time_this_iter_s: 9.248194217681885\n",
      "  time_total_s: 734.6570467948914\n",
      "  timers:\n",
      "    learn_throughput: 1746.345\n",
      "    learn_time_ms: 572.625\n",
      "    load_throughput: 215277.983\n",
      "    load_time_ms: 4.645\n",
      "    sample_throughput: 91.102\n",
      "    sample_time_ms: 10976.681\n",
      "    update_time_ms: 2.044\n",
      "  timestamp: 1631960085\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 65000\n",
      "  training_iteration: 65\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         734.657</td><td style=\"text-align: right;\">65000</td><td style=\"text-align: right;\">-0.169231</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.277</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 66000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-14-54\n",
      "  done: false\n",
      "  episode_len_mean: 996.3333333333334\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.16666666666666666\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 66\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2600330008400813\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010871170485477795\n",
      "          policy_loss: -0.07687670385671987\n",
      "          total_loss: -0.09290663893851969\n",
      "          vf_explained_var: -0.15999555587768555\n",
      "          vf_loss: 0.0029013742020146715\n",
      "    num_agent_steps_sampled: 66000\n",
      "    num_agent_steps_trained: 66000\n",
      "    num_steps_sampled: 66000\n",
      "    num_steps_trained: 66000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.5923076923077\n",
      "    ram_util_percent: 93.38461538461539\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03727358364656352\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.410440543204885\n",
      "    mean_inference_ms: 1.3426612775735915\n",
      "    mean_raw_obs_processing_ms: 0.36883504924783306\n",
      "  time_since_restore: 744.054931640625\n",
      "  time_this_iter_s: 9.397884845733643\n",
      "  time_total_s: 744.054931640625\n",
      "  timers:\n",
      "    learn_throughput: 1740.743\n",
      "    learn_time_ms: 574.467\n",
      "    load_throughput: 214488.645\n",
      "    load_time_ms: 4.662\n",
      "    sample_throughput: 91.726\n",
      "    sample_time_ms: 10902.007\n",
      "    update_time_ms: 2.038\n",
      "  timestamp: 1631960094\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 66000\n",
      "  training_iteration: 66\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         744.055</td><td style=\"text-align: right;\">66000</td><td style=\"text-align: right;\">-0.166667</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.333</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 67000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-15-04\n",
      "  done: false\n",
      "  episode_len_mean: 996.3880597014926\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.16417910447761194\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 67\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1631225029627483\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008114458839479057\n",
      "          policy_loss: -0.00018779606454902224\n",
      "          total_loss: -0.01674491481648551\n",
      "          vf_explained_var: -0.6313847899436951\n",
      "          vf_loss: 0.0023354756542378003\n",
      "    num_agent_steps_sampled: 67000\n",
      "    num_agent_steps_trained: 67000\n",
      "    num_steps_sampled: 67000\n",
      "    num_steps_trained: 67000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.95\n",
      "    ram_util_percent: 93.12857142857142\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03726655457358639\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.369064081116838\n",
      "    mean_inference_ms: 1.342357849154559\n",
      "    mean_raw_obs_processing_ms: 0.3726986220915583\n",
      "  time_since_restore: 753.2958292961121\n",
      "  time_this_iter_s: 9.24089765548706\n",
      "  time_total_s: 753.2958292961121\n",
      "  timers:\n",
      "    learn_throughput: 1744.73\n",
      "    learn_time_ms: 573.155\n",
      "    load_throughput: 214418.469\n",
      "    load_time_ms: 4.664\n",
      "    sample_throughput: 92.419\n",
      "    sample_time_ms: 10820.292\n",
      "    update_time_ms: 2.04\n",
      "  timestamp: 1631960104\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 67000\n",
      "  training_iteration: 67\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         753.296</td><td style=\"text-align: right;\">67000</td><td style=\"text-align: right;\">-0.164179</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.388</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-15-13\n",
      "  done: false\n",
      "  episode_len_mean: 996.4411764705883\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.16176470588235295\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 68\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3870711273617213\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013484920642561442\n",
      "          policy_loss: 0.0010379111601246727\n",
      "          total_loss: -0.01219221285233895\n",
      "          vf_explained_var: -0.005877804942429066\n",
      "          vf_loss: 0.006089426557688664\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.54615384615385\n",
      "    ram_util_percent: 93.0076923076923\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03725972118132562\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.3286524461666\n",
      "    mean_inference_ms: 1.3420584273976373\n",
      "    mean_raw_obs_processing_ms: 0.3763377975994206\n",
      "  time_since_restore: 762.8319742679596\n",
      "  time_this_iter_s: 9.536144971847534\n",
      "  time_total_s: 762.8319742679596\n",
      "  timers:\n",
      "    learn_throughput: 1746.377\n",
      "    learn_time_ms: 572.614\n",
      "    load_throughput: 211109.579\n",
      "    load_time_ms: 4.737\n",
      "    sample_throughput: 92.996\n",
      "    sample_time_ms: 10753.111\n",
      "    update_time_ms: 2.043\n",
      "  timestamp: 1631960113\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 68\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         762.832</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">-0.161765</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.441</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 69000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-15-23\n",
      "  done: false\n",
      "  episode_len_mean: 996.4927536231884\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.15942028985507245\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 69\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.176905319425795\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010523906018436552\n",
      "          policy_loss: -0.0286072658167945\n",
      "          total_loss: -0.04579759066303571\n",
      "          vf_explained_var: -0.6631717085838318\n",
      "          vf_loss: 0.0010269101612114659\n",
      "    num_agent_steps_sampled: 69000\n",
      "    num_agent_steps_trained: 69000\n",
      "    num_steps_sampled: 69000\n",
      "    num_steps_trained: 69000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.78571428571428\n",
      "    ram_util_percent: 92.90714285714287\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03725301002783162\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.289157844296193\n",
      "    mean_inference_ms: 1.3417628359559686\n",
      "    mean_raw_obs_processing_ms: 0.37976426798851776\n",
      "  time_since_restore: 772.287654876709\n",
      "  time_this_iter_s: 9.45568060874939\n",
      "  time_total_s: 772.287654876709\n",
      "  timers:\n",
      "    learn_throughput: 1745.06\n",
      "    learn_time_ms: 573.046\n",
      "    load_throughput: 211896.676\n",
      "    load_time_ms: 4.719\n",
      "    sample_throughput: 93.421\n",
      "    sample_time_ms: 10704.263\n",
      "    update_time_ms: 2.053\n",
      "  timestamp: 1631960123\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 69000\n",
      "  training_iteration: 69\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         772.288</td><td style=\"text-align: right;\">69000</td><td style=\"text-align: right;\">-0.15942</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.493</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 70000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-15-32\n",
      "  done: false\n",
      "  episode_len_mean: 996.5428571428571\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.15714285714285714\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 70\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.740238814883762\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012888456061343164\n",
      "          policy_loss: 0.005294753445519341\n",
      "          total_loss: -0.005845177628927761\n",
      "          vf_explained_var: -0.6892566084861755\n",
      "          vf_loss: 0.0019126034257674796\n",
      "    num_agent_steps_sampled: 70000\n",
      "    num_agent_steps_trained: 70000\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.73076923076923\n",
      "    ram_util_percent: 92.9\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03724647119108666\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.250559576659867\n",
      "    mean_inference_ms: 1.3414715902256944\n",
      "    mean_raw_obs_processing_ms: 0.38299002182563924\n",
      "  time_since_restore: 781.8122217655182\n",
      "  time_this_iter_s: 9.524566888809204\n",
      "  time_total_s: 781.8122217655182\n",
      "  timers:\n",
      "    learn_throughput: 1744.762\n",
      "    learn_time_ms: 573.144\n",
      "    load_throughput: 303280.163\n",
      "    load_time_ms: 3.297\n",
      "    sample_throughput: 111.33\n",
      "    sample_time_ms: 8982.305\n",
      "    update_time_ms: 1.959\n",
      "  timestamp: 1631960132\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 70\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         781.812</td><td style=\"text-align: right;\">70000</td><td style=\"text-align: right;\">-0.157143</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.543</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 71000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-15-42\n",
      "  done: false\n",
      "  episode_len_mean: 996.5915492957746\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.15492957746478872\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 71\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2099958525763617\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0110070932846789\n",
      "          policy_loss: -0.10320710506704119\n",
      "          total_loss: -0.11439797828594843\n",
      "          vf_explained_var: -0.3106539845466614\n",
      "          vf_loss: 0.00719419246694694\n",
      "    num_agent_steps_sampled: 71000\n",
      "    num_agent_steps_trained: 71000\n",
      "    num_steps_sampled: 71000\n",
      "    num_steps_trained: 71000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.271428571428565\n",
      "    ram_util_percent: 92.9\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03724014612355522\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.212792665783294\n",
      "    mean_inference_ms: 1.341185667890501\n",
      "    mean_raw_obs_processing_ms: 0.38602645237276706\n",
      "  time_since_restore: 791.1640992164612\n",
      "  time_this_iter_s: 9.351877450942993\n",
      "  time_total_s: 791.1640992164612\n",
      "  timers:\n",
      "    learn_throughput: 1747.142\n",
      "    learn_time_ms: 572.363\n",
      "    load_throughput: 304599.452\n",
      "    load_time_ms: 3.283\n",
      "    sample_throughput: 113.159\n",
      "    sample_time_ms: 8837.095\n",
      "    update_time_ms: 1.956\n",
      "  timestamp: 1631960142\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 71000\n",
      "  training_iteration: 71\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         791.164</td><td style=\"text-align: right;\">71000</td><td style=\"text-align: right;\">-0.15493</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.592</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-15-51\n",
      "  done: false\n",
      "  episode_len_mean: 996.6388888888889\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1527777777777778\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 72\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4089283598793876\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011029963944143685\n",
      "          policy_loss: -0.011390405231051975\n",
      "          total_loss: -0.028176533348030514\n",
      "          vf_explained_var: -0.9600671529769897\n",
      "          vf_loss: 0.0035805432436366875\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.23076923076923\n",
      "    ram_util_percent: 92.9\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03723386253126651\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.175856944364835\n",
      "    mean_inference_ms: 1.340903589884614\n",
      "    mean_raw_obs_processing_ms: 0.3888854846083372\n",
      "  time_since_restore: 800.6622250080109\n",
      "  time_this_iter_s: 9.498125791549683\n",
      "  time_total_s: 800.6622250080109\n",
      "  timers:\n",
      "    learn_throughput: 1748.059\n",
      "    learn_time_ms: 572.063\n",
      "    load_throughput: 304111.369\n",
      "    load_time_ms: 3.288\n",
      "    sample_throughput: 113.049\n",
      "    sample_time_ms: 8845.741\n",
      "    update_time_ms: 1.939\n",
      "  timestamp: 1631960151\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 72\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         800.662</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">-0.152778</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.639</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 73000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-16-01\n",
      "  done: false\n",
      "  episode_len_mean: 996.6849315068494\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1506849315068493\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 73\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.157845906416575\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010486608374559982\n",
      "          policy_loss: -0.05172949801716539\n",
      "          total_loss: -0.06717738426393932\n",
      "          vf_explained_var: -0.8243348002433777\n",
      "          vf_loss: 0.0025913429587186934\n",
      "    num_agent_steps_sampled: 73000\n",
      "    num_agent_steps_trained: 73000\n",
      "    num_steps_sampled: 73000\n",
      "    num_steps_trained: 73000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.73571428571429\n",
      "    ram_util_percent: 92.9\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037227672355074765\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.13972766120499\n",
      "    mean_inference_ms: 1.340625342285933\n",
      "    mean_raw_obs_processing_ms: 0.391575501546258\n",
      "  time_since_restore: 810.1624066829681\n",
      "  time_this_iter_s: 9.500181674957275\n",
      "  time_total_s: 810.1624066829681\n",
      "  timers:\n",
      "    learn_throughput: 1751.837\n",
      "    learn_time_ms: 570.829\n",
      "    load_throughput: 305785.295\n",
      "    load_time_ms: 3.27\n",
      "    sample_throughput: 113.074\n",
      "    sample_time_ms: 8843.801\n",
      "    update_time_ms: 1.94\n",
      "  timestamp: 1631960161\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 73000\n",
      "  training_iteration: 73\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         810.162</td><td style=\"text-align: right;\">73000</td><td style=\"text-align: right;\">-0.150685</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.685</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 74000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-16-10\n",
      "  done: false\n",
      "  episode_len_mean: 996.7297297297297\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.14864864864864866\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 74\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9251654426256815\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011617248531512983\n",
      "          policy_loss: -0.0710789515533381\n",
      "          total_loss: -0.08407848810570108\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.002331296772333897\n",
      "    num_agent_steps_sampled: 74000\n",
      "    num_agent_steps_trained: 74000\n",
      "    num_steps_sampled: 74000\n",
      "    num_steps_trained: 74000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.99230769230769\n",
      "    ram_util_percent: 92.94615384615385\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03722151299709929\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.10437069172795\n",
      "    mean_inference_ms: 1.3403503445638212\n",
      "    mean_raw_obs_processing_ms: 0.39410573617789124\n",
      "  time_since_restore: 819.638751745224\n",
      "  time_this_iter_s: 9.47634506225586\n",
      "  time_total_s: 819.638751745224\n",
      "  timers:\n",
      "    learn_throughput: 1750.57\n",
      "    learn_time_ms: 571.242\n",
      "    load_throughput: 306424.214\n",
      "    load_time_ms: 3.263\n",
      "    sample_throughput: 113.075\n",
      "    sample_time_ms: 8843.691\n",
      "    update_time_ms: 1.945\n",
      "  timestamp: 1631960170\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 74000\n",
      "  training_iteration: 74\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         819.639</td><td style=\"text-align: right;\">74000</td><td style=\"text-align: right;\">-0.148649</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 75000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-16-20\n",
      "  done: false\n",
      "  episode_len_mean: 996.7733333333333\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.14666666666666667\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 75\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5518976012865702\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009440720620272201\n",
      "          policy_loss: -0.05433649801545673\n",
      "          total_loss: -0.06373752310044235\n",
      "          vf_explained_var: -0.26456916332244873\n",
      "          vf_loss: 0.0029317080832293465\n",
      "    num_agent_steps_sampled: 75000\n",
      "    num_agent_steps_trained: 75000\n",
      "    num_steps_sampled: 75000\n",
      "    num_steps_trained: 75000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.214285714285715\n",
      "    ram_util_percent: 93.04285714285713\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037215531009239175\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.069771718604386\n",
      "    mean_inference_ms: 1.340079879152692\n",
      "    mean_raw_obs_processing_ms: 0.3964851110248196\n",
      "  time_since_restore: 829.1770870685577\n",
      "  time_this_iter_s: 9.53833532333374\n",
      "  time_total_s: 829.1770870685577\n",
      "  timers:\n",
      "    learn_throughput: 1749.988\n",
      "    learn_time_ms: 571.433\n",
      "    load_throughput: 305925.807\n",
      "    load_time_ms: 3.269\n",
      "    sample_throughput: 112.708\n",
      "    sample_time_ms: 8872.486\n",
      "    update_time_ms: 1.959\n",
      "  timestamp: 1631960180\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 75000\n",
      "  training_iteration: 75\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         829.177</td><td style=\"text-align: right;\">75000</td><td style=\"text-align: right;\">-0.146667</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.773</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-16-29\n",
      "  done: false\n",
      "  episode_len_mean: 996.8157894736842\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.14473684210526316\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 76\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3670350895987617\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01575771066045351\n",
      "          policy_loss: -0.0493748653266165\n",
      "          total_loss: -0.06500723374386629\n",
      "          vf_explained_var: -0.3038804233074188\n",
      "          vf_loss: 0.002719754211526985\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.51428571428572\n",
      "    ram_util_percent: 93.1\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03720984304515477\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.035913581824994\n",
      "    mean_inference_ms: 1.3398188255985648\n",
      "    mean_raw_obs_processing_ms: 0.3987232083068428\n",
      "  time_since_restore: 838.7980706691742\n",
      "  time_this_iter_s: 9.620983600616455\n",
      "  time_total_s: 838.7980706691742\n",
      "  timers:\n",
      "    learn_throughput: 1753.767\n",
      "    learn_time_ms: 570.201\n",
      "    load_throughput: 307151.258\n",
      "    load_time_ms: 3.256\n",
      "    sample_throughput: 112.41\n",
      "    sample_time_ms: 8896.032\n",
      "    update_time_ms: 1.962\n",
      "  timestamp: 1631960189\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 76\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         838.798</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">-0.144737</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.816</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 77000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-16-39\n",
      "  done: false\n",
      "  episode_len_mean: 996.8571428571429\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.14285714285714285\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 77\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1603705909517075\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014229375480824697\n",
      "          policy_loss: -0.07690310184326436\n",
      "          total_loss: -0.09124424705902735\n",
      "          vf_explained_var: -0.283062607049942\n",
      "          vf_loss: 0.0024601472690442784\n",
      "    num_agent_steps_sampled: 77000\n",
      "    num_agent_steps_trained: 77000\n",
      "    num_steps_sampled: 77000\n",
      "    num_steps_trained: 77000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.26428571428572\n",
      "    ram_util_percent: 93.20000000000002\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03720437945938449\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.002800943820487\n",
      "    mean_inference_ms: 1.3395666354596811\n",
      "    mean_raw_obs_processing_ms: 0.40082655959389746\n",
      "  time_since_restore: 848.5773153305054\n",
      "  time_this_iter_s: 9.779244661331177\n",
      "  time_total_s: 848.5773153305054\n",
      "  timers:\n",
      "    learn_throughput: 1754.706\n",
      "    learn_time_ms: 569.896\n",
      "    load_throughput: 306401.829\n",
      "    load_time_ms: 3.264\n",
      "    sample_throughput: 111.73\n",
      "    sample_time_ms: 8950.159\n",
      "    update_time_ms: 1.959\n",
      "  timestamp: 1631960199\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 77000\n",
      "  training_iteration: 77\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         848.577</td><td style=\"text-align: right;\">77000</td><td style=\"text-align: right;\">-0.142857</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.857</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 78000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-16-49\n",
      "  done: false\n",
      "  episode_len_mean: 996.8974358974359\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.14102564102564102\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 78\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1822681758138867\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012823828590207972\n",
      "          policy_loss: -0.0396022324450314\n",
      "          total_loss: -0.054329507570299834\n",
      "          vf_explained_var: -0.6676658987998962\n",
      "          vf_loss: 0.002767362846578989\n",
      "    num_agent_steps_sampled: 78000\n",
      "    num_agent_steps_trained: 78000\n",
      "    num_steps_sampled: 78000\n",
      "    num_steps_trained: 78000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.92857142857143\n",
      "    ram_util_percent: 93.25714285714284\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03719905028864322\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.970374053275012\n",
      "    mean_inference_ms: 1.3393219966436998\n",
      "    mean_raw_obs_processing_ms: 0.40280329683552274\n",
      "  time_since_restore: 858.1838738918304\n",
      "  time_this_iter_s: 9.606558561325073\n",
      "  time_total_s: 858.1838738918304\n",
      "  timers:\n",
      "    learn_throughput: 1749.12\n",
      "    learn_time_ms: 571.716\n",
      "    load_throughput: 314189.489\n",
      "    load_time_ms: 3.183\n",
      "    sample_throughput: 111.664\n",
      "    sample_time_ms: 8955.401\n",
      "    update_time_ms: 2.032\n",
      "  timestamp: 1631960209\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 78000\n",
      "  training_iteration: 78\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         858.184</td><td style=\"text-align: right;\">78000</td><td style=\"text-align: right;\">-0.141026</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.897</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 79000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-16-58\n",
      "  done: false\n",
      "  episode_len_mean: 996.9367088607595\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.13924050632911392\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 79\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0501876632372538\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013846790155795515\n",
      "          policy_loss: -0.0917613323053552\n",
      "          total_loss: -0.10419381814491417\n",
      "          vf_explained_var: -0.41766104102134705\n",
      "          vf_loss: 0.0033960985821775264\n",
      "    num_agent_steps_sampled: 79000\n",
      "    num_agent_steps_trained: 79000\n",
      "    num_steps_sampled: 79000\n",
      "    num_steps_trained: 79000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.823076923076925\n",
      "    ram_util_percent: 93.20000000000002\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03719379357382314\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.938598738586256\n",
      "    mean_inference_ms: 1.3390796982232067\n",
      "    mean_raw_obs_processing_ms: 0.40465835913912135\n",
      "  time_since_restore: 867.6521210670471\n",
      "  time_this_iter_s: 9.468247175216675\n",
      "  time_total_s: 867.6521210670471\n",
      "  timers:\n",
      "    learn_throughput: 1745.819\n",
      "    learn_time_ms: 572.797\n",
      "    load_throughput: 314165.955\n",
      "    load_time_ms: 3.183\n",
      "    sample_throughput: 111.662\n",
      "    sample_time_ms: 8955.582\n",
      "    update_time_ms: 2.023\n",
      "  timestamp: 1631960218\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79000\n",
      "  training_iteration: 79\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         867.652</td><td style=\"text-align: right;\">79000</td><td style=\"text-align: right;\">-0.139241</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.937</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-17-08\n",
      "  done: false\n",
      "  episode_len_mean: 996.975\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1375\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 80\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5064215077294243\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009917298886241434\n",
      "          policy_loss: -0.061112570431497364\n",
      "          total_loss: -0.08036700333986017\n",
      "          vf_explained_var: -0.9404802322387695\n",
      "          vf_loss: 0.0024626916491090217\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.800000000000004\n",
      "    ram_util_percent: 93.20714285714287\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037188615366756925\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.907471496746377\n",
      "    mean_inference_ms: 1.3388400405060956\n",
      "    mean_raw_obs_processing_ms: 0.40639921578123406\n",
      "  time_since_restore: 877.2257306575775\n",
      "  time_this_iter_s: 9.573609590530396\n",
      "  time_total_s: 877.2257306575775\n",
      "  timers:\n",
      "    learn_throughput: 1748.616\n",
      "    learn_time_ms: 571.881\n",
      "    load_throughput: 311925.334\n",
      "    load_time_ms: 3.206\n",
      "    sample_throughput: 111.59\n",
      "    sample_time_ms: 8961.368\n",
      "    update_time_ms: 2.021\n",
      "  timestamp: 1631960228\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 80\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         877.226</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> -0.1375</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.975</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 81000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-17-17\n",
      "  done: false\n",
      "  episode_len_mean: 997.0123456790124\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.13580246913580246\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 81\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9302699353959825\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012640334276966234\n",
      "          policy_loss: -0.01683764590157403\n",
      "          total_loss: -0.029392540951569877\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.002481691470731878\n",
      "    num_agent_steps_sampled: 81000\n",
      "    num_agent_steps_trained: 81000\n",
      "    num_steps_sampled: 81000\n",
      "    num_steps_trained: 81000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.607692307692304\n",
      "    ram_util_percent: 93.25384615384615\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037183473096755446\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.876955876897306\n",
      "    mean_inference_ms: 1.3386016753581225\n",
      "    mean_raw_obs_processing_ms: 0.4080306927435047\n",
      "  time_since_restore: 886.6861107349396\n",
      "  time_this_iter_s: 9.46038007736206\n",
      "  time_total_s: 886.6861107349396\n",
      "  timers:\n",
      "    learn_throughput: 1746.669\n",
      "    learn_time_ms: 572.518\n",
      "    load_throughput: 312462.118\n",
      "    load_time_ms: 3.2\n",
      "    sample_throughput: 111.463\n",
      "    sample_time_ms: 8971.586\n",
      "    update_time_ms: 2.027\n",
      "  timestamp: 1631960237\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 81000\n",
      "  training_iteration: 81\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         886.686</td><td style=\"text-align: right;\">81000</td><td style=\"text-align: right;\">-0.135802</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.012</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 82000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-17-27\n",
      "  done: false\n",
      "  episode_len_mean: 997.0487804878048\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.13414634146341464\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 82\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3029034561581083\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014199022656020391\n",
      "          policy_loss: -0.0972621076222923\n",
      "          total_loss: -0.10837482195347548\n",
      "          vf_explained_var: -0.32009801268577576\n",
      "          vf_loss: 0.0071241508833029205\n",
      "    num_agent_steps_sampled: 82000\n",
      "    num_agent_steps_trained: 82000\n",
      "    num_steps_sampled: 82000\n",
      "    num_steps_trained: 82000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.035714285714285\n",
      "    ram_util_percent: 93.33571428571427\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03717834989946307\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.84703284304079\n",
      "    mean_inference_ms: 1.338365954622754\n",
      "    mean_raw_obs_processing_ms: 0.40955839539937505\n",
      "  time_since_restore: 896.1442761421204\n",
      "  time_this_iter_s: 9.458165407180786\n",
      "  time_total_s: 896.1442761421204\n",
      "  timers:\n",
      "    learn_throughput: 1748.874\n",
      "    learn_time_ms: 571.797\n",
      "    load_throughput: 312650.779\n",
      "    load_time_ms: 3.198\n",
      "    sample_throughput: 111.504\n",
      "    sample_time_ms: 8968.283\n",
      "    update_time_ms: 2.044\n",
      "  timestamp: 1631960247\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 82000\n",
      "  training_iteration: 82\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         896.144</td><td style=\"text-align: right;\">82000</td><td style=\"text-align: right;\">-0.134146</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.049</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 83000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-17-36\n",
      "  done: false\n",
      "  episode_len_mean: 997.0843373493976\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.13253012048192772\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 83\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9952758285734389\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011374155246743189\n",
      "          policy_loss: -0.11684016349414984\n",
      "          total_loss: -0.13053088424106438\n",
      "          vf_explained_var: -0.7157347202301025\n",
      "          vf_loss: 0.002423258366373678\n",
      "    num_agent_steps_sampled: 83000\n",
      "    num_agent_steps_trained: 83000\n",
      "    num_steps_sampled: 83000\n",
      "    num_steps_trained: 83000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.08461538461539\n",
      "    ram_util_percent: 93.37692307692308\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037173297176118426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.817698421295193\n",
      "    mean_inference_ms: 1.3381339711670313\n",
      "    mean_raw_obs_processing_ms: 0.4109878263256488\n",
      "  time_since_restore: 905.713947057724\n",
      "  time_this_iter_s: 9.569670915603638\n",
      "  time_total_s: 905.713947057724\n",
      "  timers:\n",
      "    learn_throughput: 1745.412\n",
      "    learn_time_ms: 572.931\n",
      "    load_throughput: 311684.266\n",
      "    load_time_ms: 3.208\n",
      "    sample_throughput: 111.432\n",
      "    sample_time_ms: 8974.12\n",
      "    update_time_ms: 2.025\n",
      "  timestamp: 1631960256\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 83000\n",
      "  training_iteration: 83\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         905.714</td><td style=\"text-align: right;\">83000</td><td style=\"text-align: right;\">-0.13253</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.084</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-17-46\n",
      "  done: false\n",
      "  episode_len_mean: 997.1190476190476\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.13095238095238096\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 84\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.08747095796797\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01031124191841687\n",
      "          policy_loss: -0.02929033376276493\n",
      "          total_loss: -0.04488177825179365\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0018032185215916899\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.214285714285715\n",
      "    ram_util_percent: 93.40000000000002\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037168342241672064\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.788921801116514\n",
      "    mean_inference_ms: 1.3379059771054878\n",
      "    mean_raw_obs_processing_ms: 0.41232385829640644\n",
      "  time_since_restore: 915.1869413852692\n",
      "  time_this_iter_s: 9.472994327545166\n",
      "  time_total_s: 915.1869413852692\n",
      "  timers:\n",
      "    learn_throughput: 1749.639\n",
      "    learn_time_ms: 571.547\n",
      "    load_throughput: 310516.676\n",
      "    load_time_ms: 3.22\n",
      "    sample_throughput: 111.419\n",
      "    sample_time_ms: 8975.147\n",
      "    update_time_ms: 2.029\n",
      "  timestamp: 1631960266\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 84\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         915.187</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">-0.130952</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.119</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 85000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-17-56\n",
      "  done: false\n",
      "  episode_len_mean: 997.1529411764706\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.12941176470588237\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 85\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1857131481170655\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013090514691074872\n",
      "          policy_loss: -0.06537847821083334\n",
      "          total_loss: -0.07783794005711873\n",
      "          vf_explained_var: -0.4576722979545593\n",
      "          vf_loss: 0.004979620341004597\n",
      "    num_agent_steps_sampled: 85000\n",
      "    num_agent_steps_trained: 85000\n",
      "    num_steps_sampled: 85000\n",
      "    num_steps_trained: 85000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.771428571428565\n",
      "    ram_util_percent: 93.46428571428571\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037163522063710965\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.760706113213029\n",
      "    mean_inference_ms: 1.337683809115395\n",
      "    mean_raw_obs_processing_ms: 0.41357215319467067\n",
      "  time_since_restore: 924.8311367034912\n",
      "  time_this_iter_s: 9.644195318222046\n",
      "  time_total_s: 924.8311367034912\n",
      "  timers:\n",
      "    learn_throughput: 1748.454\n",
      "    learn_time_ms: 571.934\n",
      "    load_throughput: 312192.333\n",
      "    load_time_ms: 3.203\n",
      "    sample_throughput: 111.293\n",
      "    sample_time_ms: 8985.317\n",
      "    update_time_ms: 2.047\n",
      "  timestamp: 1631960276\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 85000\n",
      "  training_iteration: 85\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         924.831</td><td style=\"text-align: right;\">85000</td><td style=\"text-align: right;\">-0.129412</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.153</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 86000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-18-05\n",
      "  done: false\n",
      "  episode_len_mean: 997.1860465116279\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.12790697674418605\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 86\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0184190551439922\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012316577307518983\n",
      "          policy_loss: -0.11867087735897965\n",
      "          total_loss: -0.13204878196120262\n",
      "          vf_explained_var: -0.5750837326049805\n",
      "          vf_loss: 0.002649440100261321\n",
      "    num_agent_steps_sampled: 86000\n",
      "    num_agent_steps_trained: 86000\n",
      "    num_steps_sampled: 86000\n",
      "    num_steps_trained: 86000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.34285714285714\n",
      "    ram_util_percent: 93.5\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03715878997187587\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.733040552866191\n",
      "    mean_inference_ms: 1.337466926934465\n",
      "    mean_raw_obs_processing_ms: 0.41473640883398355\n",
      "  time_since_restore: 934.5241498947144\n",
      "  time_this_iter_s: 9.693013191223145\n",
      "  time_total_s: 934.5241498947144\n",
      "  timers:\n",
      "    learn_throughput: 1743.673\n",
      "    learn_time_ms: 573.502\n",
      "    load_throughput: 311166.307\n",
      "    load_time_ms: 3.214\n",
      "    sample_throughput: 111.223\n",
      "    sample_time_ms: 8990.942\n",
      "    update_time_ms: 2.048\n",
      "  timestamp: 1631960285\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 86000\n",
      "  training_iteration: 86\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         934.524</td><td style=\"text-align: right;\">86000</td><td style=\"text-align: right;\">-0.127907</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.186</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 87000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-18-15\n",
      "  done: false\n",
      "  episode_len_mean: 997.2183908045977\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.12643678160919541\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 87\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2169319046868217\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012042088527288216\n",
      "          policy_loss: -0.09571726541552279\n",
      "          total_loss: -0.1106333123313056\n",
      "          vf_explained_var: -0.044689204543828964\n",
      "          vf_loss: 0.003189068467408005\n",
      "    num_agent_steps_sampled: 87000\n",
      "    num_agent_steps_trained: 87000\n",
      "    num_steps_sampled: 87000\n",
      "    num_steps_trained: 87000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.58461538461539\n",
      "    ram_util_percent: 93.5076923076923\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0371541797926741\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.705898110721165\n",
      "    mean_inference_ms: 1.337254026049696\n",
      "    mean_raw_obs_processing_ms: 0.415821470535715\n",
      "  time_since_restore: 944.1268539428711\n",
      "  time_this_iter_s: 9.602704048156738\n",
      "  time_total_s: 944.1268539428711\n",
      "  timers:\n",
      "    learn_throughput: 1738.507\n",
      "    learn_time_ms: 575.206\n",
      "    load_throughput: 311605.536\n",
      "    load_time_ms: 3.209\n",
      "    sample_throughput: 111.463\n",
      "    sample_time_ms: 8971.591\n",
      "    update_time_ms: 2.051\n",
      "  timestamp: 1631960295\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 87000\n",
      "  training_iteration: 87\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         944.127</td><td style=\"text-align: right;\">87000</td><td style=\"text-align: right;\">-0.126437</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.218</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-18-24\n",
      "  done: false\n",
      "  episode_len_mean: 997.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.125\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 88\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2118651813930934\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012470235730030312\n",
      "          policy_loss: -0.03877365338719553\n",
      "          total_loss: -0.05469950990130504\n",
      "          vf_explained_var: -0.455881804227829\n",
      "          vf_loss: 0.0019840891615280675\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.00714285714285\n",
      "    ram_util_percent: 93.48571428571428\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037149664631153545\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.679245062765595\n",
      "    mean_inference_ms: 1.3370444633037815\n",
      "    mean_raw_obs_processing_ms: 0.41683033314119894\n",
      "  time_since_restore: 953.5705351829529\n",
      "  time_this_iter_s: 9.443681240081787\n",
      "  time_total_s: 953.5705351829529\n",
      "  timers:\n",
      "    learn_throughput: 1745.857\n",
      "    learn_time_ms: 572.784\n",
      "    load_throughput: 311510.65\n",
      "    load_time_ms: 3.21\n",
      "    sample_throughput: 111.635\n",
      "    sample_time_ms: 8957.79\n",
      "    update_time_ms: 1.973\n",
      "  timestamp: 1631960304\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 88\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         953.571</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">  -0.125</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            997.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 89000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-18-34\n",
      "  done: false\n",
      "  episode_len_mean: 997.2808988764045\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.12359550561797752\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 89\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2655070622762046\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012933785004902557\n",
      "          policy_loss: -0.04137669038027525\n",
      "          total_loss: -0.05633626545055045\n",
      "          vf_explained_var: -0.2809304893016815\n",
      "          vf_loss: 0.0033303407428320496\n",
      "    num_agent_steps_sampled: 89000\n",
      "    num_agent_steps_trained: 89000\n",
      "    num_steps_sampled: 89000\n",
      "    num_steps_trained: 89000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.92857142857144\n",
      "    ram_util_percent: 93.60714285714282\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037145376887589926\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.653113559795388\n",
      "    mean_inference_ms: 1.336842050320863\n",
      "    mean_raw_obs_processing_ms: 0.41776922349847706\n",
      "  time_since_restore: 963.4636650085449\n",
      "  time_this_iter_s: 9.893129825592041\n",
      "  time_total_s: 963.4636650085449\n",
      "  timers:\n",
      "    learn_throughput: 1741.789\n",
      "    learn_time_ms: 574.123\n",
      "    load_throughput: 310252.533\n",
      "    load_time_ms: 3.223\n",
      "    sample_throughput: 111.134\n",
      "    sample_time_ms: 8998.164\n",
      "    update_time_ms: 2.733\n",
      "  timestamp: 1631960314\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 89000\n",
      "  training_iteration: 89\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         963.464</td><td style=\"text-align: right;\">89000</td><td style=\"text-align: right;\">-0.123596</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.281</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 90000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-19-01\n",
      "  done: false\n",
      "  episode_len_mean: 995.8222222222222\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.12222222222222222\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 90\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.248350397745768\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014040662193493765\n",
      "          policy_loss: -0.040853378176689145\n",
      "          total_loss: -0.0531912926170561\n",
      "          vf_explained_var: -0.15068352222442627\n",
      "          vf_loss: 0.005406865075282339\n",
      "    num_agent_steps_sampled: 90000\n",
      "    num_agent_steps_trained: 90000\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.02631578947368\n",
      "    ram_util_percent: 93.64736842105262\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0371412186177663\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.627445480516634\n",
      "    mean_inference_ms: 1.336646093488373\n",
      "    mean_raw_obs_processing_ms: 0.4207196567035619\n",
      "  time_since_restore: 989.8698601722717\n",
      "  time_this_iter_s: 26.406195163726807\n",
      "  time_total_s: 989.8698601722717\n",
      "  timers:\n",
      "    learn_throughput: 1724.56\n",
      "    learn_time_ms: 579.858\n",
      "    load_throughput: 244170.033\n",
      "    load_time_ms: 4.096\n",
      "    sample_throughput: 93.679\n",
      "    sample_time_ms: 10674.803\n",
      "    update_time_ms: 2.726\n",
      "  timestamp: 1631960341\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 90\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">          989.87</td><td style=\"text-align: right;\">90000</td><td style=\"text-align: right;\">-0.122222</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           995.822</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 91000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-19-15\n",
      "  done: false\n",
      "  episode_len_mean: 995.8681318681319\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.12087912087912088\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 91\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1808314270443385\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01238612560560585\n",
      "          policy_loss: -0.010459888436728054\n",
      "          total_loss: -0.02530612550261948\n",
      "          vf_explained_var: -0.426871657371521\n",
      "          vf_loss: 0.0027817552249568204\n",
      "    num_agent_steps_sampled: 91000\n",
      "    num_agent_steps_trained: 91000\n",
      "    num_steps_sampled: 91000\n",
      "    num_steps_trained: 91000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.635000000000005\n",
      "    ram_util_percent: 93.69500000000001\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03713736244889021\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.602772163897766\n",
      "    mean_inference_ms: 1.3364596217350186\n",
      "    mean_raw_obs_processing_ms: 0.4235379331691642\n",
      "  time_since_restore: 1003.9827332496643\n",
      "  time_this_iter_s: 14.112873077392578\n",
      "  time_total_s: 1003.9827332496643\n",
      "  timers:\n",
      "    learn_throughput: 1706.935\n",
      "    learn_time_ms: 585.845\n",
      "    load_throughput: 234336.987\n",
      "    load_time_ms: 4.267\n",
      "    sample_throughput: 89.816\n",
      "    sample_time_ms: 11133.892\n",
      "    update_time_ms: 2.719\n",
      "  timestamp: 1631960355\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 91000\n",
      "  training_iteration: 91\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         1003.98</td><td style=\"text-align: right;\">91000</td><td style=\"text-align: right;\">-0.120879</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           995.868</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-19-25\n",
      "  done: false\n",
      "  episode_len_mean: 995.9130434782609\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.11956521739130435\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 92\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3976521094640098\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009352428623911103\n",
      "          policy_loss: -0.02395745155711969\n",
      "          total_loss: -0.04330570863352882\n",
      "          vf_explained_var: -0.9853339195251465\n",
      "          vf_loss: 0.0014718185376194823\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.42857142857142\n",
      "    ram_util_percent: 93.14285714285715\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03713376036614276\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.578566501852599\n",
      "    mean_inference_ms: 1.3362805771380737\n",
      "    mean_raw_obs_processing_ms: 0.4262289902680146\n",
      "  time_since_restore: 1013.9264590740204\n",
      "  time_this_iter_s: 9.943725824356079\n",
      "  time_total_s: 1013.9264590740204\n",
      "  timers:\n",
      "    learn_throughput: 1696.619\n",
      "    learn_time_ms: 589.407\n",
      "    load_throughput: 233905.731\n",
      "    load_time_ms: 4.275\n",
      "    sample_throughput: 89.455\n",
      "    sample_time_ms: 11178.86\n",
      "    update_time_ms: 2.719\n",
      "  timestamp: 1631960365\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 92\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         1013.93</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">-0.119565</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           995.913</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 93000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-19-35\n",
      "  done: false\n",
      "  episode_len_mean: 995.9569892473119\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.11827956989247312\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 93\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2791106012132434\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016022829712035353\n",
      "          policy_loss: -0.07448436406751474\n",
      "          total_loss: -0.08769543096423149\n",
      "          vf_explained_var: -0.054530609399080276\n",
      "          vf_loss: 0.004172335058036778\n",
      "    num_agent_steps_sampled: 93000\n",
      "    num_agent_steps_trained: 93000\n",
      "    num_steps_sampled: 93000\n",
      "    num_steps_trained: 93000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.73571428571428\n",
      "    ram_util_percent: 93.07142857142857\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037130292080023865\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.554827660708353\n",
      "    mean_inference_ms: 1.3361076567790247\n",
      "    mean_raw_obs_processing_ms: 0.42879849747285\n",
      "  time_since_restore: 1023.9333488941193\n",
      "  time_this_iter_s: 10.006889820098877\n",
      "  time_total_s: 1023.9333488941193\n",
      "  timers:\n",
      "    learn_throughput: 1699.028\n",
      "    learn_time_ms: 588.572\n",
      "    load_throughput: 233968.36\n",
      "    load_time_ms: 4.274\n",
      "    sample_throughput: 89.1\n",
      "    sample_time_ms: 11223.401\n",
      "    update_time_ms: 2.717\n",
      "  timestamp: 1631960375\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 93000\n",
      "  training_iteration: 93\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         1023.93</td><td style=\"text-align: right;\">93000</td><td style=\"text-align: right;\">-0.11828</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           995.957</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 94000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-19-45\n",
      "  done: false\n",
      "  episode_len_mean: 996.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.11702127659574468\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 94\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3146308369106716\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01920597025660662\n",
      "          policy_loss: -0.08226198479533195\n",
      "          total_loss: -0.08905006564325757\n",
      "          vf_explained_var: 0.04210689291357994\n",
      "          vf_loss: 0.009876211318704817\n",
      "    num_agent_steps_sampled: 94000\n",
      "    num_agent_steps_trained: 94000\n",
      "    num_steps_sampled: 94000\n",
      "    num_steps_trained: 94000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.37142857142857\n",
      "    ram_util_percent: 92.88571428571427\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03712685877569791\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.531514311249122\n",
      "    mean_inference_ms: 1.335936352266528\n",
      "    mean_raw_obs_processing_ms: 0.43125042314628026\n",
      "  time_since_restore: 1033.7046930789948\n",
      "  time_this_iter_s: 9.771344184875488\n",
      "  time_total_s: 1033.7046930789948\n",
      "  timers:\n",
      "    learn_throughput: 1693.054\n",
      "    learn_time_ms: 590.649\n",
      "    load_throughput: 234340.915\n",
      "    load_time_ms: 4.267\n",
      "    sample_throughput: 88.88\n",
      "    sample_time_ms: 11251.174\n",
      "    update_time_ms: 2.713\n",
      "  timestamp: 1631960385\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 94000\n",
      "  training_iteration: 94\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">          1033.7</td><td style=\"text-align: right;\">94000</td><td style=\"text-align: right;\">-0.117021</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">               996</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 95000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-19-54\n",
      "  done: false\n",
      "  episode_len_mean: 996.0421052631579\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.11578947368421053\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 95\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.224857375356886\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010713747562378452\n",
      "          policy_loss: -0.09158550028999647\n",
      "          total_loss: -0.10748129594657156\n",
      "          vf_explained_var: -0.5530575513839722\n",
      "          vf_loss: 0.002736884676333931\n",
      "    num_agent_steps_sampled: 95000\n",
      "    num_agent_steps_trained: 95000\n",
      "    num_steps_sampled: 95000\n",
      "    num_steps_trained: 95000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.371428571428574\n",
      "    ram_util_percent: 92.75000000000001\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03712347893964743\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.508614672574593\n",
      "    mean_inference_ms: 1.3357680075913168\n",
      "    mean_raw_obs_processing_ms: 0.43359086331598357\n",
      "  time_since_restore: 1043.442245721817\n",
      "  time_this_iter_s: 9.737552642822266\n",
      "  time_total_s: 1043.442245721817\n",
      "  timers:\n",
      "    learn_throughput: 1695.258\n",
      "    learn_time_ms: 589.881\n",
      "    load_throughput: 233781.875\n",
      "    load_time_ms: 4.277\n",
      "    sample_throughput: 88.8\n",
      "    sample_time_ms: 11261.296\n",
      "    update_time_ms: 2.704\n",
      "  timestamp: 1631960394\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 95000\n",
      "  training_iteration: 95\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         1043.44</td><td style=\"text-align: right;\">95000</td><td style=\"text-align: right;\">-0.115789</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.042</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-20-04\n",
      "  done: false\n",
      "  episode_len_mean: 996.0833333333334\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.11458333333333333\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 96\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2196968264049954\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011751038271460403\n",
      "          policy_loss: -0.0517162831293212\n",
      "          total_loss: -0.06720271454089217\n",
      "          vf_explained_var: -0.723035454750061\n",
      "          vf_loss: 0.002744561880050848\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.75714285714286\n",
      "    ram_util_percent: 92.65714285714284\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03712011106127302\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.4860957891576\n",
      "    mean_inference_ms: 1.3356009134803373\n",
      "    mean_raw_obs_processing_ms: 0.4358234646066714\n",
      "  time_since_restore: 1052.961520910263\n",
      "  time_this_iter_s: 9.519275188446045\n",
      "  time_total_s: 1052.961520910263\n",
      "  timers:\n",
      "    learn_throughput: 1698.377\n",
      "    learn_time_ms: 588.797\n",
      "    load_throughput: 235494.818\n",
      "    load_time_ms: 4.246\n",
      "    sample_throughput: 88.928\n",
      "    sample_time_ms: 11245.01\n",
      "    update_time_ms: 2.71\n",
      "  timestamp: 1631960404\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 96\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         1052.96</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">-0.114583</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.083</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 97000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-20-13\n",
      "  done: false\n",
      "  episode_len_mean: 996.1237113402062\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1134020618556701\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 97\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3587436119715375\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01514238928838717\n",
      "          policy_loss: -0.038317670631739825\n",
      "          total_loss: -0.054448519233200286\n",
      "          vf_explained_var: 0.1852199137210846\n",
      "          vf_loss: 0.002346030065220677\n",
      "    num_agent_steps_sampled: 97000\n",
      "    num_agent_steps_trained: 97000\n",
      "    num_steps_sampled: 97000\n",
      "    num_steps_trained: 97000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.721428571428575\n",
      "    ram_util_percent: 92.62857142857142\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03711683905067482\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.463939137106959\n",
      "    mean_inference_ms: 1.335438136272656\n",
      "    mean_raw_obs_processing_ms: 0.43795379515992744\n",
      "  time_since_restore: 1062.4279074668884\n",
      "  time_this_iter_s: 9.466386556625366\n",
      "  time_total_s: 1062.4279074668884\n",
      "  timers:\n",
      "    learn_throughput: 1702.898\n",
      "    learn_time_ms: 587.234\n",
      "    load_throughput: 234817.154\n",
      "    load_time_ms: 4.259\n",
      "    sample_throughput: 89.024\n",
      "    sample_time_ms: 11232.916\n",
      "    update_time_ms: 2.709\n",
      "  timestamp: 1631960413\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 97000\n",
      "  training_iteration: 97\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         1062.43</td><td style=\"text-align: right;\">97000</td><td style=\"text-align: right;\">-0.113402</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.124</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 98000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-20-23\n",
      "  done: false\n",
      "  episode_len_mean: 996.1632653061224\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.11224489795918367\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 98\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.406292457050747\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009447756607495453\n",
      "          policy_loss: -0.06978459830085436\n",
      "          total_loss: -0.08877134608725706\n",
      "          vf_explained_var: -0.5617446303367615\n",
      "          vf_loss: 0.0018875559584961997\n",
      "    num_agent_steps_sampled: 98000\n",
      "    num_agent_steps_trained: 98000\n",
      "    num_steps_sampled: 98000\n",
      "    num_steps_trained: 98000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.13076923076923\n",
      "    ram_util_percent: 92.63846153846154\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03711364186344003\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.442161267877278\n",
      "    mean_inference_ms: 1.3352795691326707\n",
      "    mean_raw_obs_processing_ms: 0.4399862978541725\n",
      "  time_since_restore: 1072.1823751926422\n",
      "  time_this_iter_s: 9.754467725753784\n",
      "  time_total_s: 1072.1823751926422\n",
      "  timers:\n",
      "    learn_throughput: 1694.44\n",
      "    learn_time_ms: 590.165\n",
      "    load_throughput: 234528.293\n",
      "    load_time_ms: 4.264\n",
      "    sample_throughput: 88.802\n",
      "    sample_time_ms: 11261.038\n",
      "    update_time_ms: 2.702\n",
      "  timestamp: 1631960423\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 98000\n",
      "  training_iteration: 98\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         1072.18</td><td style=\"text-align: right;\">98000</td><td style=\"text-align: right;\">-0.112245</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.163</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 99000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-20-33\n",
      "  done: false\n",
      "  episode_len_mean: 996.2020202020202\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1111111111111111\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 99\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4170390870836047\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012479133197284267\n",
      "          policy_loss: 0.026145242692695723\n",
      "          total_loss: 0.007764018601220515\n",
      "          vf_explained_var: -0.45132899284362793\n",
      "          vf_loss: 0.0015774552257628077\n",
      "    num_agent_steps_sampled: 99000\n",
      "    num_agent_steps_trained: 99000\n",
      "    num_steps_sampled: 99000\n",
      "    num_steps_trained: 99000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.34285714285714\n",
      "    ram_util_percent: 92.55714285714285\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037110515976602886\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.42075079135268\n",
      "    mean_inference_ms: 1.3351239130036587\n",
      "    mean_raw_obs_processing_ms: 0.44192547607840216\n",
      "  time_since_restore: 1081.92724776268\n",
      "  time_this_iter_s: 9.744872570037842\n",
      "  time_total_s: 1081.92724776268\n",
      "  timers:\n",
      "    learn_throughput: 1688.357\n",
      "    learn_time_ms: 592.292\n",
      "    load_throughput: 235514.653\n",
      "    load_time_ms: 4.246\n",
      "    sample_throughput: 88.929\n",
      "    sample_time_ms: 11244.866\n",
      "    update_time_ms: 1.96\n",
      "  timestamp: 1631960433\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 99000\n",
      "  training_iteration: 99\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         1081.93</td><td style=\"text-align: right;\">99000</td><td style=\"text-align: right;\">-0.111111</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.202</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-20-43\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.11\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 100\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3702504131529065\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01530826646832624\n",
      "          policy_loss: -0.049971630051732066\n",
      "          total_loss: -0.06614410252206855\n",
      "          vf_explained_var: -0.2079268991947174\n",
      "          vf_loss: 0.0023634914114760855\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.92142857142858\n",
      "    ram_util_percent: 92.6\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037107459022913326\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.399706097674544\n",
      "    mean_inference_ms: 1.3349715431971847\n",
      "    mean_raw_obs_processing_ms: 0.443773914006063\n",
      "  time_since_restore: 1091.7182083129883\n",
      "  time_this_iter_s: 9.790960550308228\n",
      "  time_total_s: 1091.7182083129883\n",
      "  timers:\n",
      "    learn_throughput: 1698.904\n",
      "    learn_time_ms: 588.615\n",
      "    load_throughput: 298350.725\n",
      "    load_time_ms: 3.352\n",
      "    sample_throughput: 104.298\n",
      "    sample_time_ms: 9587.926\n",
      "    update_time_ms: 1.965\n",
      "  timestamp: 1631960443\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 100\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         1091.72</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">   -0.11</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 101000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-20-53\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.09\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 101\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3454988532596164\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009368502779101204\n",
      "          policy_loss: -0.024170911353495386\n",
      "          total_loss: -0.04282171010143227\n",
      "          vf_explained_var: -0.38917243480682373\n",
      "          vf_loss: 0.0016423184010717605\n",
      "    num_agent_steps_sampled: 101000\n",
      "    num_agent_steps_trained: 101000\n",
      "    num_steps_sampled: 101000\n",
      "    num_steps_trained: 101000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.528571428571425\n",
      "    ram_util_percent: 92.58571428571427\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03707594393585992\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.904231359485934\n",
      "    mean_inference_ms: 1.3331957755265187\n",
      "    mean_raw_obs_processing_ms: 0.4484764724430952\n",
      "  time_since_restore: 1101.4148981571198\n",
      "  time_this_iter_s: 9.69668984413147\n",
      "  time_total_s: 1101.4148981571198\n",
      "  timers:\n",
      "    learn_throughput: 1718.877\n",
      "    learn_time_ms: 581.775\n",
      "    load_throughput: 314686.874\n",
      "    load_time_ms: 3.178\n",
      "    sample_throughput: 109.25\n",
      "    sample_time_ms: 9153.33\n",
      "    update_time_ms: 1.957\n",
      "  timestamp: 1631960453\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 101000\n",
      "  training_iteration: 101\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         1101.41</td><td style=\"text-align: right;\">101000</td><td style=\"text-align: right;\">   -0.09</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 102000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-21-02\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.09\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 102\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1906018071704443\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011588106536698107\n",
      "          policy_loss: -0.06915400291068687\n",
      "          total_loss: -0.08344121244218615\n",
      "          vf_explained_var: -0.8119156956672668\n",
      "          vf_loss: 0.0037078192367011476\n",
      "    num_agent_steps_sampled: 102000\n",
      "    num_agent_steps_trained: 102000\n",
      "    num_steps_sampled: 102000\n",
      "    num_steps_trained: 102000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.857142857142854\n",
      "    ram_util_percent: 92.51428571428572\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03705228885821345\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.65803562165115\n",
      "    mean_inference_ms: 1.332073205633359\n",
      "    mean_raw_obs_processing_ms: 0.45332827955691757\n",
      "  time_since_restore: 1110.7984051704407\n",
      "  time_this_iter_s: 9.383507013320923\n",
      "  time_total_s: 1110.7984051704407\n",
      "  timers:\n",
      "    learn_throughput: 1728.798\n",
      "    learn_time_ms: 578.436\n",
      "    load_throughput: 316133.71\n",
      "    load_time_ms: 3.163\n",
      "    sample_throughput: 109.882\n",
      "    sample_time_ms: 9100.66\n",
      "    update_time_ms: 1.951\n",
      "  timestamp: 1631960462\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 102000\n",
      "  training_iteration: 102\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">          1110.8</td><td style=\"text-align: right;\">102000</td><td style=\"text-align: right;\">   -0.09</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 103000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-21-12\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.09\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 103\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3332026455137465\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015461548619634552\n",
      "          policy_loss: -0.013269064037336244\n",
      "          total_loss: -0.030353167653083803\n",
      "          vf_explained_var: 0.2256825566291809\n",
      "          vf_loss: 0.0010296512737921956\n",
      "    num_agent_steps_sampled: 103000\n",
      "    num_agent_steps_trained: 103000\n",
      "    num_steps_sampled: 103000\n",
      "    num_steps_trained: 103000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.621428571428574\n",
      "    ram_util_percent: 92.52857142857142\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037030577890948255\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.497271964683033\n",
      "    mean_inference_ms: 1.3311152398683594\n",
      "    mean_raw_obs_processing_ms: 0.45785801365903345\n",
      "  time_since_restore: 1120.439923286438\n",
      "  time_this_iter_s: 9.641518115997314\n",
      "  time_total_s: 1120.439923286438\n",
      "  timers:\n",
      "    learn_throughput: 1717.585\n",
      "    learn_time_ms: 582.213\n",
      "    load_throughput: 291313.594\n",
      "    load_time_ms: 3.433\n",
      "    sample_throughput: 110.375\n",
      "    sample_time_ms: 9060.038\n",
      "    update_time_ms: 1.972\n",
      "  timestamp: 1631960472\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 103000\n",
      "  training_iteration: 103\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         1120.44</td><td style=\"text-align: right;\">103000</td><td style=\"text-align: right;\">   -0.09</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-21-21\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.09\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 104\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3891367435455324\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011973381691388596\n",
      "          policy_loss: 0.04211009720133411\n",
      "          total_loss: 0.022900828760531212\n",
      "          vf_explained_var: 0.2831714451313019\n",
      "          vf_loss: 0.0006410822857611089\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.74615384615385\n",
      "    ram_util_percent: 92.63846153846154\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037011218160010906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.370174030400923\n",
      "    mean_inference_ms: 1.3303624945963395\n",
      "    mean_raw_obs_processing_ms: 0.46246287721876883\n",
      "  time_since_restore: 1130.2054982185364\n",
      "  time_this_iter_s: 9.765574932098389\n",
      "  time_total_s: 1130.2054982185364\n",
      "  timers:\n",
      "    learn_throughput: 1708.521\n",
      "    learn_time_ms: 585.302\n",
      "    load_throughput: 290574.942\n",
      "    load_time_ms: 3.441\n",
      "    sample_throughput: 110.42\n",
      "    sample_time_ms: 9056.343\n",
      "    update_time_ms: 1.98\n",
      "  timestamp: 1631960481\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 104\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         1130.21</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">   -0.09</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 105000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-21-31\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.09\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 105\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.052562508318159\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012811820025557866\n",
      "          policy_loss: -0.11291993624634213\n",
      "          total_loss: -0.12625502070619\n",
      "          vf_explained_var: -0.5048343539237976\n",
      "          vf_loss: 0.0028665516641922296\n",
      "    num_agent_steps_sampled: 105000\n",
      "    num_agent_steps_trained: 105000\n",
      "    num_steps_sampled: 105000\n",
      "    num_steps_trained: 105000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.24285714285714\n",
      "    ram_util_percent: 92.71428571428574\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03699494049997177\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.272911543415272\n",
      "    mean_inference_ms: 1.3297590770946852\n",
      "    mean_raw_obs_processing_ms: 0.46707205205598457\n",
      "  time_since_restore: 1139.7907347679138\n",
      "  time_this_iter_s: 9.585236549377441\n",
      "  time_total_s: 1139.7907347679138\n",
      "  timers:\n",
      "    learn_throughput: 1703.324\n",
      "    learn_time_ms: 587.087\n",
      "    load_throughput: 288226.716\n",
      "    load_time_ms: 3.469\n",
      "    sample_throughput: 110.628\n",
      "    sample_time_ms: 9039.301\n",
      "    update_time_ms: 1.989\n",
      "  timestamp: 1631960491\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 105000\n",
      "  training_iteration: 105\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         1139.79</td><td style=\"text-align: right;\">105000</td><td style=\"text-align: right;\">   -0.09</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 106000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-21-41\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 106\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1587016661961873\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012932984964331122\n",
      "          policy_loss: -0.02919426483826505\n",
      "          total_loss: -0.03497290106283294\n",
      "          vf_explained_var: 0.4042890667915344\n",
      "          vf_loss: 0.011443499246767411\n",
      "    num_agent_steps_sampled: 106000\n",
      "    num_agent_steps_trained: 106000\n",
      "    num_steps_sampled: 106000\n",
      "    num_steps_trained: 106000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.699999999999996\n",
      "    ram_util_percent: 92.78571428571426\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03698153778241002\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.194349073359449\n",
      "    mean_inference_ms: 1.3292578055456274\n",
      "    mean_raw_obs_processing_ms: 0.4716645449371082\n",
      "  time_since_restore: 1149.6694009304047\n",
      "  time_this_iter_s: 9.878666162490845\n",
      "  time_total_s: 1149.6694009304047\n",
      "  timers:\n",
      "    learn_throughput: 1692.489\n",
      "    learn_time_ms: 590.846\n",
      "    load_throughput: 285225.906\n",
      "    load_time_ms: 3.506\n",
      "    sample_throughput: 110.236\n",
      "    sample_time_ms: 9071.459\n",
      "    update_time_ms: 1.984\n",
      "  timestamp: 1631960501\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 106000\n",
      "  training_iteration: 106\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         1149.67</td><td style=\"text-align: right;\">106000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 107000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-21-51\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 107\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.238070731692844\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012155563361255685\n",
      "          policy_loss: -0.02121853240662151\n",
      "          total_loss: -0.03619371659070667\n",
      "          vf_explained_var: 0.13392654061317444\n",
      "          vf_loss: 0.003303020238591772\n",
      "    num_agent_steps_sampled: 107000\n",
      "    num_agent_steps_trained: 107000\n",
      "    num_steps_sampled: 107000\n",
      "    num_steps_trained: 107000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.064285714285724\n",
      "    ram_util_percent: 92.74285714285713\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03696955711779228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.129299921046659\n",
      "    mean_inference_ms: 1.3288265076558239\n",
      "    mean_raw_obs_processing_ms: 0.4762300984355229\n",
      "  time_since_restore: 1159.5854835510254\n",
      "  time_this_iter_s: 9.916082620620728\n",
      "  time_total_s: 1159.5854835510254\n",
      "  timers:\n",
      "    learn_throughput: 1678.537\n",
      "    learn_time_ms: 595.757\n",
      "    load_throughput: 285959.025\n",
      "    load_time_ms: 3.497\n",
      "    sample_throughput: 109.751\n",
      "    sample_time_ms: 9111.548\n",
      "    update_time_ms: 1.975\n",
      "  timestamp: 1631960511\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 107000\n",
      "  training_iteration: 107\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         1159.59</td><td style=\"text-align: right;\">107000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-22-01\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 108\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.136349826388889\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015706336186954276\n",
      "          policy_loss: -0.09533599629584286\n",
      "          total_loss: -0.10808177679363225\n",
      "          vf_explained_var: 0.3203541338443756\n",
      "          vf_loss: 0.0033168294686927564\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.85000000000001\n",
      "    ram_util_percent: 92.80714285714285\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036958885945487185\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.072946598632258\n",
      "    mean_inference_ms: 1.3284329570150735\n",
      "    mean_raw_obs_processing_ms: 0.4807391672911014\n",
      "  time_since_restore: 1169.2378313541412\n",
      "  time_this_iter_s: 9.652347803115845\n",
      "  time_total_s: 1169.2378313541412\n",
      "  timers:\n",
      "    learn_throughput: 1687.036\n",
      "    learn_time_ms: 592.756\n",
      "    load_throughput: 286425.742\n",
      "    load_time_ms: 3.491\n",
      "    sample_throughput: 109.838\n",
      "    sample_time_ms: 9104.353\n",
      "    update_time_ms: 1.991\n",
      "  timestamp: 1631960521\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 108\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         1169.24</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 109000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-22-10\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 109\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1985828744040594\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01251144518320683\n",
      "          policy_loss: -0.03589234302441279\n",
      "          total_loss: -0.05131308651632733\n",
      "          vf_explained_var: -0.6736879348754883\n",
      "          vf_loss: 0.0023424722719937565\n",
      "    num_agent_steps_sampled: 109000\n",
      "    num_agent_steps_trained: 109000\n",
      "    num_steps_sampled: 109000\n",
      "    num_steps_trained: 109000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.521428571428565\n",
      "    ram_util_percent: 92.85714285714286\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03694954160266654\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.023357047178672\n",
      "    mean_inference_ms: 1.3280771759909822\n",
      "    mean_raw_obs_processing_ms: 0.4851805441234658\n",
      "  time_since_restore: 1178.8712327480316\n",
      "  time_this_iter_s: 9.63340139389038\n",
      "  time_total_s: 1178.8712327480316\n",
      "  timers:\n",
      "    learn_throughput: 1700.42\n",
      "    learn_time_ms: 588.09\n",
      "    load_throughput: 285334.567\n",
      "    load_time_ms: 3.505\n",
      "    sample_throughput: 109.916\n",
      "    sample_time_ms: 9097.869\n",
      "    update_time_ms: 1.974\n",
      "  timestamp: 1631960530\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 109000\n",
      "  training_iteration: 109\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         1178.87</td><td style=\"text-align: right;\">109000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 110000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-22-20\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 110\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2607067796919083\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01156823888619621\n",
      "          policy_loss: 0.012992616184055806\n",
      "          total_loss: -0.004464457722173797\n",
      "          vf_explained_var: -0.6268162727355957\n",
      "          vf_loss: 0.001245710513709734\n",
      "    num_agent_steps_sampled: 110000\n",
      "    num_agent_steps_trained: 110000\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.728571428571435\n",
      "    ram_util_percent: 92.85\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03694134959542975\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.979057022186666\n",
      "    mean_inference_ms: 1.3277589472586113\n",
      "    mean_raw_obs_processing_ms: 0.48960895580589076\n",
      "  time_since_restore: 1188.565021276474\n",
      "  time_this_iter_s: 9.693788528442383\n",
      "  time_total_s: 1188.565021276474\n",
      "  timers:\n",
      "    learn_throughput: 1702.385\n",
      "    learn_time_ms: 587.411\n",
      "    load_throughput: 282663.612\n",
      "    load_time_ms: 3.538\n",
      "    sample_throughput: 110.026\n",
      "    sample_time_ms: 9088.765\n",
      "    update_time_ms: 1.98\n",
      "  timestamp: 1631960540\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 110\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         1188.57</td><td style=\"text-align: right;\">110000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 111000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-22-29\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 111\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2204819546805488\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012009975130145309\n",
      "          policy_loss: -0.02577392670015494\n",
      "          total_loss: -0.04288282692432403\n",
      "          vf_explained_var: -0.33106938004493713\n",
      "          vf_loss: 0.0010425510900353807\n",
      "    num_agent_steps_sampled: 111000\n",
      "    num_agent_steps_trained: 111000\n",
      "    num_steps_sampled: 111000\n",
      "    num_steps_trained: 111000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.5\n",
      "    ram_util_percent: 92.83846153846153\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03693391503644279\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.939331465365868\n",
      "    mean_inference_ms: 1.327462228790649\n",
      "    mean_raw_obs_processing_ms: 0.4939879223078554\n",
      "  time_since_restore: 1198.01473736763\n",
      "  time_this_iter_s: 9.449716091156006\n",
      "  time_total_s: 1198.01473736763\n",
      "  timers:\n",
      "    learn_throughput: 1699.427\n",
      "    learn_time_ms: 588.434\n",
      "    load_throughput: 272559.167\n",
      "    load_time_ms: 3.669\n",
      "    sample_throughput: 110.34\n",
      "    sample_time_ms: 9062.918\n",
      "    update_time_ms: 1.975\n",
      "  timestamp: 1631960549\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 111000\n",
      "  training_iteration: 111\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         1198.01</td><td style=\"text-align: right;\">111000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-22-39\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 112\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.195283304320441\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012678977669102344\n",
      "          policy_loss: -0.011234678824742635\n",
      "          total_loss: -0.02672585758070151\n",
      "          vf_explained_var: -0.75128573179245\n",
      "          vf_loss: 0.0021824983226704513\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.38571428571429\n",
      "    ram_util_percent: 92.92857142857144\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03692740192536506\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.902914339899779\n",
      "    mean_inference_ms: 1.3271914993583847\n",
      "    mean_raw_obs_processing_ms: 0.49835125931279395\n",
      "  time_since_restore: 1207.5136148929596\n",
      "  time_this_iter_s: 9.49887752532959\n",
      "  time_total_s: 1207.5136148929596\n",
      "  timers:\n",
      "    learn_throughput: 1701.83\n",
      "    learn_time_ms: 587.603\n",
      "    load_throughput: 271887.778\n",
      "    load_time_ms: 3.678\n",
      "    sample_throughput: 110.19\n",
      "    sample_time_ms: 9075.258\n",
      "    update_time_ms: 1.981\n",
      "  timestamp: 1631960559\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 112\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         1207.51</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 113000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-22-48\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 113\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.03147318760554\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014422635364251373\n",
      "          policy_loss: -0.05765251744952467\n",
      "          total_loss: -0.07128473669290543\n",
      "          vf_explained_var: -0.5591233372688293\n",
      "          vf_loss: 0.001814873470316848\n",
      "    num_agent_steps_sampled: 113000\n",
      "    num_agent_steps_trained: 113000\n",
      "    num_steps_sampled: 113000\n",
      "    num_steps_trained: 113000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.35714285714287\n",
      "    ram_util_percent: 92.91428571428573\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03692180622304396\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.870249377449886\n",
      "    mean_inference_ms: 1.3269501726381248\n",
      "    mean_raw_obs_processing_ms: 0.502623443691909\n",
      "  time_since_restore: 1217.014672279358\n",
      "  time_this_iter_s: 9.501057386398315\n",
      "  time_total_s: 1217.014672279358\n",
      "  timers:\n",
      "    learn_throughput: 1708.099\n",
      "    learn_time_ms: 585.446\n",
      "    load_throughput: 293607.739\n",
      "    load_time_ms: 3.406\n",
      "    sample_throughput: 110.331\n",
      "    sample_time_ms: 9063.67\n",
      "    update_time_ms: 1.973\n",
      "  timestamp: 1631960568\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 113000\n",
      "  training_iteration: 113\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         1217.01</td><td style=\"text-align: right;\">113000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 114000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-22-58\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 114\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8005240930451287\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012943879572431004\n",
      "          policy_loss: 0.00804068711068895\n",
      "          total_loss: -0.0035160446746481788\n",
      "          vf_explained_var: -0.05438245087862015\n",
      "          vf_loss: 0.0020799548002994723\n",
      "    num_agent_steps_sampled: 114000\n",
      "    num_agent_steps_trained: 114000\n",
      "    num_steps_sampled: 114000\n",
      "    num_steps_trained: 114000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.599999999999994\n",
      "    ram_util_percent: 92.92307692307692\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03691703896696974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.840045452024821\n",
      "    mean_inference_ms: 1.326724655875633\n",
      "    mean_raw_obs_processing_ms: 0.5068469146323031\n",
      "  time_since_restore: 1226.4966225624084\n",
      "  time_this_iter_s: 9.481950283050537\n",
      "  time_total_s: 1226.4966225624084\n",
      "  timers:\n",
      "    learn_throughput: 1722.287\n",
      "    learn_time_ms: 580.623\n",
      "    load_throughput: 293710.54\n",
      "    load_time_ms: 3.405\n",
      "    sample_throughput: 110.618\n",
      "    sample_time_ms: 9040.133\n",
      "    update_time_ms: 1.971\n",
      "  timestamp: 1631960578\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 114000\n",
      "  training_iteration: 114\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">          1226.5</td><td style=\"text-align: right;\">114000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 115000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-23-07\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 115\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.944789997736613\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015942453237049292\n",
      "          policy_loss: -0.032724656578567296\n",
      "          total_loss: -0.04422980517976814\n",
      "          vf_explained_var: -0.13467155396938324\n",
      "          vf_loss: 0.0025621695663883455\n",
      "    num_agent_steps_sampled: 115000\n",
      "    num_agent_steps_trained: 115000\n",
      "    num_steps_sampled: 115000\n",
      "    num_steps_trained: 115000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.45714285714286\n",
      "    ram_util_percent: 92.97857142857141\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03691265913053289\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.811837162098762\n",
      "    mean_inference_ms: 1.3265045077072584\n",
      "    mean_raw_obs_processing_ms: 0.5110342519769242\n",
      "  time_since_restore: 1236.069396018982\n",
      "  time_this_iter_s: 9.572773456573486\n",
      "  time_total_s: 1236.069396018982\n",
      "  timers:\n",
      "    learn_throughput: 1725.281\n",
      "    learn_time_ms: 579.616\n",
      "    load_throughput: 296857.81\n",
      "    load_time_ms: 3.369\n",
      "    sample_throughput: 110.62\n",
      "    sample_time_ms: 9039.957\n",
      "    update_time_ms: 1.938\n",
      "  timestamp: 1631960587\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 115000\n",
      "  training_iteration: 115\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         1236.07</td><td style=\"text-align: right;\">115000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-23-17\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 116\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2152086363898382\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0123266943658897\n",
      "          policy_loss: -0.014744355902075767\n",
      "          total_loss: -0.031724430951807235\n",
      "          vf_explained_var: -0.3380115032196045\n",
      "          vf_loss: 0.0010117540433485475\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.599999999999994\n",
      "    ram_util_percent: 93.0\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036908653883756984\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.785546521772584\n",
      "    mean_inference_ms: 1.3262935287762487\n",
      "    mean_raw_obs_processing_ms: 0.5151742020466309\n",
      "  time_since_restore: 1245.5443179607391\n",
      "  time_this_iter_s: 9.474921941757202\n",
      "  time_total_s: 1245.5443179607391\n",
      "  timers:\n",
      "    learn_throughput: 1740.152\n",
      "    learn_time_ms: 574.662\n",
      "    load_throughput: 299142.293\n",
      "    load_time_ms: 3.343\n",
      "    sample_throughput: 111.055\n",
      "    sample_time_ms: 9004.554\n",
      "    update_time_ms: 1.93\n",
      "  timestamp: 1631960597\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 116\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         1245.54</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 117000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-23-26\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 117\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.903273171848721\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01490904797624042\n",
      "          policy_loss: -0.10905911365730894\n",
      "          total_loss: -0.12067067018813557\n",
      "          vf_explained_var: -0.29598376154899597\n",
      "          vf_loss: 0.002389371843956825\n",
      "    num_agent_steps_sampled: 117000\n",
      "    num_agent_steps_trained: 117000\n",
      "    num_steps_sampled: 117000\n",
      "    num_steps_trained: 117000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.207142857142856\n",
      "    ram_util_percent: 93.00714285714285\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0369052339648694\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.760976694479353\n",
      "    mean_inference_ms: 1.3260881771557904\n",
      "    mean_raw_obs_processing_ms: 0.5192534772212376\n",
      "  time_since_restore: 1255.02441239357\n",
      "  time_this_iter_s: 9.48009443283081\n",
      "  time_total_s: 1255.02441239357\n",
      "  timers:\n",
      "    learn_throughput: 1748.845\n",
      "    learn_time_ms: 571.806\n",
      "    load_throughput: 297861.292\n",
      "    load_time_ms: 3.357\n",
      "    sample_throughput: 111.56\n",
      "    sample_time_ms: 8963.773\n",
      "    update_time_ms: 1.952\n",
      "  timestamp: 1631960606\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 117000\n",
      "  training_iteration: 117\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         1255.02</td><td style=\"text-align: right;\">117000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 118000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-23-36\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 118\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.048433526357015\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013237169208938127\n",
      "          policy_loss: -0.027378410732166635\n",
      "          total_loss: -0.04057334938810931\n",
      "          vf_explained_var: -0.5003200173377991\n",
      "          vf_loss: 0.002821851820529749\n",
      "    num_agent_steps_sampled: 118000\n",
      "    num_agent_steps_trained: 118000\n",
      "    num_steps_sampled: 118000\n",
      "    num_steps_trained: 118000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.24615384615385\n",
      "    ram_util_percent: 93.0076923076923\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036901722305689034\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.73803098288983\n",
      "    mean_inference_ms: 1.3258809850063926\n",
      "    mean_raw_obs_processing_ms: 0.5233121471001584\n",
      "  time_since_restore: 1264.484563112259\n",
      "  time_this_iter_s: 9.460150718688965\n",
      "  time_total_s: 1264.484563112259\n",
      "  timers:\n",
      "    learn_throughput: 1748.878\n",
      "    learn_time_ms: 571.795\n",
      "    load_throughput: 297692.166\n",
      "    load_time_ms: 3.359\n",
      "    sample_throughput: 111.8\n",
      "    sample_time_ms: 8944.576\n",
      "    update_time_ms: 1.94\n",
      "  timestamp: 1631960616\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 118000\n",
      "  training_iteration: 118\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">         1264.48</td><td style=\"text-align: right;\">118000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 119000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-23-45\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 119\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2815362188551163\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015556366305028446\n",
      "          policy_loss: -0.13137154032786688\n",
      "          total_loss: -0.14804957293801838\n",
      "          vf_explained_var: -0.36670222878456116\n",
      "          vf_loss: 0.000887052981771477\n",
      "    num_agent_steps_sampled: 119000\n",
      "    num_agent_steps_trained: 119000\n",
      "    num_steps_sampled: 119000\n",
      "    num_steps_trained: 119000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.214285714285715\n",
      "    ram_util_percent: 93.08571428571427\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03689877750113474\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.716305463743847\n",
      "    mean_inference_ms: 1.3256872422073704\n",
      "    mean_raw_obs_processing_ms: 0.5273512893862081\n",
      "  time_since_restore: 1273.8810551166534\n",
      "  time_this_iter_s: 9.396492004394531\n",
      "  time_total_s: 1273.8810551166534\n",
      "  timers:\n",
      "    learn_throughput: 1745.254\n",
      "    learn_time_ms: 572.982\n",
      "    load_throughput: 298995.153\n",
      "    load_time_ms: 3.345\n",
      "    sample_throughput: 112.111\n",
      "    sample_time_ms: 8919.693\n",
      "    update_time_ms: 1.956\n",
      "  timestamp: 1631960625\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 119000\n",
      "  training_iteration: 119\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         1273.88</td><td style=\"text-align: right;\">119000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-24-12\n",
      "  done: false\n",
      "  episode_len_mean: 994.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 120\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7926315599017673\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011571806959652993\n",
      "          policy_loss: -0.06255546121133698\n",
      "          total_loss: -0.07482656033502685\n",
      "          vf_explained_var: -0.29377052187919617\n",
      "          vf_loss: 0.0017497330929877029\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.936842105263157\n",
      "    ram_util_percent: 93.19473684210524\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03689624478811749\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.696592630895816\n",
      "    mean_inference_ms: 1.3255090425640006\n",
      "    mean_raw_obs_processing_ms: 0.532791941472135\n",
      "  time_since_restore: 1300.4136004447937\n",
      "  time_this_iter_s: 26.53254532814026\n",
      "  time_total_s: 1300.4136004447937\n",
      "  timers:\n",
      "    learn_throughput: 1751.797\n",
      "    learn_time_ms: 570.842\n",
      "    load_throughput: 214270.593\n",
      "    load_time_ms: 4.667\n",
      "    sample_throughput: 94.3\n",
      "    sample_time_ms: 10604.428\n",
      "    update_time_ms: 1.943\n",
      "  timestamp: 1631960652\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 120\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">         1300.41</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            994.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 121000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-24-23\n",
      "  done: false\n",
      "  episode_len_mean: 994.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 121\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9004017949104308\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019933723256215223\n",
      "          policy_loss: -0.08303491115156147\n",
      "          total_loss: -0.09287803322076797\n",
      "          vf_explained_var: 0.08970654010772705\n",
      "          vf_loss: 0.002433265105355531\n",
      "    num_agent_steps_sampled: 121000\n",
      "    num_agent_steps_trained: 121000\n",
      "    num_steps_sampled: 121000\n",
      "    num_steps_trained: 121000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.36\n",
      "    ram_util_percent: 93.02\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03689336425228464\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.67846726978228\n",
      "    mean_inference_ms: 1.3253266303428504\n",
      "    mean_raw_obs_processing_ms: 0.5382003504143263\n",
      "  time_since_restore: 1310.979815006256\n",
      "  time_this_iter_s: 10.566214561462402\n",
      "  time_total_s: 1310.979815006256\n",
      "  timers:\n",
      "    learn_throughput: 1753.051\n",
      "    learn_time_ms: 570.434\n",
      "    load_throughput: 218638.949\n",
      "    load_time_ms: 4.574\n",
      "    sample_throughput: 93.314\n",
      "    sample_time_ms: 10716.562\n",
      "    update_time_ms: 1.961\n",
      "  timestamp: 1631960663\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 121000\n",
      "  training_iteration: 121\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         1310.98</td><td style=\"text-align: right;\">121000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            994.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 122000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-24-32\n",
      "  done: false\n",
      "  episode_len_mean: 994.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 122\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6416691766844855\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015290371289261234\n",
      "          policy_loss: -0.04751491298278173\n",
      "          total_loss: -0.0556906070974138\n",
      "          vf_explained_var: -0.19546794891357422\n",
      "          vf_loss: 0.0030805002328836255\n",
      "    num_agent_steps_sampled: 122000\n",
      "    num_agent_steps_trained: 122000\n",
      "    num_steps_sampled: 122000\n",
      "    num_steps_trained: 122000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.0\n",
      "    ram_util_percent: 93.0923076923077\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03689070128894573\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.661837539387777\n",
      "    mean_inference_ms: 1.3251510537357591\n",
      "    mean_raw_obs_processing_ms: 0.5435771748058191\n",
      "  time_since_restore: 1320.271107673645\n",
      "  time_this_iter_s: 9.291292667388916\n",
      "  time_total_s: 1320.271107673645\n",
      "  timers:\n",
      "    learn_throughput: 1740.735\n",
      "    learn_time_ms: 574.47\n",
      "    load_throughput: 216102.097\n",
      "    load_time_ms: 4.627\n",
      "    sample_throughput: 93.53\n",
      "    sample_time_ms: 10691.745\n",
      "    update_time_ms: 1.961\n",
      "  timestamp: 1631960672\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 122000\n",
      "  training_iteration: 122\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         1320.27</td><td style=\"text-align: right;\">122000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            994.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 123000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-24-41\n",
      "  done: false\n",
      "  episode_len_mean: 994.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 123\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6769129051102531\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008027170831247184\n",
      "          policy_loss: -0.0038810446858406067\n",
      "          total_loss: -0.015816741809248924\n",
      "          vf_explained_var: -0.1763889491558075\n",
      "          vf_loss: 0.0021242604086486\n",
      "    num_agent_steps_sampled: 123000\n",
      "    num_agent_steps_trained: 123000\n",
      "    num_steps_sampled: 123000\n",
      "    num_steps_trained: 123000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.72307692307691\n",
      "    ram_util_percent: 93.12307692307692\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036887286327512696\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.646068435188711\n",
      "    mean_inference_ms: 1.3249587757043466\n",
      "    mean_raw_obs_processing_ms: 0.5488750303851799\n",
      "  time_since_restore: 1329.3173968791962\n",
      "  time_this_iter_s: 9.046289205551147\n",
      "  time_total_s: 1329.3173968791962\n",
      "  timers:\n",
      "    learn_throughput: 1738.188\n",
      "    learn_time_ms: 575.312\n",
      "    load_throughput: 216195.665\n",
      "    load_time_ms: 4.625\n",
      "    sample_throughput: 93.937\n",
      "    sample_time_ms: 10645.437\n",
      "    update_time_ms: 1.961\n",
      "  timestamp: 1631960681\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 123000\n",
      "  training_iteration: 123\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         1329.32</td><td style=\"text-align: right;\">123000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            994.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-24-50\n",
      "  done: false\n",
      "  episode_len_mean: 994.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 124\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9074831816885207\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012276440349853132\n",
      "          policy_loss: -0.008773613162338734\n",
      "          total_loss: -0.02068428887675206\n",
      "          vf_explained_var: -0.5198344588279724\n",
      "          vf_loss: 0.003020857070158753\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.05384615384616\n",
      "    ram_util_percent: 92.96153846153847\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036883504770142274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.630884307382123\n",
      "    mean_inference_ms: 1.3247517523443861\n",
      "    mean_raw_obs_processing_ms: 0.554137452085545\n",
      "  time_since_restore: 1338.3856554031372\n",
      "  time_this_iter_s: 9.06825852394104\n",
      "  time_total_s: 1338.3856554031372\n",
      "  timers:\n",
      "    learn_throughput: 1737.931\n",
      "    learn_time_ms: 575.397\n",
      "    load_throughput: 215859.645\n",
      "    load_time_ms: 4.633\n",
      "    sample_throughput: 94.305\n",
      "    sample_time_ms: 10603.931\n",
      "    update_time_ms: 1.965\n",
      "  timestamp: 1631960690\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 124\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         1338.39</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            994.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 125000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-24-59\n",
      "  done: false\n",
      "  episode_len_mean: 994.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 125\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.166566963990529\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014849692619884121\n",
      "          policy_loss: 0.01771062132385042\n",
      "          total_loss: 0.003941038416491615\n",
      "          vf_explained_var: 0.11487376689910889\n",
      "          vf_loss: 0.002884318660168598\n",
      "    num_agent_steps_sampled: 125000\n",
      "    num_agent_steps_trained: 125000\n",
      "    num_steps_sampled: 125000\n",
      "    num_steps_trained: 125000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.75384615384616\n",
      "    ram_util_percent: 92.70769230769231\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036879432029728904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.616623457328133\n",
      "    mean_inference_ms: 1.324541823742735\n",
      "    mean_raw_obs_processing_ms: 0.559333502226158\n",
      "  time_since_restore: 1347.6085188388824\n",
      "  time_this_iter_s: 9.22286343574524\n",
      "  time_total_s: 1347.6085188388824\n",
      "  timers:\n",
      "    learn_throughput: 1740.564\n",
      "    learn_time_ms: 574.526\n",
      "    load_throughput: 216733.015\n",
      "    load_time_ms: 4.614\n",
      "    sample_throughput: 94.609\n",
      "    sample_time_ms: 10569.814\n",
      "    update_time_ms: 1.983\n",
      "  timestamp: 1631960699\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 125000\n",
      "  training_iteration: 125\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         1347.61</td><td style=\"text-align: right;\">125000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            994.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 126000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-25-09\n",
      "  done: false\n",
      "  episode_len_mean: 994.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 126\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9185214175118341\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01357564970726247\n",
      "          policy_loss: 0.0690712066160308\n",
      "          total_loss: 0.05644264734453625\n",
      "          vf_explained_var: -0.10656964033842087\n",
      "          vf_loss: 0.00197487366773809\n",
      "    num_agent_steps_sampled: 126000\n",
      "    num_agent_steps_trained: 126000\n",
      "    num_steps_sampled: 126000\n",
      "    num_steps_trained: 126000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.714285714285715\n",
      "    ram_util_percent: 92.50714285714287\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036875780579050076\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.603068171045097\n",
      "    mean_inference_ms: 1.3243430023244025\n",
      "    mean_raw_obs_processing_ms: 0.5644733777663987\n",
      "  time_since_restore: 1357.0513269901276\n",
      "  time_this_iter_s: 9.442808151245117\n",
      "  time_total_s: 1357.0513269901276\n",
      "  timers:\n",
      "    learn_throughput: 1741.879\n",
      "    learn_time_ms: 574.093\n",
      "    load_throughput: 216935.912\n",
      "    load_time_ms: 4.61\n",
      "    sample_throughput: 94.634\n",
      "    sample_time_ms: 10567.059\n",
      "    update_time_ms: 1.985\n",
      "  timestamp: 1631960709\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 126000\n",
      "  training_iteration: 126\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">         1357.05</td><td style=\"text-align: right;\">126000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            994.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 127000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-25-18\n",
      "  done: false\n",
      "  episode_len_mean: 994.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 127\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2159938388400606\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020442763721780367\n",
      "          policy_loss: -0.0038171343505382536\n",
      "          total_loss: -0.015505287465122012\n",
      "          vf_explained_var: -0.6180160641670227\n",
      "          vf_loss: 0.0035723543473674606\n",
      "    num_agent_steps_sampled: 127000\n",
      "    num_agent_steps_trained: 127000\n",
      "    num_steps_sampled: 127000\n",
      "    num_steps_trained: 127000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.853846153846156\n",
      "    ram_util_percent: 92.36153846153844\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03687239552664352\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.589962511249212\n",
      "    mean_inference_ms: 1.324149781234162\n",
      "    mean_raw_obs_processing_ms: 0.5695823303752174\n",
      "  time_since_restore: 1366.4031643867493\n",
      "  time_this_iter_s: 9.351837396621704\n",
      "  time_total_s: 1366.4031643867493\n",
      "  timers:\n",
      "    learn_throughput: 1746.278\n",
      "    learn_time_ms: 572.647\n",
      "    load_throughput: 217223.531\n",
      "    load_time_ms: 4.604\n",
      "    sample_throughput: 94.736\n",
      "    sample_time_ms: 10555.686\n",
      "    update_time_ms: 1.976\n",
      "  timestamp: 1631960718\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 127000\n",
      "  training_iteration: 127\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">          1366.4</td><td style=\"text-align: right;\">127000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            994.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-25-28\n",
      "  done: false\n",
      "  episode_len_mean: 994.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 128\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8719362788730198\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009766862901539507\n",
      "          policy_loss: -0.10272436779406335\n",
      "          total_loss: -0.11440858410464393\n",
      "          vf_explained_var: -0.705836832523346\n",
      "          vf_loss: 0.0020906725245165743\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.65714285714286\n",
      "    ram_util_percent: 92.21428571428574\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03686924865968535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.57730139235737\n",
      "    mean_inference_ms: 1.3239628624269426\n",
      "    mean_raw_obs_processing_ms: 0.5746598026817026\n",
      "  time_since_restore: 1375.816884279251\n",
      "  time_this_iter_s: 9.413719892501831\n",
      "  time_total_s: 1375.816884279251\n",
      "  timers:\n",
      "    learn_throughput: 1747.492\n",
      "    learn_time_ms: 572.249\n",
      "    load_throughput: 215906.314\n",
      "    load_time_ms: 4.632\n",
      "    sample_throughput: 94.774\n",
      "    sample_time_ms: 10551.42\n",
      "    update_time_ms: 1.972\n",
      "  timestamp: 1631960728\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 128\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">         1375.82</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            994.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 129000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-25-37\n",
      "  done: false\n",
      "  episode_len_mean: 994.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 129\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.070508886708154\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014659179829892652\n",
      "          policy_loss: -0.035004354692581625\n",
      "          total_loss: -0.03865101724449131\n",
      "          vf_explained_var: 0.13776853680610657\n",
      "          vf_loss: 0.00963721628844117\n",
      "    num_agent_steps_sampled: 129000\n",
      "    num_agent_steps_trained: 129000\n",
      "    num_steps_sampled: 129000\n",
      "    num_steps_trained: 129000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.892307692307696\n",
      "    ram_util_percent: 92.16153846153847\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03686615303978764\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.56506868771406\n",
      "    mean_inference_ms: 1.323774918896495\n",
      "    mean_raw_obs_processing_ms: 0.5796866377256957\n",
      "  time_since_restore: 1385.0347752571106\n",
      "  time_this_iter_s: 9.217890977859497\n",
      "  time_total_s: 1385.0347752571106\n",
      "  timers:\n",
      "    learn_throughput: 1752.185\n",
      "    learn_time_ms: 570.716\n",
      "    load_throughput: 216105.438\n",
      "    load_time_ms: 4.627\n",
      "    sample_throughput: 94.921\n",
      "    sample_time_ms: 10535.117\n",
      "    update_time_ms: 1.952\n",
      "  timestamp: 1631960737\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 129000\n",
      "  training_iteration: 129\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         1385.03</td><td style=\"text-align: right;\">129000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            994.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 130000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-25-46\n",
      "  done: false\n",
      "  episode_len_mean: 996.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 130\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9902697020106845\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012584147479582935\n",
      "          policy_loss: -0.04036878148714702\n",
      "          total_loss: -0.05050702235764927\n",
      "          vf_explained_var: 0.6147114038467407\n",
      "          vf_loss: 0.0033937330616431102\n",
      "    num_agent_steps_sampled: 130000\n",
      "    num_agent_steps_trained: 130000\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.89999999999999\n",
      "    ram_util_percent: 92.13846153846154\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03686233882359221\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.552970262093154\n",
      "    mean_inference_ms: 1.3235646426375292\n",
      "    mean_raw_obs_processing_ms: 0.5792507737517938\n",
      "  time_since_restore: 1394.3635804653168\n",
      "  time_this_iter_s: 9.328805208206177\n",
      "  time_total_s: 1394.3635804653168\n",
      "  timers:\n",
      "    learn_throughput: 1744.997\n",
      "    learn_time_ms: 573.067\n",
      "    load_throughput: 302907.82\n",
      "    load_time_ms: 3.301\n",
      "    sample_throughput: 113.46\n",
      "    sample_time_ms: 8813.702\n",
      "    update_time_ms: 1.968\n",
      "  timestamp: 1631960746\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 130\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">         1394.36</td><td style=\"text-align: right;\">130000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 131000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-25-56\n",
      "  done: false\n",
      "  episode_len_mean: 996.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 131\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6989320251676772\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008523167467535325\n",
      "          policy_loss: -0.056571107978622116\n",
      "          total_loss: -0.06758367969757981\n",
      "          vf_explained_var: -0.12957227230072021\n",
      "          vf_loss: 0.0016618984311612115\n",
      "    num_agent_steps_sampled: 131000\n",
      "    num_agent_steps_trained: 131000\n",
      "    num_steps_sampled: 131000\n",
      "    num_steps_trained: 131000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.721428571428575\n",
      "    ram_util_percent: 92.15\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03685810680985322\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.540767637327974\n",
      "    mean_inference_ms: 1.3233469220104923\n",
      "    mean_raw_obs_processing_ms: 0.5789363930428624\n",
      "  time_since_restore: 1403.7934534549713\n",
      "  time_this_iter_s: 9.429872989654541\n",
      "  time_total_s: 1403.7934534549713\n",
      "  timers:\n",
      "    learn_throughput: 1744.36\n",
      "    learn_time_ms: 573.276\n",
      "    load_throughput: 306717.758\n",
      "    load_time_ms: 3.26\n",
      "    sample_throughput: 114.944\n",
      "    sample_time_ms: 8699.924\n",
      "    update_time_ms: 1.966\n",
      "  timestamp: 1631960756\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 131000\n",
      "  training_iteration: 131\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         1403.79</td><td style=\"text-align: right;\">131000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-26-05\n",
      "  done: false\n",
      "  episode_len_mean: 996.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 132\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8825081864992776\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012833961562163133\n",
      "          policy_loss: -0.02297904549373521\n",
      "          total_loss: -0.030259071704414157\n",
      "          vf_explained_var: -0.2087603509426117\n",
      "          vf_loss: 0.005047860413065387\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.89999999999999\n",
      "    ram_util_percent: 92.15384615384616\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036853893957135946\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.528879532319843\n",
      "    mean_inference_ms: 1.3231309997115983\n",
      "    mean_raw_obs_processing_ms: 0.5787458163523121\n",
      "  time_since_restore: 1413.176796913147\n",
      "  time_this_iter_s: 9.38334345817566\n",
      "  time_total_s: 1413.176796913147\n",
      "  timers:\n",
      "    learn_throughput: 1750.477\n",
      "    learn_time_ms: 571.273\n",
      "    load_throughput: 311267.913\n",
      "    load_time_ms: 3.213\n",
      "    sample_throughput: 114.795\n",
      "    sample_time_ms: 8711.191\n",
      "    update_time_ms: 1.956\n",
      "  timestamp: 1631960765\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 132\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         1413.18</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 133000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-26-14\n",
      "  done: false\n",
      "  episode_len_mean: 996.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 133\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.971179927719964\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010757240102819072\n",
      "          policy_loss: -0.10405483750833405\n",
      "          total_loss: -0.11595736688209904\n",
      "          vf_explained_var: 0.08327030390501022\n",
      "          vf_loss: 0.0023634174812792075\n",
      "    num_agent_steps_sampled: 133000\n",
      "    num_agent_steps_trained: 133000\n",
      "    num_steps_sampled: 133000\n",
      "    num_steps_trained: 133000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.88461538461539\n",
      "    ram_util_percent: 92.13076923076923\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03684954908906439\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.51721614501741\n",
      "    mean_inference_ms: 1.322914485105815\n",
      "    mean_raw_obs_processing_ms: 0.5786714382099872\n",
      "  time_since_restore: 1422.4736881256104\n",
      "  time_this_iter_s: 9.296891212463379\n",
      "  time_total_s: 1422.4736881256104\n",
      "  timers:\n",
      "    learn_throughput: 1758.509\n",
      "    learn_time_ms: 568.664\n",
      "    load_throughput: 311018.635\n",
      "    load_time_ms: 3.215\n",
      "    sample_throughput: 114.432\n",
      "    sample_time_ms: 8738.828\n",
      "    update_time_ms: 1.961\n",
      "  timestamp: 1631960774\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 133000\n",
      "  training_iteration: 133\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">         1422.47</td><td style=\"text-align: right;\">133000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 134000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-26-24\n",
      "  done: false\n",
      "  episode_len_mean: 996.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 134\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7755383398797777\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011291772685214991\n",
      "          policy_loss: -0.1165037399985724\n",
      "          total_loss: -0.12484854602565368\n",
      "          vf_explained_var: 0.16474750638008118\n",
      "          vf_loss: 0.003694115740816212\n",
      "    num_agent_steps_sampled: 134000\n",
      "    num_agent_steps_trained: 134000\n",
      "    num_steps_sampled: 134000\n",
      "    num_steps_trained: 134000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.68571428571429\n",
      "    ram_util_percent: 92.1\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03684528729514165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.505859967923396\n",
      "    mean_inference_ms: 1.3227037056415816\n",
      "    mean_raw_obs_processing_ms: 0.5787007641952984\n",
      "  time_since_restore: 1431.813720703125\n",
      "  time_this_iter_s: 9.340032577514648\n",
      "  time_total_s: 1431.813720703125\n",
      "  timers:\n",
      "    learn_throughput: 1756.651\n",
      "    learn_time_ms: 569.265\n",
      "    load_throughput: 310684.582\n",
      "    load_time_ms: 3.219\n",
      "    sample_throughput: 114.084\n",
      "    sample_time_ms: 8765.465\n",
      "    update_time_ms: 1.945\n",
      "  timestamp: 1631960784\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 134000\n",
      "  training_iteration: 134\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         1431.81</td><td style=\"text-align: right;\">134000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 135000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-26-33\n",
      "  done: false\n",
      "  episode_len_mean: 996.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 135\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2063065780533684\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013702047742756636\n",
      "          policy_loss: -0.06944526640905274\n",
      "          total_loss: -0.08092948959933387\n",
      "          vf_explained_var: 0.07246273756027222\n",
      "          vf_loss: 0.0036421808750472136\n",
      "    num_agent_steps_sampled: 135000\n",
      "    num_agent_steps_trained: 135000\n",
      "    num_steps_sampled: 135000\n",
      "    num_steps_trained: 135000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.98461538461538\n",
      "    ram_util_percent: 92.1846153846154\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03684118803736205\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.494750386590027\n",
      "    mean_inference_ms: 1.3224972773421737\n",
      "    mean_raw_obs_processing_ms: 0.5788309535088453\n",
      "  time_since_restore: 1441.1548783779144\n",
      "  time_this_iter_s: 9.341157674789429\n",
      "  time_total_s: 1441.1548783779144\n",
      "  timers:\n",
      "    learn_throughput: 1755.274\n",
      "    learn_time_ms: 569.712\n",
      "    load_throughput: 309471.929\n",
      "    load_time_ms: 3.231\n",
      "    sample_throughput: 113.936\n",
      "    sample_time_ms: 8776.83\n",
      "    update_time_ms: 1.943\n",
      "  timestamp: 1631960793\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 135000\n",
      "  training_iteration: 135\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         1441.15</td><td style=\"text-align: right;\">135000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-26-43\n",
      "  done: false\n",
      "  episode_len_mean: 996.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 136\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.61362627281083\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010932732894461773\n",
      "          policy_loss: -0.07514935069613987\n",
      "          total_loss: -0.08350292278660668\n",
      "          vf_explained_var: -0.1857258528470993\n",
      "          vf_loss: 0.002247992540166403\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.73571428571428\n",
      "    ram_util_percent: 92.28571428571426\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036837199404053195\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.483902811019597\n",
      "    mean_inference_ms: 1.3222979381139475\n",
      "    mean_raw_obs_processing_ms: 0.5790523444764323\n",
      "  time_since_restore: 1450.6728682518005\n",
      "  time_this_iter_s: 9.517989873886108\n",
      "  time_total_s: 1450.6728682518005\n",
      "  timers:\n",
      "    learn_throughput: 1752.598\n",
      "    learn_time_ms: 570.582\n",
      "    load_throughput: 309734.743\n",
      "    load_time_ms: 3.229\n",
      "    sample_throughput: 113.85\n",
      "    sample_time_ms: 8783.462\n",
      "    update_time_ms: 1.952\n",
      "  timestamp: 1631960803\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 136\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         1450.67</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 137000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-26-52\n",
      "  done: false\n",
      "  episode_len_mean: 996.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 137\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7988484170701768\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008471773568183988\n",
      "          policy_loss: -0.00816361726158195\n",
      "          total_loss: -0.020159576419326995\n",
      "          vf_explained_var: -0.4965408444404602\n",
      "          vf_loss: 0.0017036884150002153\n",
      "    num_agent_steps_sampled: 137000\n",
      "    num_agent_steps_trained: 137000\n",
      "    num_steps_sampled: 137000\n",
      "    num_steps_trained: 137000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.62307692307692\n",
      "    ram_util_percent: 92.29999999999998\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03683347193490363\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.473321566969316\n",
      "    mean_inference_ms: 1.3221067157226065\n",
      "    mean_raw_obs_processing_ms: 0.5793596305195656\n",
      "  time_since_restore: 1460.0218739509583\n",
      "  time_this_iter_s: 9.349005699157715\n",
      "  time_total_s: 1460.0218739509583\n",
      "  timers:\n",
      "    learn_throughput: 1750.14\n",
      "    learn_time_ms: 571.383\n",
      "    load_throughput: 309339.548\n",
      "    load_time_ms: 3.233\n",
      "    sample_throughput: 113.864\n",
      "    sample_time_ms: 8782.388\n",
      "    update_time_ms: 1.943\n",
      "  timestamp: 1631960812\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 137000\n",
      "  training_iteration: 137\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         1460.02</td><td style=\"text-align: right;\">137000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 138000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-27-01\n",
      "  done: false\n",
      "  episode_len_mean: 996.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 138\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1532381415367126\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015103910924184187\n",
      "          policy_loss: -0.054317477448946896\n",
      "          total_loss: -0.0659006884528531\n",
      "          vf_explained_var: -0.38337206840515137\n",
      "          vf_loss: 0.0023028162928918996\n",
      "    num_agent_steps_sampled: 138000\n",
      "    num_agent_steps_trained: 138000\n",
      "    num_steps_sampled: 138000\n",
      "    num_steps_trained: 138000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.76923076923077\n",
      "    ram_util_percent: 92.33846153846153\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03682995183753126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.46295599047287\n",
      "    mean_inference_ms: 1.3219221577574933\n",
      "    mean_raw_obs_processing_ms: 0.5797487079901564\n",
      "  time_since_restore: 1469.3618068695068\n",
      "  time_this_iter_s: 9.339932918548584\n",
      "  time_total_s: 1469.3618068695068\n",
      "  timers:\n",
      "    learn_throughput: 1751.968\n",
      "    learn_time_ms: 570.787\n",
      "    load_throughput: 311637.95\n",
      "    load_time_ms: 3.209\n",
      "    sample_throughput: 113.952\n",
      "    sample_time_ms: 8775.619\n",
      "    update_time_ms: 1.948\n",
      "  timestamp: 1631960821\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 138000\n",
      "  training_iteration: 138\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         1469.36</td><td style=\"text-align: right;\">138000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 139000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-27-11\n",
      "  done: false\n",
      "  episode_len_mean: 996.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 139\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.331081189049615\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00881206997867881\n",
      "          policy_loss: -0.06376511667751604\n",
      "          total_loss: -0.08093607914116648\n",
      "          vf_explained_var: -0.783125638961792\n",
      "          vf_loss: 0.0016787370033044782\n",
      "    num_agent_steps_sampled: 139000\n",
      "    num_agent_steps_trained: 139000\n",
      "    num_steps_sampled: 139000\n",
      "    num_steps_trained: 139000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.642857142857146\n",
      "    ram_util_percent: 92.39285714285715\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03682656040354389\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.452814360366467\n",
      "    mean_inference_ms: 1.3217434493701656\n",
      "    mean_raw_obs_processing_ms: 0.5802121444196928\n",
      "  time_since_restore: 1478.6354129314423\n",
      "  time_this_iter_s: 9.273606061935425\n",
      "  time_total_s: 1478.6354129314423\n",
      "  timers:\n",
      "    learn_throughput: 1747.499\n",
      "    learn_time_ms: 572.246\n",
      "    load_throughput: 311143.224\n",
      "    load_time_ms: 3.214\n",
      "    sample_throughput: 113.899\n",
      "    sample_time_ms: 8779.687\n",
      "    update_time_ms: 1.968\n",
      "  timestamp: 1631960831\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 139000\n",
      "  training_iteration: 139\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         1478.64</td><td style=\"text-align: right;\">139000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-27-20\n",
      "  done: false\n",
      "  episode_len_mean: 996.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 140\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2185221089257134\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010929747582145311\n",
      "          policy_loss: 0.04124467602620522\n",
      "          total_loss: 0.028519010978440444\n",
      "          vf_explained_var: -0.6179059147834778\n",
      "          vf_loss: 0.003926370946121299\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.323076923076925\n",
      "    ram_util_percent: 92.41538461538462\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036823379456541884\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.442885189354353\n",
      "    mean_inference_ms: 1.3215726829207493\n",
      "    mean_raw_obs_processing_ms: 0.5807407259816613\n",
      "  time_since_restore: 1488.158357143402\n",
      "  time_this_iter_s: 9.522944211959839\n",
      "  time_total_s: 1488.158357143402\n",
      "  timers:\n",
      "    learn_throughput: 1745.158\n",
      "    learn_time_ms: 573.014\n",
      "    load_throughput: 314297.789\n",
      "    load_time_ms: 3.182\n",
      "    sample_throughput: 113.658\n",
      "    sample_time_ms: 8798.31\n",
      "    update_time_ms: 2.023\n",
      "  timestamp: 1631960840\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 140\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">         1488.16</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 141000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-27-30\n",
      "  done: false\n",
      "  episode_len_mean: 996.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 141\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2524225976732044\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01411872601177697\n",
      "          policy_loss: 0.037227728962898256\n",
      "          total_loss: 0.0241760298402773\n",
      "          vf_explained_var: -0.3649364113807678\n",
      "          vf_loss: 0.0023249186303776997\n",
      "    num_agent_steps_sampled: 141000\n",
      "    num_agent_steps_trained: 141000\n",
      "    num_steps_sampled: 141000\n",
      "    num_steps_trained: 141000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.23571428571428\n",
      "    ram_util_percent: 92.52142857142857\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0368205511681342\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.43312996372162\n",
      "    mean_inference_ms: 1.321411437759517\n",
      "    mean_raw_obs_processing_ms: 0.5813347325779251\n",
      "  time_since_restore: 1497.8529884815216\n",
      "  time_this_iter_s: 9.694631338119507\n",
      "  time_total_s: 1497.8529884815216\n",
      "  timers:\n",
      "    learn_throughput: 1745.452\n",
      "    learn_time_ms: 572.918\n",
      "    load_throughput: 313529.53\n",
      "    load_time_ms: 3.189\n",
      "    sample_throughput: 113.316\n",
      "    sample_time_ms: 8824.879\n",
      "    update_time_ms: 2.01\n",
      "  timestamp: 1631960850\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 141000\n",
      "  training_iteration: 141\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         1497.85</td><td style=\"text-align: right;\">141000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-18 10:27:33,062\tWARNING tune.py:519 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2021-09-18 10:27:33,062\tWARNING tune.py:519 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2021-09-18 10:27:33,363\tWARNING tune.py:519 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_82e40_00000:\n",
      "  agent_timesteps_total: 142000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_10-27-40\n",
      "  done: false\n",
      "  episode_len_mean: 996.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 142\n",
      "  experiment_id: 7b9a32d98e74411ea92d43be64b53a03\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.053322588072883\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008812955232567631\n",
      "          policy_loss: 0.07002799674454663\n",
      "          total_loss: 0.054626470452381504\n",
      "          vf_explained_var: 0.33093151450157166\n",
      "          vf_loss: 0.0006701410867713599\n",
      "    num_agent_steps_sampled: 142000\n",
      "    num_agent_steps_trained: 142000\n",
      "    num_steps_sampled: 142000\n",
      "    num_steps_trained: 142000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.10714285714287\n",
      "    ram_util_percent: 92.59285714285714\n",
      "  pid: 35494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03681790947128805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.423538729622713\n",
      "    mean_inference_ms: 1.3212591426510831\n",
      "    mean_raw_obs_processing_ms: 0.5819895721991988\n",
      "  time_since_restore: 1507.509423494339\n",
      "  time_this_iter_s: 9.656435012817383\n",
      "  time_total_s: 1507.509423494339\n",
      "  timers:\n",
      "    learn_throughput: 1745.471\n",
      "    learn_time_ms: 572.911\n",
      "    load_throughput: 311094.761\n",
      "    load_time_ms: 3.214\n",
      "    sample_throughput: 112.967\n",
      "    sample_time_ms: 8852.145\n",
      "    update_time_ms: 2.016\n",
      "  timestamp: 1631960860\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 142000\n",
      "  training_iteration: 142\n",
      "  trial_id: 82e40_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         1507.51</td><td style=\"text-align: right;\">142000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.3 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_10-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_82e40_00000</td><td>RUNNING </td><td>192.168.3.5:35494</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         1507.51</td><td style=\"text-align: right;\">142000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m 2021-09-18 10:27:40,094\tERROR worker.py:428 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   File \"python/ray/_raylet.pyx\", line 525, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   File \"python/ray/_raylet.pyx\", line 532, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   File \"python/ray/_raylet.pyx\", line 536, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 346, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 744, in sample\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m     item = next(self.rollout_provider)\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 651, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m     base_env.send_actions(actions_to_send)\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/env/base_env.py\", line 365, in send_actions\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m     self.vector_env.vector_step(action_vector)\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/env/vector_env.py\", line 173, in vector_step\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m     obs, r, done, info = self.envs[i].step(actions[i])\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/core.py\", line 292, in step\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m     return self.env.step(self.action(action))\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/core.py\", line 268, in step\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/wrappers/time_limit.py\", line 16, in step\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/iglu/env.py\", line 135, in step\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m     obs, reward, done, info = super().step(action)\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/minerl_patched-0.1.0-py3.7-linux-x86_64.egg/minerl_patched/env/_singleagent.py\", line 32, in step\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m     obs, rew, done, info = super().step(multi_agent_action)\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/minerl_patched-0.1.0-py3.7-linux-x86_64.egg/minerl_patched/env/_multiagent.py\", line 294, in step\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m     obs = comms.recv_message(instance.client_socket)\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/minerl_patched-0.1.0-py3.7-linux-x86_64.egg/minerl_patched/env/comms.py\", line 63, in recv_message\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m     lengthbuf = recvall(sock, 4)\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/minerl_patched-0.1.0-py3.7-linux-x86_64.egg/minerl_patched/env/comms.py\", line 73, in recvall\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m     newbuf = sock.recv(count)\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m   File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/worker.py\", line 425, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=35495)\u001b[0m SystemExit: 1\n",
      "2021-09-18 10:27:40,261\tERROR tune.py:557 -- Trials did not complete: [PPO_my_env_82e40_00000]\n",
      "2021-09-18 10:27:40,261\tINFO tune.py:561 -- Total run time: 1520.45 seconds (1520.14 seconds for the tuning loop).\n",
      "2021-09-18 10:27:40,261\tWARNING tune.py:566 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 35710<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WandbLoggerCallback.__del__ at 0x7f4ec1a691e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/tune/integration/wandb.py\", line 378, in __del__\n",
      "    for trial in self._trial_processes:\n",
      "RuntimeError: dictionary changed size during iteration\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f4ec1d8de10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/IGLU-Minecraft/wandb/run-20210918_100220-82e40_00000/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/IGLU-Minecraft/wandb/run-20210918_100220-82e40_00000/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>agent_timesteps_total</td><td>142000</td></tr><tr><td>episode_len_mean</td><td>996.03</td></tr><tr><td>episode_reward_max</td><td>3.0</td></tr><tr><td>episode_reward_mean</td><td>-0.1</td></tr><tr><td>episode_reward_min</td><td>-7.0</td></tr><tr><td>episodes_this_iter</td><td>1</td></tr><tr><td>episodes_total</td><td>142</td></tr><tr><td>info/learner/default_policy/learner_stats/cur_kl_coeff</td><td>0.50625</td></tr><tr><td>info/learner/default_policy/learner_stats/cur_lr</td><td>5e-05</td></tr><tr><td>info/learner/default_policy/learner_stats/entropy</td><td>2.05332</td></tr><tr><td>info/learner/default_policy/learner_stats/entropy_coeff</td><td>0.01</td></tr><tr><td>info/learner/default_policy/learner_stats/kl</td><td>0.00881</td></tr><tr><td>info/learner/default_policy/learner_stats/policy_loss</td><td>0.07003</td></tr><tr><td>info/learner/default_policy/learner_stats/total_loss</td><td>0.05463</td></tr><tr><td>info/learner/default_policy/learner_stats/vf_explained_var</td><td>0.33093</td></tr><tr><td>info/learner/default_policy/learner_stats/vf_loss</td><td>0.00067</td></tr><tr><td>info/num_agent_steps_sampled</td><td>142000</td></tr><tr><td>info/num_agent_steps_trained</td><td>142000</td></tr><tr><td>info/num_steps_sampled</td><td>142000</td></tr><tr><td>info/num_steps_trained</td><td>142000</td></tr><tr><td>iterations_since_restore</td><td>142</td></tr><tr><td>num_healthy_workers</td><td>1</td></tr><tr><td>perf/cpu_util_percent</td><td>42.10714</td></tr><tr><td>perf/ram_util_percent</td><td>92.59286</td></tr><tr><td>sampler_perf/mean_action_processing_ms</td><td>0.03682</td></tr><tr><td>sampler_perf/mean_env_render_ms</td><td>0.0</td></tr><tr><td>sampler_perf/mean_env_wait_ms</td><td>8.42354</td></tr><tr><td>sampler_perf/mean_inference_ms</td><td>1.32126</td></tr><tr><td>sampler_perf/mean_raw_obs_processing_ms</td><td>0.58199</td></tr><tr><td>time_since_restore</td><td>1507.50942</td></tr><tr><td>time_this_iter_s</td><td>9.65644</td></tr><tr><td>time_total_s</td><td>1507.50942</td></tr><tr><td>timers/learn_throughput</td><td>1745.471</td></tr><tr><td>timers/learn_time_ms</td><td>572.911</td></tr><tr><td>timers/load_throughput</td><td>311094.761</td></tr><tr><td>timers/load_time_ms</td><td>3.214</td></tr><tr><td>timers/sample_throughput</td><td>112.967</td></tr><tr><td>timers/sample_time_ms</td><td>8852.145</td></tr><tr><td>timers/update_time_ms</td><td>2.016</td></tr><tr><td>timestamp</td><td>1631960860</td></tr><tr><td>timesteps_since_restore</td><td>0</td></tr><tr><td>timesteps_total</td><td>142000</td></tr><tr><td>training_iteration</td><td>142</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>agent_timesteps_total</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>episode_len_mean</td><td>████████▃▃▄▄▄▅▅▅▅▃▃▃▃▄▄▄▄▂▃▃▃▃▃▃▃▁▁▁▃▃▃▃</td></tr><tr><td>episode_reward_max</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁██████████████████████████</td></tr><tr><td>episode_reward_mean</td><td>▁▅▆▇▇▇▇█████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>episode_reward_min</td><td>████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>episodes_this_iter</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>episodes_total</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>info/learner/default_policy/learner_stats/cur_kl_coeff</td><td>▂▂▂▂▂▂▄▁▁▁▁▁▁▁▁▁▁▂▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅████</td></tr><tr><td>info/learner/default_policy/learner_stats/cur_lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>info/learner/default_policy/learner_stats/entropy</td><td>▁▄▆▅▆▆█▆▇▇▆▆▇▇▇▇▆▇▆▃▇▇█▅▆▆▇▇▆▆▅▆▆▃▄▄▄▃▆▅</td></tr><tr><td>info/learner/default_policy/learner_stats/entropy_coeff</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>info/learner/default_policy/learner_stats/kl</td><td>▂▃▆▁▃▂▂▆▂▂▂▄▅▄▁▃▁▄▃▄▃▆▃▃▄▅█▂▄▄▆▄▄▄▄▅▄▃▆▂</td></tr><tr><td>info/learner/default_policy/learner_stats/policy_loss</td><td>▅█▆▅▅▄▇▂▇▄▅▆▆▅▅▅▁▃▄▆▆▅▅▅▅▅▄▅▅▅▄▆▆▅▆▇▅▄▅▇</td></tr><tr><td>info/learner/default_policy/learner_stats/total_loss</td><td>▅█▆▅▅▄▆▂▇▄▅▆▅▅▅▅▁▄▄▆▆▅▅▅▅▅▅▅▅▆▄▆▆▅▆▇▅▄▅▇</td></tr><tr><td>info/learner/default_policy/learner_stats/vf_explained_var</td><td>▆▆▅▁▂▇▅▅▁▂▃▁▁▁▁▁▃▆▅▂▁▄▁▁▃▅▆▃▂▇▇▂▄▄▃▅█▆▄▇</td></tr><tr><td>info/learner/default_policy/learner_stats/vf_loss</td><td>█▅▃▂▃▇▂▂▁▁▂▁▁▃▂▃▂▇▃▂▃▂▂▂▂▄▇▂▃█▃▂▁▂▃▂▃▃▂▁</td></tr><tr><td>info/num_agent_steps_sampled</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>info/num_agent_steps_trained</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>info/num_steps_sampled</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>info/num_steps_trained</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iterations_since_restore</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>num_healthy_workers</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>perf/cpu_util_percent</td><td>█▆▇▇▇▇▇▆▄▆▆▆▆▆▆▆▆▆▇▆▆▆▆▆▆▂▆▆▆▇▇▆▆▁▆▆▆▆▆▆</td></tr><tr><td>perf/ram_util_percent</td><td>▁▇██████▆▇▇▆▇▇▇▇▇█▇▆▆▇▇▇▇▇▆▆▆▆▆▆▆▇▆▆▆▆▆▆</td></tr><tr><td>sampler_perf/mean_action_processing_ms</td><td>█▇▆▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sampler_perf/mean_env_render_ms</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sampler_perf/mean_env_wait_ms</td><td>█▆▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sampler_perf/mean_inference_ms</td><td>█▇▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sampler_perf/mean_raw_obs_processing_ms</td><td>▁▁▁▁▁▁▁▁▁▂▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇██████</td></tr><tr><td>time_since_restore</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time_this_iter_s</td><td>▂▃▁▂▂▁▁▁█▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁█▁▁▁▁▁▁</td></tr><tr><td>time_total_s</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>timers/learn_throughput</td><td>▁▃▄▆▇▇▆▅▄▅▅▇██▇▇▇▇▇▇████▇▆▅▅▇▅▄▅▇█▇▇▇██▇</td></tr><tr><td>timers/learn_time_ms</td><td>█▆▄▃▂▂▃▄▅▄▄▂▁▁▁▂▂▂▂▂▁▁▁▁▂▃▄▄▂▄▄▄▂▁▂▂▂▁▁▂</td></tr><tr><td>timers/load_throughput</td><td>▁▂▄▇████▅▅▅██████▅▅██████▆▅▅█▇▇▇▇▅▅▅████</td></tr><tr><td>timers/load_time_ms</td><td>█▅▃▁▁▁▁▁▂▂▂▁▁▁▁▁▁▂▂▁▁▁▁▁▁▂▂▂▁▁▁▂▁▂▂▂▁▁▁▁</td></tr><tr><td>timers/sample_throughput</td><td>▁▂▄██▇██▆▆▆▇▇▇▇▇▇▆▆██████▆▆▆█████▆▆▆████</td></tr><tr><td>timers/sample_time_ms</td><td>█▅▃▁▁▁▁▁▂▂▂▁▁▁▁▁▁▂▂▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>timers/update_time_ms</td><td>▂▁▁▁▁▁▁▁▁▁▁▁▁▁███▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>timesteps_since_restore</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>timesteps_total</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">PPO C17 pretrained</strong>: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/82e40_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/82e40_00000</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 1,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 1000,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO C17 pretrained\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
