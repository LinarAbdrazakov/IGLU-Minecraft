{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef090c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install ray torch torchvision tabulate tensorboard\n",
    "#!pip3 install 'ray[rllib]'\n",
    "#!pip3 install ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "from models import VisualEncoder\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        features_dim = 512\n",
    "        self.encoder = VisualEncoder(features_dim)\n",
    "        self.encoder.load_state_dict(\n",
    "            torch.load(\"/src/Visual Autoencoder weights and models/IGLU_encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.action_head = nn.Linear(features_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(features_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.encoder.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        if self.use_cuda:\n",
    "            obs.cuda()\n",
    "            \n",
    "        features = self.encoder(obs)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "    env.update_taskset(TaskSet(preset=['C17']))\n",
    "    env = PovOnlyWrapper(env)\n",
    "    env = IgluActionWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 63.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-17 11:31:46,757\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-09-17 11:31:46,811\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[2m\u001b[36m(pid=333)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=333)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=333)\u001b[0m 2021-09-17 11:31:51,791\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=333)\u001b[0m 2021-09-17 11:31:51,791\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO C17 pretrained</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/d74d3_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/d74d3_00000</a><br/>\n",
       "                Run data is saved locally in <code>/src/wandb/run-20210917_113147-d74d3_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=333)\u001b[0m 2021-09-17 11:32:04,937\tINFO trainable.py:109 -- Trainable.setup took 16.441 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=333)\u001b[0m 2021-09-17 11:32:04,939\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-37-32\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8325520820087857\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.022896651729185865\n",
      "          policy_loss: 0.14715835692154036\n",
      "          total_loss: 0.1939159910298056\n",
      "          vf_explained_var: 0.15425756573677063\n",
      "          vf_loss: 0.06050382106461459\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.1254817987152\n",
      "    ram_util_percent: 52.7627408993576\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06560274175592475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 319.6864983180424\n",
      "    mean_inference_ms: 2.4936177751996538\n",
      "    mean_raw_obs_processing_ms: 0.3428630657367535\n",
      "  time_since_restore: 327.13253259658813\n",
      "  time_this_iter_s: 327.13253259658813\n",
      "  time_total_s: 327.13253259658813\n",
      "  timers:\n",
      "    learn_throughput: 310.746\n",
      "    learn_time_ms: 3218.061\n",
      "    load_throughput: 18299.516\n",
      "    load_time_ms: 54.646\n",
      "    sample_throughput: 3.088\n",
      "    sample_time_ms: 323839.151\n",
      "    update_time_ms: 11.294\n",
      "  timestamp: 1631878652\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         327.133</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-37-52\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -1.5\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8210402091344198\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.026401506742062757\n",
      "          policy_loss: -0.010755915939807893\n",
      "          total_loss: 0.13765250378184848\n",
      "          vf_explained_var: 0.6818528771400452\n",
      "          vf_loss: 0.15869836426443523\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.47000000000001\n",
      "    ram_util_percent: 54.37666666666666\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0652365445112385\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 243.5012209624757\n",
      "    mean_inference_ms: 2.462197458725641\n",
      "    mean_raw_obs_processing_ms: 0.31514908468359465\n",
      "  time_since_restore: 347.9575502872467\n",
      "  time_this_iter_s: 20.82501769065857\n",
      "  time_total_s: 347.9575502872467\n",
      "  timers:\n",
      "    learn_throughput: 311.252\n",
      "    learn_time_ms: 3212.834\n",
      "    load_throughput: 19255.826\n",
      "    load_time_ms: 51.932\n",
      "    sample_throughput: 5.858\n",
      "    sample_time_ms: 170697.049\n",
      "    update_time_ms: 8.724\n",
      "  timestamp: 1631878672\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         347.958</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">    -1.5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-38-16\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -3.0\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 3\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.256585693359375\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020107144171431782\n",
      "          policy_loss: 0.2360995369652907\n",
      "          total_loss: 0.33587057946456805\n",
      "          vf_explained_var: 0.3537977933883667\n",
      "          vf_loss: 0.11328868969447083\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.71515151515153\n",
      "    ram_util_percent: 54.44848484848485\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.064936864283041\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 201.4025708302291\n",
      "    mean_inference_ms: 2.4489187076421097\n",
      "    mean_raw_obs_processing_ms: 0.2999576123767731\n",
      "  time_since_restore: 371.1273512840271\n",
      "  time_this_iter_s: 23.169800996780396\n",
      "  time_total_s: 371.1273512840271\n",
      "  timers:\n",
      "    learn_throughput: 310.246\n",
      "    learn_time_ms: 3223.248\n",
      "    load_throughput: 20086.733\n",
      "    load_time_ms: 49.784\n",
      "    sample_throughput: 8.304\n",
      "    sample_time_ms: 120419.589\n",
      "    update_time_ms: 8.203\n",
      "  timestamp: 1631878696\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         371.127</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">      -3</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-38-37\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -2.25\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 4\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.877338202794393\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007898074699230845\n",
      "          policy_loss: 0.16588142895036273\n",
      "          total_loss: 0.18150627945239345\n",
      "          vf_explained_var: -0.125324085354805\n",
      "          vf_loss: 0.029067032629003127\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.37333333333332\n",
      "    ram_util_percent: 54.516666666666666\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06468560167931256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 173.96567203836315\n",
      "    mean_inference_ms: 2.436633720576882\n",
      "    mean_raw_obs_processing_ms: 0.2910161295658833\n",
      "  time_since_restore: 392.14394998550415\n",
      "  time_this_iter_s: 21.01659870147705\n",
      "  time_total_s: 392.14394998550415\n",
      "  timers:\n",
      "    learn_throughput: 310.131\n",
      "    learn_time_ms: 3224.447\n",
      "    load_throughput: 19938.696\n",
      "    load_time_ms: 50.154\n",
      "    sample_throughput: 10.555\n",
      "    sample_time_ms: 94744.491\n",
      "    update_time_ms: 8.643\n",
      "  timestamp: 1631878717\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         392.144</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">   -2.25</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-38-59\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -1.8\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 5\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1206712047259013\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007705085549517479\n",
      "          policy_loss: 0.17832471330960592\n",
      "          total_loss: 0.1922963089413113\n",
      "          vf_explained_var: 0.2709288001060486\n",
      "          vf_loss: 0.029977375594899058\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 5000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.340625\n",
      "    ram_util_percent: 54.515625\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06451864955407062\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 154.48633895085555\n",
      "    mean_inference_ms: 2.4253183320364604\n",
      "    mean_raw_obs_processing_ms: 0.28463957254439215\n",
      "  time_since_restore: 414.33223962783813\n",
      "  time_this_iter_s: 22.188289642333984\n",
      "  time_total_s: 414.33223962783813\n",
      "  timers:\n",
      "    learn_throughput: 309.713\n",
      "    learn_time_ms: 3228.795\n",
      "    load_throughput: 20115.138\n",
      "    load_time_ms: 49.714\n",
      "    sample_throughput: 12.567\n",
      "    sample_time_ms: 79571.416\n",
      "    update_time_ms: 8.167\n",
      "  timestamp: 1631878739\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         414.332</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">    -1.8</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-39-21\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -1.5\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 6\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5382269514931575\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00495691013425152\n",
      "          policy_loss: 0.1046609356171555\n",
      "          total_loss: 0.08784559451871449\n",
      "          vf_explained_var: -0.7114735245704651\n",
      "          vf_loss: 0.005221012464931442\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.50967741935483\n",
      "    ram_util_percent: 54.61935483870969\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06429623155778877\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 139.8077934134804\n",
      "    mean_inference_ms: 2.417021040954863\n",
      "    mean_raw_obs_processing_ms: 0.27890137481698346\n",
      "  time_since_restore: 436.1211564540863\n",
      "  time_this_iter_s: 21.78891682624817\n",
      "  time_total_s: 436.1211564540863\n",
      "  timers:\n",
      "    learn_throughput: 306.972\n",
      "    learn_time_ms: 3257.626\n",
      "    load_throughput: 20600.589\n",
      "    load_time_ms: 48.542\n",
      "    sample_throughput: 14.417\n",
      "    sample_time_ms: 69364.401\n",
      "    update_time_ms: 8.041\n",
      "  timestamp: 1631878761\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         436.121</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">    -1.5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 7000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-39-43\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -1.2857142857142858\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 7\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1081003692415026\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012554594395871593\n",
      "          policy_loss: 0.15321244191792277\n",
      "          total_loss: 0.14868131623499922\n",
      "          vf_explained_var: 0.08455053716897964\n",
      "          vf_loss: 0.012312703746526191\n",
      "    num_agent_steps_sampled: 7000\n",
      "    num_agent_steps_trained: 7000\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 7000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.47741935483872\n",
      "    ram_util_percent: 54.48709677419355\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06424307051288732\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 128.2889102604958\n",
      "    mean_inference_ms: 2.4106019204584794\n",
      "    mean_raw_obs_processing_ms: 0.27436459204316926\n",
      "  time_since_restore: 457.8520441055298\n",
      "  time_this_iter_s: 21.73088765144348\n",
      "  time_total_s: 457.8520441055298\n",
      "  timers:\n",
      "    learn_throughput: 308.221\n",
      "    learn_time_ms: 3244.422\n",
      "    load_throughput: 20322.337\n",
      "    load_time_ms: 49.207\n",
      "    sample_throughput: 16.104\n",
      "    sample_time_ms: 62097.342\n",
      "    update_time_ms: 7.965\n",
      "  timestamp: 1631878783\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         457.852</td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\">-1.28571</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-40-05\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -1.125\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 8\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.654929945203993\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009870582974131764\n",
      "          policy_loss: -0.15086126493083107\n",
      "          total_loss: -0.16899353398217096\n",
      "          vf_explained_var: -0.3211252987384796\n",
      "          vf_loss: 0.005085709155213812\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.90645161290323\n",
      "    ram_util_percent: 54.509677419354844\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06413267342863609\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 118.97319768959244\n",
      "    mean_inference_ms: 2.404470352372755\n",
      "    mean_raw_obs_processing_ms: 0.2703739906188105\n",
      "  time_since_restore: 479.7485864162445\n",
      "  time_this_iter_s: 21.89654231071472\n",
      "  time_total_s: 479.7485864162445\n",
      "  timers:\n",
      "    learn_throughput: 307.28\n",
      "    learn_time_ms: 3254.362\n",
      "    load_throughput: 20223.003\n",
      "    load_time_ms: 49.449\n",
      "    sample_throughput: 17.653\n",
      "    sample_time_ms: 56647.613\n",
      "    update_time_ms: 8.221\n",
      "  timestamp: 1631878805\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         479.749</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">  -1.125</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-40-27\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -1.0\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 9\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4034628338283963\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010819106921657844\n",
      "          policy_loss: 0.24843704075449044\n",
      "          total_loss: 0.23871621468828785\n",
      "          vf_explained_var: 0.05732899159193039\n",
      "          vf_loss: 0.010662354267616239\n",
      "    num_agent_steps_sampled: 9000\n",
      "    num_agent_steps_trained: 9000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.1\n",
      "    ram_util_percent: 54.55\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06406589906767096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 111.26066688788391\n",
      "    mean_inference_ms: 2.3992315430440496\n",
      "    mean_raw_obs_processing_ms: 0.2671714094178752\n",
      "  time_since_restore: 501.7281770706177\n",
      "  time_this_iter_s: 21.97959065437317\n",
      "  time_total_s: 501.7281770706177\n",
      "  timers:\n",
      "    learn_throughput: 306.999\n",
      "    learn_time_ms: 3257.335\n",
      "    load_throughput: 20197.69\n",
      "    load_time_ms: 49.511\n",
      "    sample_throughput: 19.075\n",
      "    sample_time_ms: 52423.997\n",
      "    update_time_ms: 8.068\n",
      "  timestamp: 1631878827\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         501.728</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">      -1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-40-49\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.9\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 10\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.563137952486674\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008054958621834526\n",
      "          policy_loss: 0.24835951179265975\n",
      "          total_loss: 0.2339441145459811\n",
      "          vf_explained_var: -0.6284920573234558\n",
      "          vf_loss: 0.008497433804182542\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.36129032258063\n",
      "    ram_util_percent: 54.60645161290322\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06399036302908882\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 104.75419252538916\n",
      "    mean_inference_ms: 2.3951423302173227\n",
      "    mean_raw_obs_processing_ms: 0.2649303183762856\n",
      "  time_since_restore: 523.8473618030548\n",
      "  time_this_iter_s: 22.119184732437134\n",
      "  time_total_s: 523.8473618030548\n",
      "  timers:\n",
      "    learn_throughput: 306.109\n",
      "    learn_time_ms: 3266.809\n",
      "    load_throughput: 20296.637\n",
      "    load_time_ms: 49.269\n",
      "    sample_throughput: 20.387\n",
      "    sample_time_ms: 49051.218\n",
      "    update_time_ms: 7.997\n",
      "  timestamp: 1631878849\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         523.847</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">    -0.9</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 11000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-41-12\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.8181818181818182\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 11\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.531078415446811\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012065643239943425\n",
      "          policy_loss: 0.15149310115310882\n",
      "          total_loss: 0.13705964783827465\n",
      "          vf_explained_var: -0.8338345289230347\n",
      "          vf_loss: 0.006805175863620307\n",
      "    num_agent_steps_sampled: 11000\n",
      "    num_agent_steps_trained: 11000\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 11000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.41764705882353\n",
      "    ram_util_percent: 54.67352941176471\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06390457011609246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 99.19159070638786\n",
      "    mean_inference_ms: 2.391393726872074\n",
      "    mean_raw_obs_processing_ms: 0.2628441436267689\n",
      "  time_since_restore: 547.2409060001373\n",
      "  time_this_iter_s: 23.39354419708252\n",
      "  time_total_s: 547.2409060001373\n",
      "  timers:\n",
      "    learn_throughput: 304.528\n",
      "    learn_time_ms: 3283.775\n",
      "    load_throughput: 20326.391\n",
      "    load_time_ms: 49.197\n",
      "    sample_throughput: 53.594\n",
      "    sample_time_ms: 18658.931\n",
      "    update_time_ms: 8.666\n",
      "  timestamp: 1631878872\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 11\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         547.241</td><td style=\"text-align: right;\">11000</td><td style=\"text-align: right;\">-0.818182</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-41-37\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.75\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 12\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5324205372068618\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009273919454826427\n",
      "          policy_loss: 0.15472963381972576\n",
      "          total_loss: 0.13739754921860164\n",
      "          vf_explained_var: -0.4567834138870239\n",
      "          vf_loss: 0.0048621762259345915\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.39428571428572\n",
      "    ram_util_percent: 54.68285714285714\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06381283567576083\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 94.38659429489935\n",
      "    mean_inference_ms: 2.3878843976254545\n",
      "    mean_raw_obs_processing_ms: 0.26088198089825526\n",
      "  time_since_restore: 572.2382123470306\n",
      "  time_this_iter_s: 24.99730634689331\n",
      "  time_total_s: 572.2382123470306\n",
      "  timers:\n",
      "    learn_throughput: 305.211\n",
      "    learn_time_ms: 3276.427\n",
      "    load_throughput: 20388.382\n",
      "    load_time_ms: 49.048\n",
      "    sample_throughput: 52.401\n",
      "    sample_time_ms: 19083.681\n",
      "    update_time_ms: 8.73\n",
      "  timestamp: 1631878897\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         572.238</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   -0.75</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 13000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-42-02\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6923076923076923\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 13\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4626607656478883\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007345461035274988\n",
      "          policy_loss: 0.1729673508140776\n",
      "          total_loss: 0.15344630868898498\n",
      "          vf_explained_var: -0.36099565029144287\n",
      "          vf_loss: 0.002626472667583989\n",
      "    num_agent_steps_sampled: 13000\n",
      "    num_agent_steps_trained: 13000\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 13000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.43055555555557\n",
      "    ram_util_percent: 54.56388888888889\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06372085371730878\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 90.18678668553164\n",
      "    mean_inference_ms: 2.3846682435781634\n",
      "    mean_raw_obs_processing_ms: 0.25912651637251793\n",
      "  time_since_restore: 597.146115064621\n",
      "  time_this_iter_s: 24.907902717590332\n",
      "  time_total_s: 597.146115064621\n",
      "  timers:\n",
      "    learn_throughput: 304.819\n",
      "    learn_time_ms: 3280.632\n",
      "    load_throughput: 20610.142\n",
      "    load_time_ms: 48.52\n",
      "    sample_throughput: 51.937\n",
      "    sample_time_ms: 19253.936\n",
      "    update_time_ms: 8.719\n",
      "  timestamp: 1631878922\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 13\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         597.146</td><td style=\"text-align: right;\">13000</td><td style=\"text-align: right;\">-0.692308</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 14000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-42-23\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6428571428571429\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 14\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0582600394884745\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010520209682918492\n",
      "          policy_loss: 0.14467662258280647\n",
      "          total_loss: 0.13108939511908424\n",
      "          vf_explained_var: -0.006725592073053122\n",
      "          vf_loss: 0.0034448037447873505\n",
      "    num_agent_steps_sampled: 14000\n",
      "    num_agent_steps_trained: 14000\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.93793103448277\n",
      "    ram_util_percent: 54.665517241379305\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06364543963494178\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 86.45756168406855\n",
      "    mean_inference_ms: 2.381916451400798\n",
      "    mean_raw_obs_processing_ms: 0.2576487779969626\n",
      "  time_since_restore: 617.6486186981201\n",
      "  time_this_iter_s: 20.502503633499146\n",
      "  time_total_s: 617.6486186981201\n",
      "  timers:\n",
      "    learn_throughput: 304.465\n",
      "    learn_time_ms: 3284.452\n",
      "    load_throughput: 20276.198\n",
      "    load_time_ms: 49.319\n",
      "    sample_throughput: 52.089\n",
      "    sample_time_ms: 19198.027\n",
      "    update_time_ms: 8.181\n",
      "  timestamp: 1631878943\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 14\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         617.649</td><td style=\"text-align: right;\">14000</td><td style=\"text-align: right;\">-0.642857</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-42-43\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 15\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6969514264000787\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012664542759675889\n",
      "          policy_loss: -0.15576508504649003\n",
      "          total_loss: -0.17568847772975763\n",
      "          vf_explained_var: -0.08843613415956497\n",
      "          vf_loss: 0.002771834875198288\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_agent_steps_trained: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.68620689655172\n",
      "    ram_util_percent: 54.765517241379314\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06357135702927634\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 83.12086539146291\n",
      "    mean_inference_ms: 2.379360953701136\n",
      "    mean_raw_obs_processing_ms: 0.2563104434272582\n",
      "  time_since_restore: 638.0416748523712\n",
      "  time_this_iter_s: 20.3930561542511\n",
      "  time_total_s: 638.0416748523712\n",
      "  timers:\n",
      "    learn_throughput: 304.635\n",
      "    learn_time_ms: 3282.612\n",
      "    load_throughput: 20221.963\n",
      "    load_time_ms: 49.451\n",
      "    sample_throughput: 52.576\n",
      "    sample_time_ms: 19020.192\n",
      "    update_time_ms: 8.153\n",
      "  timestamp: 1631878963\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         638.042</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">    -0.6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-43-02\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5625\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 16\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6887780666351317\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01096702016662355\n",
      "          policy_loss: -0.10591367665264341\n",
      "          total_loss: -0.1280851234992345\n",
      "          vf_explained_var: -0.13516156375408173\n",
      "          vf_loss: 0.001014961622422561\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.92962962962964\n",
      "    ram_util_percent: 54.7\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0634935767201226\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 80.10879588607186\n",
      "    mean_inference_ms: 2.376761773971311\n",
      "    mean_raw_obs_processing_ms: 0.25501305737372454\n",
      "  time_since_restore: 656.9376244544983\n",
      "  time_this_iter_s: 18.895949602127075\n",
      "  time_total_s: 656.9376244544983\n",
      "  timers:\n",
      "    learn_throughput: 305.045\n",
      "    learn_time_ms: 3278.202\n",
      "    load_throughput: 20125.861\n",
      "    load_time_ms: 49.687\n",
      "    sample_throughput: 53.376\n",
      "    sample_time_ms: 18735.159\n",
      "    update_time_ms: 8.163\n",
      "  timestamp: 1631878982\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 16\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         656.938</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\"> -0.5625</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 17000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-43-22\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5294117647058824\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 17\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6258899688720705\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014774509683753273\n",
      "          policy_loss: -0.13556641543077097\n",
      "          total_loss: -0.15573381218645307\n",
      "          vf_explained_var: -0.100551538169384\n",
      "          vf_loss: 0.0011051046196371317\n",
      "    num_agent_steps_sampled: 17000\n",
      "    num_agent_steps_trained: 17000\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 17000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.80689655172412\n",
      "    ram_util_percent: 54.706896551724135\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0634160387491873\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 77.37804710895747\n",
      "    mean_inference_ms: 2.374214473328756\n",
      "    mean_raw_obs_processing_ms: 0.25385960542313263\n",
      "  time_since_restore: 676.8519229888916\n",
      "  time_this_iter_s: 19.91429853439331\n",
      "  time_total_s: 676.8519229888916\n",
      "  timers:\n",
      "    learn_throughput: 302.73\n",
      "    learn_time_ms: 3303.278\n",
      "    load_throughput: 20271.573\n",
      "    load_time_ms: 49.33\n",
      "    sample_throughput: 53.969\n",
      "    sample_time_ms: 18529.321\n",
      "    update_time_ms: 7.989\n",
      "  timestamp: 1631879002\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 17\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         676.852</td><td style=\"text-align: right;\">17000</td><td style=\"text-align: right;\">-0.529412</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-43-40\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 18\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.660278063350254\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010121407704418179\n",
      "          policy_loss: -0.0747269931766722\n",
      "          total_loss: -0.0967298680709468\n",
      "          vf_explained_var: -0.2099701166152954\n",
      "          vf_loss: 0.0011839313296756397\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.06923076923077\n",
      "    ram_util_percent: 54.7\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06334364108582793\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 74.88388440662008\n",
      "    mean_inference_ms: 2.3716117273140354\n",
      "    mean_raw_obs_processing_ms: 0.25274273072278497\n",
      "  time_since_restore: 695.0356409549713\n",
      "  time_this_iter_s: 18.183717966079712\n",
      "  time_total_s: 695.0356409549713\n",
      "  timers:\n",
      "    learn_throughput: 300.801\n",
      "    learn_time_ms: 3324.452\n",
      "    load_throughput: 20546.757\n",
      "    load_time_ms: 48.669\n",
      "    sample_throughput: 55.131\n",
      "    sample_time_ms: 18138.591\n",
      "    update_time_ms: 7.527\n",
      "  timestamp: 1631879020\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         695.036</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">    -0.5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 19000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-44-01\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.47368421052631576\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 19\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6568399164411756\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010562322556422598\n",
      "          policy_loss: -0.12038397652407487\n",
      "          total_loss: -0.14271839052024815\n",
      "          vf_explained_var: -0.5177922248840332\n",
      "          vf_loss: 0.0006691975046932284\n",
      "    num_agent_steps_sampled: 19000\n",
      "    num_agent_steps_trained: 19000\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 19000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.5\n",
      "    ram_util_percent: 54.58965517241379\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06328325175571793\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 72.601989790356\n",
      "    mean_inference_ms: 2.3691138100956963\n",
      "    mean_raw_obs_processing_ms: 0.25167650553825166\n",
      "  time_since_restore: 715.4572961330414\n",
      "  time_this_iter_s: 20.42165517807007\n",
      "  time_total_s: 715.4572961330414\n",
      "  timers:\n",
      "    learn_throughput: 299.619\n",
      "    learn_time_ms: 3337.568\n",
      "    load_throughput: 20557.905\n",
      "    load_time_ms: 48.643\n",
      "    sample_throughput: 55.65\n",
      "    sample_time_ms: 17969.53\n",
      "    update_time_ms: 7.485\n",
      "  timestamp: 1631879041\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 19\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         715.457</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\">-0.473684</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-44-22\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.45\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 20\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.623726153373718\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011514101517073409\n",
      "          policy_loss: -0.08947496025098695\n",
      "          total_loss: -0.11106823401318656\n",
      "          vf_explained_var: -0.48777228593826294\n",
      "          vf_loss: 0.0007579742650300937\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.51290322580645\n",
      "    ram_util_percent: 54.61935483870969\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06324235884577963\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 70.5039682963666\n",
      "    mean_inference_ms: 2.366903379019453\n",
      "    mean_raw_obs_processing_ms: 0.250747983669417\n",
      "  time_since_restore: 736.9189972877502\n",
      "  time_this_iter_s: 21.461701154708862\n",
      "  time_total_s: 736.9189972877502\n",
      "  timers:\n",
      "    learn_throughput: 298.296\n",
      "    learn_time_ms: 3352.371\n",
      "    load_throughput: 20340.24\n",
      "    load_time_ms: 49.164\n",
      "    sample_throughput: 56.331\n",
      "    sample_time_ms: 17752.136\n",
      "    update_time_ms: 7.341\n",
      "  timestamp: 1631879062\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 20\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         736.919</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   -0.45</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-44-46\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.42857142857142855\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 21\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6582315233018665\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01196047772176029\n",
      "          policy_loss: -0.041588601387209365\n",
      "          total_loss: -0.0637170770102077\n",
      "          vf_explained_var: 0.10834157466888428\n",
      "          vf_loss: 0.00041717866996704184\n",
      "    num_agent_steps_sampled: 21000\n",
      "    num_agent_steps_trained: 21000\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 21000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.61818181818182\n",
      "    ram_util_percent: 54.60909090909092\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06319835486717504\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 68.57583712926704\n",
      "    mean_inference_ms: 2.3646405765501113\n",
      "    mean_raw_obs_processing_ms: 0.24984808591305724\n",
      "  time_since_restore: 760.2599501609802\n",
      "  time_this_iter_s: 23.34095287322998\n",
      "  time_total_s: 760.2599501609802\n",
      "  timers:\n",
      "    learn_throughput: 299.164\n",
      "    learn_time_ms: 3342.653\n",
      "    load_throughput: 21096.814\n",
      "    load_time_ms: 47.401\n",
      "    sample_throughput: 56.304\n",
      "    sample_time_ms: 17760.594\n",
      "    update_time_ms: 6.176\n",
      "  timestamp: 1631879086\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 21\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">          760.26</td><td style=\"text-align: right;\">21000</td><td style=\"text-align: right;\">-0.428571</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 22000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-45-06\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.4090909090909091\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 22\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.464519818623861\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007032059225960867\n",
      "          policy_loss: -0.033094367322822414\n",
      "          total_loss: -0.05438164948589272\n",
      "          vf_explained_var: -0.9026865363121033\n",
      "          vf_loss: 0.000984595540366071\n",
      "    num_agent_steps_sampled: 22000\n",
      "    num_agent_steps_trained: 22000\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.47666666666666\n",
      "    ram_util_percent: 54.63666666666666\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06315020509840599\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 66.79174464002641\n",
      "    mean_inference_ms: 2.3624804642582693\n",
      "    mean_raw_obs_processing_ms: 0.2489671377238699\n",
      "  time_since_restore: 781.0324630737305\n",
      "  time_this_iter_s: 20.772512912750244\n",
      "  time_total_s: 781.0324630737305\n",
      "  timers:\n",
      "    learn_throughput: 298.156\n",
      "    learn_time_ms: 3353.951\n",
      "    load_throughput: 21674.746\n",
      "    load_time_ms: 46.137\n",
      "    sample_throughput: 57.709\n",
      "    sample_time_ms: 17328.337\n",
      "    update_time_ms: 6.003\n",
      "  timestamp: 1631879106\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 22\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         781.032</td><td style=\"text-align: right;\">22000</td><td style=\"text-align: right;\">-0.409091</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 23000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-45-28\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.391304347826087\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 23\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9812383386823866\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013824228208887372\n",
      "          policy_loss: 0.04696925183137258\n",
      "          total_loss: 0.038301903340551585\n",
      "          vf_explained_var: 0.1569199413061142\n",
      "          vf_loss: 0.006479355992956294\n",
      "    num_agent_steps_sampled: 23000\n",
      "    num_agent_steps_trained: 23000\n",
      "    num_steps_sampled: 23000\n",
      "    num_steps_trained: 23000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.35333333333332\n",
      "    ram_util_percent: 54.66999999999999\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06309942617625339\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 65.13527353175012\n",
      "    mean_inference_ms: 2.360376689355136\n",
      "    mean_raw_obs_processing_ms: 0.24819429127772855\n",
      "  time_since_restore: 802.5782217979431\n",
      "  time_this_iter_s: 21.545758724212646\n",
      "  time_total_s: 802.5782217979431\n",
      "  timers:\n",
      "    learn_throughput: 297.416\n",
      "    learn_time_ms: 3362.288\n",
      "    load_throughput: 21228.244\n",
      "    load_time_ms: 47.107\n",
      "    sample_throughput: 59.134\n",
      "    sample_time_ms: 16910.72\n",
      "    update_time_ms: 5.755\n",
      "  timestamp: 1631879128\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 23\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         802.578</td><td style=\"text-align: right;\">23000</td><td style=\"text-align: right;\">-0.391304</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-45-50\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.375\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 24\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2728791819678413\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010273950128591084\n",
      "          policy_loss: -0.11382531647880872\n",
      "          total_loss: -0.1299478679895401\n",
      "          vf_explained_var: 0.09640176594257355\n",
      "          vf_loss: 0.003138778503570292\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.13870967741936\n",
      "    ram_util_percent: 54.70322580645161\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06304976976432748\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 63.59414182740437\n",
      "    mean_inference_ms: 2.358394187575026\n",
      "    mean_raw_obs_processing_ms: 0.24746707396167356\n",
      "  time_since_restore: 824.097683429718\n",
      "  time_this_iter_s: 21.519461631774902\n",
      "  time_total_s: 824.097683429718\n",
      "  timers:\n",
      "    learn_throughput: 297.957\n",
      "    learn_time_ms: 3356.189\n",
      "    load_throughput: 22028.965\n",
      "    load_time_ms: 45.395\n",
      "    sample_throughput: 58.752\n",
      "    sample_time_ms: 17020.613\n",
      "    update_time_ms: 5.776\n",
      "  timestamp: 1631879150\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 24\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         824.098</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">  -0.375</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 25000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-46-09\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.36\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 25\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6929480618900723\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00955669336418021\n",
      "          policy_loss: 0.005242812323073546\n",
      "          total_loss: -0.0036508367707331975\n",
      "          vf_explained_var: -0.5446525812149048\n",
      "          vf_loss: 0.0048104486896287805\n",
      "    num_agent_steps_sampled: 25000\n",
      "    num_agent_steps_trained: 25000\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 25000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.67142857142856\n",
      "    ram_util_percent: 54.69285714285713\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06300187625314935\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 62.15293334825457\n",
      "    mean_inference_ms: 2.3565156920875405\n",
      "    mean_raw_obs_processing_ms: 0.24679746901415844\n",
      "  time_since_restore: 843.6810536384583\n",
      "  time_this_iter_s: 19.583370208740234\n",
      "  time_total_s: 843.6810536384583\n",
      "  timers:\n",
      "    learn_throughput: 297.045\n",
      "    learn_time_ms: 3366.492\n",
      "    load_throughput: 22207.583\n",
      "    load_time_ms: 45.03\n",
      "    sample_throughput: 59.071\n",
      "    sample_time_ms: 16928.763\n",
      "    update_time_ms: 6.858\n",
      "  timestamp: 1631879169\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 25\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         843.681</td><td style=\"text-align: right;\">25000</td><td style=\"text-align: right;\">   -0.36</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 26000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-46-29\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.34615384615384615\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 26\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.356685909960005\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011747713391941871\n",
      "          policy_loss: 0.011826485395431519\n",
      "          total_loss: 0.020307090547349717\n",
      "          vf_explained_var: 0.14689010381698608\n",
      "          vf_loss: 0.028082609513593423\n",
      "    num_agent_steps_sampled: 26000\n",
      "    num_agent_steps_trained: 26000\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 26000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.72142857142856\n",
      "    ram_util_percent: 54.81428571428571\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06295281194582232\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 60.800868516932454\n",
      "    mean_inference_ms: 2.3546605142150714\n",
      "    mean_raw_obs_processing_ms: 0.24611681048787956\n",
      "  time_since_restore: 863.012069940567\n",
      "  time_this_iter_s: 19.331016302108765\n",
      "  time_total_s: 863.012069940567\n",
      "  timers:\n",
      "    learn_throughput: 296.61\n",
      "    learn_time_ms: 3371.434\n",
      "    load_throughput: 21965.411\n",
      "    load_time_ms: 45.526\n",
      "    sample_throughput: 59.088\n",
      "    sample_time_ms: 16923.818\n",
      "    update_time_ms: 6.912\n",
      "  timestamp: 1631879189\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 26\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         863.012</td><td style=\"text-align: right;\">26000</td><td style=\"text-align: right;\">-0.346154</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 27000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-46-47\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3333333333333333\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 27\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.67421985467275\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007387687331710553\n",
      "          policy_loss: -0.14290436191691291\n",
      "          total_loss: -0.16642245882087284\n",
      "          vf_explained_var: -0.009457924403250217\n",
      "          vf_loss: 0.0007307549356482923\n",
      "    num_agent_steps_sampled: 27000\n",
      "    num_agent_steps_trained: 27000\n",
      "    num_steps_sampled: 27000\n",
      "    num_steps_trained: 27000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.68846153846154\n",
      "    ram_util_percent: 54.73076923076923\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0629041056258242\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 59.528467273898116\n",
      "    mean_inference_ms: 2.352942563805958\n",
      "    mean_raw_obs_processing_ms: 0.2454363282999994\n",
      "  time_since_restore: 881.1626439094543\n",
      "  time_this_iter_s: 18.15057396888733\n",
      "  time_total_s: 881.1626439094543\n",
      "  timers:\n",
      "    learn_throughput: 296.855\n",
      "    learn_time_ms: 3368.642\n",
      "    load_throughput: 21867.109\n",
      "    load_time_ms: 45.731\n",
      "    sample_throughput: 59.706\n",
      "    sample_time_ms: 16748.822\n",
      "    update_time_ms: 7.084\n",
      "  timestamp: 1631879207\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 27\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         881.163</td><td style=\"text-align: right;\">27000</td><td style=\"text-align: right;\">-0.333333</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-47-04\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.32142857142857145\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 28\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.673496627807617\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010316577879610338\n",
      "          policy_loss: -0.17366463707553015\n",
      "          total_loss: -0.19620292484760285\n",
      "          vf_explained_var: 0.18298812210559845\n",
      "          vf_loss: 0.0007148317679012608\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.40416666666665\n",
      "    ram_util_percent: 54.75833333333333\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06285356384451306\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 58.32680504885504\n",
      "    mean_inference_ms: 2.3511521548972962\n",
      "    mean_raw_obs_processing_ms: 0.2448852384204185\n",
      "  time_since_restore: 897.9624266624451\n",
      "  time_this_iter_s: 16.799782752990723\n",
      "  time_total_s: 897.9624266624451\n",
      "  timers:\n",
      "    learn_throughput: 297.713\n",
      "    learn_time_ms: 3358.936\n",
      "    load_throughput: 21545.082\n",
      "    load_time_ms: 46.414\n",
      "    sample_throughput: 60.187\n",
      "    sample_time_ms: 16614.791\n",
      "    update_time_ms: 9.6\n",
      "  timestamp: 1631879224\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 28\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         897.962</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">-0.321429</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 29000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-47-22\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3103448275862069\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 29\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.585330859820048\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009386654423589925\n",
      "          policy_loss: -0.15296314118636978\n",
      "          total_loss: -0.17416918488840263\n",
      "          vf_explained_var: 0.24210402369499207\n",
      "          vf_loss: 0.0014792679193002792\n",
      "    num_agent_steps_sampled: 29000\n",
      "    num_agent_steps_trained: 29000\n",
      "    num_steps_sampled: 29000\n",
      "    num_steps_trained: 29000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.30384615384615\n",
      "    ram_util_percent: 54.81153846153846\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0628032455666326\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 57.19187501444803\n",
      "    mean_inference_ms: 2.3493511525368196\n",
      "    mean_raw_obs_processing_ms: 0.24435273796035192\n",
      "  time_since_restore: 916.2239458560944\n",
      "  time_this_iter_s: 18.261519193649292\n",
      "  time_total_s: 916.2239458560944\n",
      "  timers:\n",
      "    learn_throughput: 298.085\n",
      "    learn_time_ms: 3354.746\n",
      "    load_throughput: 21947.412\n",
      "    load_time_ms: 45.563\n",
      "    sample_throughput: 60.96\n",
      "    sample_time_ms: 16404.092\n",
      "    update_time_ms: 9.595\n",
      "  timestamp: 1631879242\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29000\n",
      "  training_iteration: 29\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         916.224</td><td style=\"text-align: right;\">29000</td><td style=\"text-align: right;\">-0.310345</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-48-03\n",
      "  done: false\n",
      "  episode_len_mean: 996.0333333333333\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 30\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5916891786787244\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007493594396164803\n",
      "          policy_loss: -0.02625422610176934\n",
      "          total_loss: -0.04901077699744039\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0006312527252399984\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.99137931034483\n",
      "    ram_util_percent: 54.682758620689654\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06275549919154523\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 56.12097734910175\n",
      "    mean_inference_ms: 2.347622291616395\n",
      "    mean_raw_obs_processing_ms: 0.26625259163594944\n",
      "  time_since_restore: 957.2438359260559\n",
      "  time_this_iter_s: 41.01989006996155\n",
      "  time_total_s: 957.2438359260559\n",
      "  timers:\n",
      "    learn_throughput: 300.528\n",
      "    learn_time_ms: 3327.482\n",
      "    load_throughput: 22700.872\n",
      "    load_time_ms: 44.051\n",
      "    sample_throughput: 53.978\n",
      "    sample_time_ms: 18526.039\n",
      "    update_time_ms: 9.704\n",
      "  timestamp: 1631879283\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 30\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         957.244</td><td style=\"text-align: right;\">30000</td><td style=\"text-align: right;\">    -0.3</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.033</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 31000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-48-24\n",
      "  done: false\n",
      "  episode_len_mean: 996.1612903225806\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.2903225806451613\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 31\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7046454906463624\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007553308414218199\n",
      "          policy_loss: 0.020122432543171778\n",
      "          total_loss: -0.004054422304034233\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0003203579966793768\n",
      "    num_agent_steps_sampled: 31000\n",
      "    num_agent_steps_trained: 31000\n",
      "    num_steps_sampled: 31000\n",
      "    num_steps_trained: 31000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.4483870967742\n",
      "    ram_util_percent: 54.87741935483873\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0627094642829184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 55.10869691980425\n",
      "    mean_inference_ms: 2.346040854573869\n",
      "    mean_raw_obs_processing_ms: 0.28605578091554185\n",
      "  time_since_restore: 978.3539719581604\n",
      "  time_this_iter_s: 21.110136032104492\n",
      "  time_total_s: 978.3539719581604\n",
      "  timers:\n",
      "    learn_throughput: 300.15\n",
      "    learn_time_ms: 3331.67\n",
      "    load_throughput: 21883.447\n",
      "    load_time_ms: 45.697\n",
      "    sample_throughput: 54.653\n",
      "    sample_time_ms: 18297.132\n",
      "    update_time_ms: 9.736\n",
      "  timestamp: 1631879304\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31000\n",
      "  training_iteration: 31\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         978.354</td><td style=\"text-align: right;\">31000</td><td style=\"text-align: right;\">-0.290323</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.161</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-48-44\n",
      "  done: false\n",
      "  episode_len_mean: 996.28125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.28125\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 32\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5962152216169567\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008306270521363797\n",
      "          policy_loss: -0.03481988054182794\n",
      "          total_loss: -0.0574555197937621\n",
      "          vf_explained_var: -0.8783384561538696\n",
      "          vf_loss: 0.000523146388554273\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.48214285714286\n",
      "    ram_util_percent: 54.982142857142854\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0626652541886749\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 54.1494042432341\n",
      "    mean_inference_ms: 2.3445330684868755\n",
      "    mean_raw_obs_processing_ms: 0.30399938433283347\n",
      "  time_since_restore: 998.4244606494904\n",
      "  time_this_iter_s: 20.070488691329956\n",
      "  time_total_s: 998.4244606494904\n",
      "  timers:\n",
      "    learn_throughput: 300.877\n",
      "    learn_time_ms: 3323.613\n",
      "    load_throughput: 22050.321\n",
      "    load_time_ms: 45.351\n",
      "    sample_throughput: 54.841\n",
      "    sample_time_ms: 18234.509\n",
      "    update_time_ms: 10.425\n",
      "  timestamp: 1631879324\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 32\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         998.424</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">-0.28125</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.281</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 33000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-49-04\n",
      "  done: false\n",
      "  episode_len_mean: 996.3939393939394\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.2727272727272727\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 33\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6951094309488934\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005184844854526559\n",
      "          policy_loss: -0.06732877666751544\n",
      "          total_loss: -0.09233222810758485\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00019775936945936538\n",
      "    num_agent_steps_sampled: 33000\n",
      "    num_agent_steps_trained: 33000\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 33000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.79642857142856\n",
      "    ram_util_percent: 55.010714285714286\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06262246296169924\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 53.238076864253514\n",
      "    mean_inference_ms: 2.3431369815572536\n",
      "    mean_raw_obs_processing_ms: 0.3202618141389786\n",
      "  time_since_restore: 1017.8884828090668\n",
      "  time_this_iter_s: 19.464022159576416\n",
      "  time_total_s: 1017.8884828090668\n",
      "  timers:\n",
      "    learn_throughput: 300.725\n",
      "    learn_time_ms: 3325.294\n",
      "    load_throughput: 22357.828\n",
      "    load_time_ms: 44.727\n",
      "    sample_throughput: 55.256\n",
      "    sample_time_ms: 18097.429\n",
      "    update_time_ms: 10.592\n",
      "  timestamp: 1631879344\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 33\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         1017.89</td><td style=\"text-align: right;\">33000</td><td style=\"text-align: right;\">-0.272727</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.394</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 34000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-49-24\n",
      "  done: false\n",
      "  episode_len_mean: 996.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.2647058823529412\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 34\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6443238364325627\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006959032444651589\n",
      "          policy_loss: -0.054323647357523444\n",
      "          total_loss: -0.07749979491862986\n",
      "          vf_explained_var: -0.9429082870483398\n",
      "          vf_loss: 0.0009184180187326597\n",
      "    num_agent_steps_sampled: 34000\n",
      "    num_agent_steps_trained: 34000\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 34000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.95333333333335\n",
      "    ram_util_percent: 55.016666666666666\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06258228016629235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 52.37208326368497\n",
      "    mean_inference_ms: 2.3418203803833335\n",
      "    mean_raw_obs_processing_ms: 0.3350194997982402\n",
      "  time_since_restore: 1038.4655034542084\n",
      "  time_this_iter_s: 20.5770206451416\n",
      "  time_total_s: 1038.4655034542084\n",
      "  timers:\n",
      "    learn_throughput: 299.355\n",
      "    learn_time_ms: 3340.51\n",
      "    load_throughput: 21955.442\n",
      "    load_time_ms: 45.547\n",
      "    sample_throughput: 55.597\n",
      "    sample_time_ms: 17986.655\n",
      "    update_time_ms: 11.028\n",
      "  timestamp: 1631879364\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 34\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         1038.47</td><td style=\"text-align: right;\">34000</td><td style=\"text-align: right;\">-0.264706</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">             996.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 35000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-49-47\n",
      "  done: false\n",
      "  episode_len_mean: 996.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.2571428571428571\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 35\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.69091395272149\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005998443806821503\n",
      "          policy_loss: 0.06256420800669325\n",
      "          total_loss: 0.0377992092528277\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00011966413213586849\n",
      "    num_agent_steps_sampled: 35000\n",
      "    num_agent_steps_trained: 35000\n",
      "    num_steps_sampled: 35000\n",
      "    num_steps_trained: 35000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.34838709677419\n",
      "    ram_util_percent: 55.12580645161289\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06254330261176687\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 51.549356474315324\n",
      "    mean_inference_ms: 2.3405828016537122\n",
      "    mean_raw_obs_processing_ms: 0.3484393871454536\n",
      "  time_since_restore: 1060.700157880783\n",
      "  time_this_iter_s: 22.234654426574707\n",
      "  time_total_s: 1060.700157880783\n",
      "  timers:\n",
      "    learn_throughput: 299.241\n",
      "    learn_time_ms: 3341.789\n",
      "    load_throughput: 21731.65\n",
      "    load_time_ms: 46.016\n",
      "    sample_throughput: 54.791\n",
      "    sample_time_ms: 18251.129\n",
      "    update_time_ms: 9.952\n",
      "  timestamp: 1631879387\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 35\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">          1060.7</td><td style=\"text-align: right;\">35000</td><td style=\"text-align: right;\">-0.257143</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">             996.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-50-07\n",
      "  done: false\n",
      "  episode_len_mean: 996.6944444444445\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.25\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 36\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.639289869202508\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008877543813081238\n",
      "          policy_loss: -0.029852394552694426\n",
      "          total_loss: -0.0523911381761233\n",
      "          vf_explained_var: -0.7377275824546814\n",
      "          vf_loss: 0.0008579833882524529\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.99\n",
      "    ram_util_percent: 55.09333333333333\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06250715490713053\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 50.765256948314104\n",
      "    mean_inference_ms: 2.3394541733994605\n",
      "    mean_raw_obs_processing_ms: 0.3606920659314906\n",
      "  time_since_restore: 1081.3092606067657\n",
      "  time_this_iter_s: 20.609102725982666\n",
      "  time_total_s: 1081.3092606067657\n",
      "  timers:\n",
      "    learn_throughput: 299.576\n",
      "    learn_time_ms: 3338.052\n",
      "    load_throughput: 21666.293\n",
      "    load_time_ms: 46.155\n",
      "    sample_throughput: 54.274\n",
      "    sample_time_ms: 18425.053\n",
      "    update_time_ms: 10.1\n",
      "  timestamp: 1631879407\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 36\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         1081.31</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">   -0.25</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.694</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 37000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-50-28\n",
      "  done: false\n",
      "  episode_len_mean: 996.7837837837837\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.24324324324324326\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 37\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6939575725131566\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00973015713329681\n",
      "          policy_loss: 0.02485754932794306\n",
      "          total_loss: 0.0014339764912923178\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00023207344428455043\n",
      "    num_agent_steps_sampled: 37000\n",
      "    num_agent_steps_trained: 37000\n",
      "    num_steps_sampled: 37000\n",
      "    num_steps_trained: 37000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.36\n",
      "    ram_util_percent: 55.16666666666666\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06247527655078753\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 50.01712777351042\n",
      "    mean_inference_ms: 2.3384871575325796\n",
      "    mean_raw_obs_processing_ms: 0.3718757841042121\n",
      "  time_since_restore: 1102.176587820053\n",
      "  time_this_iter_s: 20.867327213287354\n",
      "  time_total_s: 1102.176587820053\n",
      "  timers:\n",
      "    learn_throughput: 298.853\n",
      "    learn_time_ms: 3346.125\n",
      "    load_throughput: 23214.313\n",
      "    load_time_ms: 43.077\n",
      "    sample_throughput: 53.497\n",
      "    sample_time_ms: 18692.684\n",
      "    update_time_ms: 9.993\n",
      "  timestamp: 1631879428\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37000\n",
      "  training_iteration: 37\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         1102.18</td><td style=\"text-align: right;\">37000</td><td style=\"text-align: right;\">-0.243243</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.784</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 38000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-50-47\n",
      "  done: false\n",
      "  episode_len_mean: 996.8684210526316\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.23684210526315788\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 38\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.596448032061259\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007038465043673724\n",
      "          policy_loss: -0.024551905939976373\n",
      "          total_loss: -0.04758514016866684\n",
      "          vf_explained_var: -0.8122448325157166\n",
      "          vf_loss: 0.0005557626506844018\n",
      "    num_agent_steps_sampled: 38000\n",
      "    num_agent_steps_trained: 38000\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 38000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.08148148148148\n",
      "    ram_util_percent: 55.17777777777778\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06244726912647617\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 49.301335696286195\n",
      "    mean_inference_ms: 2.337608159299552\n",
      "    mean_raw_obs_processing_ms: 0.3820969568008426\n",
      "  time_since_restore: 1121.2061259746552\n",
      "  time_this_iter_s: 19.02953815460205\n",
      "  time_total_s: 1121.2061259746552\n",
      "  timers:\n",
      "    learn_throughput: 300.735\n",
      "    learn_time_ms: 3325.191\n",
      "    load_throughput: 23271.746\n",
      "    load_time_ms: 42.971\n",
      "    sample_throughput: 52.795\n",
      "    sample_time_ms: 18941.111\n",
      "    update_time_ms: 7.589\n",
      "  timestamp: 1631879447\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 38\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         1121.21</td><td style=\"text-align: right;\">38000</td><td style=\"text-align: right;\">-0.236842</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.868</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 39000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-51-08\n",
      "  done: false\n",
      "  episode_len_mean: 996.9487179487179\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.23076923076923078\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 39\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4770953747961255\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007799864140982196\n",
      "          policy_loss: -0.055536273452970714\n",
      "          total_loss: -0.07700087510877185\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.000673895875984777\n",
      "    num_agent_steps_sampled: 39000\n",
      "    num_agent_steps_trained: 39000\n",
      "    num_steps_sampled: 39000\n",
      "    num_steps_trained: 39000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.43103448275862\n",
      "    ram_util_percent: 55.12758620689656\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06242150924425957\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 48.61686891915991\n",
      "    mean_inference_ms: 2.3367802591573574\n",
      "    mean_raw_obs_processing_ms: 0.3914348307992725\n",
      "  time_since_restore: 1141.8891694545746\n",
      "  time_this_iter_s: 20.683043479919434\n",
      "  time_total_s: 1141.8891694545746\n",
      "  timers:\n",
      "    learn_throughput: 301.196\n",
      "    learn_time_ms: 3320.092\n",
      "    load_throughput: 22799.319\n",
      "    load_time_ms: 43.861\n",
      "    sample_throughput: 52.118\n",
      "    sample_time_ms: 19187.41\n",
      "    update_time_ms: 7.606\n",
      "  timestamp: 1631879468\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 39\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         1141.89</td><td style=\"text-align: right;\">39000</td><td style=\"text-align: right;\">-0.230769</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.949</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-51-29\n",
      "  done: false\n",
      "  episode_len_mean: 997.025\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.225\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 40\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.374901098675198\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007024680568122735\n",
      "          policy_loss: 0.010855482601457172\n",
      "          total_loss: -0.009813109557661745\n",
      "          vf_explained_var: -0.8223778605461121\n",
      "          vf_loss: 0.0007095911792324235\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.61333333333333\n",
      "    ram_util_percent: 55.15666666666666\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062398994115148855\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 47.96172347566589\n",
      "    mean_inference_ms: 2.3360262973264474\n",
      "    mean_raw_obs_processing_ms: 0.3999952793766686\n",
      "  time_since_restore: 1162.791140794754\n",
      "  time_this_iter_s: 20.901971340179443\n",
      "  time_total_s: 1162.791140794754\n",
      "  timers:\n",
      "    learn_throughput: 300.25\n",
      "    learn_time_ms: 3330.56\n",
      "    load_throughput: 22192.566\n",
      "    load_time_ms: 45.06\n",
      "    sample_throughput: 58.262\n",
      "    sample_time_ms: 17163.934\n",
      "    update_time_ms: 7.597\n",
      "  timestamp: 1631879489\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 40\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         1162.79</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  -0.225</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.025</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 41000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-51-49\n",
      "  done: false\n",
      "  episode_len_mean: 997.0975609756098\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.21951219512195122\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 41\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.593468689918518\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007608411689799333\n",
      "          policy_loss: 0.063156129916509\n",
      "          total_loss: 0.040278458346923195\n",
      "          vf_explained_var: -0.5153459906578064\n",
      "          vf_loss: 0.0004891766149537741\n",
      "    num_agent_steps_sampled: 41000\n",
      "    num_agent_steps_trained: 41000\n",
      "    num_steps_sampled: 41000\n",
      "    num_steps_trained: 41000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.07241379310346\n",
      "    ram_util_percent: 55.32758620689654\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06237884559921772\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 47.33372037693914\n",
      "    mean_inference_ms: 2.3353169806104157\n",
      "    mean_raw_obs_processing_ms: 0.40785389331237565\n",
      "  time_since_restore: 1183.160237312317\n",
      "  time_this_iter_s: 20.369096517562866\n",
      "  time_total_s: 1183.160237312317\n",
      "  timers:\n",
      "    learn_throughput: 301.109\n",
      "    learn_time_ms: 3321.054\n",
      "    load_throughput: 22357.03\n",
      "    load_time_ms: 44.729\n",
      "    sample_throughput: 58.482\n",
      "    sample_time_ms: 17099.411\n",
      "    update_time_ms: 7.897\n",
      "  timestamp: 1631879509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 41000\n",
      "  training_iteration: 41\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         1183.16</td><td style=\"text-align: right;\">41000</td><td style=\"text-align: right;\">-0.219512</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.098</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 42000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-52-11\n",
      "  done: false\n",
      "  episode_len_mean: 997.1666666666666\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.21428571428571427\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 42\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6145555867089167\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007085749145338847\n",
      "          policy_loss: 0.04156573303043842\n",
      "          total_loss: 0.01811413230995337\n",
      "          vf_explained_var: -0.6379765272140503\n",
      "          vf_loss: 0.0003025149072022436\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.6516129032258\n",
      "    ram_util_percent: 55.183870967741946\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062360570928084696\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 46.73171786927638\n",
      "    mean_inference_ms: 2.3346774113930135\n",
      "    mean_raw_obs_processing_ms: 0.41505264538279396\n",
      "  time_since_restore: 1204.7130663394928\n",
      "  time_this_iter_s: 21.552829027175903\n",
      "  time_total_s: 1204.7130663394928\n",
      "  timers:\n",
      "    learn_throughput: 298.67\n",
      "    learn_time_ms: 3348.174\n",
      "    load_throughput: 21706.581\n",
      "    load_time_ms: 46.069\n",
      "    sample_throughput: 58.073\n",
      "    sample_time_ms: 17219.612\n",
      "    update_time_ms: 7.366\n",
      "  timestamp: 1631879531\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 42\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         1204.71</td><td style=\"text-align: right;\">42000</td><td style=\"text-align: right;\">-0.214286</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.167</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 43000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-52-33\n",
      "  done: false\n",
      "  episode_len_mean: 997.2325581395348\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.20930232558139536\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 43\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5087023946974014\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01007998562820812\n",
      "          policy_loss: 0.12965340125891897\n",
      "          total_loss: 0.10854187524980968\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0005735010182737218\n",
      "    num_agent_steps_sampled: 43000\n",
      "    num_agent_steps_trained: 43000\n",
      "    num_steps_sampled: 43000\n",
      "    num_steps_trained: 43000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.19062500000001\n",
      "    ram_util_percent: 55.184375\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06234628306071158\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 46.15435442946018\n",
      "    mean_inference_ms: 2.334094242927141\n",
      "    mean_raw_obs_processing_ms: 0.42169553892096245\n",
      "  time_since_restore: 1226.797871351242\n",
      "  time_this_iter_s: 22.084805011749268\n",
      "  time_total_s: 1226.797871351242\n",
      "  timers:\n",
      "    learn_throughput: 298.736\n",
      "    learn_time_ms: 3347.44\n",
      "    load_throughput: 21735.591\n",
      "    load_time_ms: 46.007\n",
      "    sample_throughput: 57.202\n",
      "    sample_time_ms: 17481.978\n",
      "    update_time_ms: 7.623\n",
      "  timestamp: 1631879553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 43000\n",
      "  training_iteration: 43\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">          1226.8</td><td style=\"text-align: right;\">43000</td><td style=\"text-align: right;\">-0.209302</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.233</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-52-55\n",
      "  done: false\n",
      "  episode_len_mean: 997.2954545454545\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.20454545454545456\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 44\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4750684976577757\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009351251966668498\n",
      "          policy_loss: -0.030394253320991993\n",
      "          total_loss: -0.05058620549324486\n",
      "          vf_explained_var: -0.025417564436793327\n",
      "          vf_loss: 0.0014026846728585143\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.58387096774194\n",
      "    ram_util_percent: 55.20967741935485\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0623329572755529\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 45.60008940868855\n",
      "    mean_inference_ms: 2.333526022231283\n",
      "    mean_raw_obs_processing_ms: 0.42778763722920327\n",
      "  time_since_restore: 1248.7561666965485\n",
      "  time_this_iter_s: 21.958295345306396\n",
      "  time_total_s: 1248.7561666965485\n",
      "  timers:\n",
      "    learn_throughput: 298.037\n",
      "    learn_time_ms: 3355.293\n",
      "    load_throughput: 21724.052\n",
      "    load_time_ms: 46.032\n",
      "    sample_throughput: 56.777\n",
      "    sample_time_ms: 17612.806\n",
      "    update_time_ms: 7.231\n",
      "  timestamp: 1631879575\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 44\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         1248.76</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">-0.204545</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.295</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 45000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-53-16\n",
      "  done: false\n",
      "  episode_len_mean: 997.3555555555556\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.2\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 45\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5126723262998794\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011792254431345947\n",
      "          policy_loss: 0.0059526258872614965\n",
      "          total_loss: -0.0139061129755444\n",
      "          vf_explained_var: -0.57793128490448\n",
      "          vf_loss: 0.0012881045242668027\n",
      "    num_agent_steps_sampled: 45000\n",
      "    num_agent_steps_trained: 45000\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 45000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.97241379310344\n",
      "    ram_util_percent: 55.35862068965518\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06232122096267635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 45.066849624998454\n",
      "    mean_inference_ms: 2.332994948426451\n",
      "    mean_raw_obs_processing_ms: 0.43338327320593945\n",
      "  time_since_restore: 1269.2706062793732\n",
      "  time_this_iter_s: 20.514439582824707\n",
      "  time_total_s: 1269.2706062793732\n",
      "  timers:\n",
      "    learn_throughput: 298.61\n",
      "    learn_time_ms: 3348.849\n",
      "    load_throughput: 21924.937\n",
      "    load_time_ms: 45.61\n",
      "    sample_throughput: 57.314\n",
      "    sample_time_ms: 17447.653\n",
      "    update_time_ms: 7.189\n",
      "  timestamp: 1631879596\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 45\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         1269.27</td><td style=\"text-align: right;\">45000</td><td style=\"text-align: right;\">    -0.2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.356</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 46000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-53-37\n",
      "  done: false\n",
      "  episode_len_mean: 997.4130434782609\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.1956521739130435\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 46\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5665490706761678\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005192610523435764\n",
      "          policy_loss: 0.05657088855902354\n",
      "          total_loss: 0.033021997743182714\n",
      "          vf_explained_var: -0.7909436821937561\n",
      "          vf_loss: 0.00036409188873626083\n",
      "    num_agent_steps_sampled: 46000\n",
      "    num_agent_steps_trained: 46000\n",
      "    num_steps_sampled: 46000\n",
      "    num_steps_trained: 46000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.66451612903225\n",
      "    ram_util_percent: 55.303225806451614\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06231070698518591\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 44.55393504377166\n",
      "    mean_inference_ms: 2.3325231455179054\n",
      "    mean_raw_obs_processing_ms: 0.4385271587571573\n",
      "  time_since_restore: 1290.7750825881958\n",
      "  time_this_iter_s: 21.504476308822632\n",
      "  time_total_s: 1290.7750825881958\n",
      "  timers:\n",
      "    learn_throughput: 300.845\n",
      "    learn_time_ms: 3323.97\n",
      "    load_throughput: 22114.264\n",
      "    load_time_ms: 45.22\n",
      "    sample_throughput: 56.938\n",
      "    sample_time_ms: 17563.003\n",
      "    update_time_ms: 6.917\n",
      "  timestamp: 1631879617\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 46000\n",
      "  training_iteration: 46\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         1290.78</td><td style=\"text-align: right;\">46000</td><td style=\"text-align: right;\">-0.195652</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.413</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 47000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-53-57\n",
      "  done: false\n",
      "  episode_len_mean: 997.468085106383\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.19148936170212766\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 47\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5608829471800063\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008055430893460367\n",
      "          policy_loss: 0.048629298971758946\n",
      "          total_loss: 0.02673081010580063\n",
      "          vf_explained_var: -0.499594509601593\n",
      "          vf_loss: 0.0009916389102323188\n",
      "    num_agent_steps_sampled: 47000\n",
      "    num_agent_steps_trained: 47000\n",
      "    num_steps_sampled: 47000\n",
      "    num_steps_trained: 47000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.67142857142856\n",
      "    ram_util_percent: 55.46428571428571\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06230220821985765\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 44.059124808777916\n",
      "    mean_inference_ms: 2.332060883767795\n",
      "    mean_raw_obs_processing_ms: 0.4432409288464043\n",
      "  time_since_restore: 1310.1518716812134\n",
      "  time_this_iter_s: 19.376789093017578\n",
      "  time_total_s: 1310.1518716812134\n",
      "  timers:\n",
      "    learn_throughput: 301.138\n",
      "    learn_time_ms: 3320.734\n",
      "    load_throughput: 21389.11\n",
      "    load_time_ms: 46.753\n",
      "    sample_throughput: 57.423\n",
      "    sample_time_ms: 17414.506\n",
      "    update_time_ms: 7.799\n",
      "  timestamp: 1631879637\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47000\n",
      "  training_iteration: 47\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         1310.15</td><td style=\"text-align: right;\">47000</td><td style=\"text-align: right;\">-0.191489</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.468</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-54-16\n",
      "  done: false\n",
      "  episode_len_mean: 997.5208333333334\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.1875\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 48\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.572519135475159\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007534422646527015\n",
      "          policy_loss: 0.03229837454338041\n",
      "          total_loss: 0.009447801175216835\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0003317488488391973\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.62857142857142\n",
      "    ram_util_percent: 55.4\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06229395474526867\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 43.58171667319238\n",
      "    mean_inference_ms: 2.3316141224828564\n",
      "    mean_raw_obs_processing_ms: 0.44756644489561975\n",
      "  time_since_restore: 1330.0117168426514\n",
      "  time_this_iter_s: 19.85984516143799\n",
      "  time_total_s: 1330.0117168426514\n",
      "  timers:\n",
      "    learn_throughput: 300.982\n",
      "    learn_time_ms: 3322.454\n",
      "    load_throughput: 21309.792\n",
      "    load_time_ms: 46.927\n",
      "    sample_throughput: 57.158\n",
      "    sample_time_ms: 17495.308\n",
      "    update_time_ms: 7.772\n",
      "  timestamp: 1631879656\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 48\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         1330.01</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> -0.1875</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.521</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 49000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-54-37\n",
      "  done: false\n",
      "  episode_len_mean: 997.5714285714286\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.1836734693877551\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 49\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6118822866015963\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006308453570991862\n",
      "          policy_loss: -0.05633983065684636\n",
      "          total_loss: -0.08004176177912288\n",
      "          vf_explained_var: -0.9846917986869812\n",
      "          vf_loss: 0.0002877886888098955\n",
      "    num_agent_steps_sampled: 49000\n",
      "    num_agent_steps_trained: 49000\n",
      "    num_steps_sampled: 49000\n",
      "    num_steps_trained: 49000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.95172413793104\n",
      "    ram_util_percent: 55.310344827586206\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06228638968506298\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 43.120991738075865\n",
      "    mean_inference_ms: 2.3311961578151763\n",
      "    mean_raw_obs_processing_ms: 0.4515422273147965\n",
      "  time_since_restore: 1350.4076788425446\n",
      "  time_this_iter_s: 20.39596199989319\n",
      "  time_total_s: 1350.4076788425446\n",
      "  timers:\n",
      "    learn_throughput: 302.176\n",
      "    learn_time_ms: 3309.326\n",
      "    load_throughput: 21268.686\n",
      "    load_time_ms: 47.017\n",
      "    sample_throughput: 57.209\n",
      "    sample_time_ms: 17479.692\n",
      "    update_time_ms: 7.668\n",
      "  timestamp: 1631879677\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49000\n",
      "  training_iteration: 49\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         1350.41</td><td style=\"text-align: right;\">49000</td><td style=\"text-align: right;\">-0.183673</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.571</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 50000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-54-58\n",
      "  done: false\n",
      "  episode_len_mean: 997.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.18\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 50\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.572450844446818\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0063754649114050035\n",
      "          policy_loss: -0.010278782414065466\n",
      "          total_loss: -0.033548574563529755\n",
      "          vf_explained_var: -0.4415629804134369\n",
      "          vf_loss: 0.0003029969874963475\n",
      "    num_agent_steps_sampled: 50000\n",
      "    num_agent_steps_trained: 50000\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.28333333333335\n",
      "    ram_util_percent: 55.30666666666668\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062281797842873646\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 42.676227723038075\n",
      "    mean_inference_ms: 2.330841569415458\n",
      "    mean_raw_obs_processing_ms: 0.4551954352321656\n",
      "  time_since_restore: 1371.4924638271332\n",
      "  time_this_iter_s: 21.084784984588623\n",
      "  time_total_s: 1371.4924638271332\n",
      "  timers:\n",
      "    learn_throughput: 301.605\n",
      "    learn_time_ms: 3315.597\n",
      "    load_throughput: 21675.071\n",
      "    load_time_ms: 46.136\n",
      "    sample_throughput: 57.167\n",
      "    sample_time_ms: 17492.586\n",
      "    update_time_ms: 7.596\n",
      "  timestamp: 1631879698\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 50\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         1371.49</td><td style=\"text-align: right;\">50000</td><td style=\"text-align: right;\">   -0.18</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">            997.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 51000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-55-18\n",
      "  done: false\n",
      "  episode_len_mean: 997.6666666666666\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.17647058823529413\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 51\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5206660985946656\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007083583153023213\n",
      "          policy_loss: 0.1372103782163726\n",
      "          total_loss: 0.1150543651647038\n",
      "          vf_explained_var: -0.550018846988678\n",
      "          vf_loss: 0.0006599369715938034\n",
      "    num_agent_steps_sampled: 51000\n",
      "    num_agent_steps_trained: 51000\n",
      "    num_steps_sampled: 51000\n",
      "    num_steps_trained: 51000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.13928571428572\n",
      "    ram_util_percent: 55.50714285714285\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06227692747912576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 42.24612464490185\n",
      "    mean_inference_ms: 2.3305091662352098\n",
      "    mean_raw_obs_processing_ms: 0.4585486475999521\n",
      "  time_since_restore: 1391.1497950553894\n",
      "  time_this_iter_s: 19.657331228256226\n",
      "  time_total_s: 1391.1497950553894\n",
      "  timers:\n",
      "    learn_throughput: 301.514\n",
      "    learn_time_ms: 3316.591\n",
      "    load_throughput: 21725.796\n",
      "    load_time_ms: 46.028\n",
      "    sample_throughput: 57.404\n",
      "    sample_time_ms: 17420.501\n",
      "    update_time_ms: 7.369\n",
      "  timestamp: 1631879718\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 51000\n",
      "  training_iteration: 51\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         1391.15</td><td style=\"text-align: right;\">51000</td><td style=\"text-align: right;\">-0.176471</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.667</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-55-38\n",
      "  done: false\n",
      "  episode_len_mean: 997.7115384615385\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.17307692307692307\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 52\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5833336538738676\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005312949977490907\n",
      "          policy_loss: -0.013673121399349637\n",
      "          total_loss: -0.03744457515163554\n",
      "          vf_explained_var: -0.7950118780136108\n",
      "          vf_loss: 0.0002687614823419911\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.64\n",
      "    ram_util_percent: 55.47999999999999\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062271004227854206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 41.830148071850175\n",
      "    mean_inference_ms: 2.3301951844744884\n",
      "    mean_raw_obs_processing_ms: 0.4616222083821666\n",
      "  time_since_restore: 1411.571985244751\n",
      "  time_this_iter_s: 20.422190189361572\n",
      "  time_total_s: 1411.571985244751\n",
      "  timers:\n",
      "    learn_throughput: 302.021\n",
      "    learn_time_ms: 3311.03\n",
      "    load_throughput: 21714.616\n",
      "    load_time_ms: 46.052\n",
      "    sample_throughput: 57.79\n",
      "    sample_time_ms: 17304.025\n",
      "    update_time_ms: 7.211\n",
      "  timestamp: 1631879738\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 52\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         1411.57</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">-0.173077</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.712</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 53000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-55-58\n",
      "  done: false\n",
      "  episode_len_mean: 997.7547169811321\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.16981132075471697\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 53\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.474974354108175\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005471036304996524\n",
      "          policy_loss: 0.11734735932615069\n",
      "          total_loss: 0.09499752206934822\n",
      "          vf_explained_var: -0.6537927985191345\n",
      "          vf_loss: 0.0005534299874852877\n",
      "    num_agent_steps_sampled: 53000\n",
      "    num_agent_steps_trained: 53000\n",
      "    num_steps_sampled: 53000\n",
      "    num_steps_trained: 53000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.50714285714285\n",
      "    ram_util_percent: 55.41071428571428\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0622657960509811\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 41.4274648203559\n",
      "    mean_inference_ms: 2.3298972138394483\n",
      "    mean_raw_obs_processing_ms: 0.4644307402095548\n",
      "  time_since_restore: 1431.4813675880432\n",
      "  time_this_iter_s: 19.909382343292236\n",
      "  time_total_s: 1431.4813675880432\n",
      "  timers:\n",
      "    learn_throughput: 302.483\n",
      "    learn_time_ms: 3305.969\n",
      "    load_throughput: 21274.425\n",
      "    load_time_ms: 47.005\n",
      "    sample_throughput: 58.51\n",
      "    sample_time_ms: 17091.125\n",
      "    update_time_ms: 6.975\n",
      "  timestamp: 1631879758\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 53000\n",
      "  training_iteration: 53\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         1431.48</td><td style=\"text-align: right;\">53000</td><td style=\"text-align: right;\">-0.169811</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.755</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 54000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-56-19\n",
      "  done: false\n",
      "  episode_len_mean: 997.7962962962963\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.16666666666666666\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 54\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4665581782658896\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00911468132220178\n",
      "          policy_loss: 0.017988216131925583\n",
      "          total_loss: -0.0031912511938975915\n",
      "          vf_explained_var: -0.49896901845932007\n",
      "          vf_loss: 0.00040990768082135924\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_steps_sampled: 54000\n",
      "    num_steps_trained: 54000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.31333333333335\n",
      "    ram_util_percent: 55.203333333333326\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06226162153478897\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 41.037712860426666\n",
      "    mean_inference_ms: 2.329652443657535\n",
      "    mean_raw_obs_processing_ms: 0.46700620632386325\n",
      "  time_since_restore: 1452.4993348121643\n",
      "  time_this_iter_s: 21.017967224121094\n",
      "  time_total_s: 1452.4993348121643\n",
      "  timers:\n",
      "    learn_throughput: 302.527\n",
      "    learn_time_ms: 3305.486\n",
      "    load_throughput: 21278.268\n",
      "    load_time_ms: 46.996\n",
      "    sample_throughput: 58.834\n",
      "    sample_time_ms: 16997.034\n",
      "    update_time_ms: 7.337\n",
      "  timestamp: 1631879779\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 54000\n",
      "  training_iteration: 54\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">          1452.5</td><td style=\"text-align: right;\">54000</td><td style=\"text-align: right;\">-0.166667</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.796</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 55000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-56-41\n",
      "  done: false\n",
      "  episode_len_mean: 997.8363636363637\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.16363636363636364\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 55\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.522688145107693\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007964353315357482\n",
      "          policy_loss: 0.13893378617035018\n",
      "          total_loss: 0.11715057508813011\n",
      "          vf_explained_var: -0.2359781265258789\n",
      "          vf_loss: 0.0007557010152797577\n",
      "    num_agent_steps_sampled: 55000\n",
      "    num_agent_steps_trained: 55000\n",
      "    num_steps_sampled: 55000\n",
      "    num_steps_trained: 55000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.76666666666667\n",
      "    ram_util_percent: 55.309999999999995\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06225828843288526\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.66046827108609\n",
      "    mean_inference_ms: 2.3294213228999974\n",
      "    mean_raw_obs_processing_ms: 0.4693639663019125\n",
      "  time_since_restore: 1473.8527300357819\n",
      "  time_this_iter_s: 21.353395223617554\n",
      "  time_total_s: 1473.8527300357819\n",
      "  timers:\n",
      "    learn_throughput: 302.813\n",
      "    learn_time_ms: 3302.367\n",
      "    load_throughput: 21009.716\n",
      "    load_time_ms: 47.597\n",
      "    sample_throughput: 58.537\n",
      "    sample_time_ms: 17083.142\n",
      "    update_time_ms: 7.579\n",
      "  timestamp: 1631879801\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55000\n",
      "  training_iteration: 55\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         1473.85</td><td style=\"text-align: right;\">55000</td><td style=\"text-align: right;\">-0.163636</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.836</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-57-01\n",
      "  done: false\n",
      "  episode_len_mean: 997.875\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.16071428571428573\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 56\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.54894253677792\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005664155145714442\n",
      "          policy_loss: 0.20042095051871406\n",
      "          total_loss: 0.1773409925401211\n",
      "          vf_explained_var: -0.9173348546028137\n",
      "          vf_loss: 0.0004978131345220997\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.27666666666666\n",
      "    ram_util_percent: 55.25\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06225620096422487\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.29475336733962\n",
      "    mean_inference_ms: 2.329212182530804\n",
      "    mean_raw_obs_processing_ms: 0.4715219277914266\n",
      "  time_since_restore: 1494.28000497818\n",
      "  time_this_iter_s: 20.42727494239807\n",
      "  time_total_s: 1494.28000497818\n",
      "  timers:\n",
      "    learn_throughput: 299.936\n",
      "    learn_time_ms: 3334.045\n",
      "    load_throughput: 20946.97\n",
      "    load_time_ms: 47.74\n",
      "    sample_throughput: 59.02\n",
      "    sample_time_ms: 16943.289\n",
      "    update_time_ms: 7.804\n",
      "  timestamp: 1631879821\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 56\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         1494.28</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">-0.160714</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.875</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 57000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-57-22\n",
      "  done: false\n",
      "  episode_len_mean: 997.9122807017544\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.15789473684210525\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 57\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.362864046626621\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015214361851835603\n",
      "          policy_loss: 0.08532434710197978\n",
      "          total_loss: 0.0683188319620159\n",
      "          vf_explained_var: 0.13178850710391998\n",
      "          vf_loss: 0.0014882752649201495\n",
      "    num_agent_steps_sampled: 57000\n",
      "    num_agent_steps_trained: 57000\n",
      "    num_steps_sampled: 57000\n",
      "    num_steps_trained: 57000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.00344827586208\n",
      "    ram_util_percent: 55.4\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06225464141655434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 39.94023007926608\n",
      "    mean_inference_ms: 2.329024386545495\n",
      "    mean_raw_obs_processing_ms: 0.47349094560250243\n",
      "  time_since_restore: 1515.1321613788605\n",
      "  time_this_iter_s: 20.852156400680542\n",
      "  time_total_s: 1515.1321613788605\n",
      "  timers:\n",
      "    learn_throughput: 301.972\n",
      "    learn_time_ms: 3311.57\n",
      "    load_throughput: 20206.346\n",
      "    load_time_ms: 49.489\n",
      "    sample_throughput: 58.437\n",
      "    sample_time_ms: 17112.398\n",
      "    update_time_ms: 6.899\n",
      "  timestamp: 1631879842\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 57000\n",
      "  training_iteration: 57\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         1515.13</td><td style=\"text-align: right;\">57000</td><td style=\"text-align: right;\">-0.157895</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.912</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 58000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-57-42\n",
      "  done: false\n",
      "  episode_len_mean: 997.948275862069\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.15517241379310345\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 58\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5994016408920286\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003919696678871117\n",
      "          policy_loss: 0.01693007235104839\n",
      "          total_loss: -0.007387547567486763\n",
      "          vf_explained_var: -0.9963218569755554\n",
      "          vf_loss: 0.00035349804974329243\n",
      "    num_agent_steps_sampled: 58000\n",
      "    num_agent_steps_trained: 58000\n",
      "    num_steps_sampled: 58000\n",
      "    num_steps_trained: 58000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.38620689655173\n",
      "    ram_util_percent: 55.334482758620695\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0622534872026017\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 39.59606541414414\n",
      "    mean_inference_ms: 2.3288447607992553\n",
      "    mean_raw_obs_processing_ms: 0.4752890540542033\n",
      "  time_since_restore: 1535.1131126880646\n",
      "  time_this_iter_s: 19.9809513092041\n",
      "  time_total_s: 1535.1131126880646\n",
      "  timers:\n",
      "    learn_throughput: 300.687\n",
      "    learn_time_ms: 3325.713\n",
      "    load_throughput: 20453.544\n",
      "    load_time_ms: 48.891\n",
      "    sample_throughput: 58.441\n",
      "    sample_time_ms: 17111.294\n",
      "    update_time_ms: 6.919\n",
      "  timestamp: 1631879862\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 58000\n",
      "  training_iteration: 58\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         1535.11</td><td style=\"text-align: right;\">58000</td><td style=\"text-align: right;\">-0.155172</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.948</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 59000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-58-02\n",
      "  done: false\n",
      "  episode_len_mean: 997.9830508474577\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.15254237288135594\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 59\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16874999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.580409876505534\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011202735749780186\n",
      "          policy_loss: 0.028530783578753473\n",
      "          total_loss: 0.0048245202543007\n",
      "          vf_explained_var: -0.3805197477340698\n",
      "          vf_loss: 0.00020737118377130376\n",
      "    num_agent_steps_sampled: 59000\n",
      "    num_agent_steps_trained: 59000\n",
      "    num_steps_sampled: 59000\n",
      "    num_steps_trained: 59000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.45714285714287\n",
      "    ram_util_percent: 55.21071428571428\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06225235101226962\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 39.2617332807987\n",
      "    mean_inference_ms: 2.3286627873090855\n",
      "    mean_raw_obs_processing_ms: 0.47692224127152066\n",
      "  time_since_restore: 1554.6347951889038\n",
      "  time_this_iter_s: 19.521682500839233\n",
      "  time_total_s: 1554.6347951889038\n",
      "  timers:\n",
      "    learn_throughput: 300.529\n",
      "    learn_time_ms: 3327.471\n",
      "    load_throughput: 20443.047\n",
      "    load_time_ms: 48.916\n",
      "    sample_throughput: 58.752\n",
      "    sample_time_ms: 17020.763\n",
      "    update_time_ms: 7.606\n",
      "  timestamp: 1631879882\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 59000\n",
      "  training_iteration: 59\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         1554.63</td><td style=\"text-align: right;\">59000</td><td style=\"text-align: right;\">-0.152542</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.983</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-58-40\n",
      "  done: false\n",
      "  episode_len_mean: 996.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.15\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 60\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16874999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.605114483833313\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011749671108785763\n",
      "          policy_loss: 0.012571687748034795\n",
      "          total_loss: -0.01089285264412562\n",
      "          vf_explained_var: -0.750762939453125\n",
      "          vf_loss: 0.0006038495682231668\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.3962962962963\n",
      "    ram_util_percent: 55.1611111111111\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062251337604703964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.93706880110933\n",
      "    mean_inference_ms: 2.328490813110491\n",
      "    mean_raw_obs_processing_ms: 0.4833048011116775\n",
      "  time_since_restore: 1592.8425850868225\n",
      "  time_this_iter_s: 38.2077898979187\n",
      "  time_total_s: 1592.8425850868225\n",
      "  timers:\n",
      "    learn_throughput: 302.209\n",
      "    learn_time_ms: 3308.968\n",
      "    load_throughput: 19993.25\n",
      "    load_time_ms: 50.017\n",
      "    sample_throughput: 53.333\n",
      "    sample_time_ms: 18750.066\n",
      "    update_time_ms: 7.873\n",
      "  timestamp: 1631879920\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 60\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         1592.84</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">   -0.15</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">            996.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 61000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-59-01\n",
      "  done: false\n",
      "  episode_len_mean: 996.1147540983607\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.14754098360655737\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 61\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16874999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.662177054087321\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009611889711393356\n",
      "          policy_loss: 0.06219906972514259\n",
      "          total_loss: 0.03753438095251719\n",
      "          vf_explained_var: -0.9571781754493713\n",
      "          vf_loss: 0.00033507461032261036\n",
      "    num_agent_steps_sampled: 61000\n",
      "    num_agent_steps_trained: 61000\n",
      "    num_steps_sampled: 61000\n",
      "    num_steps_trained: 61000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.25333333333333\n",
      "    ram_util_percent: 55.08\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062252504100564876\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.62167677005119\n",
      "    mean_inference_ms: 2.328348101163272\n",
      "    mean_raw_obs_processing_ms: 0.4893124597752546\n",
      "  time_since_restore: 1613.6481473445892\n",
      "  time_this_iter_s: 20.805562257766724\n",
      "  time_total_s: 1613.6481473445892\n",
      "  timers:\n",
      "    learn_throughput: 302.026\n",
      "    learn_time_ms: 3310.979\n",
      "    load_throughput: 20007.298\n",
      "    load_time_ms: 49.982\n",
      "    sample_throughput: 53.016\n",
      "    sample_time_ms: 18862.384\n",
      "    update_time_ms: 8.168\n",
      "  timestamp: 1631879941\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 61000\n",
      "  training_iteration: 61\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         1613.65</td><td style=\"text-align: right;\">61000</td><td style=\"text-align: right;\">-0.147541</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.115</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 62000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-59-23\n",
      "  done: false\n",
      "  episode_len_mean: 996.1774193548387\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.14516129032258066\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 62\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16874999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.622997925016615\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0077461316806264335\n",
      "          policy_loss: -0.08818741585645411\n",
      "          total_loss: -0.11287050868074099\n",
      "          vf_explained_var: -0.6271733641624451\n",
      "          vf_loss: 0.00023972698383861117\n",
      "    num_agent_steps_sampled: 62000\n",
      "    num_agent_steps_trained: 62000\n",
      "    num_steps_sampled: 62000\n",
      "    num_steps_trained: 62000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.52580645161291\n",
      "    ram_util_percent: 55.358064516129026\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062254759267181645\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.315264903451556\n",
      "    mean_inference_ms: 2.3282640683083247\n",
      "    mean_raw_obs_processing_ms: 0.49497677326205564\n",
      "  time_since_restore: 1635.5696320533752\n",
      "  time_this_iter_s: 21.92148470878601\n",
      "  time_total_s: 1635.5696320533752\n",
      "  timers:\n",
      "    learn_throughput: 301.395\n",
      "    learn_time_ms: 3317.905\n",
      "    load_throughput: 20537.018\n",
      "    load_time_ms: 48.693\n",
      "    sample_throughput: 52.679\n",
      "    sample_time_ms: 18982.946\n",
      "    update_time_ms: 8.29\n",
      "  timestamp: 1631879963\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 62000\n",
      "  training_iteration: 62\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         1635.57</td><td style=\"text-align: right;\">62000</td><td style=\"text-align: right;\">-0.145161</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.177</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 63000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-59-43\n",
      "  done: false\n",
      "  episode_len_mean: 996.2380952380952\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.14285714285714285\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 63\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16874999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6194220675362483\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007692296037220419\n",
      "          policy_loss: -0.060676634084019396\n",
      "          total_loss: -0.08526498385601573\n",
      "          vf_explained_var: -0.6201708316802979\n",
      "          vf_loss: 0.00030779422862300027\n",
      "    num_agent_steps_sampled: 63000\n",
      "    num_agent_steps_trained: 63000\n",
      "    num_steps_sampled: 63000\n",
      "    num_steps_trained: 63000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.82333333333332\n",
      "    ram_util_percent: 55.179999999999986\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06225777874210346\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.01718290187944\n",
      "    mean_inference_ms: 2.3282205418991406\n",
      "    mean_raw_obs_processing_ms: 0.500309943193781\n",
      "  time_since_restore: 1655.8801412582397\n",
      "  time_this_iter_s: 20.310509204864502\n",
      "  time_total_s: 1655.8801412582397\n",
      "  timers:\n",
      "    learn_throughput: 301.345\n",
      "    learn_time_ms: 3318.454\n",
      "    load_throughput: 21132.582\n",
      "    load_time_ms: 47.32\n",
      "    sample_throughput: 52.566\n",
      "    sample_time_ms: 19023.871\n",
      "    update_time_ms: 8.31\n",
      "  timestamp: 1631879983\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 63000\n",
      "  training_iteration: 63\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         1655.88</td><td style=\"text-align: right;\">63000</td><td style=\"text-align: right;\">-0.142857</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.238</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-00-03\n",
      "  done: false\n",
      "  episode_len_mean: 996.296875\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.140625\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 64\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16874999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6289323621326024\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007954230135985728\n",
      "          policy_loss: -0.04383460050448775\n",
      "          total_loss: -0.06839010098742114\n",
      "          vf_explained_var: -0.7870053648948669\n",
      "          vf_loss: 0.0003915450551883421\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.8\n",
      "    ram_util_percent: 55.175\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06226199452007744\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.72694687634976\n",
      "    mean_inference_ms: 2.3282177438366887\n",
      "    mean_raw_obs_processing_ms: 0.5053304215342903\n",
      "  time_since_restore: 1675.5833129882812\n",
      "  time_this_iter_s: 19.703171730041504\n",
      "  time_total_s: 1675.5833129882812\n",
      "  timers:\n",
      "    learn_throughput: 302.744\n",
      "    learn_time_ms: 3303.124\n",
      "    load_throughput: 21779.846\n",
      "    load_time_ms: 45.914\n",
      "    sample_throughput: 52.884\n",
      "    sample_time_ms: 18909.436\n",
      "    update_time_ms: 8.228\n",
      "  timestamp: 1631880003\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 64\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         1675.58</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">-0.140625</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.297</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 65000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-00-23\n",
      "  done: false\n",
      "  episode_len_mean: 996.3538461538461\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.13846153846153847\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 65\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16874999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4319579389360215\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0285542163118468\n",
      "          policy_loss: 0.006777266330189175\n",
      "          total_loss: -0.011171819104088678\n",
      "          vf_explained_var: -0.001862167613580823\n",
      "          vf_loss: 0.0015519743492606924\n",
      "    num_agent_steps_sampled: 65000\n",
      "    num_agent_steps_trained: 65000\n",
      "    num_steps_sampled: 65000\n",
      "    num_steps_trained: 65000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.18275862068967\n",
      "    ram_util_percent: 55.317241379310346\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062266954014709476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.444453205782665\n",
      "    mean_inference_ms: 2.3282399752804275\n",
      "    mean_raw_obs_processing_ms: 0.5100543809015283\n",
      "  time_since_restore: 1696.2781670093536\n",
      "  time_this_iter_s: 20.694854021072388\n",
      "  time_total_s: 1696.2781670093536\n",
      "  timers:\n",
      "    learn_throughput: 301.296\n",
      "    learn_time_ms: 3318.999\n",
      "    load_throughput: 21992.823\n",
      "    load_time_ms: 45.469\n",
      "    sample_throughput: 53.114\n",
      "    sample_time_ms: 18827.334\n",
      "    update_time_ms: 9.051\n",
      "  timestamp: 1631880023\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 65000\n",
      "  training_iteration: 65\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         1696.28</td><td style=\"text-align: right;\">65000</td><td style=\"text-align: right;\">-0.138462</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.354</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 66000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-00-47\n",
      "  done: false\n",
      "  episode_len_mean: 996.4090909090909\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.13636363636363635\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 66\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3480296048853133\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02487002721985557\n",
      "          policy_loss: -0.2271477121445868\n",
      "          total_loss: -0.16994733545515273\n",
      "          vf_explained_var: 0.12110377103090286\n",
      "          vf_loss: 0.0643854452452312\n",
      "    num_agent_steps_sampled: 66000\n",
      "    num_agent_steps_trained: 66000\n",
      "    num_steps_sampled: 66000\n",
      "    num_steps_trained: 66000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.1441176470588\n",
      "    ram_util_percent: 55.17941176470588\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06227286045379798\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.169996343544156\n",
      "    mean_inference_ms: 2.3282916665969116\n",
      "    mean_raw_obs_processing_ms: 0.5145225604056649\n",
      "  time_since_restore: 1719.6111016273499\n",
      "  time_this_iter_s: 23.332934617996216\n",
      "  time_total_s: 1719.6111016273499\n",
      "  timers:\n",
      "    learn_throughput: 303.166\n",
      "    learn_time_ms: 3298.525\n",
      "    load_throughput: 21942.44\n",
      "    load_time_ms: 45.574\n",
      "    sample_throughput: 52.251\n",
      "    sample_time_ms: 19138.342\n",
      "    update_time_ms: 9.034\n",
      "  timestamp: 1631880047\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 66000\n",
      "  training_iteration: 66\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         1719.61</td><td style=\"text-align: right;\">66000</td><td style=\"text-align: right;\">-0.136364</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.409</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 67000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-01-08\n",
      "  done: false\n",
      "  episode_len_mean: 996.4626865671642\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.13432835820895522\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 67\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6835751480526395\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011697778905650264\n",
      "          policy_loss: -0.20479283597734238\n",
      "          total_loss: -0.2087068905433019\n",
      "          vf_explained_var: 0.4075738191604614\n",
      "          vf_loss: 0.008480196472050415\n",
      "    num_agent_steps_sampled: 67000\n",
      "    num_agent_steps_trained: 67000\n",
      "    num_steps_sampled: 67000\n",
      "    num_steps_trained: 67000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.28620689655172\n",
      "    ram_util_percent: 55.341379310344834\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06227989357439532\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 36.902715586555594\n",
      "    mean_inference_ms: 2.3283766156406567\n",
      "    mean_raw_obs_processing_ms: 0.5187286106809713\n",
      "  time_since_restore: 1740.4523351192474\n",
      "  time_this_iter_s: 20.841233491897583\n",
      "  time_total_s: 1740.4523351192474\n",
      "  timers:\n",
      "    learn_throughput: 303.609\n",
      "    learn_time_ms: 3293.71\n",
      "    load_throughput: 22293.645\n",
      "    load_time_ms: 44.856\n",
      "    sample_throughput: 52.237\n",
      "    sample_time_ms: 19143.374\n",
      "    update_time_ms: 8.985\n",
      "  timestamp: 1631880068\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 67000\n",
      "  training_iteration: 67\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         1740.45</td><td style=\"text-align: right;\">67000</td><td style=\"text-align: right;\">-0.134328</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.463</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-01-30\n",
      "  done: false\n",
      "  episode_len_mean: 996.5147058823529\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.1323529411764706\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 68\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.681647147072686\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005362766349328258\n",
      "          policy_loss: -0.146048682079547\n",
      "          total_loss: -0.17030539868606462\n",
      "          vf_explained_var: -0.7936663031578064\n",
      "          vf_loss: 0.0005235768856234952\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.546875\n",
      "    ram_util_percent: 55.41875\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06228766974810228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 36.64252401218404\n",
      "    mean_inference_ms: 2.3285033299578393\n",
      "    mean_raw_obs_processing_ms: 0.5226943064728022\n",
      "  time_since_restore: 1762.4297773838043\n",
      "  time_this_iter_s: 21.977442264556885\n",
      "  time_total_s: 1762.4297773838043\n",
      "  timers:\n",
      "    learn_throughput: 305.589\n",
      "    learn_time_ms: 3272.371\n",
      "    load_throughput: 21952.363\n",
      "    load_time_ms: 45.553\n",
      "    sample_throughput: 51.644\n",
      "    sample_time_ms: 19363.423\n",
      "    update_time_ms: 8.977\n",
      "  timestamp: 1631880090\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 68\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         1762.43</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">-0.132353</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.515</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 69000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-01-51\n",
      "  done: false\n",
      "  episode_len_mean: 996.5652173913044\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.13043478260869565\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 69\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.682385766506195\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014775814188122086\n",
      "          policy_loss: -0.1807917046878073\n",
      "          total_loss: -0.18370871825350654\n",
      "          vf_explained_var: 0.3091524839401245\n",
      "          vf_loss: 0.008296653253233267\n",
      "    num_agent_steps_sampled: 69000\n",
      "    num_agent_steps_trained: 69000\n",
      "    num_steps_sampled: 69000\n",
      "    num_steps_trained: 69000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.32333333333334\n",
      "    ram_util_percent: 55.4\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062295408429737256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 36.38890693333787\n",
      "    mean_inference_ms: 2.32868156708231\n",
      "    mean_raw_obs_processing_ms: 0.5264336422277268\n",
      "  time_since_restore: 1783.4854319095612\n",
      "  time_this_iter_s: 21.055654525756836\n",
      "  time_total_s: 1783.4854319095612\n",
      "  timers:\n",
      "    learn_throughput: 305.498\n",
      "    learn_time_ms: 3273.341\n",
      "    load_throughput: 21889.751\n",
      "    load_time_ms: 45.683\n",
      "    sample_throughput: 51.238\n",
      "    sample_time_ms: 19516.864\n",
      "    update_time_ms: 8.275\n",
      "  timestamp: 1631880111\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 69000\n",
      "  training_iteration: 69\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         1783.49</td><td style=\"text-align: right;\">69000</td><td style=\"text-align: right;\">-0.130435</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.565</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 70000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-02-15\n",
      "  done: false\n",
      "  episode_len_mean: 996.6142857142858\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.17142857142857143\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 70\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7593254407246908\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013135910085929802\n",
      "          policy_loss: -0.08781980209880405\n",
      "          total_loss: -0.06990940239694383\n",
      "          vf_explained_var: 0.3782915472984314\n",
      "          vf_loss: 0.030516113423638875\n",
      "    num_agent_steps_sampled: 70000\n",
      "    num_agent_steps_trained: 70000\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.83529411764708\n",
      "    ram_util_percent: 55.37058823529412\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062303496302535157\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 36.14225092659063\n",
      "    mean_inference_ms: 2.3288762705848005\n",
      "    mean_raw_obs_processing_ms: 0.5299561456802527\n",
      "  time_since_restore: 1807.3987393379211\n",
      "  time_this_iter_s: 23.913307428359985\n",
      "  time_total_s: 1807.3987393379211\n",
      "  timers:\n",
      "    learn_throughput: 304.958\n",
      "    learn_time_ms: 3279.14\n",
      "    load_throughput: 21800.381\n",
      "    load_time_ms: 45.871\n",
      "    sample_throughput: 55.305\n",
      "    sample_time_ms: 18081.532\n",
      "    update_time_ms: 8.335\n",
      "  timestamp: 1631880135\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 70\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">          1807.4</td><td style=\"text-align: right;\">70000</td><td style=\"text-align: right;\">-0.171429</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.614</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 71000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-02-36\n",
      "  done: false\n",
      "  episode_len_mean: 996.6619718309859\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.16901408450704225\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 71\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3313600487179227\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009249212627710375\n",
      "          policy_loss: -0.06587968009213606\n",
      "          total_loss: -0.08124723757306734\n",
      "          vf_explained_var: 0.04926378279924393\n",
      "          vf_loss: 0.004434232918235163\n",
      "    num_agent_steps_sampled: 71000\n",
      "    num_agent_steps_trained: 71000\n",
      "    num_steps_sampled: 71000\n",
      "    num_steps_trained: 71000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.10000000000001\n",
      "    ram_util_percent: 55.44666666666668\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06231236357110162\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.90173620316589\n",
      "    mean_inference_ms: 2.3290833266098074\n",
      "    mean_raw_obs_processing_ms: 0.5332764062090064\n",
      "  time_since_restore: 1828.4737935066223\n",
      "  time_this_iter_s: 21.075054168701172\n",
      "  time_total_s: 1828.4737935066223\n",
      "  timers:\n",
      "    learn_throughput: 306.469\n",
      "    learn_time_ms: 3262.969\n",
      "    load_throughput: 21756.731\n",
      "    load_time_ms: 45.963\n",
      "    sample_throughput: 55.178\n",
      "    sample_time_ms: 18123.191\n",
      "    update_time_ms: 7.905\n",
      "  timestamp: 1631880156\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 71000\n",
      "  training_iteration: 71\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         1828.47</td><td style=\"text-align: right;\">71000</td><td style=\"text-align: right;\">-0.169014</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.662</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-02-57\n",
      "  done: false\n",
      "  episode_len_mean: 996.7083333333334\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.18055555555555555\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 72\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4942849331431918\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009845601098148787\n",
      "          policy_loss: -0.040692878928449416\n",
      "          total_loss: -0.0405314928955502\n",
      "          vf_explained_var: 0.47759881615638733\n",
      "          vf_loss: 0.011365978882855012\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.92999999999999\n",
      "    ram_util_percent: 55.23\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062321732037750474\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.66713477194847\n",
      "    mean_inference_ms: 2.3293022492757474\n",
      "    mean_raw_obs_processing_ms: 0.5364018231401957\n",
      "  time_since_restore: 1849.732189655304\n",
      "  time_this_iter_s: 21.25839614868164\n",
      "  time_total_s: 1849.732189655304\n",
      "  timers:\n",
      "    learn_throughput: 307.86\n",
      "    learn_time_ms: 3248.225\n",
      "    load_throughput: 22070.359\n",
      "    load_time_ms: 45.31\n",
      "    sample_throughput: 55.237\n",
      "    sample_time_ms: 18103.709\n",
      "    update_time_ms: 8.197\n",
      "  timestamp: 1631880177\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 72\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         1849.73</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">-0.180556</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.708</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 73000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-03-18\n",
      "  done: false\n",
      "  episode_len_mean: 996.7534246575342\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.1780821917808219\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 73\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2092454532782237\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008057922017772117\n",
      "          policy_loss: -0.07164878149827321\n",
      "          total_loss: -0.07847950243287616\n",
      "          vf_explained_var: -0.22768601775169373\n",
      "          vf_loss: 0.002202245497594251\n",
      "    num_agent_steps_sampled: 73000\n",
      "    num_agent_steps_trained: 73000\n",
      "    num_steps_sampled: 73000\n",
      "    num_steps_trained: 73000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.03103448275863\n",
      "    ram_util_percent: 55.462068965517254\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06233061283751566\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.43810049096416\n",
      "    mean_inference_ms: 2.3295048894105976\n",
      "    mean_raw_obs_processing_ms: 0.539343152203499\n",
      "  time_since_restore: 1870.072815656662\n",
      "  time_this_iter_s: 20.340626001358032\n",
      "  time_total_s: 1870.072815656662\n",
      "  timers:\n",
      "    learn_throughput: 309.15\n",
      "    learn_time_ms: 3234.672\n",
      "    load_throughput: 21538.687\n",
      "    load_time_ms: 46.428\n",
      "    sample_throughput: 55.191\n",
      "    sample_time_ms: 18118.996\n",
      "    update_time_ms: 8.375\n",
      "  timestamp: 1631880198\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 73000\n",
      "  training_iteration: 73\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         1870.07</td><td style=\"text-align: right;\">73000</td><td style=\"text-align: right;\">-0.178082</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.753</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 74000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-03-38\n",
      "  done: false\n",
      "  episode_len_mean: 996.7972972972973\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.17567567567567569\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 74\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.78732021384769\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004847924844793599\n",
      "          policy_loss: 0.15035739069183668\n",
      "          total_loss: 0.12447390933003691\n",
      "          vf_explained_var: -0.2961638867855072\n",
      "          vf_loss: 0.00014902631850822622\n",
      "    num_agent_steps_sampled: 74000\n",
      "    num_agent_steps_trained: 74000\n",
      "    num_steps_sampled: 74000\n",
      "    num_steps_trained: 74000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.15517241379311\n",
      "    ram_util_percent: 55.55172413793103\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06233928964570868\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.21440321067148\n",
      "    mean_inference_ms: 2.3296991313719877\n",
      "    mean_raw_obs_processing_ms: 0.5421033363159861\n",
      "  time_since_restore: 1890.3233618736267\n",
      "  time_this_iter_s: 20.25054621696472\n",
      "  time_total_s: 1890.3233618736267\n",
      "  timers:\n",
      "    learn_throughput: 309.542\n",
      "    learn_time_ms: 3230.578\n",
      "    load_throughput: 20990.737\n",
      "    load_time_ms: 47.64\n",
      "    sample_throughput: 55.023\n",
      "    sample_time_ms: 18174.382\n",
      "    update_time_ms: 8.891\n",
      "  timestamp: 1631880218\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 74000\n",
      "  training_iteration: 74\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         1890.32</td><td style=\"text-align: right;\">74000</td><td style=\"text-align: right;\">-0.175676</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.797</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 75000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-04-00\n",
      "  done: false\n",
      "  episode_len_mean: 996.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.17333333333333334\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 75\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.747909437285529\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008864443687889903\n",
      "          policy_loss: 0.22119099961386787\n",
      "          total_loss: 0.19561366786559423\n",
      "          vf_explained_var: -0.4017302989959717\n",
      "          vf_loss: 0.00021890560714786666\n",
      "    num_agent_steps_sampled: 75000\n",
      "    num_agent_steps_trained: 75000\n",
      "    num_steps_sampled: 75000\n",
      "    num_steps_trained: 75000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.6625\n",
      "    ram_util_percent: 55.337500000000006\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06234721769467998\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 34.99613283089303\n",
      "    mean_inference_ms: 2.3298792373427486\n",
      "    mean_raw_obs_processing_ms: 0.5446940224316817\n",
      "  time_since_restore: 1912.2720172405243\n",
      "  time_this_iter_s: 21.948655366897583\n",
      "  time_total_s: 1912.2720172405243\n",
      "  timers:\n",
      "    learn_throughput: 309.784\n",
      "    learn_time_ms: 3228.051\n",
      "    load_throughput: 20873.467\n",
      "    load_time_ms: 47.908\n",
      "    sample_throughput: 54.637\n",
      "    sample_time_ms: 18302.625\n",
      "    update_time_ms: 8.502\n",
      "  timestamp: 1631880240\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 75000\n",
      "  training_iteration: 75\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         1912.27</td><td style=\"text-align: right;\">75000</td><td style=\"text-align: right;\">-0.173333</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">            996.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-04-23\n",
      "  done: false\n",
      "  episode_len_mean: 996.8815789473684\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.17105263157894737\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 76\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.651280747519599\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010167692320472968\n",
      "          policy_loss: 0.041649584223826724\n",
      "          total_loss: 0.018801196581787534\n",
      "          vf_explained_var: -0.29168176651000977\n",
      "          vf_loss: 0.0017341463677843826\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.67812500000001\n",
      "    ram_util_percent: 55.40625\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0623549115240317\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 34.78321532576569\n",
      "    mean_inference_ms: 2.3300512455242157\n",
      "    mean_raw_obs_processing_ms: 0.5471247084506622\n",
      "  time_since_restore: 1935.0349879264832\n",
      "  time_this_iter_s: 22.762970685958862\n",
      "  time_total_s: 1935.0349879264832\n",
      "  timers:\n",
      "    learn_throughput: 307.741\n",
      "    learn_time_ms: 3249.489\n",
      "    load_throughput: 20722.139\n",
      "    load_time_ms: 48.258\n",
      "    sample_throughput: 54.873\n",
      "    sample_time_ms: 18224.05\n",
      "    update_time_ms: 8.269\n",
      "  timestamp: 1631880263\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 76\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         1935.03</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">-0.171053</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.882</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 77000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-04-50\n",
      "  done: false\n",
      "  episode_len_mean: 996.922077922078\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.16883116883116883\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 77\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6505048433939615\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01739890654130302\n",
      "          policy_loss: 0.04425622605615192\n",
      "          total_loss: 0.02276866568459405\n",
      "          vf_explained_var: -0.6006844639778137\n",
      "          vf_loss: 0.0017144130583296323\n",
      "    num_agent_steps_sampled: 77000\n",
      "    num_agent_steps_trained: 77000\n",
      "    num_steps_sampled: 77000\n",
      "    num_steps_trained: 77000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.96153846153847\n",
      "    ram_util_percent: 55.49230769230769\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06236243495975082\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 34.5761322047095\n",
      "    mean_inference_ms: 2.3302202992755596\n",
      "    mean_raw_obs_processing_ms: 0.5494129296976696\n",
      "  time_since_restore: 1961.9380717277527\n",
      "  time_this_iter_s: 26.90308380126953\n",
      "  time_total_s: 1961.9380717277527\n",
      "  timers:\n",
      "    learn_throughput: 304.8\n",
      "    learn_time_ms: 3280.841\n",
      "    load_throughput: 20516.936\n",
      "    load_time_ms: 48.74\n",
      "    sample_throughput: 53.197\n",
      "    sample_time_ms: 18798.083\n",
      "    update_time_ms: 8.365\n",
      "  timestamp: 1631880290\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 77000\n",
      "  training_iteration: 77\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         1961.94</td><td style=\"text-align: right;\">77000</td><td style=\"text-align: right;\">-0.168831</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.922</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 78000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-05-12\n",
      "  done: false\n",
      "  episode_len_mean: 996.9615384615385\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.16666666666666666\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 78\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7996091736687554\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010110235263627487\n",
      "          policy_loss: 0.005160707400904761\n",
      "          total_loss: -0.007823523961835437\n",
      "          vf_explained_var: 0.022857094183564186\n",
      "          vf_loss: 0.0030924935611741\n",
      "    num_agent_steps_sampled: 78000\n",
      "    num_agent_steps_trained: 78000\n",
      "    num_steps_sampled: 78000\n",
      "    num_steps_trained: 78000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.25806451612904\n",
      "    ram_util_percent: 55.53548387096774\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062369393119975525\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 34.37389904213769\n",
      "    mean_inference_ms: 2.3303841125755107\n",
      "    mean_raw_obs_processing_ms: 0.5515558994223299\n",
      "  time_since_restore: 1984.1353023052216\n",
      "  time_this_iter_s: 22.197230577468872\n",
      "  time_total_s: 1984.1353023052216\n",
      "  timers:\n",
      "    learn_throughput: 302.529\n",
      "    learn_time_ms: 3305.464\n",
      "    load_throughput: 20667.236\n",
      "    load_time_ms: 48.386\n",
      "    sample_throughput: 53.204\n",
      "    sample_time_ms: 18795.443\n",
      "    update_time_ms: 8.743\n",
      "  timestamp: 1631880312\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 78000\n",
      "  training_iteration: 78\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         1984.14</td><td style=\"text-align: right;\">78000</td><td style=\"text-align: right;\">-0.166667</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.962</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 79000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-05-32\n",
      "  done: false\n",
      "  episode_len_mean: 997.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.16455696202531644\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 79\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5787433650758533\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011448549777362753\n",
      "          policy_loss: 0.04804096122582754\n",
      "          total_loss: 0.025097030649582543\n",
      "          vf_explained_var: -0.10338200628757477\n",
      "          vf_loss: 0.0006700698677579769\n",
      "    num_agent_steps_sampled: 79000\n",
      "    num_agent_steps_trained: 79000\n",
      "    num_steps_sampled: 79000\n",
      "    num_steps_trained: 79000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.80344827586205\n",
      "    ram_util_percent: 55.55172413793104\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06237635889591233\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 34.17599086874099\n",
      "    mean_inference_ms: 2.330542526619394\n",
      "    mean_raw_obs_processing_ms: 0.5535612973049832\n",
      "  time_since_restore: 2003.949501991272\n",
      "  time_this_iter_s: 19.814199686050415\n",
      "  time_total_s: 2003.949501991272\n",
      "  timers:\n",
      "    learn_throughput: 302.063\n",
      "    learn_time_ms: 3310.564\n",
      "    load_throughput: 21705.75\n",
      "    load_time_ms: 46.071\n",
      "    sample_throughput: 53.566\n",
      "    sample_time_ms: 18668.567\n",
      "    update_time_ms: 8.916\n",
      "  timestamp: 1631880332\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79000\n",
      "  training_iteration: 79\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         2003.95</td><td style=\"text-align: right;\">79000</td><td style=\"text-align: right;\">-0.164557</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">               997</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-05-53\n",
      "  done: false\n",
      "  episode_len_mean: 997.0375\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.1625\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 80\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7261841694513955\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007416652737251105\n",
      "          policy_loss: 0.13044191292590565\n",
      "          total_loss: 0.10491255840493573\n",
      "          vf_explained_var: -0.4378131031990051\n",
      "          vf_loss: 0.00032448224834903764\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.02903225806452\n",
      "    ram_util_percent: 55.39032258064516\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06238271272544489\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.98248783909536\n",
      "    mean_inference_ms: 2.3306884808370234\n",
      "    mean_raw_obs_processing_ms: 0.5554378851516618\n",
      "  time_since_restore: 2025.7873957157135\n",
      "  time_this_iter_s: 21.83789372444153\n",
      "  time_total_s: 2025.7873957157135\n",
      "  timers:\n",
      "    learn_throughput: 300.74\n",
      "    learn_time_ms: 3325.126\n",
      "    load_throughput: 22502.935\n",
      "    load_time_ms: 44.439\n",
      "    sample_throughput: 54.205\n",
      "    sample_time_ms: 18448.359\n",
      "    update_time_ms: 8.553\n",
      "  timestamp: 1631880353\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 80\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         2025.79</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> -0.1625</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.038</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 81000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-06-14\n",
      "  done: false\n",
      "  episode_len_mean: 997.074074074074\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.16049382716049382\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 81\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4189632336298623\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01113431852729434\n",
      "          policy_loss: 0.07782759360141224\n",
      "          total_loss: 0.0564344491602646\n",
      "          vf_explained_var: -0.07267534732818604\n",
      "          vf_loss: 0.0006827062788134855\n",
      "    num_agent_steps_sampled: 81000\n",
      "    num_agent_steps_trained: 81000\n",
      "    num_steps_sampled: 81000\n",
      "    num_steps_trained: 81000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.36333333333333\n",
      "    ram_util_percent: 55.45666666666668\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06238869151679202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.79317121804308\n",
      "    mean_inference_ms: 2.3308201485493085\n",
      "    mean_raw_obs_processing_ms: 0.5571934484379354\n",
      "  time_since_restore: 2046.7489182949066\n",
      "  time_this_iter_s: 20.961522579193115\n",
      "  time_total_s: 2046.7489182949066\n",
      "  timers:\n",
      "    learn_throughput: 297.518\n",
      "    learn_time_ms: 3361.146\n",
      "    load_throughput: 22509.432\n",
      "    load_time_ms: 44.426\n",
      "    sample_throughput: 54.338\n",
      "    sample_time_ms: 18403.285\n",
      "    update_time_ms: 8.488\n",
      "  timestamp: 1631880374\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 81000\n",
      "  training_iteration: 81\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         2046.75</td><td style=\"text-align: right;\">81000</td><td style=\"text-align: right;\">-0.160494</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.074</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 82000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-06-37\n",
      "  done: false\n",
      "  episode_len_mean: 997.109756097561\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.15853658536585366\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 82\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.599143491850959\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010127068659953355\n",
      "          policy_loss: -0.010222329820195834\n",
      "          total_loss: -0.032569019724097516\n",
      "          vf_explained_var: -0.4360887408256531\n",
      "          vf_loss: 0.0017221841505185391\n",
      "    num_agent_steps_sampled: 82000\n",
      "    num_agent_steps_trained: 82000\n",
      "    num_steps_sampled: 82000\n",
      "    num_steps_trained: 82000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.22812499999999\n",
      "    ram_util_percent: 55.521874999999994\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06239440980446674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.608165256448984\n",
      "    mean_inference_ms: 2.330948599371362\n",
      "    mean_raw_obs_processing_ms: 0.5588394402297561\n",
      "  time_since_restore: 2069.3562684059143\n",
      "  time_this_iter_s: 22.60735011100769\n",
      "  time_total_s: 2069.3562684059143\n",
      "  timers:\n",
      "    learn_throughput: 298.18\n",
      "    learn_time_ms: 3353.685\n",
      "    load_throughput: 21434.812\n",
      "    load_time_ms: 46.653\n",
      "    sample_throughput: 53.924\n",
      "    sample_time_ms: 18544.702\n",
      "    update_time_ms: 8.158\n",
      "  timestamp: 1631880397\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 82000\n",
      "  training_iteration: 82\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         2069.36</td><td style=\"text-align: right;\">82000</td><td style=\"text-align: right;\">-0.158537</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">            997.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 83000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-06-58\n",
      "  done: false\n",
      "  episode_len_mean: 997.144578313253\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.1566265060240964\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 83\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5844804207483927\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007499080314881487\n",
      "          policy_loss: 0.033319500150779884\n",
      "          total_loss: 0.0093902756460011\n",
      "          vf_explained_var: -0.8462010025978088\n",
      "          vf_loss: 0.0004919250564449208\n",
      "    num_agent_steps_sampled: 83000\n",
      "    num_agent_steps_trained: 83000\n",
      "    num_steps_sampled: 83000\n",
      "    num_steps_trained: 83000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.65333333333332\n",
      "    ram_util_percent: 55.49666666666666\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062400027580416115\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.427112005966144\n",
      "    mean_inference_ms: 2.3310777841540244\n",
      "    mean_raw_obs_processing_ms: 0.560379499052843\n",
      "  time_since_restore: 2090.609765768051\n",
      "  time_this_iter_s: 21.25349736213684\n",
      "  time_total_s: 2090.609765768051\n",
      "  timers:\n",
      "    learn_throughput: 297.167\n",
      "    learn_time_ms: 3365.114\n",
      "    load_throughput: 22249.651\n",
      "    load_time_ms: 44.945\n",
      "    sample_throughput: 53.687\n",
      "    sample_time_ms: 18626.409\n",
      "    update_time_ms: 7.966\n",
      "  timestamp: 1631880418\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 83000\n",
      "  training_iteration: 83\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         2090.61</td><td style=\"text-align: right;\">83000</td><td style=\"text-align: right;\">-0.156627</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.145</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-07-20\n",
      "  done: false\n",
      "  episode_len_mean: 997.1785714285714\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.15476190476190477\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 84\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5901665528615316\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011607688712689829\n",
      "          policy_loss: -0.062297636167042786\n",
      "          total_loss: -0.08402547396512496\n",
      "          vf_explained_var: -0.6153408288955688\n",
      "          vf_loss: 0.0019701822732006097\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.05483870967743\n",
      "    ram_util_percent: 55.641935483870974\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062405280387095016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.24988482477997\n",
      "    mean_inference_ms: 2.3312076687416\n",
      "    mean_raw_obs_processing_ms: 0.561815348450074\n",
      "  time_since_restore: 2111.9406542778015\n",
      "  time_this_iter_s: 21.330888509750366\n",
      "  time_total_s: 2111.9406542778015\n",
      "  timers:\n",
      "    learn_throughput: 295.755\n",
      "    learn_time_ms: 3381.176\n",
      "    load_throughput: 22438.361\n",
      "    load_time_ms: 44.567\n",
      "    sample_throughput: 53.416\n",
      "    sample_time_ms: 18720.961\n",
      "    update_time_ms: 7.353\n",
      "  timestamp: 1631880440\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 84\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         2111.94</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">-0.154762</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.179</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 85000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-07-43\n",
      "  done: false\n",
      "  episode_len_mean: 997.2117647058824\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.15294117647058825\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 85\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5535075558556453\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007302621317655028\n",
      "          policy_loss: -0.05762269020908409\n",
      "          total_loss: -0.08020128450459904\n",
      "          vf_explained_var: -0.6017072796821594\n",
      "          vf_loss: 0.0015701238638333355\n",
      "    num_agent_steps_sampled: 85000\n",
      "    num_agent_steps_trained: 85000\n",
      "    num_steps_sampled: 85000\n",
      "    num_steps_trained: 85000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.69393939393939\n",
      "    ram_util_percent: 55.75454545454545\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0624100166375762\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.07665650546355\n",
      "    mean_inference_ms: 2.3313288427576495\n",
      "    mean_raw_obs_processing_ms: 0.5631525880339894\n",
      "  time_since_restore: 2135.173333644867\n",
      "  time_this_iter_s: 23.23267936706543\n",
      "  time_total_s: 2135.173333644867\n",
      "  timers:\n",
      "    learn_throughput: 297.256\n",
      "    learn_time_ms: 3364.109\n",
      "    load_throughput: 22292.803\n",
      "    load_time_ms: 44.858\n",
      "    sample_throughput: 53.004\n",
      "    sample_time_ms: 18866.626\n",
      "    update_time_ms: 6.78\n",
      "  timestamp: 1631880463\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 85000\n",
      "  training_iteration: 85\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         2135.17</td><td style=\"text-align: right;\">85000</td><td style=\"text-align: right;\">-0.152941</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.212</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 86000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-08-04\n",
      "  done: false\n",
      "  episode_len_mean: 997.2441860465116\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.16279069767441862\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 86\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5879243930180869\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014863628071148858\n",
      "          policy_loss: 0.004015587601396773\n",
      "          total_loss: 0.02721075142423312\n",
      "          vf_explained_var: 0.5836988091468811\n",
      "          vf_loss: 0.03625263834579123\n",
      "    num_agent_steps_sampled: 86000\n",
      "    num_agent_steps_trained: 86000\n",
      "    num_steps_sampled: 86000\n",
      "    num_steps_trained: 86000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.48666666666668\n",
      "    ram_util_percent: 55.63666666666667\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06241426509201204\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.90695271458128\n",
      "    mean_inference_ms: 2.3314363193563454\n",
      "    mean_raw_obs_processing_ms: 0.5643962245457538\n",
      "  time_since_restore: 2155.8573496341705\n",
      "  time_this_iter_s: 20.68401598930359\n",
      "  time_total_s: 2155.8573496341705\n",
      "  timers:\n",
      "    learn_throughput: 299.235\n",
      "    learn_time_ms: 3341.857\n",
      "    load_throughput: 22603.2\n",
      "    load_time_ms: 44.242\n",
      "    sample_throughput: 53.529\n",
      "    sample_time_ms: 18681.407\n",
      "    update_time_ms: 6.735\n",
      "  timestamp: 1631880484\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 86000\n",
      "  training_iteration: 86\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         2155.86</td><td style=\"text-align: right;\">86000</td><td style=\"text-align: right;\">-0.162791</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.244</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 87000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-08-26\n",
      "  done: false\n",
      "  episode_len_mean: 997.2758620689655\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.16091954022988506\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 87\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4648949517144096\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013747318157107951\n",
      "          policy_loss: 0.036554096080362794\n",
      "          total_loss: 0.016068021787537468\n",
      "          vf_explained_var: -0.20082828402519226\n",
      "          vf_loss: 0.0015530292733779384\n",
      "    num_agent_steps_sampled: 87000\n",
      "    num_agent_steps_trained: 87000\n",
      "    num_steps_sampled: 87000\n",
      "    num_steps_trained: 87000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.73125\n",
      "    ram_util_percent: 55.646874999999994\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062418091187906845\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.740853638758516\n",
      "    mean_inference_ms: 2.3315427310980774\n",
      "    mean_raw_obs_processing_ms: 0.5655543667605951\n",
      "  time_since_restore: 2178.2634654045105\n",
      "  time_this_iter_s: 22.406115770339966\n",
      "  time_total_s: 2178.2634654045105\n",
      "  timers:\n",
      "    learn_throughput: 300.217\n",
      "    learn_time_ms: 3330.922\n",
      "    load_throughput: 22732.874\n",
      "    load_time_ms: 43.989\n",
      "    sample_throughput: 54.817\n",
      "    sample_time_ms: 18242.407\n",
      "    update_time_ms: 6.97\n",
      "  timestamp: 1631880506\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 87000\n",
      "  training_iteration: 87\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         2178.26</td><td style=\"text-align: right;\">87000</td><td style=\"text-align: right;\">-0.16092</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.276</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-08-49\n",
      "  done: false\n",
      "  episode_len_mean: 997.3068181818181\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.1590909090909091\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 88\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.551800553003947\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004263292353270545\n",
      "          policy_loss: 0.0022775776477323637\n",
      "          total_loss: -0.020095433036072387\n",
      "          vf_explained_var: -0.5439303517341614\n",
      "          vf_loss: 0.002335634621946762\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.48181818181817\n",
      "    ram_util_percent: 55.506060606060615\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06242144079048451\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.57836388801086\n",
      "    mean_inference_ms: 2.33164032746928\n",
      "    mean_raw_obs_processing_ms: 0.566631403277097\n",
      "  time_since_restore: 2201.311576128006\n",
      "  time_this_iter_s: 23.048110723495483\n",
      "  time_total_s: 2201.311576128006\n",
      "  timers:\n",
      "    learn_throughput: 302.683\n",
      "    learn_time_ms: 3303.786\n",
      "    load_throughput: 22875.543\n",
      "    load_time_ms: 43.715\n",
      "    sample_throughput: 54.479\n",
      "    sample_time_ms: 18355.62\n",
      "    update_time_ms: 6.547\n",
      "  timestamp: 1631880529\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 88\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         2201.31</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">-0.159091</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.307</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 89000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-09-11\n",
      "  done: false\n",
      "  episode_len_mean: 997.3370786516854\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.15730337078651685\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 89\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09492187500000004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.862240109178755\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02807579839754687\n",
      "          policy_loss: -0.1246907057861487\n",
      "          total_loss: -0.126204647620519\n",
      "          vf_explained_var: 0.2639685571193695\n",
      "          vf_loss: 0.014443451140282882\n",
      "    num_agent_steps_sampled: 89000\n",
      "    num_agent_steps_trained: 89000\n",
      "    num_steps_sampled: 89000\n",
      "    num_steps_trained: 89000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.85666666666664\n",
      "    ram_util_percent: 55.62\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062424665734789817\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.419103367072466\n",
      "    mean_inference_ms: 2.3317444869863726\n",
      "    mean_raw_obs_processing_ms: 0.5676286094660524\n",
      "  time_since_restore: 2222.707394361496\n",
      "  time_this_iter_s: 21.39581823348999\n",
      "  time_total_s: 2222.707394361496\n",
      "  timers:\n",
      "    learn_throughput: 300.762\n",
      "    learn_time_ms: 3324.886\n",
      "    load_throughput: 22115.336\n",
      "    load_time_ms: 45.217\n",
      "    sample_throughput: 54.08\n",
      "    sample_time_ms: 18491.168\n",
      "    update_time_ms: 6.495\n",
      "  timestamp: 1631880551\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 89000\n",
      "  training_iteration: 89\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         2222.71</td><td style=\"text-align: right;\">89000</td><td style=\"text-align: right;\">-0.157303</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           997.337</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 90000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-09-59\n",
      "  done: false\n",
      "  episode_len_mean: 996.0333333333333\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.15555555555555556\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 90\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.512342291408115\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01054850365601805\n",
      "          policy_loss: -0.044465203852289255\n",
      "          total_loss: -0.06561124492436647\n",
      "          vf_explained_var: -0.3280496299266815\n",
      "          vf_loss: 0.0024754543284264702\n",
      "    num_agent_steps_sampled: 90000\n",
      "    num_agent_steps_trained: 90000\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.34705882352944\n",
      "    ram_util_percent: 55.642647058823535\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06242775831037693\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.26337847189544\n",
      "    mean_inference_ms: 2.331853041155059\n",
      "    mean_raw_obs_processing_ms: 0.5714243327661995\n",
      "  time_since_restore: 2270.608332633972\n",
      "  time_this_iter_s: 47.900938272476196\n",
      "  time_total_s: 2270.608332633972\n",
      "  timers:\n",
      "    learn_throughput: 300.874\n",
      "    learn_time_ms: 3323.65\n",
      "    load_throughput: 21429.709\n",
      "    load_time_ms: 46.664\n",
      "    sample_throughput: 47.4\n",
      "    sample_time_ms: 21097.25\n",
      "    update_time_ms: 6.521\n",
      "  timestamp: 1631880599\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 90\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         2270.61</td><td style=\"text-align: right;\">90000</td><td style=\"text-align: right;\">-0.155556</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.033</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 91000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-10-22\n",
      "  done: false\n",
      "  episode_len_mean: 996.0769230769231\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.15384615384615385\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 91\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3378765079710218\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013296604901088829\n",
      "          policy_loss: -0.03418432409978575\n",
      "          total_loss: -0.05355495251715183\n",
      "          vf_explained_var: -0.5038247108459473\n",
      "          vf_loss: 0.0021149283675994312\n",
      "    num_agent_steps_sampled: 91000\n",
      "    num_agent_steps_trained: 91000\n",
      "    num_steps_sampled: 91000\n",
      "    num_steps_trained: 91000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.39393939393939\n",
      "    ram_util_percent: 55.824242424242435\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0624310696603718\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.1108622299207\n",
      "    mean_inference_ms: 2.3319705906609345\n",
      "    mean_raw_obs_processing_ms: 0.5750538648743408\n",
      "  time_since_restore: 2293.416888475418\n",
      "  time_this_iter_s: 22.808555841445923\n",
      "  time_total_s: 2293.416888475418\n",
      "  timers:\n",
      "    learn_throughput: 302.439\n",
      "    learn_time_ms: 3306.456\n",
      "    load_throughput: 22048.095\n",
      "    load_time_ms: 45.355\n",
      "    sample_throughput: 46.952\n",
      "    sample_time_ms: 21298.343\n",
      "    update_time_ms: 7.481\n",
      "  timestamp: 1631880622\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 91000\n",
      "  training_iteration: 91\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         2293.42</td><td style=\"text-align: right;\">91000</td><td style=\"text-align: right;\">-0.153846</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.077</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-10-41\n",
      "  done: false\n",
      "  episode_len_mean: 996.1195652173913\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.15217391304347827\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 92\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6115903509987723\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012788253404188212\n",
      "          policy_loss: -0.016317624412477017\n",
      "          total_loss: -0.03992792123721706\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.000684778971481137\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.16785714285716\n",
      "    ram_util_percent: 55.964285714285715\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06243446392508916\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.961070379325218\n",
      "    mean_inference_ms: 2.332087340017693\n",
      "    mean_raw_obs_processing_ms: 0.578530060627422\n",
      "  time_since_restore: 2312.9799304008484\n",
      "  time_this_iter_s: 19.563041925430298\n",
      "  time_total_s: 2312.9799304008484\n",
      "  timers:\n",
      "    learn_throughput: 301.257\n",
      "    learn_time_ms: 3319.427\n",
      "    load_throughput: 22370.862\n",
      "    load_time_ms: 44.701\n",
      "    sample_throughput: 47.661\n",
      "    sample_time_ms: 20981.396\n",
      "    update_time_ms: 7.711\n",
      "  timestamp: 1631880641\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 92\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         2312.98</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">-0.152174</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">            996.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 93000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-11-04\n",
      "  done: false\n",
      "  episode_len_mean: 996.1612903225806\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.15053763440860216\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 93\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6231721772087946\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009976965842619891\n",
      "          policy_loss: -0.01748005830579334\n",
      "          total_loss: -0.04120165660149521\n",
      "          vf_explained_var: -0.5270453691482544\n",
      "          vf_loss: 0.001089575985517715\n",
      "    num_agent_steps_sampled: 93000\n",
      "    num_agent_steps_trained: 93000\n",
      "    num_steps_sampled: 93000\n",
      "    num_steps_trained: 93000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.80937499999999\n",
      "    ram_util_percent: 55.74375\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06243778018754056\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.814295886642597\n",
      "    mean_inference_ms: 2.332205406982438\n",
      "    mean_raw_obs_processing_ms: 0.581854339659957\n",
      "  time_since_restore: 2335.656139612198\n",
      "  time_this_iter_s: 22.676209211349487\n",
      "  time_total_s: 2335.656139612198\n",
      "  timers:\n",
      "    learn_throughput: 302.122\n",
      "    learn_time_ms: 3309.921\n",
      "    load_throughput: 21519.757\n",
      "    load_time_ms: 46.469\n",
      "    sample_throughput: 47.323\n",
      "    sample_time_ms: 21131.49\n",
      "    update_time_ms: 7.65\n",
      "  timestamp: 1631880664\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 93000\n",
      "  training_iteration: 93\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         2335.66</td><td style=\"text-align: right;\">93000</td><td style=\"text-align: right;\">-0.150538</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.161</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 94000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-11-25\n",
      "  done: false\n",
      "  episode_len_mean: 996.2021276595744\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.14893617021276595\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 94\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.529301243358188\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011303815605560569\n",
      "          policy_loss: -0.0864819327990214\n",
      "          total_loss: -0.10834221740563711\n",
      "          vf_explained_var: -0.6209126710891724\n",
      "          vf_loss: 0.00182325813996916\n",
      "    num_agent_steps_sampled: 94000\n",
      "    num_agent_steps_trained: 94000\n",
      "    num_steps_sampled: 94000\n",
      "    num_steps_trained: 94000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.06129032258063\n",
      "    ram_util_percent: 55.78387096774193\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06244142769355807\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.67030150869905\n",
      "    mean_inference_ms: 2.332324410642002\n",
      "    mean_raw_obs_processing_ms: 0.585036099237996\n",
      "  time_since_restore: 2357.212571620941\n",
      "  time_this_iter_s: 21.556432008743286\n",
      "  time_total_s: 2357.212571620941\n",
      "  timers:\n",
      "    learn_throughput: 301.736\n",
      "    learn_time_ms: 3314.158\n",
      "    load_throughput: 21259.846\n",
      "    load_time_ms: 47.037\n",
      "    sample_throughput: 47.285\n",
      "    sample_time_ms: 21148.216\n",
      "    update_time_ms: 7.765\n",
      "  timestamp: 1631880685\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 94000\n",
      "  training_iteration: 94\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         2357.21</td><td style=\"text-align: right;\">94000</td><td style=\"text-align: right;\">-0.148936</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.202</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 95000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-11-47\n",
      "  done: false\n",
      "  episode_len_mean: 996.2421052631579\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.14736842105263157\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 95\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7282590044869317\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01093716756770162\n",
      "          policy_loss: -0.08470574178629452\n",
      "          total_loss: -0.1101939257648256\n",
      "          vf_explained_var: -0.9687694907188416\n",
      "          vf_loss: 0.00023713670088909566\n",
      "    num_agent_steps_sampled: 95000\n",
      "    num_agent_steps_trained: 95000\n",
      "    num_steps_sampled: 95000\n",
      "    num_steps_trained: 95000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.5875\n",
      "    ram_util_percent: 55.962500000000006\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06244596033986463\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.5290639203101\n",
      "    mean_inference_ms: 2.332441995731955\n",
      "    mean_raw_obs_processing_ms: 0.5880768973429824\n",
      "  time_since_restore: 2379.172951221466\n",
      "  time_this_iter_s: 21.960379600524902\n",
      "  time_total_s: 2379.172951221466\n",
      "  timers:\n",
      "    learn_throughput: 300.129\n",
      "    learn_time_ms: 3331.9\n",
      "    load_throughput: 21296.44\n",
      "    load_time_ms: 46.956\n",
      "    sample_throughput: 47.611\n",
      "    sample_time_ms: 21003.379\n",
      "    update_time_ms: 7.729\n",
      "  timestamp: 1631880707\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 95000\n",
      "  training_iteration: 95\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         2379.17</td><td style=\"text-align: right;\">95000</td><td style=\"text-align: right;\">-0.147368</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.242</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-12-08\n",
      "  done: false\n",
      "  episode_len_mean: 996.28125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.14583333333333334\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 96\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.597190899319119\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010360204543609234\n",
      "          policy_loss: -0.0689450555998418\n",
      "          total_loss: -0.09245996433827612\n",
      "          vf_explained_var: -0.9270338416099548\n",
      "          vf_loss: 0.0009818850685809999\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.4896551724138\n",
      "    ram_util_percent: 55.77586206896551\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06245021287804214\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.390394477411434\n",
      "    mean_inference_ms: 2.332553263094528\n",
      "    mean_raw_obs_processing_ms: 0.5909831708515935\n",
      "  time_since_restore: 2400.0239491462708\n",
      "  time_this_iter_s: 20.850997924804688\n",
      "  time_total_s: 2400.0239491462708\n",
      "  timers:\n",
      "    learn_throughput: 299.102\n",
      "    learn_time_ms: 3343.341\n",
      "    load_throughput: 21767.899\n",
      "    load_time_ms: 45.939\n",
      "    sample_throughput: 47.597\n",
      "    sample_time_ms: 21009.633\n",
      "    update_time_ms: 7.812\n",
      "  timestamp: 1631880728\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 96\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         2400.02</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">-0.145833</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.281</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 97000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-12-30\n",
      "  done: false\n",
      "  episode_len_mean: 996.319587628866\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.14432989690721648\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 97\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5748609251446193\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009554114888837834\n",
      "          policy_loss: -0.04073672856514653\n",
      "          total_loss: -0.064096922479156\n",
      "          vf_explained_var: -0.5242765545845032\n",
      "          vf_loss: 0.0010280759392319143\n",
      "    num_agent_steps_sampled: 97000\n",
      "    num_agent_steps_trained: 97000\n",
      "    num_steps_sampled: 97000\n",
      "    num_steps_trained: 97000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.87741935483872\n",
      "    ram_util_percent: 55.92258064516128\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06245470912964609\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.254284761617523\n",
      "    mean_inference_ms: 2.332666684878887\n",
      "    mean_raw_obs_processing_ms: 0.5937624288216569\n",
      "  time_since_restore: 2421.512412548065\n",
      "  time_this_iter_s: 21.488463401794434\n",
      "  time_total_s: 2421.512412548065\n",
      "  timers:\n",
      "    learn_throughput: 300.066\n",
      "    learn_time_ms: 3332.595\n",
      "    load_throughput: 21593.377\n",
      "    load_time_ms: 46.31\n",
      "    sample_throughput: 47.781\n",
      "    sample_time_ms: 20928.703\n",
      "    update_time_ms: 7.552\n",
      "  timestamp: 1631880750\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 97000\n",
      "  training_iteration: 97\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         2421.51</td><td style=\"text-align: right;\">97000</td><td style=\"text-align: right;\">-0.14433</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">            996.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 98000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-12-51\n",
      "  done: false\n",
      "  episode_len_mean: 996.3571428571429\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.14285714285714285\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 98\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6256618552737767\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01371157633164195\n",
      "          policy_loss: 0.043156652980380586\n",
      "          total_loss: 0.019874976244237688\n",
      "          vf_explained_var: -0.6642675995826721\n",
      "          vf_loss: 0.001022648128047068\n",
      "    num_agent_steps_sampled: 98000\n",
      "    num_agent_steps_trained: 98000\n",
      "    num_steps_sampled: 98000\n",
      "    num_steps_trained: 98000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.67096774193547\n",
      "    ram_util_percent: 55.980645161290326\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06245935006443357\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.12064059605456\n",
      "    mean_inference_ms: 2.3327828547298655\n",
      "    mean_raw_obs_processing_ms: 0.5964192203844043\n",
      "  time_since_restore: 2442.926000356674\n",
      "  time_this_iter_s: 21.41358780860901\n",
      "  time_total_s: 2442.926000356674\n",
      "  timers:\n",
      "    learn_throughput: 297.831\n",
      "    learn_time_ms: 3357.61\n",
      "    load_throughput: 21821.33\n",
      "    load_time_ms: 45.827\n",
      "    sample_throughput: 48.215\n",
      "    sample_time_ms: 20740.547\n",
      "    update_time_ms: 7.649\n",
      "  timestamp: 1631880771\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 98000\n",
      "  training_iteration: 98\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         2442.93</td><td style=\"text-align: right;\">98000</td><td style=\"text-align: right;\">-0.142857</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.357</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 99000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-13-14\n",
      "  done: false\n",
      "  episode_len_mean: 996.3939393939394\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.1414141414141414\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 99\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.443727069430881\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015180123717132473\n",
      "          policy_loss: -0.004589583145247565\n",
      "          total_loss: -0.02541354956726233\n",
      "          vf_explained_var: -0.7978007197380066\n",
      "          vf_loss: 0.0014519160561677483\n",
      "    num_agent_steps_sampled: 99000\n",
      "    num_agent_steps_trained: 99000\n",
      "    num_steps_sampled: 99000\n",
      "    num_steps_trained: 99000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.94545454545454\n",
      "    ram_util_percent: 55.89999999999999\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06246394154846485\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.98956485194497\n",
      "    mean_inference_ms: 2.3329008847992125\n",
      "    mean_raw_obs_processing_ms: 0.5989571232023225\n",
      "  time_since_restore: 2465.9992661476135\n",
      "  time_this_iter_s: 23.07326579093933\n",
      "  time_total_s: 2465.9992661476135\n",
      "  timers:\n",
      "    learn_throughput: 298.735\n",
      "    learn_time_ms: 3347.451\n",
      "    load_throughput: 21595.045\n",
      "    load_time_ms: 46.307\n",
      "    sample_throughput: 47.813\n",
      "    sample_time_ms: 20914.907\n",
      "    update_time_ms: 8.033\n",
      "  timestamp: 1631880794\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 99000\n",
      "  training_iteration: 99\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">            2466</td><td style=\"text-align: right;\">99000</td><td style=\"text-align: right;\">-0.141414</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">           996.394</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-13-37\n",
      "  done: false\n",
      "  episode_len_mean: 996.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.14\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 100\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3950306627485487\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011573837640274736\n",
      "          policy_loss: -0.043028517233000864\n",
      "          total_loss: -0.06447875483168496\n",
      "          vf_explained_var: -0.6765676140785217\n",
      "          vf_loss: 0.0008521546250752483\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.36875\n",
      "    ram_util_percent: 56.0375\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06246853947673805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.860908524149988\n",
      "    mean_inference_ms: 2.333023639452804\n",
      "    mean_raw_obs_processing_ms: 0.6013816402880647\n",
      "  time_since_restore: 2488.3167386054993\n",
      "  time_this_iter_s: 22.317472457885742\n",
      "  time_total_s: 2488.3167386054993\n",
      "  timers:\n",
      "    learn_throughput: 299.079\n",
      "    learn_time_ms: 3343.599\n",
      "    load_throughput: 22071.091\n",
      "    load_time_ms: 45.308\n",
      "    sample_throughput: 54.463\n",
      "    sample_time_ms: 18361.116\n",
      "    update_time_ms: 8.039\n",
      "  timestamp: 1631880817\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 100\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         2488.32</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">   -0.14</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">            996.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 101000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-13-59\n",
      "  done: false\n",
      "  episode_len_mean: 996.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.14\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 101\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.437473177909851\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014073381750692802\n",
      "          policy_loss: -0.01898759798043304\n",
      "          total_loss: -0.04005986009207037\n",
      "          vf_explained_var: -0.9911442399024963\n",
      "          vf_loss: 0.001298660193828659\n",
      "    num_agent_steps_sampled: 101000\n",
      "    num_agent_steps_trained: 101000\n",
      "    num_steps_sampled: 101000\n",
      "    num_steps_trained: 101000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.215625\n",
      "    ram_util_percent: 55.88125000000001\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06244199328925447\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.845123649378394\n",
      "    mean_inference_ms: 2.3315404022810564\n",
      "    mean_raw_obs_processing_ms: 0.6063057846518687\n",
      "  time_since_restore: 2510.860783100128\n",
      "  time_this_iter_s: 22.544044494628906\n",
      "  time_total_s: 2510.860783100128\n",
      "  timers:\n",
      "    learn_throughput: 299.518\n",
      "    learn_time_ms: 3338.7\n",
      "    load_throughput: 21508.237\n",
      "    load_time_ms: 46.494\n",
      "    sample_throughput: 54.525\n",
      "    sample_time_ms: 18340.145\n",
      "    update_time_ms: 7.182\n",
      "  timestamp: 1631880839\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 101000\n",
      "  training_iteration: 101\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         2510.86</td><td style=\"text-align: right;\">101000</td><td style=\"text-align: right;\">   -0.14</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">            996.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 102000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-14-23\n",
      "  done: false\n",
      "  episode_len_mean: 996.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.11\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 102\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.400231358740065\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011387287457032554\n",
      "          policy_loss: -0.05097810071375635\n",
      "          total_loss: -0.07185932066705492\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0014997381173695128\n",
      "    num_agent_steps_sampled: 102000\n",
      "    num_agent_steps_trained: 102000\n",
      "    num_steps_sampled: 102000\n",
      "    num_steps_trained: 102000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.55151515151515\n",
      "    ram_util_percent: 55.90909090909091\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062422613304772924\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.352941796392752\n",
      "    mean_inference_ms: 2.330687062492825\n",
      "    mean_raw_obs_processing_ms: 0.611724678490766\n",
      "  time_since_restore: 2534.0204174518585\n",
      "  time_this_iter_s: 23.159634351730347\n",
      "  time_total_s: 2534.0204174518585\n",
      "  timers:\n",
      "    learn_throughput: 299.95\n",
      "    learn_time_ms: 3333.893\n",
      "    load_throughput: 21953.477\n",
      "    load_time_ms: 45.551\n",
      "    sample_throughput: 53.459\n",
      "    sample_time_ms: 18705.752\n",
      "    update_time_ms: 6.993\n",
      "  timestamp: 1631880863\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 102000\n",
      "  training_iteration: 102\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         2534.02</td><td style=\"text-align: right;\">102000</td><td style=\"text-align: right;\">   -0.11</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">            996.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 103000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-14-46\n",
      "  done: false\n",
      "  episode_len_mean: 996.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 103\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.449418558014764\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010848966769631355\n",
      "          policy_loss: -0.010474602174427774\n",
      "          total_loss: -0.03251426236497031\n",
      "          vf_explained_var: -0.4952675998210907\n",
      "          vf_loss: 0.0009098165733222333\n",
      "    num_agent_steps_sampled: 103000\n",
      "    num_agent_steps_trained: 103000\n",
      "    num_steps_sampled: 103000\n",
      "    num_steps_trained: 103000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.93333333333334\n",
      "    ram_util_percent: 55.97575757575758\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06240876321869354\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.361796923542066\n",
      "    mean_inference_ms: 2.3299143303264254\n",
      "    mean_raw_obs_processing_ms: 0.6172613158209654\n",
      "  time_since_restore: 2557.5547184944153\n",
      "  time_this_iter_s: 23.534301042556763\n",
      "  time_total_s: 2557.5547184944153\n",
      "  timers:\n",
      "    learn_throughput: 297.769\n",
      "    learn_time_ms: 3358.311\n",
      "    load_throughput: 22487.214\n",
      "    load_time_ms: 44.47\n",
      "    sample_throughput: 53.284\n",
      "    sample_time_ms: 18767.478\n",
      "    update_time_ms: 7.412\n",
      "  timestamp: 1631880886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 103000\n",
      "  training_iteration: 103\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         2557.55</td><td style=\"text-align: right;\">103000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-15-11\n",
      "  done: false\n",
      "  episode_len_mean: 996.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 104\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3971771028306748\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010057135934312199\n",
      "          policy_loss: -0.0492396314524942\n",
      "          total_loss: -0.07026630114350053\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0015131378067760831\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.31666666666666\n",
      "    ram_util_percent: 55.922222222222224\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06239988241229896\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.62620698582667\n",
      "    mean_inference_ms: 2.32937112628199\n",
      "    mean_raw_obs_processing_ms: 0.6227962221165415\n",
      "  time_since_restore: 2582.3627927303314\n",
      "  time_this_iter_s: 24.808074235916138\n",
      "  time_total_s: 2582.3627927303314\n",
      "  timers:\n",
      "    learn_throughput: 298.624\n",
      "    learn_time_ms: 3348.693\n",
      "    load_throughput: 22944.846\n",
      "    load_time_ms: 43.583\n",
      "    sample_throughput: 52.344\n",
      "    sample_time_ms: 19104.287\n",
      "    update_time_ms: 7.255\n",
      "  timestamp: 1631880911\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 104\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         2582.36</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 105000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-15-35\n",
      "  done: false\n",
      "  episode_len_mean: 996.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 105\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.445719567934672\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01062537140738427\n",
      "          policy_loss: -0.02658760998811987\n",
      "          total_loss: -0.04884598371055391\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.000685951970000234\n",
      "    num_agent_steps_sampled: 105000\n",
      "    num_agent_steps_trained: 105000\n",
      "    num_steps_sampled: 105000\n",
      "    num_steps_trained: 105000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.35999999999999\n",
      "    ram_util_percent: 55.825714285714284\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06239342692056063\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.041465204598566\n",
      "    mean_inference_ms: 2.3290293151472716\n",
      "    mean_raw_obs_processing_ms: 0.6283249591565634\n",
      "  time_since_restore: 2606.6329550743103\n",
      "  time_this_iter_s: 24.270162343978882\n",
      "  time_total_s: 2606.6329550743103\n",
      "  timers:\n",
      "    learn_throughput: 298.008\n",
      "    learn_time_ms: 3355.61\n",
      "    load_throughput: 23114.5\n",
      "    load_time_ms: 43.263\n",
      "    sample_throughput: 51.738\n",
      "    sample_time_ms: 19328.291\n",
      "    update_time_ms: 7.298\n",
      "  timestamp: 1631880935\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 105000\n",
      "  training_iteration: 105\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         2606.63</td><td style=\"text-align: right;\">105000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 106000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-15-59\n",
      "  done: false\n",
      "  episode_len_mean: 996.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 106\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3610691600375704\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013947366054081797\n",
      "          policy_loss: -0.07662792404492696\n",
      "          total_loss: -0.09692705762055186\n",
      "          vf_explained_var: -0.4752286374568939\n",
      "          vf_loss: 0.0013256908633694467\n",
      "    num_agent_steps_sampled: 106000\n",
      "    num_agent_steps_trained: 106000\n",
      "    num_steps_sampled: 106000\n",
      "    num_steps_trained: 106000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.53333333333333\n",
      "    ram_util_percent: 55.96363636363637\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062393729264517354\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.55818369672909\n",
      "    mean_inference_ms: 2.3287436006515856\n",
      "    mean_raw_obs_processing_ms: 0.6338875046346761\n",
      "  time_since_restore: 2630.06028342247\n",
      "  time_this_iter_s: 23.42732834815979\n",
      "  time_total_s: 2630.06028342247\n",
      "  timers:\n",
      "    learn_throughput: 298.631\n",
      "    learn_time_ms: 3348.614\n",
      "    load_throughput: 22166.704\n",
      "    load_time_ms: 45.113\n",
      "    sample_throughput: 51.043\n",
      "    sample_time_ms: 19591.34\n",
      "    update_time_ms: 7.254\n",
      "  timestamp: 1631880959\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 106000\n",
      "  training_iteration: 106\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         2630.06</td><td style=\"text-align: right;\">106000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 107000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-16-20\n",
      "  done: false\n",
      "  episode_len_mean: 996.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 107\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4248368289735582\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009437246918515404\n",
      "          policy_loss: -0.10654901617930995\n",
      "          total_loss: -0.12868357089658577\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0007701107578921235\n",
      "    num_agent_steps_sampled: 107000\n",
      "    num_agent_steps_trained: 107000\n",
      "    num_steps_sampled: 107000\n",
      "    num_steps_trained: 107000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.82903225806452\n",
      "    ram_util_percent: 56.00322580645162\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062386479343867346\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.14702279537234\n",
      "    mean_inference_ms: 2.3285003119823706\n",
      "    mean_raw_obs_processing_ms: 0.6394282337416967\n",
      "  time_since_restore: 2651.453639268875\n",
      "  time_this_iter_s: 21.39335584640503\n",
      "  time_total_s: 2651.453639268875\n",
      "  timers:\n",
      "    learn_throughput: 299.198\n",
      "    learn_time_ms: 3342.269\n",
      "    load_throughput: 22739.887\n",
      "    load_time_ms: 43.976\n",
      "    sample_throughput: 51.055\n",
      "    sample_time_ms: 19586.688\n",
      "    update_time_ms: 8.377\n",
      "  timestamp: 1631880980\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 107000\n",
      "  training_iteration: 107\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         2651.45</td><td style=\"text-align: right;\">107000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-16-43\n",
      "  done: false\n",
      "  episode_len_mean: 996.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 108\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0410037928157383\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02125752388742986\n",
      "          policy_loss: -0.08330033847855198\n",
      "          total_loss: -0.06261907857325343\n",
      "          vf_explained_var: 0.4351070523262024\n",
      "          vf_loss: 0.038064589548028174\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.009375\n",
      "    ram_util_percent: 55.978125000000006\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06238503576327267\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.789870087106195\n",
      "    mean_inference_ms: 2.3283616298965963\n",
      "    mean_raw_obs_processing_ms: 0.6449604207765894\n",
      "  time_since_restore: 2674.4010438919067\n",
      "  time_this_iter_s: 22.947404623031616\n",
      "  time_total_s: 2674.4010438919067\n",
      "  timers:\n",
      "    learn_throughput: 299.512\n",
      "    learn_time_ms: 3338.762\n",
      "    load_throughput: 23297.431\n",
      "    load_time_ms: 42.923\n",
      "    sample_throughput: 50.648\n",
      "    sample_time_ms: 19743.992\n",
      "    update_time_ms: 8.944\n",
      "  timestamp: 1631881003\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 108\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">          2674.4</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 109000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-17-05\n",
      "  done: false\n",
      "  episode_len_mean: 996.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 109\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21357421875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.445371145672268\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009782441690906139\n",
      "          policy_loss: -0.009773344463772244\n",
      "          total_loss: -0.030494355327553218\n",
      "          vf_explained_var: -0.8929105401039124\n",
      "          vf_loss: 0.0016434229186011685\n",
      "    num_agent_steps_sampled: 109000\n",
      "    num_agent_steps_trained: 109000\n",
      "    num_steps_sampled: 109000\n",
      "    num_steps_trained: 109000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.8967741935484\n",
      "    ram_util_percent: 56.00967741935484\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062381755654683\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.474467305730062\n",
      "    mean_inference_ms: 2.3282670998608967\n",
      "    mean_raw_obs_processing_ms: 0.6504528561640619\n",
      "  time_since_restore: 2695.8044164180756\n",
      "  time_this_iter_s: 21.403372526168823\n",
      "  time_total_s: 2695.8044164180756\n",
      "  timers:\n",
      "    learn_throughput: 298.054\n",
      "    learn_time_ms: 3355.098\n",
      "    load_throughput: 23861.547\n",
      "    load_time_ms: 41.908\n",
      "    sample_throughput: 51.112\n",
      "    sample_time_ms: 19564.826\n",
      "    update_time_ms: 8.496\n",
      "  timestamp: 1631881025\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 109000\n",
      "  training_iteration: 109\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">          2695.8</td><td style=\"text-align: right;\">109000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 110000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-17-26\n",
      "  done: false\n",
      "  episode_len_mean: 996.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 110\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21357421875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5558599578009713\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011409046787479976\n",
      "          policy_loss: 0.020525161052743594\n",
      "          total_loss: -0.0017047274029917186\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0008920330994038119\n",
      "    num_agent_steps_sampled: 110000\n",
      "    num_agent_steps_trained: 110000\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.37666666666668\n",
      "    ram_util_percent: 56.10999999999999\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06238054179949412\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.19244651854623\n",
      "    mean_inference_ms: 2.3281641057250244\n",
      "    mean_raw_obs_processing_ms: 0.6558595682032293\n",
      "  time_since_restore: 2716.893082857132\n",
      "  time_this_iter_s: 21.088666439056396\n",
      "  time_total_s: 2716.893082857132\n",
      "  timers:\n",
      "    learn_throughput: 299.363\n",
      "    learn_time_ms: 3340.421\n",
      "    load_throughput: 24340.204\n",
      "    load_time_ms: 41.084\n",
      "    sample_throughput: 51.394\n",
      "    sample_time_ms: 19457.712\n",
      "    update_time_ms: 8.673\n",
      "  timestamp: 1631881046\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 110\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         2716.89</td><td style=\"text-align: right;\">110000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 111000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-17-47\n",
      "  done: false\n",
      "  episode_len_mean: 996.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 111\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21357421875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5189634482065837\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012090204302106767\n",
      "          policy_loss: -0.048534168303012847\n",
      "          total_loss: -0.07024448501567046\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0008971612046783169\n",
      "    num_agent_steps_sampled: 111000\n",
      "    num_agent_steps_trained: 111000\n",
      "    num_steps_sampled: 111000\n",
      "    num_steps_trained: 111000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.28387096774192\n",
      "    ram_util_percent: 55.958064516129035\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062381735302537586\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.93651055122443\n",
      "    mean_inference_ms: 2.328100183285349\n",
      "    mean_raw_obs_processing_ms: 0.661243669309295\n",
      "  time_since_restore: 2738.408723115921\n",
      "  time_this_iter_s: 21.515640258789062\n",
      "  time_total_s: 2738.408723115921\n",
      "  timers:\n",
      "    learn_throughput: 299.204\n",
      "    learn_time_ms: 3342.2\n",
      "    load_throughput: 25425.418\n",
      "    load_time_ms: 39.331\n",
      "    sample_throughput: 51.666\n",
      "    sample_time_ms: 19355.119\n",
      "    update_time_ms: 8.642\n",
      "  timestamp: 1631881067\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 111000\n",
      "  training_iteration: 111\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         2738.41</td><td style=\"text-align: right;\">111000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-18-10\n",
      "  done: false\n",
      "  episode_len_mean: 996.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 112\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21357421875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4473704126146103\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010531518075151503\n",
      "          policy_loss: 0.00040603743659125435\n",
      "          total_loss: -0.020472222090595297\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0013461831058116836\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.3125\n",
      "    ram_util_percent: 56.03125\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06238581380229236\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.70076265367713\n",
      "    mean_inference_ms: 2.328079284806588\n",
      "    mean_raw_obs_processing_ms: 0.6666052376255428\n",
      "  time_since_restore: 2760.824190378189\n",
      "  time_this_iter_s: 22.415467262268066\n",
      "  time_total_s: 2760.824190378189\n",
      "  timers:\n",
      "    learn_throughput: 298.481\n",
      "    learn_time_ms: 3350.301\n",
      "    load_throughput: 24547.717\n",
      "    load_time_ms: 40.737\n",
      "    sample_throughput: 51.892\n",
      "    sample_time_ms: 19270.831\n",
      "    update_time_ms: 9.068\n",
      "  timestamp: 1631881090\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 112\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         2760.82</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 113000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-18-32\n",
      "  done: false\n",
      "  episode_len_mean: 996.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 113\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21357421875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3859406338797675\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00834848841114323\n",
      "          policy_loss: -0.07417795504960749\n",
      "          total_loss: -0.09495690365632375\n",
      "          vf_explained_var: -0.8835589289665222\n",
      "          vf_loss: 0.001297433768114489\n",
      "    num_agent_steps_sampled: 113000\n",
      "    num_agent_steps_trained: 113000\n",
      "    num_steps_sampled: 113000\n",
      "    num_steps_trained: 113000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.16129032258064\n",
      "    ram_util_percent: 56.02903225806452\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062391844223911884\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.482249297391505\n",
      "    mean_inference_ms: 2.3280907729794995\n",
      "    mean_raw_obs_processing_ms: 0.6719301113057241\n",
      "  time_since_restore: 2782.7441947460175\n",
      "  time_this_iter_s: 21.92000436782837\n",
      "  time_total_s: 2782.7441947460175\n",
      "  timers:\n",
      "    learn_throughput: 299.167\n",
      "    learn_time_ms: 3342.618\n",
      "    load_throughput: 26387.369\n",
      "    load_time_ms: 37.897\n",
      "    sample_throughput: 52.301\n",
      "    sample_time_ms: 19119.944\n",
      "    update_time_ms: 8.703\n",
      "  timestamp: 1631881112\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 113000\n",
      "  training_iteration: 113\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         2782.74</td><td style=\"text-align: right;\">113000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 114000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-18-52\n",
      "  done: false\n",
      "  episode_len_mean: 996.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 114\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21357421875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3800995349884033\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01146774601683589\n",
      "          policy_loss: -0.09023776488999526\n",
      "          total_loss: -0.1103533337927527\n",
      "          vf_explained_var: -0.834629237651825\n",
      "          vf_loss: 0.0012362120039243665\n",
      "    num_agent_steps_sampled: 114000\n",
      "    num_agent_steps_trained: 114000\n",
      "    num_steps_sampled: 114000\n",
      "    num_steps_trained: 114000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.27333333333333\n",
      "    ram_util_percent: 56.06333333333332\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062397518123467534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.28156332573661\n",
      "    mean_inference_ms: 2.328097703492489\n",
      "    mean_raw_obs_processing_ms: 0.67720284234669\n",
      "  time_since_restore: 2803.401764392853\n",
      "  time_this_iter_s: 20.657569646835327\n",
      "  time_total_s: 2803.401764392853\n",
      "  timers:\n",
      "    learn_throughput: 299.676\n",
      "    learn_time_ms: 3336.942\n",
      "    load_throughput: 26873.679\n",
      "    load_time_ms: 37.211\n",
      "    sample_throughput: 53.444\n",
      "    sample_time_ms: 18711.161\n",
      "    update_time_ms: 8.729\n",
      "  timestamp: 1631881132\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 114000\n",
      "  training_iteration: 114\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">          2803.4</td><td style=\"text-align: right;\">114000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 115000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-19-16\n",
      "  done: false\n",
      "  episode_len_mean: 996.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 115\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21357421875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3574768013424343\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011144591132150774\n",
      "          policy_loss: -0.09405082940227455\n",
      "          total_loss: -0.11400963150792652\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0012357685908985635\n",
      "    num_agent_steps_sampled: 115000\n",
      "    num_agent_steps_trained: 115000\n",
      "    num_steps_sampled: 115000\n",
      "    num_steps_trained: 115000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.69999999999999\n",
      "    ram_util_percent: 56.087878787878786\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06240436675680093\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.096515801941088\n",
      "    mean_inference_ms: 2.3281246984472075\n",
      "    mean_raw_obs_processing_ms: 0.6824365603451815\n",
      "  time_since_restore: 2826.5094060897827\n",
      "  time_this_iter_s: 23.10764169692993\n",
      "  time_total_s: 2826.5094060897827\n",
      "  timers:\n",
      "    learn_throughput: 301.454\n",
      "    learn_time_ms: 3317.254\n",
      "    load_throughput: 27968.571\n",
      "    load_time_ms: 35.754\n",
      "    sample_throughput: 53.717\n",
      "    sample_time_ms: 18616.25\n",
      "    update_time_ms: 8.687\n",
      "  timestamp: 1631881156\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 115000\n",
      "  training_iteration: 115\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         2826.51</td><td style=\"text-align: right;\">115000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-19-37\n",
      "  done: false\n",
      "  episode_len_mean: 996.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 116\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21357421875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8898015247450934\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013397774594389547\n",
      "          policy_loss: -0.11173594792683919\n",
      "          total_loss: -0.11933159687452846\n",
      "          vf_explained_var: 0.3663696348667145\n",
      "          vf_loss: 0.008440946435762776\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.77999999999999\n",
      "    ram_util_percent: 56.04999999999999\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062413109184723786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.92603661071289\n",
      "    mean_inference_ms: 2.328206319734618\n",
      "    mean_raw_obs_processing_ms: 0.6876437919765447\n",
      "  time_since_restore: 2847.951604127884\n",
      "  time_this_iter_s: 21.442198038101196\n",
      "  time_total_s: 2847.951604127884\n",
      "  timers:\n",
      "    learn_throughput: 300.023\n",
      "    learn_time_ms: 3333.081\n",
      "    load_throughput: 28633.11\n",
      "    load_time_ms: 34.925\n",
      "    sample_throughput: 54.342\n",
      "    sample_time_ms: 18402.101\n",
      "    update_time_ms: 9.024\n",
      "  timestamp: 1631881177\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 116\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         2847.95</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 117000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-19-59\n",
      "  done: false\n",
      "  episode_len_mean: 996.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 117\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21357421875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.469111699528164\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0121672635629873\n",
      "          policy_loss: -0.07043854304485851\n",
      "          total_loss: -0.0909836760825581\n",
      "          vf_explained_var: -0.5555711984634399\n",
      "          vf_loss: 0.0015473710761095087\n",
      "    num_agent_steps_sampled: 117000\n",
      "    num_agent_steps_trained: 117000\n",
      "    num_steps_sampled: 117000\n",
      "    num_steps_trained: 117000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.24838709677421\n",
      "    ram_util_percent: 55.91935483870967\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062423330183680355\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.76779297707614\n",
      "    mean_inference_ms: 2.3283273407454503\n",
      "    mean_raw_obs_processing_ms: 0.6928061635448626\n",
      "  time_since_restore: 2869.6355905532837\n",
      "  time_this_iter_s: 21.68398642539978\n",
      "  time_total_s: 2869.6355905532837\n",
      "  timers:\n",
      "    learn_throughput: 299.838\n",
      "    learn_time_ms: 3335.138\n",
      "    load_throughput: 29481.503\n",
      "    load_time_ms: 33.92\n",
      "    sample_throughput: 54.255\n",
      "    sample_time_ms: 18431.433\n",
      "    update_time_ms: 8.671\n",
      "  timestamp: 1631881199\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 117000\n",
      "  training_iteration: 117\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         2869.64</td><td style=\"text-align: right;\">117000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 118000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-20-20\n",
      "  done: false\n",
      "  episode_len_mean: 996.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 118\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21357421875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3818835894266766\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012859240416590565\n",
      "          policy_loss: -0.006342281483941608\n",
      "          total_loss: -0.025570384671704638\n",
      "          vf_explained_var: -0.9837234616279602\n",
      "          vf_loss: 0.0018443290299425521\n",
      "    num_agent_steps_sampled: 118000\n",
      "    num_agent_steps_trained: 118000\n",
      "    num_steps_sampled: 118000\n",
      "    num_steps_trained: 118000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.42999999999999\n",
      "    ram_util_percent: 56.07333333333334\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06243416776309308\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.621316047788643\n",
      "    mean_inference_ms: 2.3285078838302744\n",
      "    mean_raw_obs_processing_ms: 0.6979415627056662\n",
      "  time_since_restore: 2890.3718254566193\n",
      "  time_this_iter_s: 20.73623490333557\n",
      "  time_total_s: 2890.3718254566193\n",
      "  timers:\n",
      "    learn_throughput: 301.36\n",
      "    learn_time_ms: 3318.285\n",
      "    load_throughput: 27908.461\n",
      "    load_time_ms: 35.831\n",
      "    sample_throughput: 54.869\n",
      "    sample_time_ms: 18225.373\n",
      "    update_time_ms: 8.141\n",
      "  timestamp: 1631881220\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 118000\n",
      "  training_iteration: 118\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">         2890.37</td><td style=\"text-align: right;\">118000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 119000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-20-40\n",
      "  done: false\n",
      "  episode_len_mean: 996.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 119\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21357421875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2835214243994817\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013155861325511055\n",
      "          policy_loss: -0.12420820337202813\n",
      "          total_loss: -0.14127405385176342\n",
      "          vf_explained_var: -0.5896868109703064\n",
      "          vf_loss: 0.002959610789548606\n",
      "    num_agent_steps_sampled: 119000\n",
      "    num_agent_steps_trained: 119000\n",
      "    num_steps_sampled: 119000\n",
      "    num_steps_trained: 119000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.75666666666665\n",
      "    ram_util_percent: 55.90666666666667\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062443976372852726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.484134416199836\n",
      "    mean_inference_ms: 2.3287162402131414\n",
      "    mean_raw_obs_processing_ms: 0.7030446959474113\n",
      "  time_since_restore: 2911.2039206027985\n",
      "  time_this_iter_s: 20.8320951461792\n",
      "  time_total_s: 2911.2039206027985\n",
      "  timers:\n",
      "    learn_throughput: 303.226\n",
      "    learn_time_ms: 3297.871\n",
      "    load_throughput: 27086.361\n",
      "    load_time_ms: 36.919\n",
      "    sample_throughput: 54.986\n",
      "    sample_time_ms: 18186.456\n",
      "    update_time_ms: 8.382\n",
      "  timestamp: 1631881240\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 119000\n",
      "  training_iteration: 119\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">          2911.2</td><td style=\"text-align: right;\">119000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-21-22\n",
      "  done: false\n",
      "  episode_len_mean: 994.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 120\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21357421875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7736882978015476\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.033166344244157625\n",
      "          policy_loss: -0.0555082396707601\n",
      "          total_loss: -0.045292250832749736\n",
      "          vf_explained_var: -0.1948496699333191\n",
      "          vf_loss: 0.020869393759251884\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.23728813559322\n",
      "    ram_util_percent: 55.979661016949144\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06245092359382142\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.355578511996992\n",
      "    mean_inference_ms: 2.3289126139426597\n",
      "    mean_raw_obs_processing_ms: 0.7097914233306671\n",
      "  time_since_restore: 2952.5046813488007\n",
      "  time_this_iter_s: 41.3007607460022\n",
      "  time_total_s: 2952.5046813488007\n",
      "  timers:\n",
      "    learn_throughput: 302.459\n",
      "    learn_time_ms: 3306.237\n",
      "    load_throughput: 25993.568\n",
      "    load_time_ms: 38.471\n",
      "    sample_throughput: 49.511\n",
      "    sample_time_ms: 20197.455\n",
      "    update_time_ms: 8.602\n",
      "  timestamp: 1631881282\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 120\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">          2952.5</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 121000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-21-45\n",
      "  done: false\n",
      "  episode_len_mean: 994.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 121\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3374123202429877\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013050459843795131\n",
      "          policy_loss: -0.02246841471642256\n",
      "          total_loss: -0.037462924503617816\n",
      "          vf_explained_var: -0.5238310694694519\n",
      "          vf_loss: 0.004198753391392529\n",
      "    num_agent_steps_sampled: 121000\n",
      "    num_agent_steps_trained: 121000\n",
      "    num_steps_sampled: 121000\n",
      "    num_steps_trained: 121000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.6818181818182\n",
      "    ram_util_percent: 55.96060606060607\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062459306910252685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.23328699422104\n",
      "    mean_inference_ms: 2.3291666714103068\n",
      "    mean_raw_obs_processing_ms: 0.7164935666652564\n",
      "  time_since_restore: 2976.0444915294647\n",
      "  time_this_iter_s: 23.539810180664062\n",
      "  time_total_s: 2976.0444915294647\n",
      "  timers:\n",
      "    learn_throughput: 302.909\n",
      "    learn_time_ms: 3301.321\n",
      "    load_throughput: 26892.598\n",
      "    load_time_ms: 37.185\n",
      "    sample_throughput: 49.005\n",
      "    sample_time_ms: 20405.929\n",
      "    update_time_ms: 8.638\n",
      "  timestamp: 1631881305\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 121000\n",
      "  training_iteration: 121\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         2976.04</td><td style=\"text-align: right;\">121000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 122000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-22-09\n",
      "  done: false\n",
      "  episode_len_mean: 994.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 122\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0404961625734965\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00928683659396055\n",
      "          policy_loss: -0.10122576053771708\n",
      "          total_loss: -0.09320877840121587\n",
      "          vf_explained_var: -0.057225652039051056\n",
      "          vf_loss: 0.025446799343141417\n",
      "    num_agent_steps_sampled: 122000\n",
      "    num_agent_steps_trained: 122000\n",
      "    num_steps_sampled: 122000\n",
      "    num_steps_trained: 122000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.81764705882352\n",
      "    ram_util_percent: 55.98529411764706\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062469419473101515\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.117835358048232\n",
      "    mean_inference_ms: 2.329444135888475\n",
      "    mean_raw_obs_processing_ms: 0.7231527372744375\n",
      "  time_since_restore: 2999.6302404403687\n",
      "  time_this_iter_s: 23.58574891090393\n",
      "  time_total_s: 2999.6302404403687\n",
      "  timers:\n",
      "    learn_throughput: 302.334\n",
      "    learn_time_ms: 3307.601\n",
      "    load_throughput: 26391.205\n",
      "    load_time_ms: 37.891\n",
      "    sample_throughput: 48.742\n",
      "    sample_time_ms: 20516.33\n",
      "    update_time_ms: 8.293\n",
      "  timestamp: 1631881329\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 122000\n",
      "  training_iteration: 122\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         2999.63</td><td style=\"text-align: right;\">122000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 123000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-22-28\n",
      "  done: false\n",
      "  episode_len_mean: 994.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.06\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 123\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9267613887786865\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019908543532838822\n",
      "          policy_loss: -0.07351846396923065\n",
      "          total_loss: -0.07842791279157003\n",
      "          vf_explained_var: 0.5549955368041992\n",
      "          vf_loss: 0.007980233860305615\n",
      "    num_agent_steps_sampled: 123000\n",
      "    num_agent_steps_trained: 123000\n",
      "    num_steps_sampled: 123000\n",
      "    num_steps_trained: 123000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.4962962962963\n",
      "    ram_util_percent: 56.25185185185186\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062480966017355095\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.00834762235953\n",
      "    mean_inference_ms: 2.3297463747510827\n",
      "    mean_raw_obs_processing_ms: 0.7297542176215803\n",
      "  time_since_restore: 3019.0502138137817\n",
      "  time_this_iter_s: 19.419973373413086\n",
      "  time_total_s: 3019.0502138137817\n",
      "  timers:\n",
      "    learn_throughput: 302.607\n",
      "    learn_time_ms: 3304.621\n",
      "    load_throughput: 24382.922\n",
      "    load_time_ms: 41.012\n",
      "    sample_throughput: 49.346\n",
      "    sample_time_ms: 20265.043\n",
      "    update_time_ms: 8.661\n",
      "  timestamp: 1631881348\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 123000\n",
      "  training_iteration: 123\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         3019.05</td><td style=\"text-align: right;\">123000</td><td style=\"text-align: right;\">   -0.06</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-22-51\n",
      "  done: false\n",
      "  episode_len_mean: 994.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.06\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 124\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9405799720022414\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009753102485843712\n",
      "          policy_loss: 0.0282860999306043\n",
      "          total_loss: 0.019036925294333033\n",
      "          vf_explained_var: -0.16663862764835358\n",
      "          vf_loss: 0.007032110261368669\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.96666666666667\n",
      "    ram_util_percent: 56.11818181818182\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06249320600640698\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.904202089555863\n",
      "    mean_inference_ms: 2.3300608323501075\n",
      "    mean_raw_obs_processing_ms: 0.7363081105844351\n",
      "  time_since_restore: 3041.5225982666016\n",
      "  time_this_iter_s: 22.472384452819824\n",
      "  time_total_s: 3041.5225982666016\n",
      "  timers:\n",
      "    learn_throughput: 302.9\n",
      "    learn_time_ms: 3301.423\n",
      "    load_throughput: 23494.421\n",
      "    load_time_ms: 42.563\n",
      "    sample_throughput: 48.905\n",
      "    sample_time_ms: 20447.69\n",
      "    update_time_ms: 8.837\n",
      "  timestamp: 1631881371\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 124\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         3041.52</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">   -0.06</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 125000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-23-11\n",
      "  done: false\n",
      "  episode_len_mean: 994.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 125\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.944806350602044\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012197409976933561\n",
      "          policy_loss: -0.09054802656173706\n",
      "          total_loss: -0.0783381309774187\n",
      "          vf_explained_var: 0.04035874083638191\n",
      "          vf_loss: 0.027750381015034187\n",
      "    num_agent_steps_sampled: 125000\n",
      "    num_agent_steps_trained: 125000\n",
      "    num_steps_sampled: 125000\n",
      "    num_steps_trained: 125000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.17857142857143\n",
      "    ram_util_percent: 56.19285714285714\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06250597374773298\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.805650470570683\n",
      "    mean_inference_ms: 2.3303851409622767\n",
      "    mean_raw_obs_processing_ms: 0.7428083186298418\n",
      "  time_since_restore: 3061.6575050354004\n",
      "  time_this_iter_s: 20.134906768798828\n",
      "  time_total_s: 3061.6575050354004\n",
      "  timers:\n",
      "    learn_throughput: 307.173\n",
      "    learn_time_ms: 3255.494\n",
      "    load_throughput: 23193.286\n",
      "    load_time_ms: 43.116\n",
      "    sample_throughput: 49.515\n",
      "    sample_time_ms: 20195.982\n",
      "    update_time_ms: 8.903\n",
      "  timestamp: 1631881391\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 125000\n",
      "  training_iteration: 125\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         3061.66</td><td style=\"text-align: right;\">125000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 126000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-23-32\n",
      "  done: false\n",
      "  episode_len_mean: 994.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 126\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9929669936498007\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014712239395109804\n",
      "          policy_loss: -0.027322612785630754\n",
      "          total_loss: -0.037600276950332856\n",
      "          vf_explained_var: 0.15046650171279907\n",
      "          vf_loss: 0.0049387739428008596\n",
      "    num_agent_steps_sampled: 126000\n",
      "    num_agent_steps_trained: 126000\n",
      "    num_steps_sampled: 126000\n",
      "    num_steps_trained: 126000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.10666666666667\n",
      "    ram_util_percent: 56.146666666666654\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06251990945768962\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.712488720014697\n",
      "    mean_inference_ms: 2.3307415403149\n",
      "    mean_raw_obs_processing_ms: 0.7492716539804143\n",
      "  time_since_restore: 3082.412533521652\n",
      "  time_this_iter_s: 20.75502848625183\n",
      "  time_total_s: 3082.412533521652\n",
      "  timers:\n",
      "    learn_throughput: 306.388\n",
      "    learn_time_ms: 3263.833\n",
      "    load_throughput: 23094.455\n",
      "    load_time_ms: 43.3\n",
      "    sample_throughput: 49.703\n",
      "    sample_time_ms: 20119.373\n",
      "    update_time_ms: 8.537\n",
      "  timestamp: 1631881412\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 126000\n",
      "  training_iteration: 126\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">         3082.41</td><td style=\"text-align: right;\">126000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 127000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-23-53\n",
      "  done: false\n",
      "  episode_len_mean: 994.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 127\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9122215774324205\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013576780514786594\n",
      "          policy_loss: -0.07478177535037199\n",
      "          total_loss: -0.0850123543292284\n",
      "          vf_explained_var: -0.1157212182879448\n",
      "          vf_loss: 0.004542158657891883\n",
      "    num_agent_steps_sampled: 127000\n",
      "    num_agent_steps_trained: 127000\n",
      "    num_steps_sampled: 127000\n",
      "    num_steps_trained: 127000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.61333333333332\n",
      "    ram_util_percent: 56.08\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06253490071522184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.624627585854267\n",
      "    mean_inference_ms: 2.3310913448228607\n",
      "    mean_raw_obs_processing_ms: 0.7556961312468943\n",
      "  time_since_restore: 3103.2626168727875\n",
      "  time_this_iter_s: 20.850083351135254\n",
      "  time_total_s: 3103.2626168727875\n",
      "  timers:\n",
      "    learn_throughput: 304.579\n",
      "    learn_time_ms: 3283.215\n",
      "    load_throughput: 22336.064\n",
      "    load_time_ms: 44.771\n",
      "    sample_throughput: 49.961\n",
      "    sample_time_ms: 20015.788\n",
      "    update_time_ms: 8.232\n",
      "  timestamp: 1631881433\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 127000\n",
      "  training_iteration: 127\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         3103.26</td><td style=\"text-align: right;\">127000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-24-16\n",
      "  done: false\n",
      "  episode_len_mean: 994.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 128\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.316945383283827\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010088099887949677\n",
      "          policy_loss: -0.05141940421114365\n",
      "          total_loss: -0.06917586074107224\n",
      "          vf_explained_var: -0.8578599691390991\n",
      "          vf_loss: 0.002181160177052435\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.6\n",
      "    ram_util_percent: 56.162499999999994\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06255128149117604\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.542318117503463\n",
      "    mean_inference_ms: 2.3314943143575637\n",
      "    mean_raw_obs_processing_ms: 0.7620463298201757\n",
      "  time_since_restore: 3125.8719034194946\n",
      "  time_this_iter_s: 22.609286546707153\n",
      "  time_total_s: 3125.8719034194946\n",
      "  timers:\n",
      "    learn_throughput: 303.167\n",
      "    learn_time_ms: 3298.509\n",
      "    load_throughput: 22434.496\n",
      "    load_time_ms: 44.574\n",
      "    sample_throughput: 49.533\n",
      "    sample_time_ms: 20188.72\n",
      "    update_time_ms: 8.053\n",
      "  timestamp: 1631881456\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 128\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">         3125.87</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 129000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-24-40\n",
      "  done: false\n",
      "  episode_len_mean: 994.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 129\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0479537977112665\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010306951849753516\n",
      "          policy_loss: -0.06498321096102397\n",
      "          total_loss: -0.07578105040722423\n",
      "          vf_explained_var: 0.19519323110580444\n",
      "          vf_loss: 0.006379751399314652\n",
      "    num_agent_steps_sampled: 129000\n",
      "    num_agent_steps_trained: 129000\n",
      "    num_steps_sampled: 129000\n",
      "    num_steps_trained: 129000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.46571428571428\n",
      "    ram_util_percent: 56.18\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06256833775059191\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.464746596703424\n",
      "    mean_inference_ms: 2.331930298481496\n",
      "    mean_raw_obs_processing_ms: 0.7683515136939337\n",
      "  time_since_restore: 3150.341016769409\n",
      "  time_this_iter_s: 24.46911334991455\n",
      "  time_total_s: 3150.341016769409\n",
      "  timers:\n",
      "    learn_throughput: 302.441\n",
      "    learn_time_ms: 3306.432\n",
      "    load_throughput: 23835.728\n",
      "    load_time_ms: 41.954\n",
      "    sample_throughput: 48.669\n",
      "    sample_time_ms: 20546.877\n",
      "    update_time_ms: 8.527\n",
      "  timestamp: 1631881480\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 129000\n",
      "  training_iteration: 129\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         3150.34</td><td style=\"text-align: right;\">129000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 130000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-25-01\n",
      "  done: false\n",
      "  episode_len_mean: 996.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 130\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.420143307579888\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012572116114495647\n",
      "          policy_loss: -0.04213267929024166\n",
      "          total_loss: -0.06050246088869042\n",
      "          vf_explained_var: -0.4564734101295471\n",
      "          vf_loss: 0.0018040305915443847\n",
      "    num_agent_steps_sampled: 130000\n",
      "    num_agent_steps_trained: 130000\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.94827586206897\n",
      "    ram_util_percent: 56.30689655172414\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06258554713023914\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.39041675155362\n",
      "    mean_inference_ms: 2.332378173290186\n",
      "    mean_raw_obs_processing_ms: 0.7678876755992053\n",
      "  time_since_restore: 3170.9356949329376\n",
      "  time_this_iter_s: 20.594678163528442\n",
      "  time_total_s: 3170.9356949329376\n",
      "  timers:\n",
      "    learn_throughput: 301.275\n",
      "    learn_time_ms: 3319.224\n",
      "    load_throughput: 23791.786\n",
      "    load_time_ms: 42.031\n",
      "    sample_throughput: 54.16\n",
      "    sample_time_ms: 18463.804\n",
      "    update_time_ms: 8.154\n",
      "  timestamp: 1631881501\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 130\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">         3170.94</td><td style=\"text-align: right;\">130000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 131000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-25-21\n",
      "  done: false\n",
      "  episode_len_mean: 996.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 131\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.458908012178209\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010099060792395198\n",
      "          policy_loss: -0.06306235481881434\n",
      "          total_loss: -0.08367393580265343\n",
      "          vf_explained_var: -0.32355350255966187\n",
      "          vf_loss: 0.0007421532427542843\n",
      "    num_agent_steps_sampled: 131000\n",
      "    num_agent_steps_trained: 131000\n",
      "    num_steps_sampled: 131000\n",
      "    num_steps_trained: 131000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.65172413793104\n",
      "    ram_util_percent: 56.18275862068966\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06260302720122207\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.319077121922245\n",
      "    mean_inference_ms: 2.3328145236989655\n",
      "    mean_raw_obs_processing_ms: 0.767586235178201\n",
      "  time_since_restore: 3191.2142992019653\n",
      "  time_this_iter_s: 20.27860426902771\n",
      "  time_total_s: 3191.2142992019653\n",
      "  timers:\n",
      "    learn_throughput: 301.077\n",
      "    learn_time_ms: 3321.411\n",
      "    load_throughput: 22166.564\n",
      "    load_time_ms: 45.113\n",
      "    sample_throughput: 55.163\n",
      "    sample_time_ms: 18128.069\n",
      "    update_time_ms: 9.865\n",
      "  timestamp: 1631881521\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 131000\n",
      "  training_iteration: 131\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         3191.21</td><td style=\"text-align: right;\">131000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-25-42\n",
      "  done: false\n",
      "  episode_len_mean: 996.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 132\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4483855379952324\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012007816454606492\n",
      "          policy_loss: 0.027592906086809105\n",
      "          total_loss: 0.008000983810052275\n",
      "          vf_explained_var: -0.9837244749069214\n",
      "          vf_loss: 0.0010450927570572175\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.60967741935484\n",
      "    ram_util_percent: 56.16129032258065\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06262131994761963\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.25081423500441\n",
      "    mean_inference_ms: 2.3332688422708747\n",
      "    mean_raw_obs_processing_ms: 0.767437289503096\n",
      "  time_since_restore: 3212.3387401103973\n",
      "  time_this_iter_s: 21.124440908432007\n",
      "  time_total_s: 3212.3387401103973\n",
      "  timers:\n",
      "    learn_throughput: 301.784\n",
      "    learn_time_ms: 3313.629\n",
      "    load_throughput: 22643.163\n",
      "    load_time_ms: 44.163\n",
      "    sample_throughput: 55.9\n",
      "    sample_time_ms: 17888.964\n",
      "    update_time_ms: 10.296\n",
      "  timestamp: 1631881542\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 132\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         3212.34</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 133000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-26-02\n",
      "  done: false\n",
      "  episode_len_mean: 996.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 133\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.491351482603285\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008600589458638492\n",
      "          policy_loss: -0.04028805270791054\n",
      "          total_loss: -0.06205409901837508\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00039217389979360936\n",
      "    num_agent_steps_sampled: 133000\n",
      "    num_agent_steps_trained: 133000\n",
      "    num_steps_sampled: 133000\n",
      "    num_steps_trained: 133000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.68928571428572\n",
      "    ram_util_percent: 56.13928571428571\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0626407548852202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.18559625454601\n",
      "    mean_inference_ms: 2.3337296983724776\n",
      "    mean_raw_obs_processing_ms: 0.7674388491281405\n",
      "  time_since_restore: 3232.1181943416595\n",
      "  time_this_iter_s: 19.779454231262207\n",
      "  time_total_s: 3232.1181943416595\n",
      "  timers:\n",
      "    learn_throughput: 301.528\n",
      "    learn_time_ms: 3316.437\n",
      "    load_throughput: 22220.513\n",
      "    load_time_ms: 45.003\n",
      "    sample_throughput: 55.795\n",
      "    sample_time_ms: 17922.765\n",
      "    update_time_ms: 9.933\n",
      "  timestamp: 1631881562\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 133000\n",
      "  training_iteration: 133\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">         3232.12</td><td style=\"text-align: right;\">133000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 134000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-26-22\n",
      "  done: false\n",
      "  episode_len_mean: 996.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 134\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.529958481258816\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011932382636034017\n",
      "          policy_loss: -0.02629579363597764\n",
      "          total_loss: -0.04720821003946993\n",
      "          vf_explained_var: -0.7050359845161438\n",
      "          vf_loss: 0.0005644907673639762\n",
      "    num_agent_steps_sampled: 134000\n",
      "    num_agent_steps_trained: 134000\n",
      "    num_steps_sampled: 134000\n",
      "    num_steps_trained: 134000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.38214285714285\n",
      "    ram_util_percent: 56.271428571428565\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06266075510891747\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.122886916600724\n",
      "    mean_inference_ms: 2.3342035203144995\n",
      "    mean_raw_obs_processing_ms: 0.7675823175445048\n",
      "  time_since_restore: 3252.01012468338\n",
      "  time_this_iter_s: 19.89193034172058\n",
      "  time_total_s: 3252.01012468338\n",
      "  timers:\n",
      "    learn_throughput: 299.678\n",
      "    learn_time_ms: 3336.92\n",
      "    load_throughput: 22120.41\n",
      "    load_time_ms: 45.207\n",
      "    sample_throughput: 56.674\n",
      "    sample_time_ms: 17644.621\n",
      "    update_time_ms: 9.661\n",
      "  timestamp: 1631881582\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 134000\n",
      "  training_iteration: 134\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         3252.01</td><td style=\"text-align: right;\">134000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 135000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-26-43\n",
      "  done: false\n",
      "  episode_len_mean: 996.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 135\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.554200953907437\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011163853777264043\n",
      "          policy_loss: -0.02106411928931872\n",
      "          total_loss: -0.04284183431623711\n",
      "          vf_explained_var: -0.9647737145423889\n",
      "          vf_loss: 0.00018782899100592152\n",
      "    num_agent_steps_sampled: 135000\n",
      "    num_agent_steps_trained: 135000\n",
      "    num_steps_sampled: 135000\n",
      "    num_steps_trained: 135000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.7548387096774\n",
      "    ram_util_percent: 56.23870967741936\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06268139577415927\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.06215804966812\n",
      "    mean_inference_ms: 2.334685377861162\n",
      "    mean_raw_obs_processing_ms: 0.7678548141347409\n",
      "  time_since_restore: 3273.1929132938385\n",
      "  time_this_iter_s: 21.182788610458374\n",
      "  time_total_s: 3273.1929132938385\n",
      "  timers:\n",
      "    learn_throughput: 294.774\n",
      "    learn_time_ms: 3392.43\n",
      "    load_throughput: 21580.478\n",
      "    load_time_ms: 46.338\n",
      "    sample_throughput: 56.523\n",
      "    sample_time_ms: 17691.858\n",
      "    update_time_ms: 10.061\n",
      "  timestamp: 1631881603\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 135000\n",
      "  training_iteration: 135\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         3273.19</td><td style=\"text-align: right;\">135000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-27-03\n",
      "  done: false\n",
      "  episode_len_mean: 996.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 136\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5076779656940036\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008498940811222096\n",
      "          policy_loss: -0.05248340935342842\n",
      "          total_loss: -0.0739362835056252\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0009011746455346131\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.43214285714285\n",
      "    ram_util_percent: 56.310714285714276\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06270221507877462\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.003699760520245\n",
      "    mean_inference_ms: 2.3351653748184624\n",
      "    mean_raw_obs_processing_ms: 0.7682366476531071\n",
      "  time_since_restore: 3293.3277337551117\n",
      "  time_this_iter_s: 20.134820461273193\n",
      "  time_total_s: 3293.3277337551117\n",
      "  timers:\n",
      "    learn_throughput: 295.418\n",
      "    learn_time_ms: 3385.031\n",
      "    load_throughput: 21767.255\n",
      "    load_time_ms: 45.941\n",
      "    sample_throughput: 56.698\n",
      "    sample_time_ms: 17637.332\n",
      "    update_time_ms: 10.203\n",
      "  timestamp: 1631881623\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 136\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         3293.33</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 137000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-27-23\n",
      "  done: false\n",
      "  episode_len_mean: 996.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 137\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4930771695242986\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01339896581705461\n",
      "          policy_loss: -0.04156754621201091\n",
      "          total_loss: -0.061105486129721005\n",
      "          vf_explained_var: -0.8102798461914062\n",
      "          vf_loss: 0.0011003181422387974\n",
      "    num_agent_steps_sampled: 137000\n",
      "    num_agent_steps_trained: 137000\n",
      "    num_steps_sampled: 137000\n",
      "    num_steps_trained: 137000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.65862068965517\n",
      "    ram_util_percent: 56.34827586206896\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06272261073142586\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.947332274053228\n",
      "    mean_inference_ms: 2.335622697032635\n",
      "    mean_raw_obs_processing_ms: 0.7687259748639295\n",
      "  time_since_restore: 3313.209511756897\n",
      "  time_this_iter_s: 19.88177800178528\n",
      "  time_total_s: 3313.209511756897\n",
      "  timers:\n",
      "    learn_throughput: 296.417\n",
      "    learn_time_ms: 3373.626\n",
      "    load_throughput: 21970.98\n",
      "    load_time_ms: 45.515\n",
      "    sample_throughput: 56.971\n",
      "    sample_time_ms: 17552.868\n",
      "    update_time_ms: 10.087\n",
      "  timestamp: 1631881643\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 137000\n",
      "  training_iteration: 137\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         3313.21</td><td style=\"text-align: right;\">137000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 138000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-27-45\n",
      "  done: false\n",
      "  episode_len_mean: 996.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 138\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5110334396362304\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008643838421109202\n",
      "          policy_loss: -0.0371304295439687\n",
      "          total_loss: -0.05905163950390286\n",
      "          vf_explained_var: -0.9766775369644165\n",
      "          vf_loss: 0.0004199709575510092\n",
      "    num_agent_steps_sampled: 138000\n",
      "    num_agent_steps_trained: 138000\n",
      "    num_steps_sampled: 138000\n",
      "    num_steps_trained: 138000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.65483870967743\n",
      "    ram_util_percent: 56.2516129032258\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06274251026378258\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.8934861704255\n",
      "    mean_inference_ms: 2.3360786847266626\n",
      "    mean_raw_obs_processing_ms: 0.7693188137087814\n",
      "  time_since_restore: 3334.940976381302\n",
      "  time_this_iter_s: 21.731464624404907\n",
      "  time_total_s: 3334.940976381302\n",
      "  timers:\n",
      "    learn_throughput: 296.228\n",
      "    learn_time_ms: 3375.777\n",
      "    load_throughput: 21853.335\n",
      "    load_time_ms: 45.76\n",
      "    sample_throughput: 57.267\n",
      "    sample_time_ms: 17462.211\n",
      "    update_time_ms: 10.465\n",
      "  timestamp: 1631881665\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 138000\n",
      "  training_iteration: 138\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         3334.94</td><td style=\"text-align: right;\">138000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 139000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-28-06\n",
      "  done: false\n",
      "  episode_len_mean: 996.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 139\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.501179035504659\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0052663108218657445\n",
      "          policy_loss: -0.08662640932533476\n",
      "          total_loss: -0.1095280236668057\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00042305211035353646\n",
      "    num_agent_steps_sampled: 139000\n",
      "    num_agent_steps_trained: 139000\n",
      "    num_steps_sampled: 139000\n",
      "    num_steps_trained: 139000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.12666666666665\n",
      "    ram_util_percent: 56.21666666666667\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06276248892051878\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.841540133278375\n",
      "    mean_inference_ms: 2.3365487014426294\n",
      "    mean_raw_obs_processing_ms: 0.7700125704497056\n",
      "  time_since_restore: 3355.805289030075\n",
      "  time_this_iter_s: 20.864312648773193\n",
      "  time_total_s: 3355.805289030075\n",
      "  timers:\n",
      "    learn_throughput: 298.573\n",
      "    learn_time_ms: 3349.26\n",
      "    load_throughput: 20842.764\n",
      "    load_time_ms: 47.978\n",
      "    sample_throughput: 58.387\n",
      "    sample_time_ms: 17127.057\n",
      "    update_time_ms: 9.928\n",
      "  timestamp: 1631881686\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 139000\n",
      "  training_iteration: 139\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         3355.81</td><td style=\"text-align: right;\">139000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-28-27\n",
      "  done: false\n",
      "  episode_len_mean: 996.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 140\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4377379284964666\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009546864238779899\n",
      "          policy_loss: -0.06849491078820494\n",
      "          total_loss: -0.08850019880466992\n",
      "          vf_explained_var: -0.8471523523330688\n",
      "          vf_loss: 0.0013136437284022881\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.41724137931034\n",
      "    ram_util_percent: 56.4\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.062781985232253\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.791343260149937\n",
      "    mean_inference_ms: 2.337016692699219\n",
      "    mean_raw_obs_processing_ms: 0.7707885398705286\n",
      "  time_since_restore: 3376.674330472946\n",
      "  time_this_iter_s: 20.869041442871094\n",
      "  time_total_s: 3376.674330472946\n",
      "  timers:\n",
      "    learn_throughput: 297.957\n",
      "    learn_time_ms: 3356.183\n",
      "    load_throughput: 21082.466\n",
      "    load_time_ms: 47.433\n",
      "    sample_throughput: 58.316\n",
      "    sample_time_ms: 17147.995\n",
      "    update_time_ms: 9.83\n",
      "  timestamp: 1631881707\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 140\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">         3376.67</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 141000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-28-50\n",
      "  done: false\n",
      "  episode_len_mean: 996.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 141\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.450490776697795\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010521490339220218\n",
      "          policy_loss: -0.043889830489125516\n",
      "          total_loss: -0.06360063722564115\n",
      "          vf_explained_var: -0.8855666518211365\n",
      "          vf_loss: 0.0014234219406110546\n",
      "    num_agent_steps_sampled: 141000\n",
      "    num_agent_steps_trained: 141000\n",
      "    num_steps_sampled: 141000\n",
      "    num_steps_trained: 141000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.44545454545454\n",
      "    ram_util_percent: 56.34848484848485\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0628013531692317\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.743078403465958\n",
      "    mean_inference_ms: 2.337493499344339\n",
      "    mean_raw_obs_processing_ms: 0.7716414504836939\n",
      "  time_since_restore: 3399.6678307056427\n",
      "  time_this_iter_s: 22.993500232696533\n",
      "  time_total_s: 3399.6678307056427\n",
      "  timers:\n",
      "    learn_throughput: 298.198\n",
      "    learn_time_ms: 3353.473\n",
      "    load_throughput: 20915.55\n",
      "    load_time_ms: 47.811\n",
      "    sample_throughput: 57.387\n",
      "    sample_time_ms: 17425.623\n",
      "    update_time_ms: 8.454\n",
      "  timestamp: 1631881730\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 141000\n",
      "  training_iteration: 141\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         3399.67</td><td style=\"text-align: right;\">141000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 142000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-29-12\n",
      "  done: false\n",
      "  episode_len_mean: 996.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 142\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4462637450959948\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008315613971717652\n",
      "          policy_loss: 0.016792931270578668\n",
      "          total_loss: -0.004580179632951816\n",
      "          vf_explained_var: -0.8604872822761536\n",
      "          vf_loss: 0.00042552469725099705\n",
      "    num_agent_steps_sampled: 142000\n",
      "    num_agent_steps_trained: 142000\n",
      "    num_steps_sampled: 142000\n",
      "    num_steps_trained: 142000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.21290322580644\n",
      "    ram_util_percent: 56.18064516129032\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06282074121222267\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.696307416136037\n",
      "    mean_inference_ms: 2.3379676125415814\n",
      "    mean_raw_obs_processing_ms: 0.7725779455368793\n",
      "  time_since_restore: 3421.3377521038055\n",
      "  time_this_iter_s: 21.669921398162842\n",
      "  time_total_s: 3421.3377521038055\n",
      "  timers:\n",
      "    learn_throughput: 298.617\n",
      "    learn_time_ms: 3348.769\n",
      "    load_throughput: 21320.158\n",
      "    load_time_ms: 46.904\n",
      "    sample_throughput: 57.186\n",
      "    sample_time_ms: 17486.794\n",
      "    update_time_ms: 7.99\n",
      "  timestamp: 1631881752\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 142000\n",
      "  training_iteration: 142\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         3421.34</td><td style=\"text-align: right;\">142000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 143000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-29-33\n",
      "  done: false\n",
      "  episode_len_mean: 996.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 143\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2402521279123095\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012325041808585032\n",
      "          policy_loss: -0.09700226253933376\n",
      "          total_loss: -0.11311782966885302\n",
      "          vf_explained_var: -0.673058032989502\n",
      "          vf_loss: 0.0023384869978245763\n",
      "    num_agent_steps_sampled: 143000\n",
      "    num_agent_steps_trained: 143000\n",
      "    num_steps_sampled: 143000\n",
      "    num_steps_trained: 143000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.48064516129034\n",
      "    ram_util_percent: 56.24193548387097\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06283918940812279\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.65085946152933\n",
      "    mean_inference_ms: 2.3384409263568804\n",
      "    mean_raw_obs_processing_ms: 0.7735697230554237\n",
      "  time_since_restore: 3443.2219779491425\n",
      "  time_this_iter_s: 21.884225845336914\n",
      "  time_total_s: 3443.2219779491425\n",
      "  timers:\n",
      "    learn_throughput: 299.154\n",
      "    learn_time_ms: 3342.765\n",
      "    load_throughput: 21508.755\n",
      "    load_time_ms: 46.493\n",
      "    sample_throughput: 56.484\n",
      "    sample_time_ms: 17704.278\n",
      "    update_time_ms: 7.818\n",
      "  timestamp: 1631881773\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 143000\n",
      "  training_iteration: 143\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">         3443.22</td><td style=\"text-align: right;\">143000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-29-56\n",
      "  done: false\n",
      "  episode_len_mean: 996.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 144\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.211416419347127\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010504637477082272\n",
      "          policy_loss: -0.06921374439779256\n",
      "          total_loss: -0.08610630689395798\n",
      "          vf_explained_var: -0.9461559653282166\n",
      "          vf_loss: 0.0018563204115101446\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.99375\n",
      "    ram_util_percent: 56.221875000000004\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06285776433967839\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.606683367972675\n",
      "    mean_inference_ms: 2.338930595950621\n",
      "    mean_raw_obs_processing_ms: 0.7746319610195908\n",
      "  time_since_restore: 3465.6756489276886\n",
      "  time_this_iter_s: 22.453670978546143\n",
      "  time_total_s: 3465.6756489276886\n",
      "  timers:\n",
      "    learn_throughput: 300.283\n",
      "    learn_time_ms: 3330.188\n",
      "    load_throughput: 21582.743\n",
      "    load_time_ms: 46.333\n",
      "    sample_throughput: 55.769\n",
      "    sample_time_ms: 17930.983\n",
      "    update_time_ms: 7.728\n",
      "  timestamp: 1631881796\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 144\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         3465.68</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 145000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-30-19\n",
      "  done: false\n",
      "  episode_len_mean: 996.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 145\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8596008989546033\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014272596840122356\n",
      "          policy_loss: -0.06997817369798819\n",
      "          total_loss: -0.08093187782085604\n",
      "          vf_explained_var: -0.47597622871398926\n",
      "          vf_loss: 0.0030699168008545205\n",
      "    num_agent_steps_sampled: 145000\n",
      "    num_agent_steps_trained: 145000\n",
      "    num_steps_sampled: 145000\n",
      "    num_steps_trained: 145000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.82424242424243\n",
      "    ram_util_percent: 56.354545454545466\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0628762416403714\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.564050272469842\n",
      "    mean_inference_ms: 2.3394295137646024\n",
      "    mean_raw_obs_processing_ms: 0.7757582260553044\n",
      "  time_since_restore: 3488.3550159931183\n",
      "  time_this_iter_s: 22.679367065429688\n",
      "  time_total_s: 3488.3550159931183\n",
      "  timers:\n",
      "    learn_throughput: 299.182\n",
      "    learn_time_ms: 3342.442\n",
      "    load_throughput: 21839.055\n",
      "    load_time_ms: 45.79\n",
      "    sample_throughput: 55.341\n",
      "    sample_time_ms: 18069.896\n",
      "    update_time_ms: 7.289\n",
      "  timestamp: 1631881819\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 145000\n",
      "  training_iteration: 145\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   145</td><td style=\"text-align: right;\">         3488.36</td><td style=\"text-align: right;\">145000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 146000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-30-42\n",
      "  done: false\n",
      "  episode_len_mean: 996.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 146\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.952524483203888\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010818561085446217\n",
      "          policy_loss: -0.04485704455938604\n",
      "          total_loss: -0.058808583786918056\n",
      "          vf_explained_var: -0.9600870609283447\n",
      "          vf_loss: 0.00210785731065294\n",
      "    num_agent_steps_sampled: 146000\n",
      "    num_agent_steps_trained: 146000\n",
      "    num_steps_sampled: 146000\n",
      "    num_steps_trained: 146000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.11818181818182\n",
      "    ram_util_percent: 56.35151515151515\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06289475197158771\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.52267760999107\n",
      "    mean_inference_ms: 2.3399247743275344\n",
      "    mean_raw_obs_processing_ms: 0.776941816750185\n",
      "  time_since_restore: 3511.3058671951294\n",
      "  time_this_iter_s: 22.95085120201111\n",
      "  time_total_s: 3511.3058671951294\n",
      "  timers:\n",
      "    learn_throughput: 299.183\n",
      "    learn_time_ms: 3342.44\n",
      "    load_throughput: 21684.742\n",
      "    load_time_ms: 46.115\n",
      "    sample_throughput: 54.491\n",
      "    sample_time_ms: 18351.56\n",
      "    update_time_ms: 7.123\n",
      "  timestamp: 1631881842\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 146000\n",
      "  training_iteration: 146\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         3511.31</td><td style=\"text-align: right;\">146000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 147000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-31-03\n",
      "  done: false\n",
      "  episode_len_mean: 996.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 147\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.237096310986413\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009844050061072288\n",
      "          policy_loss: -0.019409024549855127\n",
      "          total_loss: -0.03710847823984093\n",
      "          vf_explained_var: -0.7488583922386169\n",
      "          vf_loss: 0.0015178567487358426\n",
      "    num_agent_steps_sampled: 147000\n",
      "    num_agent_steps_trained: 147000\n",
      "    num_steps_sampled: 147000\n",
      "    num_steps_trained: 147000\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.29666666666667\n",
      "    ram_util_percent: 56.46666666666667\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06291292715067236\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.48290770981728\n",
      "    mean_inference_ms: 2.3404364863350575\n",
      "    mean_raw_obs_processing_ms: 0.7781890908143437\n",
      "  time_since_restore: 3532.7055213451385\n",
      "  time_this_iter_s: 21.399654150009155\n",
      "  time_total_s: 3532.7055213451385\n",
      "  timers:\n",
      "    learn_throughput: 299.986\n",
      "    learn_time_ms: 3333.494\n",
      "    load_throughput: 21318.153\n",
      "    load_time_ms: 46.908\n",
      "    sample_throughput: 54.02\n",
      "    sample_time_ms: 18511.528\n",
      "    update_time_ms: 7.038\n",
      "  timestamp: 1631881863\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 147000\n",
      "  training_iteration: 147\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         3532.71</td><td style=\"text-align: right;\">147000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-31-26\n",
      "  done: false\n",
      "  episode_len_mean: 996.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 148\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8128536012437608\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010980895245667326\n",
      "          policy_loss: -0.02079895345701112\n",
      "          total_loss: -0.03331697094771597\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0020926659008384577\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.67878787878789\n",
      "    ram_util_percent: 56.41212121212121\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06293163365652676\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.444633830561216\n",
      "    mean_inference_ms: 2.34096447500628\n",
      "    mean_raw_obs_processing_ms: 0.7794922481672742\n",
      "  time_since_restore: 3555.599936246872\n",
      "  time_this_iter_s: 22.8944149017334\n",
      "  time_total_s: 3555.599936246872\n",
      "  timers:\n",
      "    learn_throughput: 301.184\n",
      "    learn_time_ms: 3320.231\n",
      "    load_throughput: 21992.477\n",
      "    load_time_ms: 45.47\n",
      "    sample_throughput: 53.64\n",
      "    sample_time_ms: 18642.764\n",
      "    update_time_ms: 6.867\n",
      "  timestamp: 1631881886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 148\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">          3555.6</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 149000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-31-48\n",
      "  done: false\n",
      "  episode_len_mean: 996.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 149\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8678631358676487\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011706085978709405\n",
      "          policy_loss: -0.027267458248469566\n",
      "          total_loss: -0.03997246024923192\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0022234510112967756\n",
      "    num_agent_steps_sampled: 149000\n",
      "    num_agent_steps_trained: 149000\n",
      "    num_steps_sampled: 149000\n",
      "    num_steps_trained: 149000\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.38749999999999\n",
      "    ram_util_percent: 56.390625\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06295059743006223\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.407633668800415\n",
      "    mean_inference_ms: 2.34150228003052\n",
      "    mean_raw_obs_processing_ms: 0.7808455677363315\n",
      "  time_since_restore: 3577.8496153354645\n",
      "  time_this_iter_s: 22.24967908859253\n",
      "  time_total_s: 3577.8496153354645\n",
      "  timers:\n",
      "    learn_throughput: 299.462\n",
      "    learn_time_ms: 3339.319\n",
      "    load_throughput: 21473.517\n",
      "    load_time_ms: 46.569\n",
      "    sample_throughput: 53.301\n",
      "    sample_time_ms: 18761.409\n",
      "    update_time_ms: 6.77\n",
      "  timestamp: 1631881908\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 149000\n",
      "  training_iteration: 149\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         3577.85</td><td style=\"text-align: right;\">149000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 150000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-32-29\n",
      "  done: false\n",
      "  episode_len_mean: 994.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 150\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.828073391649458\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011733039304530053\n",
      "          policy_loss: -0.0720957869456874\n",
      "          total_loss: -0.08403067261808449\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.002587034929698954\n",
      "    num_agent_steps_sampled: 150000\n",
      "    num_agent_steps_trained: 150000\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.14827586206897\n",
      "    ram_util_percent: 56.327586206896555\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0629685446118045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.371705882032987\n",
      "    mean_inference_ms: 2.342030004223807\n",
      "    mean_raw_obs_processing_ms: 0.7835473366889104\n",
      "  time_since_restore: 3618.7121171951294\n",
      "  time_this_iter_s: 40.86250185966492\n",
      "  time_total_s: 3618.7121171951294\n",
      "  timers:\n",
      "    learn_throughput: 300.146\n",
      "    learn_time_ms: 3331.707\n",
      "    load_throughput: 21724.277\n",
      "    load_time_ms: 46.031\n",
      "    sample_throughput: 48.151\n",
      "    sample_time_ms: 20768.143\n",
      "    update_time_ms: 7.05\n",
      "  timestamp: 1631881949\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 150\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         3618.71</td><td style=\"text-align: right;\">150000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 151000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-32-52\n",
      "  done: false\n",
      "  episode_len_mean: 994.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 151\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.212010701497396\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013314972859247239\n",
      "          policy_loss: -0.05696420412924555\n",
      "          total_loss: -0.07312988332576222\n",
      "          vf_explained_var: -0.43057429790496826\n",
      "          vf_loss: 0.0016888272715732456\n",
      "    num_agent_steps_sampled: 151000\n",
      "    num_agent_steps_trained: 151000\n",
      "    num_steps_sampled: 151000\n",
      "    num_steps_trained: 151000\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.140625\n",
      "    ram_util_percent: 56.465624999999996\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0629872185574617\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.33713143121029\n",
      "    mean_inference_ms: 2.342564178620246\n",
      "    mean_raw_obs_processing_ms: 0.7862847957817904\n",
      "  time_since_restore: 3641.337580680847\n",
      "  time_this_iter_s: 22.625463485717773\n",
      "  time_total_s: 3641.337580680847\n",
      "  timers:\n",
      "    learn_throughput: 298.886\n",
      "    learn_time_ms: 3345.758\n",
      "    load_throughput: 21833.609\n",
      "    load_time_ms: 45.801\n",
      "    sample_throughput: 48.267\n",
      "    sample_time_ms: 20718.288\n",
      "    update_time_ms: 6.633\n",
      "  timestamp: 1631881972\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 151000\n",
      "  training_iteration: 151\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         3641.34</td><td style=\"text-align: right;\">151000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-33-13\n",
      "  done: false\n",
      "  episode_len_mean: 994.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 152\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1537937535179985\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01648995029004368\n",
      "          policy_loss: -0.010548469548424085\n",
      "          total_loss: -0.02534708482109838\n",
      "          vf_explained_var: -0.6186804175376892\n",
      "          vf_loss: 0.0014565794340645273\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.83225806451613\n",
      "    ram_util_percent: 56.59354838709677\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06300685359055057\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.303660413363943\n",
      "    mean_inference_ms: 2.343106597396007\n",
      "    mean_raw_obs_processing_ms: 0.7890581189511348\n",
      "  time_since_restore: 3662.6854712963104\n",
      "  time_this_iter_s: 21.347890615463257\n",
      "  time_total_s: 3662.6854712963104\n",
      "  timers:\n",
      "    learn_throughput: 300.466\n",
      "    learn_time_ms: 3328.168\n",
      "    load_throughput: 21034.993\n",
      "    load_time_ms: 47.54\n",
      "    sample_throughput: 48.303\n",
      "    sample_time_ms: 20702.824\n",
      "    update_time_ms: 6.507\n",
      "  timestamp: 1631881993\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 152\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         3662.69</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 153000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-33-35\n",
      "  done: false\n",
      "  episode_len_mean: 994.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 153\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9260594089825949\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01132183843695142\n",
      "          policy_loss: -0.06022522548834483\n",
      "          total_loss: -0.07402798264390892\n",
      "          vf_explained_var: -0.9774143695831299\n",
      "          vf_loss: 0.0018307589476333104\n",
      "    num_agent_steps_sampled: 153000\n",
      "    num_agent_steps_trained: 153000\n",
      "    num_steps_sampled: 153000\n",
      "    num_steps_trained: 153000\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.22333333333333\n",
      "    ram_util_percent: 56.62999999999999\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06302639125108396\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.271319279191918\n",
      "    mean_inference_ms: 2.3436519120155386\n",
      "    mean_raw_obs_processing_ms: 0.7918664592530545\n",
      "  time_since_restore: 3683.922568321228\n",
      "  time_this_iter_s: 21.237097024917603\n",
      "  time_total_s: 3683.922568321228\n",
      "  timers:\n",
      "    learn_throughput: 300.352\n",
      "    learn_time_ms: 3329.421\n",
      "    load_throughput: 22172.868\n",
      "    load_time_ms: 45.1\n",
      "    sample_throughput: 48.452\n",
      "    sample_time_ms: 20639.015\n",
      "    update_time_ms: 6.524\n",
      "  timestamp: 1631882015\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 153000\n",
      "  training_iteration: 153\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">         3683.92</td><td style=\"text-align: right;\">153000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 154000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-33-55\n",
      "  done: false\n",
      "  episode_len_mean: 994.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 154\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.133923077583313\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011304170291635558\n",
      "          policy_loss: -0.02418375060790115\n",
      "          total_loss: -0.04049466347528829\n",
      "          vf_explained_var: -0.7788010239601135\n",
      "          vf_loss: 0.001406897423213296\n",
      "    num_agent_steps_sampled: 154000\n",
      "    num_agent_steps_trained: 154000\n",
      "    num_steps_sampled: 154000\n",
      "    num_steps_trained: 154000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.46333333333334\n",
      "    ram_util_percent: 56.65333333333332\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06304567037801244\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.2398603855296\n",
      "    mean_inference_ms: 2.3441783494470845\n",
      "    mean_raw_obs_processing_ms: 0.794703299092277\n",
      "  time_since_restore: 3704.591030359268\n",
      "  time_this_iter_s: 20.66846203804016\n",
      "  time_total_s: 3704.591030359268\n",
      "  timers:\n",
      "    learn_throughput: 299.465\n",
      "    learn_time_ms: 3339.291\n",
      "    load_throughput: 22411.899\n",
      "    load_time_ms: 44.619\n",
      "    sample_throughput: 48.798\n",
      "    sample_time_ms: 20492.707\n",
      "    update_time_ms: 7.12\n",
      "  timestamp: 1631882035\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 154000\n",
      "  training_iteration: 154\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         3704.59</td><td style=\"text-align: right;\">154000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 155000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-34-15\n",
      "  done: false\n",
      "  episode_len_mean: 994.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 155\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3161039961708916\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011992398676154044\n",
      "          policy_loss: -0.08401796685324775\n",
      "          total_loss: -0.10255945407681995\n",
      "          vf_explained_var: -0.5244120955467224\n",
      "          vf_loss: 0.0007776464005776992\n",
      "    num_agent_steps_sampled: 155000\n",
      "    num_agent_steps_trained: 155000\n",
      "    num_steps_sampled: 155000\n",
      "    num_steps_trained: 155000\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.01071428571429\n",
      "    ram_util_percent: 56.66428571428571\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06306487921461106\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.209073464845357\n",
      "    mean_inference_ms: 2.3447088489074925\n",
      "    mean_raw_obs_processing_ms: 0.7975659550365757\n",
      "  time_since_restore: 3724.109055042267\n",
      "  time_this_iter_s: 19.518024682998657\n",
      "  time_total_s: 3724.109055042267\n",
      "  timers:\n",
      "    learn_throughput: 302.32\n",
      "    learn_time_ms: 3307.758\n",
      "    load_throughput: 22209.206\n",
      "    load_time_ms: 45.026\n",
      "    sample_throughput: 49.488\n",
      "    sample_time_ms: 20206.906\n",
      "    update_time_ms: 7.278\n",
      "  timestamp: 1631882055\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 155000\n",
      "  training_iteration: 155\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">         3724.11</td><td style=\"text-align: right;\">155000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-34-39\n",
      "  done: false\n",
      "  episode_len_mean: 994.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 156\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1101482020484075\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01273304493528949\n",
      "          policy_loss: -0.0608680527864231\n",
      "          total_loss: -0.07662692528424991\n",
      "          vf_explained_var: -0.8579201698303223\n",
      "          vf_loss: 0.0012634346281023075\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.78529411764706\n",
      "    ram_util_percent: 56.6235294117647\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06308389205523612\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.17940297657614\n",
      "    mean_inference_ms: 2.34523809964225\n",
      "    mean_raw_obs_processing_ms: 0.8004514895423366\n",
      "  time_since_restore: 3748.0192234516144\n",
      "  time_this_iter_s: 23.910168409347534\n",
      "  time_total_s: 3748.0192234516144\n",
      "  timers:\n",
      "    learn_throughput: 303.13\n",
      "    learn_time_ms: 3298.914\n",
      "    load_throughput: 22311.659\n",
      "    load_time_ms: 44.82\n",
      "    sample_throughput: 49.233\n",
      "    sample_time_ms: 20311.704\n",
      "    update_time_ms: 7.393\n",
      "  timestamp: 1631882079\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 156\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   156</td><td style=\"text-align: right;\">         3748.02</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 157000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-35-01\n",
      "  done: false\n",
      "  episode_len_mean: 994.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 157\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8197116057078044\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006822921506126889\n",
      "          policy_loss: -0.08421174919025766\n",
      "          total_loss: -0.09916790910065174\n",
      "          vf_explained_var: -0.2679310739040375\n",
      "          vf_loss: 0.0010551560983812023\n",
      "    num_agent_steps_sampled: 157000\n",
      "    num_agent_steps_trained: 157000\n",
      "    num_steps_sampled: 157000\n",
      "    num_steps_trained: 157000\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.81935483870969\n",
      "    ram_util_percent: 56.68387096774194\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06310291290248166\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.150573291487408\n",
      "    mean_inference_ms: 2.34576617697407\n",
      "    mean_raw_obs_processing_ms: 0.8033592411468321\n",
      "  time_since_restore: 3769.999830007553\n",
      "  time_this_iter_s: 21.98060655593872\n",
      "  time_total_s: 3769.999830007553\n",
      "  timers:\n",
      "    learn_throughput: 302.453\n",
      "    learn_time_ms: 3306.298\n",
      "    load_throughput: 23080.552\n",
      "    load_time_ms: 43.327\n",
      "    sample_throughput: 49.107\n",
      "    sample_time_ms: 20363.85\n",
      "    update_time_ms: 7.193\n",
      "  timestamp: 1631882101\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 157000\n",
      "  training_iteration: 157\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">            3770</td><td style=\"text-align: right;\">157000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 158000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-35-22\n",
      "  done: false\n",
      "  episode_len_mean: 994.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 158\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2133744253052605\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007080793649837997\n",
      "          policy_loss: -0.029157265772422156\n",
      "          total_loss: -0.03811273587246736\n",
      "          vf_explained_var: -0.5103279948234558\n",
      "          vf_loss: 0.0009098596651003593\n",
      "    num_agent_steps_sampled: 158000\n",
      "    num_agent_steps_trained: 158000\n",
      "    num_steps_sampled: 158000\n",
      "    num_steps_trained: 158000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.73666666666668\n",
      "    ram_util_percent: 56.68333333333332\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06312264839408162\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.122655945624153\n",
      "    mean_inference_ms: 2.3463016171667173\n",
      "    mean_raw_obs_processing_ms: 0.8062875167402703\n",
      "  time_since_restore: 3790.9789402484894\n",
      "  time_this_iter_s: 20.97911024093628\n",
      "  time_total_s: 3790.9789402484894\n",
      "  timers:\n",
      "    learn_throughput: 300.514\n",
      "    learn_time_ms: 3327.632\n",
      "    load_throughput: 23268.002\n",
      "    load_time_ms: 42.977\n",
      "    sample_throughput: 49.634\n",
      "    sample_time_ms: 20147.387\n",
      "    update_time_ms: 7.251\n",
      "  timestamp: 1631882122\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 158000\n",
      "  training_iteration: 158\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">         3790.98</td><td style=\"text-align: right;\">158000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 159000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-35-42\n",
      "  done: false\n",
      "  episode_len_mean: 994.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 159\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9229062226083544\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009496264842429901\n",
      "          policy_loss: -0.06700451225042343\n",
      "          total_loss: -0.08211902408964104\n",
      "          vf_explained_var: -0.5830950140953064\n",
      "          vf_loss: 0.001072315273065922\n",
      "    num_agent_steps_sampled: 159000\n",
      "    num_agent_steps_trained: 159000\n",
      "    num_steps_sampled: 159000\n",
      "    num_steps_trained: 159000\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.94999999999999\n",
      "    ram_util_percent: 56.71666666666666\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06314277104413107\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.095635074167664\n",
      "    mean_inference_ms: 2.3468491729101064\n",
      "    mean_raw_obs_processing_ms: 0.8092383354825232\n",
      "  time_since_restore: 3811.619457960129\n",
      "  time_this_iter_s: 20.640517711639404\n",
      "  time_total_s: 3811.619457960129\n",
      "  timers:\n",
      "    learn_throughput: 299.739\n",
      "    learn_time_ms: 3336.235\n",
      "    load_throughput: 23451.177\n",
      "    load_time_ms: 42.642\n",
      "    sample_throughput: 50.057\n",
      "    sample_time_ms: 19977.299\n",
      "    update_time_ms: 7.13\n",
      "  timestamp: 1631882142\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 159000\n",
      "  training_iteration: 159\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">         3811.62</td><td style=\"text-align: right;\">159000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            994.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-36-05\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 160\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5375478982925415\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012381180438810554\n",
      "          policy_loss: -0.05140774490104781\n",
      "          total_loss: -0.060831688758399755\n",
      "          vf_explained_var: 0.060454294085502625\n",
      "          vf_loss: 0.0019850851256503826\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.99375\n",
      "    ram_util_percent: 56.790625\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06316317711435593\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.06942124189558\n",
      "    mean_inference_ms: 2.347408032923968\n",
      "    mean_raw_obs_processing_ms: 0.8092691487599423\n",
      "  time_since_restore: 3834.0607137680054\n",
      "  time_this_iter_s: 22.441255807876587\n",
      "  time_total_s: 3834.0607137680054\n",
      "  timers:\n",
      "    learn_throughput: 299.344\n",
      "    learn_time_ms: 3340.636\n",
      "    load_throughput: 22921.222\n",
      "    load_time_ms: 43.628\n",
      "    sample_throughput: 55.155\n",
      "    sample_time_ms: 18130.859\n",
      "    update_time_ms: 6.753\n",
      "  timestamp: 1631882165\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 160\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         3834.06</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 161000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-36-26\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 161\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9031488332483504\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011671738904660639\n",
      "          policy_loss: -0.0023719746619462966\n",
      "          total_loss: -0.016241512530379826\n",
      "          vf_explained_var: -0.07053528726100922\n",
      "          vf_loss: 0.0014227746879138673\n",
      "    num_agent_steps_sampled: 161000\n",
      "    num_agent_steps_trained: 161000\n",
      "    num_steps_sampled: 161000\n",
      "    num_steps_trained: 161000\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.16000000000003\n",
      "    ram_util_percent: 56.69000000000001\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0631828763267222\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.04389412590384\n",
      "    mean_inference_ms: 2.3479633792234766\n",
      "    mean_raw_obs_processing_ms: 0.809363943571345\n",
      "  time_since_restore: 3855.2260949611664\n",
      "  time_this_iter_s: 21.16538119316101\n",
      "  time_total_s: 3855.2260949611664\n",
      "  timers:\n",
      "    learn_throughput: 298.985\n",
      "    learn_time_ms: 3344.653\n",
      "    load_throughput: 23142.699\n",
      "    load_time_ms: 43.21\n",
      "    sample_throughput: 55.613\n",
      "    sample_time_ms: 17981.284\n",
      "    update_time_ms: 6.741\n",
      "  timestamp: 1631882186\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 161000\n",
      "  training_iteration: 161\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">         3855.23</td><td style=\"text-align: right;\">161000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 162000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-36-47\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 162\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9414480553732978\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013559135910729225\n",
      "          policy_loss: -0.04931067584289445\n",
      "          total_loss: -0.062319658531083004\n",
      "          vf_explained_var: -0.4272163510322571\n",
      "          vf_loss: 0.0020616766880266367\n",
      "    num_agent_steps_sampled: 162000\n",
      "    num_agent_steps_trained: 162000\n",
      "    num_steps_sampled: 162000\n",
      "    num_steps_trained: 162000\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.34666666666666\n",
      "    ram_util_percent: 56.70333333333333\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06320222788403038\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.01893886515852\n",
      "    mean_inference_ms: 2.34849940579062\n",
      "    mean_raw_obs_processing_ms: 0.8095141991309996\n",
      "  time_since_restore: 3876.2278735637665\n",
      "  time_this_iter_s: 21.001778602600098\n",
      "  time_total_s: 3876.2278735637665\n",
      "  timers:\n",
      "    learn_throughput: 296.282\n",
      "    learn_time_ms: 3375.167\n",
      "    load_throughput: 22891.011\n",
      "    load_time_ms: 43.685\n",
      "    sample_throughput: 55.818\n",
      "    sample_time_ms: 17915.266\n",
      "    update_time_ms: 6.792\n",
      "  timestamp: 1631882207\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 162000\n",
      "  training_iteration: 162\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         3876.23</td><td style=\"text-align: right;\">162000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 163000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-37-09\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 163\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.052362717522515\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01045439754165792\n",
      "          policy_loss: -0.06539859386781852\n",
      "          total_loss: -0.08117583679656187\n",
      "          vf_explained_var: -0.6981022953987122\n",
      "          vf_loss: 0.001397198118502274\n",
      "    num_agent_steps_sampled: 163000\n",
      "    num_agent_steps_trained: 163000\n",
      "    num_steps_sampled: 163000\n",
      "    num_steps_trained: 163000\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.29677419354839\n",
      "    ram_util_percent: 56.670967741935485\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06322158926217844\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.994755871072357\n",
      "    mean_inference_ms: 2.349024157920095\n",
      "    mean_raw_obs_processing_ms: 0.8097214288216753\n",
      "  time_since_restore: 3898.042073726654\n",
      "  time_this_iter_s: 21.814200162887573\n",
      "  time_total_s: 3898.042073726654\n",
      "  timers:\n",
      "    learn_throughput: 295.433\n",
      "    learn_time_ms: 3384.865\n",
      "    load_throughput: 22557.65\n",
      "    load_time_ms: 44.331\n",
      "    sample_throughput: 55.672\n",
      "    sample_time_ms: 17962.365\n",
      "    update_time_ms: 7.102\n",
      "  timestamp: 1631882229\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 163000\n",
      "  training_iteration: 163\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   163</td><td style=\"text-align: right;\">         3898.04</td><td style=\"text-align: right;\">163000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-37-31\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 164\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8629295614030625\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010411088165436391\n",
      "          policy_loss: 0.004538694272438685\n",
      "          total_loss: -0.009293776295251317\n",
      "          vf_explained_var: -0.6668480634689331\n",
      "          vf_loss: 0.0014615147183778593\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.58709677419354\n",
      "    ram_util_percent: 56.64193548387096\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06324049596599886\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.971424600629188\n",
      "    mean_inference_ms: 2.349534241005186\n",
      "    mean_raw_obs_processing_ms: 0.8099854226694516\n",
      "  time_since_restore: 3920.01265668869\n",
      "  time_this_iter_s: 21.970582962036133\n",
      "  time_total_s: 3920.01265668869\n",
      "  timers:\n",
      "    learn_throughput: 296.283\n",
      "    learn_time_ms: 3375.148\n",
      "    load_throughput: 24563.028\n",
      "    load_time_ms: 40.712\n",
      "    sample_throughput: 55.228\n",
      "    sample_time_ms: 18106.621\n",
      "    update_time_ms: 6.561\n",
      "  timestamp: 1631882251\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 164\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         3920.01</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 165000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-37-52\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 165\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3019887030124664\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012157731157288826\n",
      "          policy_loss: -0.06137906619244152\n",
      "          total_loss: -0.06914592625366317\n",
      "          vf_explained_var: -0.9362831115722656\n",
      "          vf_loss: 0.001358162499188135\n",
      "    num_agent_steps_sampled: 165000\n",
      "    num_agent_steps_trained: 165000\n",
      "    num_steps_sampled: 165000\n",
      "    num_steps_trained: 165000\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.30000000000001\n",
      "    ram_util_percent: 56.63548387096775\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06325924312646816\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.948741267980417\n",
      "    mean_inference_ms: 2.350042212538176\n",
      "    mean_raw_obs_processing_ms: 0.8103049901225675\n",
      "  time_since_restore: 3941.2981326580048\n",
      "  time_this_iter_s: 21.285475969314575\n",
      "  time_total_s: 3941.2981326580048\n",
      "  timers:\n",
      "    learn_throughput: 295.973\n",
      "    learn_time_ms: 3378.688\n",
      "    load_throughput: 26183.994\n",
      "    load_time_ms: 38.191\n",
      "    sample_throughput: 54.695\n",
      "    sample_time_ms: 18283.05\n",
      "    update_time_ms: 6.376\n",
      "  timestamp: 1631882272\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 165000\n",
      "  training_iteration: 165\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">          3941.3</td><td style=\"text-align: right;\">165000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 166000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-38-13\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 166\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8888964268896316\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01088152402166575\n",
      "          policy_loss: 0.01325711500313547\n",
      "          total_loss: -0.0007654618471860886\n",
      "          vf_explained_var: -0.5768797993659973\n",
      "          vf_loss: 0.0013803680252749474\n",
      "    num_agent_steps_sampled: 166000\n",
      "    num_agent_steps_trained: 166000\n",
      "    num_steps_sampled: 166000\n",
      "    num_steps_trained: 166000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.71724137931034\n",
      "    ram_util_percent: 56.61724137931035\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06327769467616569\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.92624206416522\n",
      "    mean_inference_ms: 2.3505433039131383\n",
      "    mean_raw_obs_processing_ms: 0.8106625979138403\n",
      "  time_since_restore: 3961.8129308223724\n",
      "  time_this_iter_s: 20.514798164367676\n",
      "  time_total_s: 3961.8129308223724\n",
      "  timers:\n",
      "    learn_throughput: 298.544\n",
      "    learn_time_ms: 3349.59\n",
      "    load_throughput: 25758.791\n",
      "    load_time_ms: 38.822\n",
      "    sample_throughput: 55.642\n",
      "    sample_time_ms: 17972.083\n",
      "    update_time_ms: 6.243\n",
      "  timestamp: 1631882293\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 166000\n",
      "  training_iteration: 166\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   166</td><td style=\"text-align: right;\">         3961.81</td><td style=\"text-align: right;\">166000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 167000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-38-34\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 167\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2537896593411764\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023187873543705568\n",
      "          policy_loss: -0.04290616363286972\n",
      "          total_loss: -0.04662536842127641\n",
      "          vf_explained_var: -0.14490056037902832\n",
      "          vf_loss: 0.0013901956948555178\n",
      "    num_agent_steps_sampled: 167000\n",
      "    num_agent_steps_trained: 167000\n",
      "    num_steps_sampled: 167000\n",
      "    num_steps_trained: 167000\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.36129032258064\n",
      "    ram_util_percent: 56.61290322580645\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06329573122810407\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.904306018771905\n",
      "    mean_inference_ms: 2.3510318025938544\n",
      "    mean_raw_obs_processing_ms: 0.811070278340758\n",
      "  time_since_restore: 3983.2696499824524\n",
      "  time_this_iter_s: 21.456719160079956\n",
      "  time_total_s: 3983.2696499824524\n",
      "  timers:\n",
      "    learn_throughput: 298.25\n",
      "    learn_time_ms: 3352.897\n",
      "    load_throughput: 25015.814\n",
      "    load_time_ms: 39.975\n",
      "    sample_throughput: 55.819\n",
      "    sample_time_ms: 17915.104\n",
      "    update_time_ms: 6.416\n",
      "  timestamp: 1631882314\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 167000\n",
      "  training_iteration: 167\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         3983.27</td><td style=\"text-align: right;\">167000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-38-54\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 168\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9731562415758768\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010086247661471386\n",
      "          policy_loss: -0.04666903391480446\n",
      "          total_loss: -0.06050267008443674\n",
      "          vf_explained_var: -0.2763653099536896\n",
      "          vf_loss: 0.0010510609514312818\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.90714285714284\n",
      "    ram_util_percent: 56.625\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06331332286660635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.882675455061495\n",
      "    mean_inference_ms: 2.3514990612270354\n",
      "    mean_raw_obs_processing_ms: 0.8115214926367371\n",
      "  time_since_restore: 4003.186995744705\n",
      "  time_this_iter_s: 19.917345762252808\n",
      "  time_total_s: 4003.186995744705\n",
      "  timers:\n",
      "    learn_throughput: 297.298\n",
      "    learn_time_ms: 3363.628\n",
      "    load_throughput: 23665.293\n",
      "    load_time_ms: 42.256\n",
      "    sample_throughput: 56.18\n",
      "    sample_time_ms: 17799.997\n",
      "    update_time_ms: 6.234\n",
      "  timestamp: 1631882334\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 168\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">         4003.19</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 169000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-39-16\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 169\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.808015563752916\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00807083169733649\n",
      "          policy_loss: -0.06510322985963689\n",
      "          total_loss: -0.07802431504759523\n",
      "          vf_explained_var: -0.7681087851524353\n",
      "          vf_loss: 0.0012806937845501428\n",
      "    num_agent_steps_sampled: 169000\n",
      "    num_agent_steps_trained: 169000\n",
      "    num_steps_sampled: 169000\n",
      "    num_steps_trained: 169000\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.85806451612905\n",
      "    ram_util_percent: 56.67419354838711\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06333107955540954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.861576470760728\n",
      "    mean_inference_ms: 2.3519407749682397\n",
      "    mean_raw_obs_processing_ms: 0.8120154193915581\n",
      "  time_since_restore: 4024.4549584388733\n",
      "  time_this_iter_s: 21.26796269416809\n",
      "  time_total_s: 4024.4549584388733\n",
      "  timers:\n",
      "    learn_throughput: 296.929\n",
      "    learn_time_ms: 3367.808\n",
      "    load_throughput: 25790.977\n",
      "    load_time_ms: 38.773\n",
      "    sample_throughput: 55.983\n",
      "    sample_time_ms: 17862.608\n",
      "    update_time_ms: 6.319\n",
      "  timestamp: 1631882356\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 169000\n",
      "  training_iteration: 169\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         4024.45</td><td style=\"text-align: right;\">169000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 170000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-39-37\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 170\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8732129732767742\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01494741758274642\n",
      "          policy_loss: -0.06217225169142087\n",
      "          total_loss: -0.07261846156583893\n",
      "          vf_explained_var: -0.12025958299636841\n",
      "          vf_loss: 0.0011030560044067292\n",
      "    num_agent_steps_sampled: 170000\n",
      "    num_agent_steps_trained: 170000\n",
      "    num_steps_sampled: 170000\n",
      "    num_steps_trained: 170000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.50333333333334\n",
      "    ram_util_percent: 56.71999999999999\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06334857006308946\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.84058302304667\n",
      "    mean_inference_ms: 2.3523737356537175\n",
      "    mean_raw_obs_processing_ms: 0.8125527494755144\n",
      "  time_since_restore: 4046.026031970978\n",
      "  time_this_iter_s: 21.571073532104492\n",
      "  time_total_s: 4046.026031970978\n",
      "  timers:\n",
      "    learn_throughput: 299.618\n",
      "    learn_time_ms: 3337.581\n",
      "    load_throughput: 25629.973\n",
      "    load_time_ms: 39.017\n",
      "    sample_throughput: 56.163\n",
      "    sample_time_ms: 17805.322\n",
      "    update_time_ms: 6.476\n",
      "  timestamp: 1631882377\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 170000\n",
      "  training_iteration: 170\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">         4046.03</td><td style=\"text-align: right;\">170000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 171000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-39-58\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 171\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8517875777350532\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0075463426812566875\n",
      "          policy_loss: -0.06459145405226284\n",
      "          total_loss: -0.07882903392116229\n",
      "          vf_explained_var: -0.514999270439148\n",
      "          vf_loss: 0.0006539617911686138\n",
      "    num_agent_steps_sampled: 171000\n",
      "    num_agent_steps_trained: 171000\n",
      "    num_steps_sampled: 171000\n",
      "    num_steps_trained: 171000\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.18666666666667\n",
      "    ram_util_percent: 56.666666666666664\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06336568609249593\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.819996604119837\n",
      "    mean_inference_ms: 2.352804549097212\n",
      "    mean_raw_obs_processing_ms: 0.8131296230757507\n",
      "  time_since_restore: 4066.471582174301\n",
      "  time_this_iter_s: 20.445550203323364\n",
      "  time_total_s: 4066.471582174301\n",
      "  timers:\n",
      "    learn_throughput: 301.193\n",
      "    learn_time_ms: 3320.131\n",
      "    load_throughput: 25152.478\n",
      "    load_time_ms: 39.758\n",
      "    sample_throughput: 56.338\n",
      "    sample_time_ms: 17749.883\n",
      "    update_time_ms: 6.52\n",
      "  timestamp: 1631882398\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 171000\n",
      "  training_iteration: 171\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         4066.47</td><td style=\"text-align: right;\">171000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-40-20\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 172\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8555278327729967\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008513789398525477\n",
      "          policy_loss: -0.10576918911602762\n",
      "          total_loss: -0.11930989631348186\n",
      "          vf_explained_var: -0.6051615476608276\n",
      "          vf_loss: 0.0009233383022041785\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.8741935483871\n",
      "    ram_util_percent: 56.751612903225805\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06338250382231778\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.799864695469456\n",
      "    mean_inference_ms: 2.3532360991528813\n",
      "    mean_raw_obs_processing_ms: 0.8137469487671474\n",
      "  time_since_restore: 4088.227724313736\n",
      "  time_this_iter_s: 21.756142139434814\n",
      "  time_total_s: 4088.227724313736\n",
      "  timers:\n",
      "    learn_throughput: 302.398\n",
      "    learn_time_ms: 3306.899\n",
      "    load_throughput: 24900.361\n",
      "    load_time_ms: 40.16\n",
      "    sample_throughput: 56.059\n",
      "    sample_time_ms: 17838.342\n",
      "    update_time_ms: 6.539\n",
      "  timestamp: 1631882420\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 172\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">         4088.23</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 173000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-40-41\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 173\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.908419312371148\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012835154938535136\n",
      "          policy_loss: -0.056669689135419\n",
      "          total_loss: -0.06728586935334735\n",
      "          vf_explained_var: -0.5707488656044006\n",
      "          vf_loss: 0.002300182845081306\n",
      "    num_agent_steps_sampled: 173000\n",
      "    num_agent_steps_trained: 173000\n",
      "    num_steps_sampled: 173000\n",
      "    num_steps_trained: 173000\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.54333333333334\n",
      "    ram_util_percent: 56.666666666666664\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06339965960681741\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.780249910588346\n",
      "    mean_inference_ms: 2.3536809929163716\n",
      "    mean_raw_obs_processing_ms: 0.8144019837027272\n",
      "  time_since_restore: 4109.4809901714325\n",
      "  time_this_iter_s: 21.253265857696533\n",
      "  time_total_s: 4109.4809901714325\n",
      "  timers:\n",
      "    learn_throughput: 304.141\n",
      "    learn_time_ms: 3287.947\n",
      "    load_throughput: 23955.953\n",
      "    load_time_ms: 41.743\n",
      "    sample_throughput: 56.183\n",
      "    sample_time_ms: 17798.897\n",
      "    update_time_ms: 6.926\n",
      "  timestamp: 1631882441\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 173000\n",
      "  training_iteration: 173\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         4109.48</td><td style=\"text-align: right;\">173000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 174000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-41-02\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 174\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8957646833525763\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00956577505889585\n",
      "          policy_loss: -0.018407573882076474\n",
      "          total_loss: -0.03147956869668431\n",
      "          vf_explained_var: -0.835473895072937\n",
      "          vf_loss: 0.0012888919707620516\n",
      "    num_agent_steps_sampled: 174000\n",
      "    num_agent_steps_trained: 174000\n",
      "    num_steps_sampled: 174000\n",
      "    num_steps_trained: 174000\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.71333333333334\n",
      "    ram_util_percent: 56.78666666666666\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06341716930450016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.761142974688756\n",
      "    mean_inference_ms: 2.3541401110809277\n",
      "    mean_raw_obs_processing_ms: 0.8150997495863337\n",
      "  time_since_restore: 4130.671837806702\n",
      "  time_this_iter_s: 21.190847635269165\n",
      "  time_total_s: 4130.671837806702\n",
      "  timers:\n",
      "    learn_throughput: 304.491\n",
      "    learn_time_ms: 3284.164\n",
      "    load_throughput: 22455.106\n",
      "    load_time_ms: 44.533\n",
      "    sample_throughput: 56.428\n",
      "    sample_time_ms: 17721.578\n",
      "    update_time_ms: 6.976\n",
      "  timestamp: 1631882462\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 174000\n",
      "  training_iteration: 174\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   174</td><td style=\"text-align: right;\">         4130.67</td><td style=\"text-align: right;\">174000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 175000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-41-23\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 175\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.181014213297102\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00871430510691863\n",
      "          policy_loss: -0.014583014986581273\n",
      "          total_loss: -0.03146728517280684\n",
      "          vf_explained_var: -0.44637179374694824\n",
      "          vf_loss: 0.0007382844631340251\n",
      "    num_agent_steps_sampled: 175000\n",
      "    num_agent_steps_trained: 175000\n",
      "    num_steps_sampled: 175000\n",
      "    num_steps_trained: 175000\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.13000000000001\n",
      "    ram_util_percent: 56.60666666666666\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06343563523498476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.742295789350408\n",
      "    mean_inference_ms: 2.354616708466302\n",
      "    mean_raw_obs_processing_ms: 0.8158384685923664\n",
      "  time_since_restore: 4151.637649536133\n",
      "  time_this_iter_s: 20.965811729431152\n",
      "  time_total_s: 4151.637649536133\n",
      "  timers:\n",
      "    learn_throughput: 302.286\n",
      "    learn_time_ms: 3308.13\n",
      "    load_throughput: 21117.399\n",
      "    load_time_ms: 47.354\n",
      "    sample_throughput: 56.62\n",
      "    sample_time_ms: 17661.635\n",
      "    update_time_ms: 7.759\n",
      "  timestamp: 1631882483\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 175000\n",
      "  training_iteration: 175\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         4151.64</td><td style=\"text-align: right;\">175000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-41-44\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 176\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9866928180058798\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010989709013833861\n",
      "          policy_loss: -0.05865188104410966\n",
      "          total_loss: -0.07190047740522358\n",
      "          vf_explained_var: -0.9817169904708862\n",
      "          vf_loss: 0.0013373152521024975\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.83333333333333\n",
      "    ram_util_percent: 56.643333333333324\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06345433560612306\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.723600209255558\n",
      "    mean_inference_ms: 2.355102225617305\n",
      "    mean_raw_obs_processing_ms: 0.8166143874655565\n",
      "  time_since_restore: 4172.353628158569\n",
      "  time_this_iter_s: 20.715978622436523\n",
      "  time_total_s: 4172.353628158569\n",
      "  timers:\n",
      "    learn_throughput: 299.735\n",
      "    learn_time_ms: 3336.278\n",
      "    load_throughput: 21205.909\n",
      "    load_time_ms: 47.157\n",
      "    sample_throughput: 56.648\n",
      "    sample_time_ms: 17652.987\n",
      "    update_time_ms: 8.393\n",
      "  timestamp: 1631882504\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 176\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   176</td><td style=\"text-align: right;\">         4172.35</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 177000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-42-05\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 177\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9152248991860283\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012769913084982716\n",
      "          policy_loss: -0.07508023654421171\n",
      "          total_loss: -0.08684204684363471\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0012539586239856564\n",
      "    num_agent_steps_sampled: 177000\n",
      "    num_agent_steps_trained: 177000\n",
      "    num_steps_sampled: 177000\n",
      "    num_steps_trained: 177000\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.65\n",
      "    ram_util_percent: 56.54333333333333\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06347328851769396\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.70455122110033\n",
      "    mean_inference_ms: 2.3555933241846456\n",
      "    mean_raw_obs_processing_ms: 0.8174232376199673\n",
      "  time_since_restore: 4193.479369163513\n",
      "  time_this_iter_s: 21.125741004943848\n",
      "  time_total_s: 4193.479369163513\n",
      "  timers:\n",
      "    learn_throughput: 300.287\n",
      "    learn_time_ms: 3330.151\n",
      "    load_throughput: 20976.596\n",
      "    load_time_ms: 47.672\n",
      "    sample_throughput: 56.738\n",
      "    sample_time_ms: 17624.983\n",
      "    update_time_ms: 8.274\n",
      "  timestamp: 1631882525\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 177000\n",
      "  training_iteration: 177\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         4193.48</td><td style=\"text-align: right;\">177000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 178000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-42-29\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 178\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9787060446209377\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009866772151086502\n",
      "          policy_loss: 0.027354151838355593\n",
      "          total_loss: 0.013159998754660288\n",
      "          vf_explained_var: -0.5583335757255554\n",
      "          vf_loss: 0.0008515084919054061\n",
      "    num_agent_steps_sampled: 178000\n",
      "    num_agent_steps_trained: 178000\n",
      "    num_steps_sampled: 178000\n",
      "    num_steps_trained: 178000\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.91818181818181\n",
      "    ram_util_percent: 56.7\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0634926325911625\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.685869079353008\n",
      "    mean_inference_ms: 2.3560920519714155\n",
      "    mean_raw_obs_processing_ms: 0.8182686276416731\n",
      "  time_since_restore: 4216.97718501091\n",
      "  time_this_iter_s: 23.49781584739685\n",
      "  time_total_s: 4216.97718501091\n",
      "  timers:\n",
      "    learn_throughput: 301.486\n",
      "    learn_time_ms: 3316.909\n",
      "    load_throughput: 21292.667\n",
      "    load_time_ms: 46.965\n",
      "    sample_throughput: 55.565\n",
      "    sample_time_ms: 17996.912\n",
      "    update_time_ms: 8.289\n",
      "  timestamp: 1631882549\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 178000\n",
      "  training_iteration: 178\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   178</td><td style=\"text-align: right;\">         4216.98</td><td style=\"text-align: right;\">178000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 179000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-42-49\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 179\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9158910459942289\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01372321819357675\n",
      "          policy_loss: -0.00395935003956159\n",
      "          total_loss: -0.014950409200456408\n",
      "          vf_explained_var: -0.5404573678970337\n",
      "          vf_loss: 0.0015732685453258455\n",
      "    num_agent_steps_sampled: 179000\n",
      "    num_agent_steps_trained: 179000\n",
      "    num_steps_sampled: 179000\n",
      "    num_steps_trained: 179000\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.36999999999999\n",
      "    ram_util_percent: 56.74666666666666\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0635120220469284\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.667669445358143\n",
      "    mean_inference_ms: 2.3565992419615758\n",
      "    mean_raw_obs_processing_ms: 0.8191487775478602\n",
      "  time_since_restore: 4237.552829027176\n",
      "  time_this_iter_s: 20.57564401626587\n",
      "  time_total_s: 4237.552829027176\n",
      "  timers:\n",
      "    learn_throughput: 302.98\n",
      "    learn_time_ms: 3300.546\n",
      "    load_throughput: 21289.5\n",
      "    load_time_ms: 46.972\n",
      "    sample_throughput: 55.73\n",
      "    sample_time_ms: 17943.779\n",
      "    update_time_ms: 8.614\n",
      "  timestamp: 1631882569\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 179000\n",
      "  training_iteration: 179\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         4237.55</td><td style=\"text-align: right;\">179000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-43-34\n",
      "  done: false\n",
      "  episode_len_mean: 994.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 180\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.910135226779514\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009820711336038339\n",
      "          policy_loss: -0.014753762694696585\n",
      "          total_loss: -0.027690749636126888\n",
      "          vf_explained_var: -0.5704227685928345\n",
      "          vf_loss: 0.001445098905565424\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.04603174603176\n",
      "    ram_util_percent: 56.87777777777777\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06353201984644503\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.64976505013831\n",
      "    mean_inference_ms: 2.3571241415742885\n",
      "    mean_raw_obs_processing_ms: 0.821367915926154\n",
      "  time_since_restore: 4281.910037755966\n",
      "  time_this_iter_s: 44.35720872879028\n",
      "  time_total_s: 4281.910037755966\n",
      "  timers:\n",
      "    learn_throughput: 301.2\n",
      "    learn_time_ms: 3320.057\n",
      "    load_throughput: 21161.892\n",
      "    load_time_ms: 47.255\n",
      "    sample_throughput: 49.499\n",
      "    sample_time_ms: 20202.56\n",
      "    update_time_ms: 8.638\n",
      "  timestamp: 1631882614\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 180\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         4281.91</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 181000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-43-54\n",
      "  done: false\n",
      "  episode_len_mean: 994.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 181\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.795401806301541\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008676564348699988\n",
      "          policy_loss: -0.07895863975087801\n",
      "          total_loss: -0.09121870882809162\n",
      "          vf_explained_var: -0.6625661849975586\n",
      "          vf_loss: 0.0015244932735287067\n",
      "    num_agent_steps_sampled: 181000\n",
      "    num_agent_steps_trained: 181000\n",
      "    num_steps_sampled: 181000\n",
      "    num_steps_trained: 181000\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.52413793103447\n",
      "    ram_util_percent: 56.91724137931034\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06355243924713727\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.632186753453073\n",
      "    mean_inference_ms: 2.357662646206456\n",
      "    mean_raw_obs_processing_ms: 0.8236087152856646\n",
      "  time_since_restore: 4302.376465082169\n",
      "  time_this_iter_s: 20.466427326202393\n",
      "  time_total_s: 4302.376465082169\n",
      "  timers:\n",
      "    learn_throughput: 299.663\n",
      "    learn_time_ms: 3337.082\n",
      "    load_throughput: 21090.343\n",
      "    load_time_ms: 47.415\n",
      "    sample_throughput: 49.539\n",
      "    sample_time_ms: 20186.161\n",
      "    update_time_ms: 8.832\n",
      "  timestamp: 1631882634\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 181000\n",
      "  training_iteration: 181\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   181</td><td style=\"text-align: right;\">         4302.38</td><td style=\"text-align: right;\">181000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 182000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-44-14\n",
      "  done: false\n",
      "  episode_len_mean: 994.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 182\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.803948590490553\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010481161689088065\n",
      "          policy_loss: -0.11352839446109202\n",
      "          total_loss: -0.12500043412567013\n",
      "          vf_explained_var: -0.7762566208839417\n",
      "          vf_loss: 0.0015308059519156814\n",
      "    num_agent_steps_sampled: 182000\n",
      "    num_agent_steps_trained: 182000\n",
      "    num_steps_sampled: 182000\n",
      "    num_steps_trained: 182000\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.07241379310346\n",
      "    ram_util_percent: 57.05172413793103\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06357340427869236\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.61468196562338\n",
      "    mean_inference_ms: 2.3582117050255436\n",
      "    mean_raw_obs_processing_ms: 0.8258668353964276\n",
      "  time_since_restore: 4322.256494998932\n",
      "  time_this_iter_s: 19.880029916763306\n",
      "  time_total_s: 4322.256494998932\n",
      "  timers:\n",
      "    learn_throughput: 300.799\n",
      "    learn_time_ms: 3324.484\n",
      "    load_throughput: 21936.909\n",
      "    load_time_ms: 45.585\n",
      "    sample_throughput: 49.968\n",
      "    sample_time_ms: 20012.748\n",
      "    update_time_ms: 9.064\n",
      "  timestamp: 1631882654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 182000\n",
      "  training_iteration: 182\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">         4322.26</td><td style=\"text-align: right;\">182000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 183000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-44-35\n",
      "  done: false\n",
      "  episode_len_mean: 994.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 183\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9157222933239406\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011354019971735832\n",
      "          policy_loss: -0.04537493917677138\n",
      "          total_loss: -0.05807631876733568\n",
      "          vf_explained_var: -0.8247475624084473\n",
      "          vf_loss: 0.0009997572483068022\n",
      "    num_agent_steps_sampled: 183000\n",
      "    num_agent_steps_trained: 183000\n",
      "    num_steps_sampled: 183000\n",
      "    num_steps_trained: 183000\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.0206896551724\n",
      "    ram_util_percent: 57.07241379310346\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06359487372391946\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.5974640796503\n",
      "    mean_inference_ms: 2.3587610020340826\n",
      "    mean_raw_obs_processing_ms: 0.8281435692768799\n",
      "  time_since_restore: 4342.977040052414\n",
      "  time_this_iter_s: 20.720545053482056\n",
      "  time_total_s: 4342.977040052414\n",
      "  timers:\n",
      "    learn_throughput: 300.028\n",
      "    learn_time_ms: 3333.017\n",
      "    load_throughput: 22442.731\n",
      "    load_time_ms: 44.558\n",
      "    sample_throughput: 50.121\n",
      "    sample_time_ms: 19951.553\n",
      "    update_time_ms: 8.768\n",
      "  timestamp: 1631882675\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 183000\n",
      "  training_iteration: 183\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">         4342.98</td><td style=\"text-align: right;\">183000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-44-56\n",
      "  done: false\n",
      "  episode_len_mean: 994.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 184\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7450766152805752\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011487178622993996\n",
      "          policy_loss: -0.043839367230733235\n",
      "          total_loss: -0.05462666046288278\n",
      "          vf_explained_var: -0.4535786509513855\n",
      "          vf_loss: 0.001143397972919047\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.92258064516129\n",
      "    ram_util_percent: 56.98064516129032\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06361691533413023\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.580562920882087\n",
      "    mean_inference_ms: 2.3593157288336557\n",
      "    mean_raw_obs_processing_ms: 0.8304400775272381\n",
      "  time_since_restore: 4364.556730031967\n",
      "  time_this_iter_s: 21.579689979553223\n",
      "  time_total_s: 4364.556730031967\n",
      "  timers:\n",
      "    learn_throughput: 299.051\n",
      "    learn_time_ms: 3343.916\n",
      "    load_throughput: 22724.327\n",
      "    load_time_ms: 44.006\n",
      "    sample_throughput: 50.049\n",
      "    sample_time_ms: 19980.546\n",
      "    update_time_ms: 8.638\n",
      "  timestamp: 1631882696\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 184\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">         4364.56</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 185000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-45-18\n",
      "  done: false\n",
      "  episode_len_mean: 994.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 185\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7458354817496406\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010862149467837057\n",
      "          policy_loss: -0.10133874672982428\n",
      "          total_loss: -0.11190385483205319\n",
      "          vf_explained_var: 0.25032415986061096\n",
      "          vf_loss: 0.0016735253454599943\n",
      "    num_agent_steps_sampled: 185000\n",
      "    num_agent_steps_trained: 185000\n",
      "    num_steps_sampled: 185000\n",
      "    num_steps_trained: 185000\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.0\n",
      "    ram_util_percent: 57.09999999999999\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06363953447802989\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.56372139654045\n",
      "    mean_inference_ms: 2.3598799618757327\n",
      "    mean_raw_obs_processing_ms: 0.8327574510522706\n",
      "  time_since_restore: 4386.181686639786\n",
      "  time_this_iter_s: 21.624956607818604\n",
      "  time_total_s: 4386.181686639786\n",
      "  timers:\n",
      "    learn_throughput: 299.471\n",
      "    learn_time_ms: 3339.221\n",
      "    load_throughput: 23079.129\n",
      "    load_time_ms: 43.329\n",
      "    sample_throughput: 49.868\n",
      "    sample_time_ms: 20052.759\n",
      "    update_time_ms: 7.864\n",
      "  timestamp: 1631882718\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 185000\n",
      "  training_iteration: 185\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   185</td><td style=\"text-align: right;\">         4386.18</td><td style=\"text-align: right;\">185000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 186000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-45-41\n",
      "  done: false\n",
      "  episode_len_mean: 994.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 186\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.58891415198644\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008099312289120114\n",
      "          policy_loss: -0.05437823683023453\n",
      "          total_loss: -0.06517029090060128\n",
      "          vf_explained_var: -0.797540545463562\n",
      "          vf_loss: 0.0012050279257689706\n",
      "    num_agent_steps_sampled: 186000\n",
      "    num_agent_steps_trained: 186000\n",
      "    num_steps_sampled: 186000\n",
      "    num_steps_trained: 186000\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.62727272727274\n",
      "    ram_util_percent: 57.09090909090909\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06366262570210811\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.54728689283685\n",
      "    mean_inference_ms: 2.3604609749978605\n",
      "    mean_raw_obs_processing_ms: 0.8350923163826662\n",
      "  time_since_restore: 4408.982817649841\n",
      "  time_this_iter_s: 22.801131010055542\n",
      "  time_total_s: 4408.982817649841\n",
      "  timers:\n",
      "    learn_throughput: 299.378\n",
      "    learn_time_ms: 3340.255\n",
      "    load_throughput: 23188.939\n",
      "    load_time_ms: 43.124\n",
      "    sample_throughput: 49.355\n",
      "    sample_time_ms: 20261.229\n",
      "    update_time_ms: 7.331\n",
      "  timestamp: 1631882741\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 186000\n",
      "  training_iteration: 186\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   186</td><td style=\"text-align: right;\">         4408.98</td><td style=\"text-align: right;\">186000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 187000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-46-02\n",
      "  done: false\n",
      "  episode_len_mean: 994.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 187\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9042450057135687\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014524281695022416\n",
      "          policy_loss: -0.035537341982126235\n",
      "          total_loss: -0.04676181260082457\n",
      "          vf_explained_var: -0.7296751141548157\n",
      "          vf_loss: 0.0008384496562131163\n",
      "    num_agent_steps_sampled: 187000\n",
      "    num_agent_steps_trained: 187000\n",
      "    num_steps_sampled: 187000\n",
      "    num_steps_trained: 187000\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.53103448275863\n",
      "    ram_util_percent: 57.04827586206896\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06368623025047108\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.530983617477\n",
      "    mean_inference_ms: 2.361046792950708\n",
      "    mean_raw_obs_processing_ms: 0.8374398927786872\n",
      "  time_since_restore: 4429.766911268234\n",
      "  time_this_iter_s: 20.784093618392944\n",
      "  time_total_s: 4429.766911268234\n",
      "  timers:\n",
      "    learn_throughput: 298.363\n",
      "    learn_time_ms: 3351.627\n",
      "    load_throughput: 23380.755\n",
      "    load_time_ms: 42.77\n",
      "    sample_throughput: 49.463\n",
      "    sample_time_ms: 20216.965\n",
      "    update_time_ms: 7.234\n",
      "  timestamp: 1631882762\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 187000\n",
      "  training_iteration: 187\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">         4429.77</td><td style=\"text-align: right;\">187000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-46-23\n",
      "  done: false\n",
      "  episode_len_mean: 994.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 188\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6492479681968688\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014711113687248527\n",
      "          policy_loss: -0.04128913324740198\n",
      "          total_loss: -0.04906219123966164\n",
      "          vf_explained_var: -0.41207683086395264\n",
      "          vf_loss: 0.0016501133278426197\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.54838709677419\n",
      "    ram_util_percent: 57.12258064516128\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06371057220939655\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.514727772728275\n",
      "    mean_inference_ms: 2.3616447868468247\n",
      "    mean_raw_obs_processing_ms: 0.8398006404907588\n",
      "  time_since_restore: 4451.06627202034\n",
      "  time_this_iter_s: 21.299360752105713\n",
      "  time_total_s: 4451.06627202034\n",
      "  timers:\n",
      "    learn_throughput: 299.028\n",
      "    learn_time_ms: 3344.168\n",
      "    load_throughput: 23352.168\n",
      "    load_time_ms: 42.823\n",
      "    sample_throughput: 49.989\n",
      "    sample_time_ms: 20004.541\n",
      "    update_time_ms: 7.239\n",
      "  timestamp: 1631882783\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 188\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   188</td><td style=\"text-align: right;\">         4451.07</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 189000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-46-44\n",
      "  done: false\n",
      "  episode_len_mean: 994.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 189\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.664021733072069\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008349495163351022\n",
      "          policy_loss: -0.013494716460506121\n",
      "          total_loss: -0.02474330889268054\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0013793414885488648\n",
      "    num_agent_steps_sampled: 189000\n",
      "    num_agent_steps_trained: 189000\n",
      "    num_steps_sampled: 189000\n",
      "    num_steps_trained: 189000\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.2\n",
      "    ram_util_percent: 57.12333333333333\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06373518762666383\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.49875440473381\n",
      "    mean_inference_ms: 2.3622425603233843\n",
      "    mean_raw_obs_processing_ms: 0.8421759561899778\n",
      "  time_since_restore: 4472.289743423462\n",
      "  time_this_iter_s: 21.22347140312195\n",
      "  time_total_s: 4472.289743423462\n",
      "  timers:\n",
      "    learn_throughput: 300.056\n",
      "    learn_time_ms: 3332.707\n",
      "    load_throughput: 21791.207\n",
      "    load_time_ms: 45.89\n",
      "    sample_throughput: 49.806\n",
      "    sample_time_ms: 20078.058\n",
      "    update_time_ms: 7.099\n",
      "  timestamp: 1631882804\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 189000\n",
      "  training_iteration: 189\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">         4472.29</td><td style=\"text-align: right;\">189000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 190000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-47-06\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 190\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6111059175597298\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008772848596345363\n",
      "          policy_loss: -0.10891874374614821\n",
      "          total_loss: -0.11977002268864048\n",
      "          vf_explained_var: -0.809903621673584\n",
      "          vf_loss: 0.0010440596275859409\n",
      "    num_agent_steps_sampled: 190000\n",
      "    num_agent_steps_trained: 190000\n",
      "    num_steps_sampled: 190000\n",
      "    num_steps_trained: 190000\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.06774193548387\n",
      "    ram_util_percent: 57.051612903225795\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0637601356614225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.48269129951612\n",
      "    mean_inference_ms: 2.3628457867900474\n",
      "    mean_raw_obs_processing_ms: 0.8419783394921186\n",
      "  time_since_restore: 4493.815924882889\n",
      "  time_this_iter_s: 21.52618145942688\n",
      "  time_total_s: 4493.815924882889\n",
      "  timers:\n",
      "    learn_throughput: 299.509\n",
      "    learn_time_ms: 3338.801\n",
      "    load_throughput: 22320.207\n",
      "    load_time_ms: 44.802\n",
      "    sample_throughput: 56.213\n",
      "    sample_time_ms: 17789.379\n",
      "    update_time_ms: 7.719\n",
      "  timestamp: 1631882826\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 190000\n",
      "  training_iteration: 190\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   190</td><td style=\"text-align: right;\">         4493.82</td><td style=\"text-align: right;\">190000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 191000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-47-27\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 191\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6813222487767538\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009958217426012736\n",
      "          policy_loss: -0.020095432032313613\n",
      "          total_loss: -0.03132448929051558\n",
      "          vf_explained_var: -0.7511095404624939\n",
      "          vf_loss: 0.0007988218176049283\n",
      "    num_agent_steps_sampled: 191000\n",
      "    num_agent_steps_trained: 191000\n",
      "    num_steps_sampled: 191000\n",
      "    num_steps_trained: 191000\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.25666666666666\n",
      "    ram_util_percent: 57.069999999999986\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06378506532902643\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.466719625790102\n",
      "    mean_inference_ms: 2.363446407544\n",
      "    mean_raw_obs_processing_ms: 0.8418209637206392\n",
      "  time_since_restore: 4515.0125505924225\n",
      "  time_this_iter_s: 21.19662570953369\n",
      "  time_total_s: 4515.0125505924225\n",
      "  timers:\n",
      "    learn_throughput: 299.628\n",
      "    learn_time_ms: 3337.471\n",
      "    load_throughput: 22759.976\n",
      "    load_time_ms: 43.937\n",
      "    sample_throughput: 55.975\n",
      "    sample_time_ms: 17865.186\n",
      "    update_time_ms: 8.133\n",
      "  timestamp: 1631882847\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 191000\n",
      "  training_iteration: 191\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">         4515.01</td><td style=\"text-align: right;\">191000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-47-49\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 192\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7498871445655824\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0148068818839772\n",
      "          policy_loss: -0.06780821060140928\n",
      "          total_loss: -0.07628695741295814\n",
      "          vf_explained_var: -0.29771891236305237\n",
      "          vf_loss: 0.0019047951678253917\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.06774193548388\n",
      "    ram_util_percent: 57.077419354838696\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06381004107199723\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.45121986471451\n",
      "    mean_inference_ms: 2.364052166260563\n",
      "    mean_raw_obs_processing_ms: 0.8416978895658039\n",
      "  time_since_restore: 4536.553126573563\n",
      "  time_this_iter_s: 21.540575981140137\n",
      "  time_total_s: 4536.553126573563\n",
      "  timers:\n",
      "    learn_throughput: 300.003\n",
      "    learn_time_ms: 3333.3\n",
      "    load_throughput: 22562.552\n",
      "    load_time_ms: 44.321\n",
      "    sample_throughput: 55.447\n",
      "    sample_time_ms: 18035.335\n",
      "    update_time_ms: 7.862\n",
      "  timestamp: 1631882869\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 192\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">         4536.55</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 193000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-48-12\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 193\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7184889965587191\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010668925517259157\n",
      "          policy_loss: -0.02153450660407543\n",
      "          total_loss: -0.03300156949294938\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0005909589782176125\n",
      "    num_agent_steps_sampled: 193000\n",
      "    num_agent_steps_trained: 193000\n",
      "    num_steps_sampled: 193000\n",
      "    num_steps_trained: 193000\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.34848484848484\n",
      "    ram_util_percent: 56.97272727272727\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0638352210723569\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.435901857190323\n",
      "    mean_inference_ms: 2.3646640298564217\n",
      "    mean_raw_obs_processing_ms: 0.8416145556704281\n",
      "  time_since_restore: 4559.436215639114\n",
      "  time_this_iter_s: 22.883089065551758\n",
      "  time_total_s: 4559.436215639114\n",
      "  timers:\n",
      "    learn_throughput: 301.381\n",
      "    learn_time_ms: 3318.063\n",
      "    load_throughput: 21603.442\n",
      "    load_time_ms: 46.289\n",
      "    sample_throughput: 54.747\n",
      "    sample_time_ms: 18265.995\n",
      "    update_time_ms: 7.723\n",
      "  timestamp: 1631882892\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 193000\n",
      "  training_iteration: 193\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   193</td><td style=\"text-align: right;\">         4559.44</td><td style=\"text-align: right;\">193000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 194000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-48-33\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 194\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7440305034319559\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0072871291910290535\n",
      "          policy_loss: 0.08529128084580104\n",
      "          total_loss: 0.07199059832427236\n",
      "          vf_explained_var: -0.990918755531311\n",
      "          vf_loss: 0.0006378488870622176\n",
      "    num_agent_steps_sampled: 194000\n",
      "    num_agent_steps_trained: 194000\n",
      "    num_steps_sampled: 194000\n",
      "    num_steps_trained: 194000\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.7\n",
      "    ram_util_percent: 57.07999999999999\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06386058662162632\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.420809421415015\n",
      "    mean_inference_ms: 2.3652796578103055\n",
      "    mean_raw_obs_processing_ms: 0.8415654326078993\n",
      "  time_since_restore: 4580.594081878662\n",
      "  time_this_iter_s: 21.15786623954773\n",
      "  time_total_s: 4580.594081878662\n",
      "  timers:\n",
      "    learn_throughput: 302.822\n",
      "    learn_time_ms: 3302.274\n",
      "    load_throughput: 20843.882\n",
      "    load_time_ms: 47.976\n",
      "    sample_throughput: 54.832\n",
      "    sample_time_ms: 18237.553\n",
      "    update_time_ms: 8.044\n",
      "  timestamp: 1631882913\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 194000\n",
      "  training_iteration: 194\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   194</td><td style=\"text-align: right;\">         4580.59</td><td style=\"text-align: right;\">194000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 195000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-48-53\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 195\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.919088606039683\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012559574235208825\n",
      "          policy_loss: -0.03851765270034472\n",
      "          total_loss: -0.048753797966572976\n",
      "          vf_explained_var: -0.4083922207355499\n",
      "          vf_loss: 0.002919334766920656\n",
      "    num_agent_steps_sampled: 195000\n",
      "    num_agent_steps_trained: 195000\n",
      "    num_steps_sampled: 195000\n",
      "    num_steps_trained: 195000\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.30344827586208\n",
      "    ram_util_percent: 57.037931034482774\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06388533125833548\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.405847163577963\n",
      "    mean_inference_ms: 2.3659013026673357\n",
      "    mean_raw_obs_processing_ms: 0.8415533866523967\n",
      "  time_since_restore: 4600.950623989105\n",
      "  time_this_iter_s: 20.356542110443115\n",
      "  time_total_s: 4600.950623989105\n",
      "  timers:\n",
      "    learn_throughput: 304.714\n",
      "    learn_time_ms: 3281.768\n",
      "    load_throughput: 21750.267\n",
      "    load_time_ms: 45.976\n",
      "    sample_throughput: 55.147\n",
      "    sample_time_ms: 18133.429\n",
      "    update_time_ms: 8.084\n",
      "  timestamp: 1631882933\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 195000\n",
      "  training_iteration: 195\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   195</td><td style=\"text-align: right;\">         4600.95</td><td style=\"text-align: right;\">195000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-49-15\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 196\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9819213469823203\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012313274533789257\n",
      "          policy_loss: -0.036238116895159087\n",
      "          total_loss: -0.04866375244326061\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0014765292090790658\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.30333333333333\n",
      "    ram_util_percent: 56.983333333333334\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0639104495744939\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.391145983449125\n",
      "    mean_inference_ms: 2.366534324740467\n",
      "    mean_raw_obs_processing_ms: 0.8415777837616578\n",
      "  time_since_restore: 4622.219841957092\n",
      "  time_this_iter_s: 21.26921796798706\n",
      "  time_total_s: 4622.219841957092\n",
      "  timers:\n",
      "    learn_throughput: 304.251\n",
      "    learn_time_ms: 3286.76\n",
      "    load_throughput: 21674.433\n",
      "    load_time_ms: 46.137\n",
      "    sample_throughput: 55.632\n",
      "    sample_time_ms: 17975.171\n",
      "    update_time_ms: 7.947\n",
      "  timestamp: 1631882955\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 196\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   196</td><td style=\"text-align: right;\">         4622.22</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 197000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-49-39\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 197\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7812488476435344\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010176056760228083\n",
      "          policy_loss: -0.032043586547176046\n",
      "          total_loss: -0.04407579863650931\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.000890255652484484\n",
      "    num_agent_steps_sampled: 197000\n",
      "    num_agent_steps_trained: 197000\n",
      "    num_steps_sampled: 197000\n",
      "    num_steps_trained: 197000\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.25428571428571\n",
      "    ram_util_percent: 57.1142857142857\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06393539206580004\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.376789546986213\n",
      "    mean_inference_ms: 2.367170078681632\n",
      "    mean_raw_obs_processing_ms: 0.8416348264108328\n",
      "  time_since_restore: 4646.256106853485\n",
      "  time_this_iter_s: 24.036264896392822\n",
      "  time_total_s: 4646.256106853485\n",
      "  timers:\n",
      "    learn_throughput: 305.437\n",
      "    learn_time_ms: 3274.002\n",
      "    load_throughput: 21673.257\n",
      "    load_time_ms: 46.14\n",
      "    sample_throughput: 54.606\n",
      "    sample_time_ms: 18313.027\n",
      "    update_time_ms: 7.757\n",
      "  timestamp: 1631882979\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 197000\n",
      "  training_iteration: 197\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   197</td><td style=\"text-align: right;\">         4646.26</td><td style=\"text-align: right;\">197000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 198000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-50-01\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 198\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8324817167388068\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013513632676657197\n",
      "          policy_loss: -0.022153770758046046\n",
      "          total_loss: -0.0325771763920784\n",
      "          vf_explained_var: -0.2777040898799896\n",
      "          vf_loss: 0.0014075398904323164\n",
      "    num_agent_steps_sampled: 198000\n",
      "    num_agent_steps_trained: 198000\n",
      "    num_steps_sampled: 198000\n",
      "    num_steps_trained: 198000\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.65806451612902\n",
      "    ram_util_percent: 57.148387096774194\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06396031957231137\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.362672995230152\n",
      "    mean_inference_ms: 2.3678104672080176\n",
      "    mean_raw_obs_processing_ms: 0.8417238766668556\n",
      "  time_since_restore: 4668.148624658585\n",
      "  time_this_iter_s: 21.892517805099487\n",
      "  time_total_s: 4668.148624658585\n",
      "  timers:\n",
      "    learn_throughput: 304.812\n",
      "    learn_time_ms: 3280.715\n",
      "    load_throughput: 21893.225\n",
      "    load_time_ms: 45.676\n",
      "    sample_throughput: 54.45\n",
      "    sample_time_ms: 18365.626\n",
      "    update_time_ms: 7.849\n",
      "  timestamp: 1631883001\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 198000\n",
      "  training_iteration: 198\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   198</td><td style=\"text-align: right;\">         4668.15</td><td style=\"text-align: right;\">198000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 199000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-50-22\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 199\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.800882871945699\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010523390834674965\n",
      "          policy_loss: -0.042553014390998414\n",
      "          total_loss: -0.05471200665665998\n",
      "          vf_explained_var: -0.9289901852607727\n",
      "          vf_loss: 0.0007929032752549068\n",
      "    num_agent_steps_sampled: 199000\n",
      "    num_agent_steps_trained: 199000\n",
      "    num_steps_sampled: 199000\n",
      "    num_steps_trained: 199000\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.67333333333332\n",
      "    ram_util_percent: 57.06333333333334\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0639853437747016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.348586481846112\n",
      "    mean_inference_ms: 2.368453286971909\n",
      "    mean_raw_obs_processing_ms: 0.8418465128769371\n",
      "  time_since_restore: 4689.121193408966\n",
      "  time_this_iter_s: 20.97256875038147\n",
      "  time_total_s: 4689.121193408966\n",
      "  timers:\n",
      "    learn_throughput: 304.501\n",
      "    learn_time_ms: 3284.065\n",
      "    load_throughput: 21982.633\n",
      "    load_time_ms: 45.49\n",
      "    sample_throughput: 54.537\n",
      "    sample_time_ms: 18336.142\n",
      "    update_time_ms: 8.152\n",
      "  timestamp: 1631883022\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 199000\n",
      "  training_iteration: 199\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         4689.12</td><td style=\"text-align: right;\">199000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-50-43\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 200\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8304377476374307\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010281148994502803\n",
      "          policy_loss: -0.04973201841736833\n",
      "          total_loss: -0.06157998217062818\n",
      "          vf_explained_var: -0.6602421998977661\n",
      "          vf_loss: 0.00151589323197388\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.9193548387097\n",
      "    ram_util_percent: 57.07741935483871\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06401041063212856\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.33464680922109\n",
      "    mean_inference_ms: 2.369093083098187\n",
      "    mean_raw_obs_processing_ms: 0.8420010466006286\n",
      "  time_since_restore: 4710.895777225494\n",
      "  time_this_iter_s: 21.77458381652832\n",
      "  time_total_s: 4710.895777225494\n",
      "  timers:\n",
      "    learn_throughput: 306.706\n",
      "    learn_time_ms: 3260.45\n",
      "    load_throughput: 23726.405\n",
      "    load_time_ms: 42.147\n",
      "    sample_throughput: 54.397\n",
      "    sample_time_ms: 18383.223\n",
      "    update_time_ms: 7.498\n",
      "  timestamp: 1631883043\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 200\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">          4710.9</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 201000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-51-05\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 201\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9192161917686463\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010069269177125787\n",
      "          policy_loss: -0.025942989997565745\n",
      "          total_loss: -0.039240504511528544\n",
      "          vf_explained_var: -0.8082243204116821\n",
      "          vf_loss: 0.0010559389574660194\n",
      "    num_agent_steps_sampled: 201000\n",
      "    num_agent_steps_trained: 201000\n",
      "    num_steps_sampled: 201000\n",
      "    num_steps_trained: 201000\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.63870967741936\n",
      "    ram_util_percent: 57.06451612903226\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06403549278936164\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.320806882153764\n",
      "    mean_inference_ms: 2.369738029156363\n",
      "    mean_raw_obs_processing_ms: 0.842186183378165\n",
      "  time_since_restore: 4732.707542419434\n",
      "  time_this_iter_s: 21.81176519393921\n",
      "  time_total_s: 4732.707542419434\n",
      "  timers:\n",
      "    learn_throughput: 307.52\n",
      "    learn_time_ms: 3251.82\n",
      "    load_throughput: 23278.165\n",
      "    load_time_ms: 42.959\n",
      "    sample_throughput: 54.192\n",
      "    sample_time_ms: 18453.014\n",
      "    update_time_ms: 7.165\n",
      "  timestamp: 1631883065\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 201000\n",
      "  training_iteration: 201\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   201</td><td style=\"text-align: right;\">         4732.71</td><td style=\"text-align: right;\">201000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 202000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-51-27\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 202\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8210470212830439\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012264260502264958\n",
      "          policy_loss: -0.039335542172193524\n",
      "          total_loss: -0.05038734732402696\n",
      "          vf_explained_var: -0.6723785996437073\n",
      "          vf_loss: 0.001265173601374651\n",
      "    num_agent_steps_sampled: 202000\n",
      "    num_agent_steps_trained: 202000\n",
      "    num_steps_sampled: 202000\n",
      "    num_steps_trained: 202000\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.90322580645163\n",
      "    ram_util_percent: 57.28387096774194\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06406094488252827\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.307007948491005\n",
      "    mean_inference_ms: 2.370387106500341\n",
      "    mean_raw_obs_processing_ms: 0.8423998126554012\n",
      "  time_since_restore: 4754.54287815094\n",
      "  time_this_iter_s: 21.835335731506348\n",
      "  time_total_s: 4754.54287815094\n",
      "  timers:\n",
      "    learn_throughput: 305.339\n",
      "    learn_time_ms: 3275.049\n",
      "    load_throughput: 23113.494\n",
      "    load_time_ms: 43.265\n",
      "    sample_throughput: 54.178\n",
      "    sample_time_ms: 18457.668\n",
      "    update_time_ms: 7.197\n",
      "  timestamp: 1631883087\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 202000\n",
      "  training_iteration: 202\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   202</td><td style=\"text-align: right;\">         4754.54</td><td style=\"text-align: right;\">202000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 203000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-51-49\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 203\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8247246901194254\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009390285385000634\n",
      "          policy_loss: -0.04217991601261828\n",
      "          total_loss: -0.05516249918275409\n",
      "          vf_explained_var: -0.872680127620697\n",
      "          vf_loss: 0.0007522372554780709\n",
      "    num_agent_steps_sampled: 203000\n",
      "    num_agent_steps_trained: 203000\n",
      "    num_steps_sampled: 203000\n",
      "    num_steps_trained: 203000\n",
      "  iterations_since_restore: 203\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.55806451612901\n",
      "    ram_util_percent: 57.16451612903225\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06408642616838682\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.2931897582528\n",
      "    mean_inference_ms: 2.3710478180646213\n",
      "    mean_raw_obs_processing_ms: 0.8426438984428033\n",
      "  time_since_restore: 4776.00420331955\n",
      "  time_this_iter_s: 21.46132516860962\n",
      "  time_total_s: 4776.00420331955\n",
      "  timers:\n",
      "    learn_throughput: 302.215\n",
      "    learn_time_ms: 3308.901\n",
      "    load_throughput: 23101.731\n",
      "    load_time_ms: 43.287\n",
      "    sample_throughput: 54.7\n",
      "    sample_time_ms: 18281.6\n",
      "    update_time_ms: 7.394\n",
      "  timestamp: 1631883109\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 203000\n",
      "  training_iteration: 203\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   203</td><td style=\"text-align: right;\">            4776</td><td style=\"text-align: right;\">203000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-52-11\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 204\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8415960192680358\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009413781317034632\n",
      "          policy_loss: -0.023581356472439235\n",
      "          total_loss: -0.03662644773721695\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0008471510851652258\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.753125\n",
      "    ram_util_percent: 57.11875\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06411123902507901\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.279265943979937\n",
      "    mean_inference_ms: 2.3717115016968835\n",
      "    mean_raw_obs_processing_ms: 0.8429152067194926\n",
      "  time_since_restore: 4798.100012779236\n",
      "  time_this_iter_s: 22.09580945968628\n",
      "  time_total_s: 4798.100012779236\n",
      "  timers:\n",
      "    learn_throughput: 300.404\n",
      "    learn_time_ms: 3328.855\n",
      "    load_throughput: 23632.305\n",
      "    load_time_ms: 42.315\n",
      "    sample_throughput: 54.477\n",
      "    sample_time_ms: 18356.405\n",
      "    update_time_ms: 7.213\n",
      "  timestamp: 1631883131\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 204\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   204</td><td style=\"text-align: right;\">          4798.1</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 205000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-52-32\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 205\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8630988041559855\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015687619843919594\n",
      "          policy_loss: -0.05484868082114392\n",
      "          total_loss: -0.0647807346449958\n",
      "          vf_explained_var: -0.741744339466095\n",
      "          vf_loss: 0.0011603720415021396\n",
      "    num_agent_steps_sampled: 205000\n",
      "    num_agent_steps_trained: 205000\n",
      "    num_steps_sampled: 205000\n",
      "    num_steps_trained: 205000\n",
      "  iterations_since_restore: 205\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.65333333333335\n",
      "    ram_util_percent: 57.19000000000001\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06413468226238549\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.26528176208649\n",
      "    mean_inference_ms: 2.372379404324579\n",
      "    mean_raw_obs_processing_ms: 0.8432135720998245\n",
      "  time_since_restore: 4819.687150716782\n",
      "  time_this_iter_s: 21.587137937545776\n",
      "  time_total_s: 4819.687150716782\n",
      "  timers:\n",
      "    learn_throughput: 299.152\n",
      "    learn_time_ms: 3342.778\n",
      "    load_throughput: 23227.696\n",
      "    load_time_ms: 43.052\n",
      "    sample_throughput: 54.157\n",
      "    sample_time_ms: 18464.788\n",
      "    update_time_ms: 7.256\n",
      "  timestamp: 1631883152\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 205000\n",
      "  training_iteration: 205\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   205</td><td style=\"text-align: right;\">         4819.69</td><td style=\"text-align: right;\">205000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 206000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-52-56\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 206\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8110047234429254\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013717515309154245\n",
      "          policy_loss: -0.061805362357861465\n",
      "          total_loss: -0.0676212588014702\n",
      "          vf_explained_var: -0.21270030736923218\n",
      "          vf_loss: 0.005702308095189639\n",
      "    num_agent_steps_sampled: 206000\n",
      "    num_agent_steps_trained: 206000\n",
      "    num_steps_sampled: 206000\n",
      "    num_steps_trained: 206000\n",
      "  iterations_since_restore: 206\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.17428571428572\n",
      "    ram_util_percent: 57.208571428571425\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06415817641279412\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.25142848650246\n",
      "    mean_inference_ms: 2.373043316732287\n",
      "    mean_raw_obs_processing_ms: 0.8435375062646386\n",
      "  time_since_restore: 4843.65863442421\n",
      "  time_this_iter_s: 23.97148370742798\n",
      "  time_total_s: 4843.65863442421\n",
      "  timers:\n",
      "    learn_throughput: 300.793\n",
      "    learn_time_ms: 3324.544\n",
      "    load_throughput: 23279.87\n",
      "    load_time_ms: 42.956\n",
      "    sample_throughput: 53.327\n",
      "    sample_time_ms: 18752.364\n",
      "    update_time_ms: 7.721\n",
      "  timestamp: 1631883176\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 206000\n",
      "  training_iteration: 206\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   206</td><td style=\"text-align: right;\">         4843.66</td><td style=\"text-align: right;\">206000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 207000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-53-19\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 207\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.726768562528822\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016913280265167075\n",
      "          policy_loss: -0.08474570529328453\n",
      "          total_loss: -0.08905459766586622\n",
      "          vf_explained_var: -0.05947989225387573\n",
      "          vf_loss: 0.004831251477460481\n",
      "    num_agent_steps_sampled: 207000\n",
      "    num_agent_steps_trained: 207000\n",
      "    num_steps_sampled: 207000\n",
      "    num_steps_trained: 207000\n",
      "  iterations_since_restore: 207\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.359375\n",
      "    ram_util_percent: 57.184375\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06418203063039425\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.23784298080442\n",
      "    mean_inference_ms: 2.373707591974143\n",
      "    mean_raw_obs_processing_ms: 0.8438856361304223\n",
      "  time_since_restore: 4866.446892499924\n",
      "  time_this_iter_s: 22.78825807571411\n",
      "  time_total_s: 4866.446892499924\n",
      "  timers:\n",
      "    learn_throughput: 301.616\n",
      "    learn_time_ms: 3315.478\n",
      "    load_throughput: 23435.741\n",
      "    load_time_ms: 42.67\n",
      "    sample_throughput: 53.657\n",
      "    sample_time_ms: 18636.987\n",
      "    update_time_ms: 7.96\n",
      "  timestamp: 1631883199\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 207000\n",
      "  training_iteration: 207\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   207</td><td style=\"text-align: right;\">         4866.45</td><td style=\"text-align: right;\">207000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-53-41\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 208\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8419441368844773\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01246316005813286\n",
      "          policy_loss: -0.11542360813667377\n",
      "          total_loss: -0.12631067455642753\n",
      "          vf_explained_var: -0.7849888205528259\n",
      "          vf_loss: 0.0015433044554406983\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.77741935483873\n",
      "    ram_util_percent: 57.125806451612895\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06420582452446907\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.224305708508453\n",
      "    mean_inference_ms: 2.374378217136844\n",
      "    mean_raw_obs_processing_ms: 0.8442606826361312\n",
      "  time_since_restore: 4888.0481243133545\n",
      "  time_this_iter_s: 21.601231813430786\n",
      "  time_total_s: 4888.0481243133545\n",
      "  timers:\n",
      "    learn_throughput: 302.257\n",
      "    learn_time_ms: 3308.448\n",
      "    load_throughput: 23877.236\n",
      "    load_time_ms: 41.881\n",
      "    sample_throughput: 53.716\n",
      "    sample_time_ms: 18616.305\n",
      "    update_time_ms: 7.696\n",
      "  timestamp: 1631883221\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 208\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   208</td><td style=\"text-align: right;\">         4888.05</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 209000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-54-02\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 209\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.785466229915619\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009874135860669203\n",
      "          policy_loss: -0.06448472989723086\n",
      "          total_loss: -0.07631923022369544\n",
      "          vf_explained_var: -0.780852198600769\n",
      "          vf_loss: 0.0012752251518476341\n",
      "    num_agent_steps_sampled: 209000\n",
      "    num_agent_steps_trained: 209000\n",
      "    num_steps_sampled: 209000\n",
      "    num_steps_trained: 209000\n",
      "  iterations_since_restore: 209\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.30967741935484\n",
      "    ram_util_percent: 57.2290322580645\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06422988211740878\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.21096618059697\n",
      "    mean_inference_ms: 2.3750530997147674\n",
      "    mean_raw_obs_processing_ms: 0.8446560107360712\n",
      "  time_since_restore: 4909.497069358826\n",
      "  time_this_iter_s: 21.44894504547119\n",
      "  time_total_s: 4909.497069358826\n",
      "  timers:\n",
      "    learn_throughput: 300.389\n",
      "    learn_time_ms: 3329.014\n",
      "    load_throughput: 24772.53\n",
      "    load_time_ms: 40.367\n",
      "    sample_throughput: 53.63\n",
      "    sample_time_ms: 18646.267\n",
      "    update_time_ms: 7.312\n",
      "  timestamp: 1631883242\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 209000\n",
      "  training_iteration: 209\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   209</td><td style=\"text-align: right;\">          4909.5</td><td style=\"text-align: right;\">209000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 210000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-54-48\n",
      "  done: false\n",
      "  episode_len_mean: 994.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 210\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8835110068321228\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010695370312755435\n",
      "          policy_loss: 0.016877177812986904\n",
      "          total_loss: 0.004887766080598036\n",
      "          vf_explained_var: -0.10867059230804443\n",
      "          vf_loss: 0.0017061243396407613\n",
      "    num_agent_steps_sampled: 210000\n",
      "    num_agent_steps_trained: 210000\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.80923076923078\n",
      "    ram_util_percent: 57.10923076923078\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06425417418934577\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.197986669530597\n",
      "    mean_inference_ms: 2.3757318654702626\n",
      "    mean_raw_obs_processing_ms: 0.8460802243307082\n",
      "  time_since_restore: 4955.468570232391\n",
      "  time_this_iter_s: 45.971500873565674\n",
      "  time_total_s: 4955.468570232391\n",
      "  timers:\n",
      "    learn_throughput: 300.302\n",
      "    learn_time_ms: 3329.985\n",
      "    load_throughput: 22590.806\n",
      "    load_time_ms: 44.266\n",
      "    sample_throughput: 47.471\n",
      "    sample_time_ms: 21065.306\n",
      "    update_time_ms: 7.312\n",
      "  timestamp: 1631883288\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 210\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   210</td><td style=\"text-align: right;\">         4955.47</td><td style=\"text-align: right;\">210000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 211000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-55-09\n",
      "  done: false\n",
      "  episode_len_mean: 994.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 211\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.069602886835734\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013167202549691403\n",
      "          policy_loss: -0.05420509084231324\n",
      "          total_loss: -0.06682654039727318\n",
      "          vf_explained_var: -0.4852379560470581\n",
      "          vf_loss: 0.0017471854018771813\n",
      "    num_agent_steps_sampled: 211000\n",
      "    num_agent_steps_trained: 211000\n",
      "    num_steps_sampled: 211000\n",
      "    num_steps_trained: 211000\n",
      "  iterations_since_restore: 211\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.86333333333336\n",
      "    ram_util_percent: 57.32\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06427888511848105\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.185124675231958\n",
      "    mean_inference_ms: 2.376422178042173\n",
      "    mean_raw_obs_processing_ms: 0.8475216437622801\n",
      "  time_since_restore: 4976.446640253067\n",
      "  time_this_iter_s: 20.97807002067566\n",
      "  time_total_s: 4976.446640253067\n",
      "  timers:\n",
      "    learn_throughput: 298.935\n",
      "    learn_time_ms: 3345.209\n",
      "    load_throughput: 22671.35\n",
      "    load_time_ms: 44.109\n",
      "    sample_throughput: 47.694\n",
      "    sample_time_ms: 20967.166\n",
      "    update_time_ms: 7.048\n",
      "  timestamp: 1631883309\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 211000\n",
      "  training_iteration: 211\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   211</td><td style=\"text-align: right;\">         4976.45</td><td style=\"text-align: right;\">211000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-55-30\n",
      "  done: false\n",
      "  episode_len_mean: 994.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 212\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.196355687247382\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013378352572863367\n",
      "          policy_loss: -0.021953478683200148\n",
      "          total_loss: -0.03654515182392465\n",
      "          vf_explained_var: -0.7393118143081665\n",
      "          vf_loss: 0.0009430235775653272\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 212\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.1\n",
      "    ram_util_percent: 57.3551724137931\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06430335707854616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.17227796990158\n",
      "    mean_inference_ms: 2.3771185350098105\n",
      "    mean_raw_obs_processing_ms: 0.848980143092539\n",
      "  time_since_restore: 4996.575035810471\n",
      "  time_this_iter_s: 20.128395557403564\n",
      "  time_total_s: 4996.575035810471\n",
      "  timers:\n",
      "    learn_throughput: 298.831\n",
      "    learn_time_ms: 3346.368\n",
      "    load_throughput: 23495.764\n",
      "    load_time_ms: 42.561\n",
      "    sample_throughput: 48.082\n",
      "    sample_time_ms: 20797.93\n",
      "    update_time_ms: 7.202\n",
      "  timestamp: 1631883330\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 212\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   212</td><td style=\"text-align: right;\">         4996.58</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 213000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-55-50\n",
      "  done: false\n",
      "  episode_len_mean: 994.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 213\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0415684633784825\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012957344788403309\n",
      "          policy_loss: -0.10081625088221496\n",
      "          total_loss: -0.11417060539954238\n",
      "          vf_explained_var: -0.2644387483596802\n",
      "          vf_loss: 0.0008347808102068181\n",
      "    num_agent_steps_sampled: 213000\n",
      "    num_agent_steps_trained: 213000\n",
      "    num_steps_sampled: 213000\n",
      "    num_steps_trained: 213000\n",
      "  iterations_since_restore: 213\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.41000000000001\n",
      "    ram_util_percent: 57.39666666666666\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0643282526966014\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.15950517354088\n",
      "    mean_inference_ms: 2.377819910778551\n",
      "    mean_raw_obs_processing_ms: 0.8504555396434932\n",
      "  time_since_restore: 5017.217927694321\n",
      "  time_this_iter_s: 20.642891883850098\n",
      "  time_total_s: 5017.217927694321\n",
      "  timers:\n",
      "    learn_throughput: 299.294\n",
      "    learn_time_ms: 3341.197\n",
      "    load_throughput: 23844.401\n",
      "    load_time_ms: 41.939\n",
      "    sample_throughput: 48.261\n",
      "    sample_time_ms: 20720.497\n",
      "    update_time_ms: 8.077\n",
      "  timestamp: 1631883350\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 213000\n",
      "  training_iteration: 213\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   213</td><td style=\"text-align: right;\">         5017.22</td><td style=\"text-align: right;\">213000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 214000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-56-13\n",
      "  done: false\n",
      "  episode_len_mean: 994.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 214\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1427780707677204\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009424977445997958\n",
      "          policy_loss: -0.07874864826185836\n",
      "          total_loss: -0.0948236836741368\n",
      "          vf_explained_var: -0.044945694506168365\n",
      "          vf_loss: 0.0008236479030327043\n",
      "    num_agent_steps_sampled: 214000\n",
      "    num_agent_steps_trained: 214000\n",
      "    num_steps_sampled: 214000\n",
      "    num_steps_trained: 214000\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.06774193548384\n",
      "    ram_util_percent: 57.40645161290323\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06435326493541091\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.146984101592167\n",
      "    mean_inference_ms: 2.3785317708278884\n",
      "    mean_raw_obs_processing_ms: 0.8519469524775702\n",
      "  time_since_restore: 5039.471650838852\n",
      "  time_this_iter_s: 22.25372314453125\n",
      "  time_total_s: 5039.471650838852\n",
      "  timers:\n",
      "    learn_throughput: 300.006\n",
      "    learn_time_ms: 3333.264\n",
      "    load_throughput: 24030.439\n",
      "    load_time_ms: 41.614\n",
      "    sample_throughput: 48.206\n",
      "    sample_time_ms: 20744.45\n",
      "    update_time_ms: 8.149\n",
      "  timestamp: 1631883373\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 214000\n",
      "  training_iteration: 214\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   214</td><td style=\"text-align: right;\">         5039.47</td><td style=\"text-align: right;\">214000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 215000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-56-33\n",
      "  done: false\n",
      "  episode_len_mean: 994.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 215\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0050293617778356\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013246689226210289\n",
      "          policy_loss: -0.06719138386348883\n",
      "          total_loss: -0.07974726342492633\n",
      "          vf_explained_var: -0.618224561214447\n",
      "          vf_loss: 0.001128822560228097\n",
      "    num_agent_steps_sampled: 215000\n",
      "    num_agent_steps_trained: 215000\n",
      "    num_steps_sampled: 215000\n",
      "    num_steps_trained: 215000\n",
      "  iterations_since_restore: 215\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.88275862068967\n",
      "    ram_util_percent: 57.337931034482764\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06437875097284469\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.13440612938142\n",
      "    mean_inference_ms: 2.379253472363277\n",
      "    mean_raw_obs_processing_ms: 0.853453667761099\n",
      "  time_since_restore: 5059.850550413132\n",
      "  time_this_iter_s: 20.378899574279785\n",
      "  time_total_s: 5059.850550413132\n",
      "  timers:\n",
      "    learn_throughput: 298.801\n",
      "    learn_time_ms: 3346.714\n",
      "    load_throughput: 24064.038\n",
      "    load_time_ms: 41.556\n",
      "    sample_throughput: 48.519\n",
      "    sample_time_ms: 20610.309\n",
      "    update_time_ms: 8.047\n",
      "  timestamp: 1631883393\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 215000\n",
      "  training_iteration: 215\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   215</td><td style=\"text-align: right;\">         5059.85</td><td style=\"text-align: right;\">215000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-56-54\n",
      "  done: false\n",
      "  episode_len_mean: 994.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 216\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9327411360210842\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012801307043459116\n",
      "          policy_loss: -0.09322091941204336\n",
      "          total_loss: -0.10426265303459432\n",
      "          vf_explained_var: -0.24551036953926086\n",
      "          vf_loss: 0.0021341110974188067\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.92666666666666\n",
      "    ram_util_percent: 57.41666666666667\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06440488688268124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.121945167768633\n",
      "    mean_inference_ms: 2.379983132163239\n",
      "    mean_raw_obs_processing_ms: 0.8549760353299161\n",
      "  time_since_restore: 5080.494108438492\n",
      "  time_this_iter_s: 20.643558025360107\n",
      "  time_total_s: 5080.494108438492\n",
      "  timers:\n",
      "    learn_throughput: 297.42\n",
      "    learn_time_ms: 3362.249\n",
      "    load_throughput: 24265.136\n",
      "    load_time_ms: 41.211\n",
      "    sample_throughput: 49.352\n",
      "    sample_time_ms: 20262.781\n",
      "    update_time_ms: 7.837\n",
      "  timestamp: 1631883414\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 216\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   216</td><td style=\"text-align: right;\">         5080.49</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 217000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-57-13\n",
      "  done: false\n",
      "  episode_len_mean: 994.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 217\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.367240701781379\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009859587919861587\n",
      "          policy_loss: -0.10452458615311318\n",
      "          total_loss: -0.12291349884536532\n",
      "          vf_explained_var: -0.8213814496994019\n",
      "          vf_loss: 0.0005455480740541437\n",
      "    num_agent_steps_sampled: 217000\n",
      "    num_agent_steps_trained: 217000\n",
      "    num_steps_sampled: 217000\n",
      "    num_steps_trained: 217000\n",
      "  iterations_since_restore: 217\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.50000000000001\n",
      "    ram_util_percent: 57.34814814814815\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06443112406990087\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.10950090475754\n",
      "    mean_inference_ms: 2.3807184173520457\n",
      "    mean_raw_obs_processing_ms: 0.8565132633474597\n",
      "  time_since_restore: 5099.587072134018\n",
      "  time_this_iter_s: 19.092963695526123\n",
      "  time_total_s: 5099.587072134018\n",
      "  timers:\n",
      "    learn_throughput: 297.316\n",
      "    learn_time_ms: 3363.43\n",
      "    load_throughput: 25337.807\n",
      "    load_time_ms: 39.467\n",
      "    sample_throughput: 50.266\n",
      "    sample_time_ms: 19894.069\n",
      "    update_time_ms: 7.582\n",
      "  timestamp: 1631883433\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 217000\n",
      "  training_iteration: 217\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   217</td><td style=\"text-align: right;\">         5099.59</td><td style=\"text-align: right;\">217000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 218000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-57-35\n",
      "  done: false\n",
      "  episode_len_mean: 994.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 218\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8402519742647807\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014111878109713653\n",
      "          policy_loss: -0.08514437810000446\n",
      "          total_loss: -0.09209464989188644\n",
      "          vf_explained_var: -0.43059518933296204\n",
      "          vf_loss: 0.004670893127978262\n",
      "    num_agent_steps_sampled: 218000\n",
      "    num_agent_steps_trained: 218000\n",
      "    num_steps_sampled: 218000\n",
      "    num_steps_trained: 218000\n",
      "  iterations_since_restore: 218\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.321875\n",
      "    ram_util_percent: 57.475\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06445758207336873\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.097255694965014\n",
      "    mean_inference_ms: 2.381459337631158\n",
      "    mean_raw_obs_processing_ms: 0.85806319445903\n",
      "  time_since_restore: 5121.769492387772\n",
      "  time_this_iter_s: 22.182420253753662\n",
      "  time_total_s: 5121.769492387772\n",
      "  timers:\n",
      "    learn_throughput: 297.481\n",
      "    learn_time_ms: 3361.556\n",
      "    load_throughput: 24496.091\n",
      "    load_time_ms: 40.823\n",
      "    sample_throughput: 50.119\n",
      "    sample_time_ms: 19952.441\n",
      "    update_time_ms: 7.705\n",
      "  timestamp: 1631883455\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 218000\n",
      "  training_iteration: 218\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   218</td><td style=\"text-align: right;\">         5121.77</td><td style=\"text-align: right;\">218000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 219000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-57-55\n",
      "  done: false\n",
      "  episode_len_mean: 994.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 219\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0937465800179376\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008736657669738534\n",
      "          policy_loss: -0.08168211297856437\n",
      "          total_loss: -0.09720296478933758\n",
      "          vf_explained_var: -0.8143491744995117\n",
      "          vf_loss: 0.001218285424773866\n",
      "    num_agent_steps_sampled: 219000\n",
      "    num_agent_steps_trained: 219000\n",
      "    num_steps_sampled: 219000\n",
      "    num_steps_trained: 219000\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.77857142857142\n",
      "    ram_util_percent: 57.52499999999999\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06448434563057816\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.085118620678724\n",
      "    mean_inference_ms: 2.3822098775176297\n",
      "    mean_raw_obs_processing_ms: 0.8596285457163242\n",
      "  time_since_restore: 5141.443259000778\n",
      "  time_this_iter_s: 19.673766613006592\n",
      "  time_total_s: 5141.443259000778\n",
      "  timers:\n",
      "    learn_throughput: 298.099\n",
      "    learn_time_ms: 3354.588\n",
      "    load_throughput: 24880.435\n",
      "    load_time_ms: 40.192\n",
      "    sample_throughput: 50.55\n",
      "    sample_time_ms: 19782.496\n",
      "    update_time_ms: 7.556\n",
      "  timestamp: 1631883475\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 219000\n",
      "  training_iteration: 219\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   219</td><td style=\"text-align: right;\">         5141.44</td><td style=\"text-align: right;\">219000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-58-16\n",
      "  done: false\n",
      "  episode_len_mean: 996.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 220\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.968561413553026\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011473198705135306\n",
      "          policy_loss: -0.024857106028745572\n",
      "          total_loss: -0.03776720892637968\n",
      "          vf_explained_var: -0.21506379544734955\n",
      "          vf_loss: 0.001262158933807061\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.59032258064516\n",
      "    ram_util_percent: 55.12903225806452\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06451161153681585\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.073152100980764\n",
      "    mean_inference_ms: 2.3829715053776304\n",
      "    mean_raw_obs_processing_ms: 0.8595150440666584\n",
      "  time_since_restore: 5163.1082084178925\n",
      "  time_this_iter_s: 21.664949417114258\n",
      "  time_total_s: 5163.1082084178925\n",
      "  timers:\n",
      "    learn_throughput: 295.417\n",
      "    learn_time_ms: 3385.044\n",
      "    load_throughput: 24838.87\n",
      "    load_time_ms: 40.259\n",
      "    sample_throughput: 57.729\n",
      "    sample_time_ms: 17322.3\n",
      "    update_time_ms: 7.741\n",
      "  timestamp: 1631883496\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 220\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   220</td><td style=\"text-align: right;\">         5163.11</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            996.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 221000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-58-37\n",
      "  done: false\n",
      "  episode_len_mean: 996.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 221\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9733978470166524\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01598886112817439\n",
      "          policy_loss: -0.05756007917225361\n",
      "          total_loss: -0.06784268048488432\n",
      "          vf_explained_var: -0.2295282930135727\n",
      "          vf_loss: 0.0017680523477287757\n",
      "    num_agent_steps_sampled: 221000\n",
      "    num_agent_steps_trained: 221000\n",
      "    num_steps_sampled: 221000\n",
      "    num_steps_trained: 221000\n",
      "  iterations_since_restore: 221\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.40344827586206\n",
      "    ram_util_percent: 54.31724137931034\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06453904390930275\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.06108446310773\n",
      "    mean_inference_ms: 2.383734840943527\n",
      "    mean_raw_obs_processing_ms: 0.859428914978032\n",
      "  time_since_restore: 5183.295953035355\n",
      "  time_this_iter_s: 20.187744617462158\n",
      "  time_total_s: 5183.295953035355\n",
      "  timers:\n",
      "    learn_throughput: 296.425\n",
      "    learn_time_ms: 3373.531\n",
      "    load_throughput: 26224.923\n",
      "    load_time_ms: 38.132\n",
      "    sample_throughput: 57.948\n",
      "    sample_time_ms: 17256.781\n",
      "    update_time_ms: 7.834\n",
      "  timestamp: 1631883517\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 221000\n",
      "  training_iteration: 221\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   221</td><td style=\"text-align: right;\">          5183.3</td><td style=\"text-align: right;\">221000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            996.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 222000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-58-56\n",
      "  done: false\n",
      "  episode_len_mean: 996.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 222\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2903773466746014\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010352948184403575\n",
      "          policy_loss: -0.029747795768909985\n",
      "          total_loss: -0.047199611986676854\n",
      "          vf_explained_var: -0.6179518103599548\n",
      "          vf_loss: 0.0004769305760116064\n",
      "    num_agent_steps_sampled: 222000\n",
      "    num_agent_steps_trained: 222000\n",
      "    num_steps_sampled: 222000\n",
      "    num_steps_trained: 222000\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.01851851851852\n",
      "    ram_util_percent: 54.31851851851852\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06456670323527326\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.04888380770436\n",
      "    mean_inference_ms: 2.384501307313168\n",
      "    mean_raw_obs_processing_ms: 0.8593699541581288\n",
      "  time_since_restore: 5202.520559549332\n",
      "  time_this_iter_s: 19.22460651397705\n",
      "  time_total_s: 5202.520559549332\n",
      "  timers:\n",
      "    learn_throughput: 296.143\n",
      "    learn_time_ms: 3376.747\n",
      "    load_throughput: 26808.287\n",
      "    load_time_ms: 37.302\n",
      "    sample_throughput: 58.26\n",
      "    sample_time_ms: 17164.341\n",
      "    update_time_ms: 7.664\n",
      "  timestamp: 1631883536\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 222000\n",
      "  training_iteration: 222\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   222</td><td style=\"text-align: right;\">         5202.52</td><td style=\"text-align: right;\">222000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            996.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 223000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-59-17\n",
      "  done: false\n",
      "  episode_len_mean: 996.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 223\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2018638836012947\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009554359965701585\n",
      "          policy_loss: -0.057626703986898065\n",
      "          total_loss: -0.0742123673359553\n",
      "          vf_explained_var: -0.141108900308609\n",
      "          vf_loss: 0.0008417026238021208\n",
      "    num_agent_steps_sampled: 223000\n",
      "    num_agent_steps_trained: 223000\n",
      "    num_steps_sampled: 223000\n",
      "    num_steps_trained: 223000\n",
      "  iterations_since_restore: 223\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.96666666666665\n",
      "    ram_util_percent: 54.223333333333336\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06459465126485169\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.036942764102136\n",
      "    mean_inference_ms: 2.3852786827712906\n",
      "    mean_raw_obs_processing_ms: 0.8593323663262925\n",
      "  time_since_restore: 5223.090195178986\n",
      "  time_this_iter_s: 20.56963562965393\n",
      "  time_total_s: 5223.090195178986\n",
      "  timers:\n",
      "    learn_throughput: 297.077\n",
      "    learn_time_ms: 3366.133\n",
      "    load_throughput: 28811.485\n",
      "    load_time_ms: 34.708\n",
      "    sample_throughput: 58.247\n",
      "    sample_time_ms: 17168.202\n",
      "    update_time_ms: 8.775\n",
      "  timestamp: 1631883557\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 223000\n",
      "  training_iteration: 223\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   223</td><td style=\"text-align: right;\">         5223.09</td><td style=\"text-align: right;\">223000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_12-59-38\n",
      "  done: false\n",
      "  episode_len_mean: 996.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 224\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8922201050652399\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011891495723562724\n",
      "          policy_loss: -0.07822657111618254\n",
      "          total_loss: -0.08957069135374493\n",
      "          vf_explained_var: -0.6731227040290833\n",
      "          vf_loss: 0.0018637216093743013\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.52666666666666\n",
      "    ram_util_percent: 54.223333333333336\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06462276482912835\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.02500995723574\n",
      "    mean_inference_ms: 2.3860630486226504\n",
      "    mean_raw_obs_processing_ms: 0.8593186930565879\n",
      "  time_since_restore: 5244.193081855774\n",
      "  time_this_iter_s: 21.10288667678833\n",
      "  time_total_s: 5244.193081855774\n",
      "  timers:\n",
      "    learn_throughput: 296.105\n",
      "    learn_time_ms: 3377.184\n",
      "    load_throughput: 27734.236\n",
      "    load_time_ms: 36.057\n",
      "    sample_throughput: 58.681\n",
      "    sample_time_ms: 17041.256\n",
      "    update_time_ms: 8.531\n",
      "  timestamp: 1631883578\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 224\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   224</td><td style=\"text-align: right;\">         5244.19</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 225000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-00-01\n",
      "  done: false\n",
      "  episode_len_mean: 996.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 225\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.325787483321296\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009563224092002356\n",
      "          policy_loss: -0.007777372416522768\n",
      "          total_loss: -0.025763917879925834\n",
      "          vf_explained_var: -0.87129145860672\n",
      "          vf_loss: 0.0006757955251183982\n",
      "    num_agent_steps_sampled: 225000\n",
      "    num_agent_steps_trained: 225000\n",
      "    num_steps_sampled: 225000\n",
      "    num_steps_trained: 225000\n",
      "  iterations_since_restore: 225\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.45588235294119\n",
      "    ram_util_percent: 54.161764705882355\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06465106231657418\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.01335447832198\n",
      "    mean_inference_ms: 2.386858130946109\n",
      "    mean_raw_obs_processing_ms: 0.8593307623755375\n",
      "  time_since_restore: 5267.968113899231\n",
      "  time_this_iter_s: 23.77503204345703\n",
      "  time_total_s: 5267.968113899231\n",
      "  timers:\n",
      "    learn_throughput: 298.011\n",
      "    learn_time_ms: 3355.586\n",
      "    load_throughput: 26540.147\n",
      "    load_time_ms: 37.679\n",
      "    sample_throughput: 57.469\n",
      "    sample_time_ms: 17400.696\n",
      "    update_time_ms: 8.688\n",
      "  timestamp: 1631883601\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 225000\n",
      "  training_iteration: 225\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   225</td><td style=\"text-align: right;\">         5267.97</td><td style=\"text-align: right;\">225000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 226000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-00-23\n",
      "  done: false\n",
      "  episode_len_mean: 996.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 226\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.88915982776218\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01358278661515812\n",
      "          policy_loss: -0.06094362164537112\n",
      "          total_loss: -0.07158148280448384\n",
      "          vf_explained_var: -0.5859333276748657\n",
      "          vf_loss: 0.00172663720505726\n",
      "    num_agent_steps_sampled: 226000\n",
      "    num_agent_steps_trained: 226000\n",
      "    num_steps_sampled: 226000\n",
      "    num_steps_trained: 226000\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.72333333333333\n",
      "    ram_util_percent: 54.266666666666666\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06467955461817074\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.0018796909896\n",
      "    mean_inference_ms: 2.387658636754364\n",
      "    mean_raw_obs_processing_ms: 0.8593683903421109\n",
      "  time_since_restore: 5289.137322187424\n",
      "  time_this_iter_s: 21.16920828819275\n",
      "  time_total_s: 5289.137322187424\n",
      "  timers:\n",
      "    learn_throughput: 298.617\n",
      "    learn_time_ms: 3348.767\n",
      "    load_throughput: 26254.701\n",
      "    load_time_ms: 38.088\n",
      "    sample_throughput: 57.273\n",
      "    sample_time_ms: 17460.315\n",
      "    update_time_ms: 8.438\n",
      "  timestamp: 1631883623\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 226000\n",
      "  training_iteration: 226\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   226</td><td style=\"text-align: right;\">         5289.14</td><td style=\"text-align: right;\">226000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 227000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-00-43\n",
      "  done: false\n",
      "  episode_len_mean: 996.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 227\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5116786479949953\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010258098989230193\n",
      "          policy_loss: -0.030051898087064426\n",
      "          total_loss: -0.04987878517972098\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00036045141039519674\n",
      "    num_agent_steps_sampled: 227000\n",
      "    num_agent_steps_trained: 227000\n",
      "    num_steps_sampled: 227000\n",
      "    num_steps_trained: 227000\n",
      "  iterations_since_restore: 227\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.03448275862068\n",
      "    ram_util_percent: 54.35517241379311\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06470797945539677\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.990491201468295\n",
      "    mean_inference_ms: 2.3884710607436865\n",
      "    mean_raw_obs_processing_ms: 0.8594323524348522\n",
      "  time_since_restore: 5309.114752292633\n",
      "  time_this_iter_s: 19.97743010520935\n",
      "  time_total_s: 5309.114752292633\n",
      "  timers:\n",
      "    learn_throughput: 296.743\n",
      "    learn_time_ms: 3369.918\n",
      "    load_throughput: 24807.299\n",
      "    load_time_ms: 40.311\n",
      "    sample_throughput: 57.064\n",
      "    sample_time_ms: 17524.215\n",
      "    update_time_ms: 9.425\n",
      "  timestamp: 1631883643\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 227000\n",
      "  training_iteration: 227\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   227</td><td style=\"text-align: right;\">         5309.11</td><td style=\"text-align: right;\">227000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-01-05\n",
      "  done: false\n",
      "  episode_len_mean: 996.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 228\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.401009111934238\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00972322257254182\n",
      "          policy_loss: -0.05260191737777657\n",
      "          total_loss: -0.07146546548853318\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00047412764745078875\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.84375\n",
      "    ram_util_percent: 54.349999999999994\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06473666603645287\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.97917619697949\n",
      "    mean_inference_ms: 2.389292629419673\n",
      "    mean_raw_obs_processing_ms: 0.8595201183065929\n",
      "  time_since_restore: 5331.731253147125\n",
      "  time_this_iter_s: 22.616500854492188\n",
      "  time_total_s: 5331.731253147125\n",
      "  timers:\n",
      "    learn_throughput: 296.324\n",
      "    learn_time_ms: 3374.688\n",
      "    load_throughput: 25032.192\n",
      "    load_time_ms: 39.949\n",
      "    sample_throughput: 56.938\n",
      "    sample_time_ms: 17563.105\n",
      "    update_time_ms: 9.522\n",
      "  timestamp: 1631883665\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 228\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   228</td><td style=\"text-align: right;\">         5331.73</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 229000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-01-28\n",
      "  done: false\n",
      "  episode_len_mean: 996.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 229\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9264569030867682\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011252371818146025\n",
      "          policy_loss: -0.0819086945719189\n",
      "          total_loss: -0.09393675873676936\n",
      "          vf_explained_var: -0.024482425302267075\n",
      "          vf_loss: 0.0018292679076289966\n",
      "    num_agent_steps_sampled: 229000\n",
      "    num_agent_steps_trained: 229000\n",
      "    num_steps_sampled: 229000\n",
      "    num_steps_trained: 229000\n",
      "  iterations_since_restore: 229\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.609375\n",
      "    ram_util_percent: 54.3\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06476572056224814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.967778501706206\n",
      "    mean_inference_ms: 2.3901246932668183\n",
      "    mean_raw_obs_processing_ms: 0.8596302953780024\n",
      "  time_since_restore: 5354.1165635585785\n",
      "  time_this_iter_s: 22.385310411453247\n",
      "  time_total_s: 5354.1165635585785\n",
      "  timers:\n",
      "    learn_throughput: 295.343\n",
      "    learn_time_ms: 3385.892\n",
      "    load_throughput: 23622.243\n",
      "    load_time_ms: 42.333\n",
      "    sample_throughput: 56.114\n",
      "    sample_time_ms: 17820.814\n",
      "    update_time_ms: 9.565\n",
      "  timestamp: 1631883688\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 229000\n",
      "  training_iteration: 229\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   229</td><td style=\"text-align: right;\">         5354.12</td><td style=\"text-align: right;\">229000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 230000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-01-49\n",
      "  done: false\n",
      "  episode_len_mean: 996.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 230\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4067123254140217\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0159808436475539\n",
      "          policy_loss: -0.05798084607554806\n",
      "          total_loss: -0.07368578298224343\n",
      "          vf_explained_var: -0.5717572569847107\n",
      "          vf_loss: 0.000682720208230118\n",
      "    num_agent_steps_sampled: 230000\n",
      "    num_agent_steps_trained: 230000\n",
      "    num_steps_sampled: 230000\n",
      "    num_steps_trained: 230000\n",
      "  iterations_since_restore: 230\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.66\n",
      "    ram_util_percent: 54.39000000000001\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06479491873279825\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.95653362624148\n",
      "    mean_inference_ms: 2.390963357875189\n",
      "    mean_raw_obs_processing_ms: 0.8597626087023477\n",
      "  time_since_restore: 5375.00900053978\n",
      "  time_this_iter_s: 20.892436981201172\n",
      "  time_total_s: 5375.00900053978\n",
      "  timers:\n",
      "    learn_throughput: 295.0\n",
      "    learn_time_ms: 3389.832\n",
      "    load_throughput: 24514.345\n",
      "    load_time_ms: 40.792\n",
      "    sample_throughput: 56.39\n",
      "    sample_time_ms: 17733.692\n",
      "    update_time_ms: 9.276\n",
      "  timestamp: 1631883709\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 230000\n",
      "  training_iteration: 230\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   230</td><td style=\"text-align: right;\">         5375.01</td><td style=\"text-align: right;\">230000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 231000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-02-09\n",
      "  done: false\n",
      "  episode_len_mean: 996.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 231\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.375819969177246\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016918729714040737\n",
      "          policy_loss: -0.04716336644358105\n",
      "          total_loss: -0.06241649819744958\n",
      "          vf_explained_var: -0.5044807195663452\n",
      "          vf_loss: 0.0003749067761721866\n",
      "    num_agent_steps_sampled: 231000\n",
      "    num_agent_steps_trained: 231000\n",
      "    num_steps_sampled: 231000\n",
      "    num_steps_trained: 231000\n",
      "  iterations_since_restore: 231\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.91428571428571\n",
      "    ram_util_percent: 54.44285714285714\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06482444951411132\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.945428364842552\n",
      "    mean_inference_ms: 2.39180621791869\n",
      "    mean_raw_obs_processing_ms: 0.8599176511067756\n",
      "  time_since_restore: 5394.931007385254\n",
      "  time_this_iter_s: 19.922006845474243\n",
      "  time_total_s: 5394.931007385254\n",
      "  timers:\n",
      "    learn_throughput: 296.54\n",
      "    learn_time_ms: 3372.225\n",
      "    load_throughput: 23334.733\n",
      "    load_time_ms: 42.855\n",
      "    sample_throughput: 56.425\n",
      "    sample_time_ms: 17722.541\n",
      "    update_time_ms: 9.371\n",
      "  timestamp: 1631883729\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 231000\n",
      "  training_iteration: 231\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   231</td><td style=\"text-align: right;\">         5394.93</td><td style=\"text-align: right;\">231000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-02-32\n",
      "  done: false\n",
      "  episode_len_mean: 996.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 232\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6927730268902248\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011777262272475436\n",
      "          policy_loss: -0.20777333395348654\n",
      "          total_loss: -0.21773468322224088\n",
      "          vf_explained_var: 0.18922178447246552\n",
      "          vf_loss: 0.0013069160253508017\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "  iterations_since_restore: 232\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.36969696969696\n",
      "    ram_util_percent: 54.418181818181814\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06485369787803079\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.93455548865061\n",
      "    mean_inference_ms: 2.3926455376633937\n",
      "    mean_raw_obs_processing_ms: 0.8600930847685349\n",
      "  time_since_restore: 5418.200408697128\n",
      "  time_this_iter_s: 23.26940131187439\n",
      "  time_total_s: 5418.200408697128\n",
      "  timers:\n",
      "    learn_throughput: 298.018\n",
      "    learn_time_ms: 3355.497\n",
      "    load_throughput: 22462.755\n",
      "    load_time_ms: 44.518\n",
      "    sample_throughput: 55.127\n",
      "    sample_time_ms: 18140.074\n",
      "    update_time_ms: 10.687\n",
      "  timestamp: 1631883752\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 232\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   232</td><td style=\"text-align: right;\">          5418.2</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 233000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-02-52\n",
      "  done: false\n",
      "  episode_len_mean: 996.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 233\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1705028494199117\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008380091982388526\n",
      "          policy_loss: -0.09298102768758933\n",
      "          total_loss: -0.10974223961432775\n",
      "          vf_explained_var: -0.24728107452392578\n",
      "          vf_loss: 0.0009168300077564911\n",
      "    num_agent_steps_sampled: 233000\n",
      "    num_agent_steps_trained: 233000\n",
      "    num_steps_sampled: 233000\n",
      "    num_steps_trained: 233000\n",
      "  iterations_since_restore: 233\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.19655172413792\n",
      "    ram_util_percent: 54.47931034482759\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06488254495000968\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.923863363540764\n",
      "    mean_inference_ms: 2.393475341107685\n",
      "    mean_raw_obs_processing_ms: 0.8602875243207793\n",
      "  time_since_restore: 5438.077037811279\n",
      "  time_this_iter_s: 19.876629114151\n",
      "  time_total_s: 5438.077037811279\n",
      "  timers:\n",
      "    learn_throughput: 297.575\n",
      "    learn_time_ms: 3360.499\n",
      "    load_throughput: 21232.66\n",
      "    load_time_ms: 47.097\n",
      "    sample_throughput: 55.352\n",
      "    sample_time_ms: 18066.353\n",
      "    update_time_ms: 8.662\n",
      "  timestamp: 1631883772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 233000\n",
      "  training_iteration: 233\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   233</td><td style=\"text-align: right;\">         5438.08</td><td style=\"text-align: right;\">233000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 234000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-03-11\n",
      "  done: false\n",
      "  episode_len_mean: 996.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 234\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4491851541731093\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009736742219176501\n",
      "          policy_loss: -0.04025728586647245\n",
      "          total_loss: -0.059479636864529714\n",
      "          vf_explained_var: -0.5977715253829956\n",
      "          vf_loss: 0.0005905855042025603\n",
      "    num_agent_steps_sampled: 234000\n",
      "    num_agent_steps_trained: 234000\n",
      "    num_steps_sampled: 234000\n",
      "    num_steps_trained: 234000\n",
      "  iterations_since_restore: 234\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.05185185185185\n",
      "    ram_util_percent: 54.45925925925926\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06491081589552887\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.913305828399942\n",
      "    mean_inference_ms: 2.394295337967065\n",
      "    mean_raw_obs_processing_ms: 0.8604992677620015\n",
      "  time_since_restore: 5456.883095502853\n",
      "  time_this_iter_s: 18.806057691574097\n",
      "  time_total_s: 5456.883095502853\n",
      "  timers:\n",
      "    learn_throughput: 298.985\n",
      "    learn_time_ms: 3344.648\n",
      "    load_throughput: 22082.141\n",
      "    load_time_ms: 45.285\n",
      "    sample_throughput: 56.038\n",
      "    sample_time_ms: 17844.973\n",
      "    update_time_ms: 8.852\n",
      "  timestamp: 1631883791\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 234000\n",
      "  training_iteration: 234\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   234</td><td style=\"text-align: right;\">         5456.88</td><td style=\"text-align: right;\">234000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 235000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-03-30\n",
      "  done: false\n",
      "  episode_len_mean: 996.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 235\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3519351694318984\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008716062051584277\n",
      "          policy_loss: -0.04767108221227924\n",
      "          total_loss: -0.06615736271358198\n",
      "          vf_explained_var: -0.5999708771705627\n",
      "          vf_loss: 0.00084463380305048\n",
      "    num_agent_steps_sampled: 235000\n",
      "    num_agent_steps_trained: 235000\n",
      "    num_steps_sampled: 235000\n",
      "    num_steps_trained: 235000\n",
      "  iterations_since_restore: 235\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.78571428571429\n",
      "    ram_util_percent: 54.38214285714287\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06493885856890488\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.9028186861909\n",
      "    mean_inference_ms: 2.395107522930642\n",
      "    mean_raw_obs_processing_ms: 0.86072809913131\n",
      "  time_since_restore: 5476.639860630035\n",
      "  time_this_iter_s: 19.756765127182007\n",
      "  time_total_s: 5476.639860630035\n",
      "  timers:\n",
      "    learn_throughput: 297.988\n",
      "    learn_time_ms: 3355.84\n",
      "    load_throughput: 22130.366\n",
      "    load_time_ms: 45.187\n",
      "    sample_throughput: 57.367\n",
      "    sample_time_ms: 17431.655\n",
      "    update_time_ms: 8.994\n",
      "  timestamp: 1631883810\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 235000\n",
      "  training_iteration: 235\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   235</td><td style=\"text-align: right;\">         5476.64</td><td style=\"text-align: right;\">235000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-03-51\n",
      "  done: false\n",
      "  episode_len_mean: 996.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 236\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5584807634353637\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0038307869018370786\n",
      "          policy_loss: -0.03296334677272373\n",
      "          total_loss: -0.05634311352752977\n",
      "          vf_explained_var: -0.8542150855064392\n",
      "          vf_loss: 0.00036419278191412253\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_agent_steps_trained: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "  iterations_since_restore: 236\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.77931034482756\n",
      "    ram_util_percent: 54.427586206896564\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06496663374799143\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.892516323792513\n",
      "    mean_inference_ms: 2.3959103385272638\n",
      "    mean_raw_obs_processing_ms: 0.8609729455614666\n",
      "  time_since_restore: 5497.110242128372\n",
      "  time_this_iter_s: 20.470381498336792\n",
      "  time_total_s: 5497.110242128372\n",
      "  timers:\n",
      "    learn_throughput: 297.967\n",
      "    learn_time_ms: 3356.078\n",
      "    load_throughput: 22900.173\n",
      "    load_time_ms: 43.668\n",
      "    sample_throughput: 57.594\n",
      "    sample_time_ms: 17363.023\n",
      "    update_time_ms: 8.958\n",
      "  timestamp: 1631883831\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 236\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   236</td><td style=\"text-align: right;\">         5497.11</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 237000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-04-12\n",
      "  done: false\n",
      "  episode_len_mean: 996.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 237\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2402709960937499\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2920024342007106\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012525019140303408\n",
      "          policy_loss: -0.08468877714541223\n",
      "          total_loss: -0.10317967029081451\n",
      "          vf_explained_var: -0.8052874803543091\n",
      "          vf_loss: 0.0014197276471046886\n",
      "    num_agent_steps_sampled: 237000\n",
      "    num_agent_steps_trained: 237000\n",
      "    num_steps_sampled: 237000\n",
      "    num_steps_trained: 237000\n",
      "  iterations_since_restore: 237\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.20333333333335\n",
      "    ram_util_percent: 54.323333333333345\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06499419616901131\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.882428085493167\n",
      "    mean_inference_ms: 2.3967047100263774\n",
      "    mean_raw_obs_processing_ms: 0.8612352908874682\n",
      "  time_since_restore: 5518.245272159576\n",
      "  time_this_iter_s: 21.135030031204224\n",
      "  time_total_s: 5518.245272159576\n",
      "  timers:\n",
      "    learn_throughput: 298.933\n",
      "    learn_time_ms: 3345.232\n",
      "    load_throughput: 23720.286\n",
      "    load_time_ms: 42.158\n",
      "    sample_throughput: 57.171\n",
      "    sample_time_ms: 17491.348\n",
      "    update_time_ms: 8.653\n",
      "  timestamp: 1631883852\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 237000\n",
      "  training_iteration: 237\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   237</td><td style=\"text-align: right;\">         5518.25</td><td style=\"text-align: right;\">237000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 238000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-04-31\n",
      "  done: false\n",
      "  episode_len_mean: 996.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 238\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2402709960937499\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4051309400134615\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009668669065648396\n",
      "          policy_loss: -0.04426674925618702\n",
      "          total_loss: -0.06556955153743425\n",
      "          vf_explained_var: -0.5549781918525696\n",
      "          vf_loss: 0.00042540446199078964\n",
      "    num_agent_steps_sampled: 238000\n",
      "    num_agent_steps_trained: 238000\n",
      "    num_steps_sampled: 238000\n",
      "    num_steps_trained: 238000\n",
      "  iterations_since_restore: 238\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.91111111111111\n",
      "    ram_util_percent: 54.41481481481482\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06502154887762876\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.872337932653206\n",
      "    mean_inference_ms: 2.397490521169101\n",
      "    mean_raw_obs_processing_ms: 0.8615101909095303\n",
      "  time_since_restore: 5537.138881206512\n",
      "  time_this_iter_s: 18.893609046936035\n",
      "  time_total_s: 5537.138881206512\n",
      "  timers:\n",
      "    learn_throughput: 300.597\n",
      "    learn_time_ms: 3326.718\n",
      "    load_throughput: 24369.577\n",
      "    load_time_ms: 41.035\n",
      "    sample_throughput: 58.348\n",
      "    sample_time_ms: 17138.515\n",
      "    update_time_ms: 9.052\n",
      "  timestamp: 1631883871\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 238000\n",
      "  training_iteration: 238\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   238</td><td style=\"text-align: right;\">         5537.14</td><td style=\"text-align: right;\">238000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 239000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-05-10\n",
      "  done: false\n",
      "  episode_len_mean: 994.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 240\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2402709960937499\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5302413596047295\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008992739588868166\n",
      "          policy_loss: -0.06487356589900123\n",
      "          total_loss: -0.07560779410931799\n",
      "          vf_explained_var: -0.08387229591608047\n",
      "          vf_loss: 0.002407490720765458\n",
      "    num_agent_steps_sampled: 239000\n",
      "    num_agent_steps_trained: 239000\n",
      "    num_steps_sampled: 239000\n",
      "    num_steps_trained: 239000\n",
      "  iterations_since_restore: 239\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.13928571428572\n",
      "    ram_util_percent: 54.519642857142856\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06507526703913691\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.8526465140887\n",
      "    mean_inference_ms: 2.3990215223846985\n",
      "    mean_raw_obs_processing_ms: 0.8636525432718756\n",
      "  time_since_restore: 5576.43842792511\n",
      "  time_this_iter_s: 39.29954671859741\n",
      "  time_total_s: 5576.43842792511\n",
      "  timers:\n",
      "    learn_throughput: 303.13\n",
      "    learn_time_ms: 3298.915\n",
      "    load_throughput: 24921.252\n",
      "    load_time_ms: 40.126\n",
      "    sample_throughput: 53.028\n",
      "    sample_time_ms: 18857.802\n",
      "    update_time_ms: 8.97\n",
      "  timestamp: 1631883910\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 239000\n",
      "  training_iteration: 239\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   239</td><td style=\"text-align: right;\">         5576.44</td><td style=\"text-align: right;\">239000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             994.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-05-32\n",
      "  done: false\n",
      "  episode_len_mean: 994.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 241\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2402709960937499\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5727473550372655\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012811994657241253\n",
      "          policy_loss: -0.05757978981774714\n",
      "          total_loss: -0.08006197597003645\n",
      "          vf_explained_var: -0.681026816368103\n",
      "          vf_loss: 0.00016693572882407656\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 240\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.63870967741937\n",
      "    ram_util_percent: 54.49032258064516\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06510168532360991\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.842886208229913\n",
      "    mean_inference_ms: 2.3997729640625507\n",
      "    mean_raw_obs_processing_ms: 0.8647553635281892\n",
      "  time_since_restore: 5597.618897676468\n",
      "  time_this_iter_s: 21.180469751358032\n",
      "  time_total_s: 5597.618897676468\n",
      "  timers:\n",
      "    learn_throughput: 305.063\n",
      "    learn_time_ms: 3278.016\n",
      "    load_throughput: 24914.427\n",
      "    load_time_ms: 40.137\n",
      "    sample_throughput: 52.869\n",
      "    sample_time_ms: 18914.849\n",
      "    update_time_ms: 9.206\n",
      "  timestamp: 1631883932\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 240\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   240</td><td style=\"text-align: right;\">         5597.62</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             994.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 241000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-05-53\n",
      "  done: false\n",
      "  episode_len_mean: 994.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 242\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2402709960937499\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5172870026694403\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012729663930558487\n",
      "          policy_loss: -0.014022427631749048\n",
      "          total_loss: -0.03557681787448625\n",
      "          vf_explained_var: -0.8781890869140625\n",
      "          vf_loss: 0.0005599146766851643\n",
      "    num_agent_steps_sampled: 241000\n",
      "    num_agent_steps_trained: 241000\n",
      "    num_steps_sampled: 241000\n",
      "    num_steps_trained: 241000\n",
      "  iterations_since_restore: 241\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.60645161290324\n",
      "    ram_util_percent: 54.42258064516129\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06512778797267893\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.833232757130094\n",
      "    mean_inference_ms: 2.400513559119951\n",
      "    mean_raw_obs_processing_ms: 0.8658666095727764\n",
      "  time_since_restore: 5619.363477945328\n",
      "  time_this_iter_s: 21.744580268859863\n",
      "  time_total_s: 5619.363477945328\n",
      "  timers:\n",
      "    learn_throughput: 303.012\n",
      "    learn_time_ms: 3300.204\n",
      "    load_throughput: 25566.263\n",
      "    load_time_ms: 39.114\n",
      "    sample_throughput: 52.422\n",
      "    sample_time_ms: 19075.979\n",
      "    update_time_ms: 9.165\n",
      "  timestamp: 1631883953\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 241000\n",
      "  training_iteration: 241\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   241</td><td style=\"text-align: right;\">         5619.36</td><td style=\"text-align: right;\">241000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             994.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 242000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-06-16\n",
      "  done: false\n",
      "  episode_len_mean: 994.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 243\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2402709960937499\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8226065384017096\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021640508631334102\n",
      "          policy_loss: -0.022951164096593858\n",
      "          total_loss: -0.03153983900944392\n",
      "          vf_explained_var: 0.27084046602249146\n",
      "          vf_loss: 0.004437802041259905\n",
      "    num_agent_steps_sampled: 242000\n",
      "    num_agent_steps_trained: 242000\n",
      "    num_steps_sampled: 242000\n",
      "    num_steps_trained: 242000\n",
      "  iterations_since_restore: 242\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.15625\n",
      "    ram_util_percent: 54.587500000000006\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06515354059336814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.82369190937917\n",
      "    mean_inference_ms: 2.4012475425874675\n",
      "    mean_raw_obs_processing_ms: 0.866989169530801\n",
      "  time_since_restore: 5641.797409772873\n",
      "  time_this_iter_s: 22.433931827545166\n",
      "  time_total_s: 5641.797409772873\n",
      "  timers:\n",
      "    learn_throughput: 302.705\n",
      "    learn_time_ms: 3303.544\n",
      "    load_throughput: 26278.338\n",
      "    load_time_ms: 38.054\n",
      "    sample_throughput: 52.655\n",
      "    sample_time_ms: 18991.557\n",
      "    update_time_ms: 8.389\n",
      "  timestamp: 1631883976\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 242000\n",
      "  training_iteration: 242\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   242</td><td style=\"text-align: right;\">          5641.8</td><td style=\"text-align: right;\">242000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             994.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 243000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-06-35\n",
      "  done: false\n",
      "  episode_len_mean: 994.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 244\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2193796462482878\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015399175165676512\n",
      "          policy_loss: -0.07748763422585196\n",
      "          total_loss: -0.09247404902966486\n",
      "          vf_explained_var: -0.5581624507904053\n",
      "          vf_loss: 0.0016574208833036311\n",
      "    num_agent_steps_sampled: 243000\n",
      "    num_agent_steps_trained: 243000\n",
      "    num_steps_sampled: 243000\n",
      "    num_steps_trained: 243000\n",
      "  iterations_since_restore: 243\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.39642857142859\n",
      "    ram_util_percent: 54.73214285714287\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06517919394312231\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.814128034157964\n",
      "    mean_inference_ms: 2.4019705859808815\n",
      "    mean_raw_obs_processing_ms: 0.8681229794391969\n",
      "  time_since_restore: 5661.2656836509705\n",
      "  time_this_iter_s: 19.468273878097534\n",
      "  time_total_s: 5661.2656836509705\n",
      "  timers:\n",
      "    learn_throughput: 302.077\n",
      "    learn_time_ms: 3310.419\n",
      "    load_throughput: 26181.199\n",
      "    load_time_ms: 38.195\n",
      "    sample_throughput: 52.786\n",
      "    sample_time_ms: 18944.443\n",
      "    update_time_ms: 7.926\n",
      "  timestamp: 1631883995\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 243000\n",
      "  training_iteration: 243\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   243</td><td style=\"text-align: right;\">         5661.27</td><td style=\"text-align: right;\">243000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             994.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-06-55\n",
      "  done: false\n",
      "  episode_len_mean: 994.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 245\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1183500872717964\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011411988366341008\n",
      "          policy_loss: -0.00787665301726924\n",
      "          total_loss: -0.023327657083670298\n",
      "          vf_explained_var: -0.1909130960702896\n",
      "          vf_loss: 0.0016195433744643298\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_agent_steps_trained: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "  iterations_since_restore: 244\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.9\n",
      "    ram_util_percent: 54.68518518518518\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06520484042960717\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.804516311256197\n",
      "    mean_inference_ms: 2.4026783062434554\n",
      "    mean_raw_obs_processing_ms: 0.8692668529928117\n",
      "  time_since_restore: 5680.767412662506\n",
      "  time_this_iter_s: 19.501729011535645\n",
      "  time_total_s: 5680.767412662506\n",
      "  timers:\n",
      "    learn_throughput: 301.528\n",
      "    learn_time_ms: 3316.444\n",
      "    load_throughput: 24916.204\n",
      "    load_time_ms: 40.135\n",
      "    sample_throughput: 52.59\n",
      "    sample_time_ms: 19014.914\n",
      "    update_time_ms: 7.927\n",
      "  timestamp: 1631884015\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 244\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   244</td><td style=\"text-align: right;\">         5680.77</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             994.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 245000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-07-17\n",
      "  done: false\n",
      "  episode_len_mean: 994.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 246\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6699415524800618\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007410018533563011\n",
      "          policy_loss: -0.014593663232194052\n",
      "          total_loss: -0.02762328452534146\n",
      "          vf_explained_var: 0.06282100826501846\n",
      "          vf_loss: 0.0009991751414620215\n",
      "    num_agent_steps_sampled: 245000\n",
      "    num_agent_steps_trained: 245000\n",
      "    num_steps_sampled: 245000\n",
      "    num_steps_trained: 245000\n",
      "  iterations_since_restore: 245\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.31515151515151\n",
      "    ram_util_percent: 54.669696969696965\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06523030551008391\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.794949154354335\n",
      "    mean_inference_ms: 2.403374964111886\n",
      "    mean_raw_obs_processing_ms: 0.8704226567698925\n",
      "  time_since_restore: 5703.217685461044\n",
      "  time_this_iter_s: 22.450272798538208\n",
      "  time_total_s: 5703.217685461044\n",
      "  timers:\n",
      "    learn_throughput: 301.294\n",
      "    learn_time_ms: 3319.017\n",
      "    load_throughput: 25192.089\n",
      "    load_time_ms: 39.695\n",
      "    sample_throughput: 51.861\n",
      "    sample_time_ms: 19282.362\n",
      "    update_time_ms: 7.833\n",
      "  timestamp: 1631884037\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 245000\n",
      "  training_iteration: 245\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   245</td><td style=\"text-align: right;\">         5703.22</td><td style=\"text-align: right;\">245000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             994.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 246000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-07-36\n",
      "  done: false\n",
      "  episode_len_mean: 994.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 247\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6288456016116672\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00848820469784819\n",
      "          policy_loss: -0.07270787155462635\n",
      "          total_loss: -0.09552950834234555\n",
      "          vf_explained_var: -0.5303865075111389\n",
      "          vf_loss: 0.0004076127384097264\n",
      "    num_agent_steps_sampled: 246000\n",
      "    num_agent_steps_trained: 246000\n",
      "    num_steps_sampled: 246000\n",
      "    num_steps_trained: 246000\n",
      "  iterations_since_restore: 246\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.26538461538462\n",
      "    ram_util_percent: 54.7576923076923\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06525534826563378\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.78535553491422\n",
      "    mean_inference_ms: 2.4040586390576824\n",
      "    mean_raw_obs_processing_ms: 0.8715869174168592\n",
      "  time_since_restore: 5721.421979427338\n",
      "  time_this_iter_s: 18.204293966293335\n",
      "  time_total_s: 5721.421979427338\n",
      "  timers:\n",
      "    learn_throughput: 301.728\n",
      "    learn_time_ms: 3314.24\n",
      "    load_throughput: 25736.3\n",
      "    load_time_ms: 38.856\n",
      "    sample_throughput: 52.462\n",
      "    sample_time_ms: 19061.366\n",
      "    update_time_ms: 7.82\n",
      "  timestamp: 1631884056\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 246000\n",
      "  training_iteration: 246\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   246</td><td style=\"text-align: right;\">         5721.42</td><td style=\"text-align: right;\">246000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             994.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 247000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-07-53\n",
      "  done: false\n",
      "  episode_len_mean: 994.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 248\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6399842474195694\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007505974543899422\n",
      "          policy_loss: -0.028431431328256925\n",
      "          total_loss: -0.051923176998065576\n",
      "          vf_explained_var: -0.8157765865325928\n",
      "          vf_loss: 0.00020289487120963814\n",
      "    num_agent_steps_sampled: 247000\n",
      "    num_agent_steps_trained: 247000\n",
      "    num_steps_sampled: 247000\n",
      "    num_steps_trained: 247000\n",
      "  iterations_since_restore: 247\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.20416666666667\n",
      "    ram_util_percent: 54.6875\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06527987166382342\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.775586099068825\n",
      "    mean_inference_ms: 2.4047248369085676\n",
      "    mean_raw_obs_processing_ms: 0.8727596275479093\n",
      "  time_since_restore: 5738.544070005417\n",
      "  time_this_iter_s: 17.122090578079224\n",
      "  time_total_s: 5738.544070005417\n",
      "  timers:\n",
      "    learn_throughput: 299.735\n",
      "    learn_time_ms: 3336.275\n",
      "    load_throughput: 25200.293\n",
      "    load_time_ms: 39.682\n",
      "    sample_throughput: 53.653\n",
      "    sample_time_ms: 18638.241\n",
      "    update_time_ms: 7.161\n",
      "  timestamp: 1631884073\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 247000\n",
      "  training_iteration: 247\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   247</td><td style=\"text-align: right;\">         5738.54</td><td style=\"text-align: right;\">247000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             994.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-08-12\n",
      "  done: false\n",
      "  episode_len_mean: 994.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 249\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4170959181255762\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010257711211463634\n",
      "          policy_loss: -0.008020046022203233\n",
      "          total_loss: -0.02808950493733088\n",
      "          vf_explained_var: -0.3131256401538849\n",
      "          vf_loss: 0.000404552183479407\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "  iterations_since_restore: 248\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.1\n",
      "    ram_util_percent: 54.68518518518518\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06530398285784376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.765767731974577\n",
      "    mean_inference_ms: 2.4053747615787078\n",
      "    mean_raw_obs_processing_ms: 0.8739396439385777\n",
      "  time_since_restore: 5757.266045808792\n",
      "  time_this_iter_s: 18.721975803375244\n",
      "  time_total_s: 5757.266045808792\n",
      "  timers:\n",
      "    learn_throughput: 299.527\n",
      "    learn_time_ms: 3338.601\n",
      "    load_throughput: 23980.099\n",
      "    load_time_ms: 41.701\n",
      "    sample_throughput: 53.716\n",
      "    sample_time_ms: 18616.26\n",
      "    update_time_ms: 7.423\n",
      "  timestamp: 1631884092\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 248\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 68.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   248</td><td style=\"text-align: right;\">         5757.27</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             994.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 249000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-08-30\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 250\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.60210837788052\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004698857526665121\n",
      "          policy_loss: -0.041860949703388745\n",
      "          total_loss: -0.06606315546151664\n",
      "          vf_explained_var: -0.14511190354824066\n",
      "          vf_loss: 0.00012537952568689233\n",
      "    num_agent_steps_sampled: 249000\n",
      "    num_agent_steps_trained: 249000\n",
      "    num_steps_sampled: 249000\n",
      "    num_steps_trained: 249000\n",
      "  iterations_since_restore: 249\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.95384615384616\n",
      "    ram_util_percent: 54.93076923076922\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06532777032854234\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.755947242999312\n",
      "    mean_inference_ms: 2.406008263628033\n",
      "    mean_raw_obs_processing_ms: 0.8738282052773712\n",
      "  time_since_restore: 5775.700050830841\n",
      "  time_this_iter_s: 18.43400502204895\n",
      "  time_total_s: 5775.700050830841\n",
      "  timers:\n",
      "    learn_throughput: 298.212\n",
      "    learn_time_ms: 3353.321\n",
      "    load_throughput: 23526.656\n",
      "    load_time_ms: 42.505\n",
      "    sample_throughput: 60.552\n",
      "    sample_time_ms: 16514.779\n",
      "    update_time_ms: 7.609\n",
      "  timestamp: 1631884110\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 249000\n",
      "  training_iteration: 249\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   249</td><td style=\"text-align: right;\">          5775.7</td><td style=\"text-align: right;\">249000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 250000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-08-51\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 251\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18020324707031254\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1764312744140626\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00884762589463038\n",
      "          policy_loss: -0.046544923674729136\n",
      "          total_loss: -0.06564622355831994\n",
      "          vf_explained_var: 0.05611061677336693\n",
      "          vf_loss: 0.0010686416013009471\n",
      "    num_agent_steps_sampled: 250000\n",
      "    num_agent_steps_trained: 250000\n",
      "    num_steps_sampled: 250000\n",
      "    num_steps_trained: 250000\n",
      "  iterations_since_restore: 250\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.64666666666668\n",
      "    ram_util_percent: 54.96666666666667\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06535134549893366\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.746123493904733\n",
      "    mean_inference_ms: 2.4066330365192563\n",
      "    mean_raw_obs_processing_ms: 0.873735295783085\n",
      "  time_since_restore: 5796.512920856476\n",
      "  time_this_iter_s: 20.812870025634766\n",
      "  time_total_s: 5796.512920856476\n",
      "  timers:\n",
      "    learn_throughput: 298.841\n",
      "    learn_time_ms: 3346.262\n",
      "    load_throughput: 22725.46\n",
      "    load_time_ms: 44.004\n",
      "    sample_throughput: 60.665\n",
      "    sample_time_ms: 16483.923\n",
      "    update_time_ms: 7.492\n",
      "  timestamp: 1631884131\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 250000\n",
      "  training_iteration: 250\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   250</td><td style=\"text-align: right;\">         5796.51</td><td style=\"text-align: right;\">250000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 251000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-09-11\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 252\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18020324707031254\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.598739269044664\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0100361169889656\n",
      "          policy_loss: -0.02394912954316371\n",
      "          total_loss: -0.04791394385198752\n",
      "          vf_explained_var: -0.35088589787483215\n",
      "          vf_loss: 0.00021403972202986348\n",
      "    num_agent_steps_sampled: 251000\n",
      "    num_agent_steps_trained: 251000\n",
      "    num_steps_sampled: 251000\n",
      "    num_steps_trained: 251000\n",
      "  iterations_since_restore: 251\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.0642857142857\n",
      "    ram_util_percent: 54.90714285714286\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06537463029354376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.736338744968883\n",
      "    mean_inference_ms: 2.4072462940676465\n",
      "    mean_raw_obs_processing_ms: 0.8736598227961528\n",
      "  time_since_restore: 5816.332168579102\n",
      "  time_this_iter_s: 19.819247722625732\n",
      "  time_total_s: 5816.332168579102\n",
      "  timers:\n",
      "    learn_throughput: 301.36\n",
      "    learn_time_ms: 3318.287\n",
      "    load_throughput: 22035.192\n",
      "    load_time_ms: 45.382\n",
      "    sample_throughput: 61.426\n",
      "    sample_time_ms: 16279.816\n",
      "    update_time_ms: 7.326\n",
      "  timestamp: 1631884151\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 251000\n",
      "  training_iteration: 251\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   251</td><td style=\"text-align: right;\">         5816.33</td><td style=\"text-align: right;\">251000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-09-29\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 253\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18020324707031254\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.404777634143829\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011342534185933692\n",
      "          policy_loss: -0.00515492997235722\n",
      "          total_loss: -0.026935775991943148\n",
      "          vf_explained_var: -0.6658045053482056\n",
      "          vf_loss: 0.00022296775976226652\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_agent_steps_trained: 252000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "  iterations_since_restore: 252\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.92592592592592\n",
      "    ram_util_percent: 55.03333333333333\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06539775182879923\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.72654145150818\n",
      "    mean_inference_ms: 2.4078543412949687\n",
      "    mean_raw_obs_processing_ms: 0.8736041276527119\n",
      "  time_since_restore: 5834.804697036743\n",
      "  time_this_iter_s: 18.4725284576416\n",
      "  time_total_s: 5834.804697036743\n",
      "  timers:\n",
      "    learn_throughput: 302.0\n",
      "    learn_time_ms: 3311.259\n",
      "    load_throughput: 21258.122\n",
      "    load_time_ms: 47.041\n",
      "    sample_throughput: 62.934\n",
      "    sample_time_ms: 15889.555\n",
      "    update_time_ms: 6.794\n",
      "  timestamp: 1631884169\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 252\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   252</td><td style=\"text-align: right;\">          5834.8</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 253000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-09-49\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 254\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18020324707031254\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.464255279964871\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00703289825658509\n",
      "          policy_loss: 0.011814820766448974\n",
      "          total_loss: -0.01139675122168329\n",
      "          vf_explained_var: -0.3809989392757416\n",
      "          vf_loss: 0.00016362950208430346\n",
      "    num_agent_steps_sampled: 253000\n",
      "    num_agent_steps_trained: 253000\n",
      "    num_steps_sampled: 253000\n",
      "    num_steps_trained: 253000\n",
      "  iterations_since_restore: 253\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.08214285714287\n",
      "    ram_util_percent: 55.04285714285714\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06542072693138723\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.71682962440345\n",
      "    mean_inference_ms: 2.4084585981379885\n",
      "    mean_raw_obs_processing_ms: 0.873564397095773\n",
      "  time_since_restore: 5854.646896839142\n",
      "  time_this_iter_s: 19.84219980239868\n",
      "  time_total_s: 5854.646896839142\n",
      "  timers:\n",
      "    learn_throughput: 303.857\n",
      "    learn_time_ms: 3291.018\n",
      "    load_throughput: 21159.437\n",
      "    load_time_ms: 47.26\n",
      "    sample_throughput: 62.709\n",
      "    sample_time_ms: 15946.657\n",
      "    update_time_ms: 7.029\n",
      "  timestamp: 1631884189\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 253000\n",
      "  training_iteration: 253\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   253</td><td style=\"text-align: right;\">         5854.65</td><td style=\"text-align: right;\">253000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 254000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-10-10\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 255\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18020324707031254\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.172269121143553\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008866148927023568\n",
      "          policy_loss: -0.002254860517051485\n",
      "          total_loss: -0.01016233538587888\n",
      "          vf_explained_var: -0.08020985871553421\n",
      "          vf_loss: 0.0022175042940135526\n",
      "    num_agent_steps_sampled: 254000\n",
      "    num_agent_steps_trained: 254000\n",
      "    num_steps_sampled: 254000\n",
      "    num_steps_trained: 254000\n",
      "  iterations_since_restore: 254\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.99666666666667\n",
      "    ram_util_percent: 55.04333333333334\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06544340622586922\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.70730208421349\n",
      "    mean_inference_ms: 2.4090587701517365\n",
      "    mean_raw_obs_processing_ms: 0.8735419534686747\n",
      "  time_since_restore: 5875.575488328934\n",
      "  time_this_iter_s: 20.92859148979187\n",
      "  time_total_s: 5875.575488328934\n",
      "  timers:\n",
      "    learn_throughput: 305.292\n",
      "    learn_time_ms: 3275.554\n",
      "    load_throughput: 21120.94\n",
      "    load_time_ms: 47.346\n",
      "    sample_throughput: 62.094\n",
      "    sample_time_ms: 16104.562\n",
      "    update_time_ms: 7.035\n",
      "  timestamp: 1631884210\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 254000\n",
      "  training_iteration: 254\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   254</td><td style=\"text-align: right;\">         5875.58</td><td style=\"text-align: right;\">254000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 255000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-10-31\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 256\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18020324707031254\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3873217066129047\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02298204412364342\n",
      "          policy_loss: 0.030240821093320845\n",
      "          total_loss: 0.011448577418923378\n",
      "          vf_explained_var: -0.547936737537384\n",
      "          vf_loss: 0.0009395352946537767\n",
      "    num_agent_steps_sampled: 255000\n",
      "    num_agent_steps_trained: 255000\n",
      "    num_steps_sampled: 255000\n",
      "    num_steps_trained: 255000\n",
      "  iterations_since_restore: 255\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.10666666666665\n",
      "    ram_util_percent: 54.88666666666668\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06546575207352573\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.697690149345668\n",
      "    mean_inference_ms: 2.4096546090298427\n",
      "    mean_raw_obs_processing_ms: 0.8735359963980361\n",
      "  time_since_restore: 5896.700392484665\n",
      "  time_this_iter_s: 21.1249041557312\n",
      "  time_total_s: 5896.700392484665\n",
      "  timers:\n",
      "    learn_throughput: 305.403\n",
      "    learn_time_ms: 3274.367\n",
      "    load_throughput: 20843.199\n",
      "    load_time_ms: 47.977\n",
      "    sample_throughput: 62.606\n",
      "    sample_time_ms: 15972.807\n",
      "    update_time_ms: 6.807\n",
      "  timestamp: 1631884231\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 255000\n",
      "  training_iteration: 255\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   255</td><td style=\"text-align: right;\">          5896.7</td><td style=\"text-align: right;\">255000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-10-52\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 257\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048706054688\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0639383316040039\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013952982865125405\n",
      "          policy_loss: 0.05028067599568102\n",
      "          total_loss: 0.044030386871761744\n",
      "          vf_explained_var: -0.03897345811128616\n",
      "          vf_loss: 0.0006175309279771884\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "  iterations_since_restore: 256\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.59666666666666\n",
      "    ram_util_percent: 54.84666666666667\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06548805963948101\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.68811633826841\n",
      "    mean_inference_ms: 2.4102456786479367\n",
      "    mean_raw_obs_processing_ms: 0.8735510192780849\n",
      "  time_since_restore: 5917.854937076569\n",
      "  time_this_iter_s: 21.154544591903687\n",
      "  time_total_s: 5917.854937076569\n",
      "  timers:\n",
      "    learn_throughput: 304.882\n",
      "    learn_time_ms: 3279.96\n",
      "    load_throughput: 20056.713\n",
      "    load_time_ms: 49.859\n",
      "    sample_throughput: 61.5\n",
      "    sample_time_ms: 16260.142\n",
      "    update_time_ms: 6.953\n",
      "  timestamp: 1631884252\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 256\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   256</td><td style=\"text-align: right;\">         5917.85</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 257000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-11-11\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 258\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048706054688\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.089183260334863\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011181906765771361\n",
      "          policy_loss: 0.04003968040148417\n",
      "          total_loss: 0.022593638445768092\n",
      "          vf_explained_var: -0.5010351538658142\n",
      "          vf_loss: 0.00042326368867280607\n",
      "    num_agent_steps_sampled: 257000\n",
      "    num_agent_steps_trained: 257000\n",
      "    num_steps_sampled: 257000\n",
      "    num_steps_trained: 257000\n",
      "  iterations_since_restore: 257\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.96666666666667\n",
      "    ram_util_percent: 54.84814814814815\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06550958605034983\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.67857767811481\n",
      "    mean_inference_ms: 2.410832554533724\n",
      "    mean_raw_obs_processing_ms: 0.8735806731531148\n",
      "  time_since_restore: 5936.697016954422\n",
      "  time_this_iter_s: 18.842079877853394\n",
      "  time_total_s: 5936.697016954422\n",
      "  timers:\n",
      "    learn_throughput: 309.074\n",
      "    learn_time_ms: 3235.469\n",
      "    load_throughput: 19344.931\n",
      "    load_time_ms: 51.693\n",
      "    sample_throughput: 60.702\n",
      "    sample_time_ms: 16474.003\n",
      "    update_time_ms: 7.413\n",
      "  timestamp: 1631884271\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 257000\n",
      "  training_iteration: 257\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   257</td><td style=\"text-align: right;\">          5936.7</td><td style=\"text-align: right;\">257000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 258000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-11-36\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 259\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048706054688\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5433738811148539\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010662395507604651\n",
      "          policy_loss: 0.01581247817311022\n",
      "          total_loss: 0.00558922580546803\n",
      "          vf_explained_var: -0.5498093962669373\n",
      "          vf_loss: 0.0023283872692445584\n",
      "    num_agent_steps_sampled: 258000\n",
      "    num_agent_steps_trained: 258000\n",
      "    num_steps_sampled: 258000\n",
      "    num_steps_trained: 258000\n",
      "  iterations_since_restore: 258\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.23142857142855\n",
      "    ram_util_percent: 54.751428571428576\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06553078752878048\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.669308716430262\n",
      "    mean_inference_ms: 2.411415004384392\n",
      "    mean_raw_obs_processing_ms: 0.8736252028944891\n",
      "  time_since_restore: 5961.301643371582\n",
      "  time_this_iter_s: 24.604626417160034\n",
      "  time_total_s: 5961.301643371582\n",
      "  timers:\n",
      "    learn_throughput: 309.046\n",
      "    learn_time_ms: 3235.761\n",
      "    load_throughput: 19522.51\n",
      "    load_time_ms: 51.223\n",
      "    sample_throughput: 58.609\n",
      "    sample_time_ms: 17062.133\n",
      "    update_time_ms: 7.935\n",
      "  timestamp: 1631884296\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 258000\n",
      "  training_iteration: 258\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   258</td><td style=\"text-align: right;\">          5961.3</td><td style=\"text-align: right;\">258000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 259000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-11-57\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 260\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048706054688\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.338711112075382\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012111252115523819\n",
      "          policy_loss: 0.03643445461574528\n",
      "          total_loss: 0.01714893157283465\n",
      "          vf_explained_var: 0.12745891511440277\n",
      "          vf_loss: 0.0008278600753884449\n",
      "    num_agent_steps_sampled: 259000\n",
      "    num_agent_steps_trained: 259000\n",
      "    num_steps_sampled: 259000\n",
      "    num_steps_trained: 259000\n",
      "  iterations_since_restore: 259\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.06333333333335\n",
      "    ram_util_percent: 54.92333333333333\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06555170686772135\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.660066957162375\n",
      "    mean_inference_ms: 2.4119872561142013\n",
      "    mean_raw_obs_processing_ms: 0.8736835570810452\n",
      "  time_since_restore: 5982.549743652344\n",
      "  time_this_iter_s: 21.24810028076172\n",
      "  time_total_s: 5982.549743652344\n",
      "  timers:\n",
      "    learn_throughput: 309.155\n",
      "    learn_time_ms: 3234.618\n",
      "    load_throughput: 20440.656\n",
      "    load_time_ms: 48.922\n",
      "    sample_throughput: 57.647\n",
      "    sample_time_ms: 17346.938\n",
      "    update_time_ms: 7.907\n",
      "  timestamp: 1631884317\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 259000\n",
      "  training_iteration: 259\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   259</td><td style=\"text-align: right;\">         5982.55</td><td style=\"text-align: right;\">259000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-12-16\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 261\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048706054688\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.477712015310923\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007478591137112368\n",
      "          policy_loss: -0.051330424265729056\n",
      "          total_loss: -0.07359165789352523\n",
      "          vf_explained_var: -0.02806364931166172\n",
      "          vf_loss: 0.0004943814887155895\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_agent_steps_trained: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "  iterations_since_restore: 260\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.92592592592592\n",
      "    ram_util_percent: 54.96666666666667\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0655722997490599\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.65082705055193\n",
      "    mean_inference_ms: 2.412548828216408\n",
      "    mean_raw_obs_processing_ms: 0.8737537363465882\n",
      "  time_since_restore: 6001.051754236221\n",
      "  time_this_iter_s: 18.502010583877563\n",
      "  time_total_s: 6001.051754236221\n",
      "  timers:\n",
      "    learn_throughput: 308.747\n",
      "    learn_time_ms: 3238.894\n",
      "    load_throughput: 22219.1\n",
      "    load_time_ms: 45.006\n",
      "    sample_throughput: 58.428\n",
      "    sample_time_ms: 17115.097\n",
      "    update_time_ms: 7.96\n",
      "  timestamp: 1631884336\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 260\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   260</td><td style=\"text-align: right;\">         6001.05</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 261000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-12-34\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 262\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048706054688\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4529397792286343\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01885646320976784\n",
      "          policy_loss: 0.017440967427359687\n",
      "          total_loss: -0.0015133948789702522\n",
      "          vf_explained_var: -0.006633467972278595\n",
      "          vf_loss: 0.0004780429225623569\n",
      "    num_agent_steps_sampled: 261000\n",
      "    num_agent_steps_trained: 261000\n",
      "    num_steps_sampled: 261000\n",
      "    num_steps_trained: 261000\n",
      "  iterations_since_restore: 261\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.0576923076923\n",
      "    ram_util_percent: 54.98076923076924\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06559272140299297\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.641596658252954\n",
      "    mean_inference_ms: 2.4130980427375754\n",
      "    mean_raw_obs_processing_ms: 0.8738381839231697\n",
      "  time_since_restore: 6019.238510847092\n",
      "  time_this_iter_s: 18.18675661087036\n",
      "  time_total_s: 6019.238510847092\n",
      "  timers:\n",
      "    learn_throughput: 308.168\n",
      "    learn_time_ms: 3244.981\n",
      "    load_throughput: 22457.968\n",
      "    load_time_ms: 44.528\n",
      "    sample_throughput: 58.876\n",
      "    sample_time_ms: 16984.799\n",
      "    update_time_ms: 7.861\n",
      "  timestamp: 1631884354\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 261000\n",
      "  training_iteration: 261\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   261</td><td style=\"text-align: right;\">         6019.24</td><td style=\"text-align: right;\">261000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 262000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-12-56\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 263\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048706054688\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5738446897930567\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007366168234181379\n",
      "          policy_loss: -0.010512118879705667\n",
      "          total_loss: -0.033688547027607756\n",
      "          vf_explained_var: 0.009148666635155678\n",
      "          vf_loss: 0.0005709096788551606\n",
      "    num_agent_steps_sampled: 262000\n",
      "    num_agent_steps_trained: 262000\n",
      "    num_steps_sampled: 262000\n",
      "    num_steps_trained: 262000\n",
      "  iterations_since_restore: 262\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.60967741935484\n",
      "    ram_util_percent: 55.022580645161284\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06561268069662195\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.63244235371738\n",
      "    mean_inference_ms: 2.413636345314382\n",
      "    mean_raw_obs_processing_ms: 0.8739363208999604\n",
      "  time_since_restore: 6040.872642278671\n",
      "  time_this_iter_s: 21.63413143157959\n",
      "  time_total_s: 6040.872642278671\n",
      "  timers:\n",
      "    learn_throughput: 307.075\n",
      "    learn_time_ms: 3256.535\n",
      "    load_throughput: 22314.234\n",
      "    load_time_ms: 44.814\n",
      "    sample_throughput: 57.846\n",
      "    sample_time_ms: 17287.365\n",
      "    update_time_ms: 8.654\n",
      "  timestamp: 1631884376\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 262000\n",
      "  training_iteration: 262\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   262</td><td style=\"text-align: right;\">         6040.87</td><td style=\"text-align: right;\">262000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 263000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-13-15\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 264\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048706054688\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5150874071651037\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009870686889307064\n",
      "          policy_loss: -0.08928388361301687\n",
      "          total_loss: -0.1113191194832325\n",
      "          vf_explained_var: -0.2960982322692871\n",
      "          vf_loss: 0.0004475411179555522\n",
      "    num_agent_steps_sampled: 263000\n",
      "    num_agent_steps_trained: 263000\n",
      "    num_steps_sampled: 263000\n",
      "    num_steps_trained: 263000\n",
      "  iterations_since_restore: 263\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.11071428571428\n",
      "    ram_util_percent: 54.964285714285715\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563227204478438\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.623264779982303\n",
      "    mean_inference_ms: 2.4141657829907057\n",
      "    mean_raw_obs_processing_ms: 0.874047028605585\n",
      "  time_since_restore: 6060.44224023819\n",
      "  time_this_iter_s: 19.569597959518433\n",
      "  time_total_s: 6060.44224023819\n",
      "  timers:\n",
      "    learn_throughput: 305.947\n",
      "    learn_time_ms: 3268.543\n",
      "    load_throughput: 22365.22\n",
      "    load_time_ms: 44.712\n",
      "    sample_throughput: 57.978\n",
      "    sample_time_ms: 17248.048\n",
      "    update_time_ms: 8.482\n",
      "  timestamp: 1631884395\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 263000\n",
      "  training_iteration: 263\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   263</td><td style=\"text-align: right;\">         6060.44</td><td style=\"text-align: right;\">263000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-13-35\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 265\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048706054688\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4838998529646132\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006964994946142748\n",
      "          policy_loss: -0.05879223350849416\n",
      "          total_loss: -0.08042348954930073\n",
      "          vf_explained_var: -0.34815871715545654\n",
      "          vf_loss: 0.001325071110901869\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "  iterations_since_restore: 264\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.75000000000001\n",
      "    ram_util_percent: 55.046428571428564\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06565150279842485\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.614127842856448\n",
      "    mean_inference_ms: 2.4146823681723326\n",
      "    mean_raw_obs_processing_ms: 0.8741713236734576\n",
      "  time_since_restore: 6080.579000234604\n",
      "  time_this_iter_s: 20.136759996414185\n",
      "  time_total_s: 6080.579000234604\n",
      "  timers:\n",
      "    learn_throughput: 305.643\n",
      "    learn_time_ms: 3271.788\n",
      "    load_throughput: 23213.786\n",
      "    load_time_ms: 43.078\n",
      "    sample_throughput: 58.249\n",
      "    sample_time_ms: 17167.541\n",
      "    update_time_ms: 8.581\n",
      "  timestamp: 1631884415\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 264\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   264</td><td style=\"text-align: right;\">         6080.58</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 265000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-13-54\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 266\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048706054688\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6568623595767553\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0067920278052539295\n",
      "          policy_loss: -0.05886666588485241\n",
      "          total_loss: -0.08298657859882547\n",
      "          vf_explained_var: -0.5992338061332703\n",
      "          vf_loss: 0.0006127939402277762\n",
      "    num_agent_steps_sampled: 265000\n",
      "    num_agent_steps_trained: 265000\n",
      "    num_steps_sampled: 265000\n",
      "    num_steps_trained: 265000\n",
      "  iterations_since_restore: 265\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.85384615384615\n",
      "    ram_util_percent: 55.06923076923076\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06567029942115404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.604995385507433\n",
      "    mean_inference_ms: 2.4151881070670775\n",
      "    mean_raw_obs_processing_ms: 0.8743086288695484\n",
      "  time_since_restore: 6098.747233390808\n",
      "  time_this_iter_s: 18.168233156204224\n",
      "  time_total_s: 6098.747233390808\n",
      "  timers:\n",
      "    learn_throughput: 307.224\n",
      "    learn_time_ms: 3254.955\n",
      "    load_throughput: 23256.7\n",
      "    load_time_ms: 42.998\n",
      "    sample_throughput: 59.21\n",
      "    sample_time_ms: 16889.044\n",
      "    update_time_ms: 8.512\n",
      "  timestamp: 1631884434\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 265000\n",
      "  training_iteration: 265\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   265</td><td style=\"text-align: right;\">         6098.75</td><td style=\"text-align: right;\">265000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 266000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-14-13\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 267\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048706054688\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.495101903544532\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008789616749868657\n",
      "          policy_loss: -0.09074656379719576\n",
      "          total_loss: -0.11272072237398889\n",
      "          vf_explained_var: -0.8771919012069702\n",
      "          vf_loss: 0.0006009842327330261\n",
      "    num_agent_steps_sampled: 266000\n",
      "    num_agent_steps_trained: 266000\n",
      "    num_steps_sampled: 266000\n",
      "    num_steps_trained: 266000\n",
      "  iterations_since_restore: 266\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.63928571428572\n",
      "    ram_util_percent: 55.02499999999999\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.065688755787744\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.59585142266395\n",
      "    mean_inference_ms: 2.4156870122787804\n",
      "    mean_raw_obs_processing_ms: 0.8744593149112735\n",
      "  time_since_restore: 6117.765465021133\n",
      "  time_this_iter_s: 19.018231630325317\n",
      "  time_total_s: 6117.765465021133\n",
      "  timers:\n",
      "    learn_throughput: 307.188\n",
      "    learn_time_ms: 3255.338\n",
      "    load_throughput: 23690.315\n",
      "    load_time_ms: 42.211\n",
      "    sample_throughput: 59.966\n",
      "    sample_time_ms: 16675.99\n",
      "    update_time_ms: 8.446\n",
      "  timestamp: 1631884453\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 266000\n",
      "  training_iteration: 266\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   266</td><td style=\"text-align: right;\">         6117.77</td><td style=\"text-align: right;\">266000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 267000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-14-35\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 268\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048706054688\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.605266242557102\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008884117093942124\n",
      "          policy_loss: -0.02031387612223625\n",
      "          total_loss: -0.0435525575445758\n",
      "          vf_explained_var: -0.6254823207855225\n",
      "          vf_loss: 0.0004125600045881583\n",
      "    num_agent_steps_sampled: 267000\n",
      "    num_agent_steps_trained: 267000\n",
      "    num_steps_sampled: 267000\n",
      "    num_steps_trained: 267000\n",
      "  iterations_since_restore: 267\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.33548387096775\n",
      "    ram_util_percent: 55.08387096774194\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06570711009505724\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.586910997580038\n",
      "    mean_inference_ms: 2.4161815359276884\n",
      "    mean_raw_obs_processing_ms: 0.8746225691531077\n",
      "  time_since_restore: 6139.692167758942\n",
      "  time_this_iter_s: 21.926702737808228\n",
      "  time_total_s: 6139.692167758942\n",
      "  timers:\n",
      "    learn_throughput: 305.451\n",
      "    learn_time_ms: 3273.847\n",
      "    load_throughput: 25430.335\n",
      "    load_time_ms: 39.323\n",
      "    sample_throughput: 58.93\n",
      "    sample_time_ms: 16969.367\n",
      "    update_time_ms: 8.077\n",
      "  timestamp: 1631884475\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 267000\n",
      "  training_iteration: 267\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   267</td><td style=\"text-align: right;\">         6139.69</td><td style=\"text-align: right;\">267000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 268000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-14-53\n",
      "  done: false\n",
      "  episode_len_mean: 996.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 269\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048706054688\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.065948888990614\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008675385659342262\n",
      "          policy_loss: -0.012590409484174517\n",
      "          total_loss: -0.030268286830849117\n",
      "          vf_explained_var: -0.9930423498153687\n",
      "          vf_loss: 0.0006366129956101456\n",
      "    num_agent_steps_sampled: 268000\n",
      "    num_agent_steps_trained: 268000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "  iterations_since_restore: 268\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.22307692307692\n",
      "    ram_util_percent: 55.16153846153846\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06572521585603487\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.577958386276755\n",
      "    mean_inference_ms: 2.416665164157098\n",
      "    mean_raw_obs_processing_ms: 0.8747967355934918\n",
      "  time_since_restore: 6158.125022888184\n",
      "  time_this_iter_s: 18.432855129241943\n",
      "  time_total_s: 6158.125022888184\n",
      "  timers:\n",
      "    learn_throughput: 304.153\n",
      "    learn_time_ms: 3287.818\n",
      "    load_throughput: 25786.711\n",
      "    load_time_ms: 38.78\n",
      "    sample_throughput: 61.199\n",
      "    sample_time_ms: 16340.057\n",
      "    update_time_ms: 6.774\n",
      "  timestamp: 1631884493\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 268\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   268</td><td style=\"text-align: right;\">         6158.13</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 269000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-15-34\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 270\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048706054688\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4288941701253255\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0185153060813357\n",
      "          policy_loss: -0.001324428700738483\n",
      "          total_loss: -0.020217958009905286\n",
      "          vf_explained_var: -0.36393865942955017\n",
      "          vf_loss: 0.00039063461734056344\n",
      "    num_agent_steps_sampled: 269000\n",
      "    num_agent_steps_trained: 269000\n",
      "    num_steps_sampled: 269000\n",
      "    num_steps_trained: 269000\n",
      "  iterations_since_restore: 269\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.74137931034484\n",
      "    ram_util_percent: 54.92068965517242\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06574321076750082\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.568965213853712\n",
      "    mean_inference_ms: 2.4171442419206755\n",
      "    mean_raw_obs_processing_ms: 0.8757866672161972\n",
      "  time_since_restore: 6198.476944923401\n",
      "  time_this_iter_s: 40.351922035217285\n",
      "  time_total_s: 6198.476944923401\n",
      "  timers:\n",
      "    learn_throughput: 304.726\n",
      "    learn_time_ms: 3281.633\n",
      "    load_throughput: 25070.436\n",
      "    load_time_ms: 39.888\n",
      "    sample_throughput: 54.778\n",
      "    sample_time_ms: 18255.53\n",
      "    update_time_ms: 6.987\n",
      "  timestamp: 1631884534\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 269000\n",
      "  training_iteration: 269\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   269</td><td style=\"text-align: right;\">         6198.48</td><td style=\"text-align: right;\">269000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 270000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-15-53\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 271\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048706054688\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7947929753197562\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003679657428587014\n",
      "          policy_loss: -0.06740568661027485\n",
      "          total_loss: -0.09431537257300483\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 4.3615165163323077e-05\n",
      "    num_agent_steps_sampled: 270000\n",
      "    num_agent_steps_trained: 270000\n",
      "    num_steps_sampled: 270000\n",
      "    num_steps_trained: 270000\n",
      "  iterations_since_restore: 270\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.04814814814816\n",
      "    ram_util_percent: 54.86666666666667\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06576110587345223\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.560013252598274\n",
      "    mean_inference_ms: 2.417616178062979\n",
      "    mean_raw_obs_processing_ms: 0.8767848266936192\n",
      "  time_since_restore: 6217.591356992722\n",
      "  time_this_iter_s: 19.11441206932068\n",
      "  time_total_s: 6217.591356992722\n",
      "  timers:\n",
      "    learn_throughput: 303.944\n",
      "    learn_time_ms: 3290.083\n",
      "    load_throughput: 22723.859\n",
      "    load_time_ms: 44.007\n",
      "    sample_throughput: 54.631\n",
      "    sample_time_ms: 18304.622\n",
      "    update_time_ms: 6.886\n",
      "  timestamp: 1631884553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 270000\n",
      "  training_iteration: 270\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   270</td><td style=\"text-align: right;\">         6217.59</td><td style=\"text-align: right;\">270000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 271000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-16-12\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 272\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1351524353027344\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2668595472971598\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012661088515012015\n",
      "          policy_loss: -0.0035423156287935046\n",
      "          total_loss: -0.024239884979195065\n",
      "          vf_explained_var: -0.21099282801151276\n",
      "          vf_loss: 0.0002598455196372621\n",
      "    num_agent_steps_sampled: 271000\n",
      "    num_agent_steps_trained: 271000\n",
      "    num_steps_sampled: 271000\n",
      "    num_steps_trained: 271000\n",
      "  iterations_since_restore: 271\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.3074074074074\n",
      "    ram_util_percent: 54.96296296296296\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06577882549505219\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.551032921759454\n",
      "    mean_inference_ms: 2.418077624535753\n",
      "    mean_raw_obs_processing_ms: 0.8777908452939173\n",
      "  time_since_restore: 6236.600405693054\n",
      "  time_this_iter_s: 19.00904870033264\n",
      "  time_total_s: 6236.600405693054\n",
      "  timers:\n",
      "    learn_throughput: 301.596\n",
      "    learn_time_ms: 3315.69\n",
      "    load_throughput: 23631.453\n",
      "    load_time_ms: 42.316\n",
      "    sample_throughput: 54.458\n",
      "    sample_time_ms: 18362.781\n",
      "    update_time_ms: 7.008\n",
      "  timestamp: 1631884572\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 271000\n",
      "  training_iteration: 271\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   271</td><td style=\"text-align: right;\">          6236.6</td><td style=\"text-align: right;\">271000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-16-33\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 273\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1351524353027344\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4588063054614597\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.027354001990714177\n",
      "          policy_loss: 0.01135951206088066\n",
      "          total_loss: -0.008425585242609184\n",
      "          vf_explained_var: 0.05565381050109863\n",
      "          vf_loss: 0.0011060083175026293\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_agent_steps_trained: 272000\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "  iterations_since_restore: 272\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.43333333333335\n",
      "    ram_util_percent: 55.12333333333333\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06579649845536006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.542105055471346\n",
      "    mean_inference_ms: 2.418538286115798\n",
      "    mean_raw_obs_processing_ms: 0.8788077885540067\n",
      "  time_since_restore: 6257.400937080383\n",
      "  time_this_iter_s: 20.8005313873291\n",
      "  time_total_s: 6257.400937080383\n",
      "  timers:\n",
      "    learn_throughput: 302.866\n",
      "    learn_time_ms: 3301.792\n",
      "    load_throughput: 24017.215\n",
      "    load_time_ms: 41.637\n",
      "    sample_throughput: 54.658\n",
      "    sample_time_ms: 18295.42\n",
      "    update_time_ms: 6.235\n",
      "  timestamp: 1631884593\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 272\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   272</td><td style=\"text-align: right;\">          6257.4</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 273000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-16-57\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 274\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20272865295410147\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7228121439615887\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006555908932268236\n",
      "          policy_loss: -0.056072154579063255\n",
      "          total_loss: -0.0818501996083392\n",
      "          vf_explained_var: -0.9795089960098267\n",
      "          vf_loss: 0.0001210060645664473\n",
      "    num_agent_steps_sampled: 273000\n",
      "    num_agent_steps_trained: 273000\n",
      "    num_steps_sampled: 273000\n",
      "    num_steps_trained: 273000\n",
      "  iterations_since_restore: 273\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.26285714285714\n",
      "    ram_util_percent: 55.16\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06581383355732762\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.533379384720334\n",
      "    mean_inference_ms: 2.4189894794622946\n",
      "    mean_raw_obs_processing_ms: 0.8798336990719\n",
      "  time_since_restore: 6281.854563713074\n",
      "  time_this_iter_s: 24.45362663269043\n",
      "  time_total_s: 6281.854563713074\n",
      "  timers:\n",
      "    learn_throughput: 304.344\n",
      "    learn_time_ms: 3285.752\n",
      "    load_throughput: 25165.94\n",
      "    load_time_ms: 39.736\n",
      "    sample_throughput: 53.186\n",
      "    sample_time_ms: 18801.878\n",
      "    update_time_ms: 6.268\n",
      "  timestamp: 1631884617\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 273000\n",
      "  training_iteration: 273\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   273</td><td style=\"text-align: right;\">         6281.85</td><td style=\"text-align: right;\">273000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 274000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-17-16\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 275\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20272865295410147\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.796656839052836\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.002929389044971289\n",
      "          policy_loss: -0.05718997925933864\n",
      "          total_loss: -0.0845182174195846\n",
      "          vf_explained_var: -0.8385451436042786\n",
      "          vf_loss: 4.4457897537439646e-05\n",
      "    num_agent_steps_sampled: 274000\n",
      "    num_agent_steps_trained: 274000\n",
      "    num_steps_sampled: 274000\n",
      "    num_steps_trained: 274000\n",
      "  iterations_since_restore: 274\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.08148148148148\n",
      "    ram_util_percent: 55.196296296296296\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06583058468761656\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.524691441889495\n",
      "    mean_inference_ms: 2.4194296028650566\n",
      "    mean_raw_obs_processing_ms: 0.8808661477761285\n",
      "  time_since_restore: 6301.156710624695\n",
      "  time_this_iter_s: 19.302146911621094\n",
      "  time_total_s: 6301.156710624695\n",
      "  timers:\n",
      "    learn_throughput: 304.617\n",
      "    learn_time_ms: 3282.813\n",
      "    load_throughput: 24437.888\n",
      "    load_time_ms: 40.92\n",
      "    sample_throughput: 53.418\n",
      "    sample_time_ms: 18720.381\n",
      "    update_time_ms: 5.983\n",
      "  timestamp: 1631884636\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 274000\n",
      "  training_iteration: 274\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   274</td><td style=\"text-align: right;\">         6301.16</td><td style=\"text-align: right;\">274000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 275000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-17-36\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 276\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10136432647705074\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.743808952967326\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005968940498635611\n",
      "          policy_loss: -0.14591676187184122\n",
      "          total_loss: -0.17268128825558557\n",
      "          vf_explained_var: -0.6891257762908936\n",
      "          vf_loss: 6.852521623336037e-05\n",
      "    num_agent_steps_sampled: 275000\n",
      "    num_agent_steps_trained: 275000\n",
      "    num_steps_sampled: 275000\n",
      "    num_steps_trained: 275000\n",
      "  iterations_since_restore: 275\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.04285714285716\n",
      "    ram_util_percent: 55.14285714285713\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06584739149242769\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.516037186358\n",
      "    mean_inference_ms: 2.419864236613945\n",
      "    mean_raw_obs_processing_ms: 0.8819057980223242\n",
      "  time_since_restore: 6320.4636290073395\n",
      "  time_this_iter_s: 19.306918382644653\n",
      "  time_total_s: 6320.4636290073395\n",
      "  timers:\n",
      "    learn_throughput: 302.742\n",
      "    learn_time_ms: 3303.142\n",
      "    load_throughput: 24559.935\n",
      "    load_time_ms: 40.717\n",
      "    sample_throughput: 53.155\n",
      "    sample_time_ms: 18813.05\n",
      "    update_time_ms: 6.204\n",
      "  timestamp: 1631884656\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 275000\n",
      "  training_iteration: 275\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   275</td><td style=\"text-align: right;\">         6320.46</td><td style=\"text-align: right;\">275000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 276000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-17-56\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 277\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10136432647705074\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.456915643480089\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008951282872195055\n",
      "          policy_loss: -0.025808141064933603\n",
      "          total_loss: -0.049170313361618256\n",
      "          vf_explained_var: -0.5455197095870972\n",
      "          vf_loss: 0.00029964175125011633\n",
      "    num_agent_steps_sampled: 276000\n",
      "    num_agent_steps_trained: 276000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "  iterations_since_restore: 276\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.50344827586207\n",
      "    ram_util_percent: 55.09655172413793\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06586395505795424\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.507417108065056\n",
      "    mean_inference_ms: 2.420293369559527\n",
      "    mean_raw_obs_processing_ms: 0.8829492544516939\n",
      "  time_since_restore: 6341.0414452552795\n",
      "  time_this_iter_s: 20.577816247940063\n",
      "  time_total_s: 6341.0414452552795\n",
      "  timers:\n",
      "    learn_throughput: 301.895\n",
      "    learn_time_ms: 3312.408\n",
      "    load_throughput: 24031.636\n",
      "    load_time_ms: 41.612\n",
      "    sample_throughput: 52.747\n",
      "    sample_time_ms: 18958.551\n",
      "    update_time_ms: 6.33\n",
      "  timestamp: 1631884676\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 276\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   276</td><td style=\"text-align: right;\">         6341.04</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 277000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-18-17\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 278\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10136432647705074\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9432082699404822\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01930281784279969\n",
      "          policy_loss: -0.06910237587160534\n",
      "          total_loss: -0.08511004828744465\n",
      "          vf_explained_var: 0.20948821306228638\n",
      "          vf_loss: 0.0014677918149472032\n",
      "    num_agent_steps_sampled: 277000\n",
      "    num_agent_steps_trained: 277000\n",
      "    num_steps_sampled: 277000\n",
      "    num_steps_trained: 277000\n",
      "  iterations_since_restore: 277\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.27241379310344\n",
      "    ram_util_percent: 55.08620689655172\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06588046169291523\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.498707889069678\n",
      "    mean_inference_ms: 2.4207184596924005\n",
      "    mean_raw_obs_processing_ms: 0.8839993043102349\n",
      "  time_since_restore: 6361.359825372696\n",
      "  time_this_iter_s: 20.318380117416382\n",
      "  time_total_s: 6361.359825372696\n",
      "  timers:\n",
      "    learn_throughput: 299.148\n",
      "    learn_time_ms: 3342.823\n",
      "    load_throughput: 23300.369\n",
      "    load_time_ms: 42.918\n",
      "    sample_throughput: 53.292\n",
      "    sample_time_ms: 18764.575\n",
      "    update_time_ms: 7.137\n",
      "  timestamp: 1631884697\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 277000\n",
      "  training_iteration: 277\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   277</td><td style=\"text-align: right;\">         6361.36</td><td style=\"text-align: right;\">277000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 278000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-18-39\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 279\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10136432647705074\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.151294015513526\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.026719063651283213\n",
      "          policy_loss: -0.007707224579321013\n",
      "          total_loss: -0.025066928565502168\n",
      "          vf_explained_var: 0.4342817962169647\n",
      "          vf_loss: 0.0014448768426922874\n",
      "    num_agent_steps_sampled: 278000\n",
      "    num_agent_steps_trained: 278000\n",
      "    num_steps_sampled: 278000\n",
      "    num_steps_trained: 278000\n",
      "  iterations_since_restore: 278\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.34375\n",
      "    ram_util_percent: 55.20625\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06589684492352359\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.490132183179103\n",
      "    mean_inference_ms: 2.4211398075072554\n",
      "    mean_raw_obs_processing_ms: 0.8850564077425144\n",
      "  time_since_restore: 6383.322300672531\n",
      "  time_this_iter_s: 21.962475299835205\n",
      "  time_total_s: 6383.322300672531\n",
      "  timers:\n",
      "    learn_throughput: 300.38\n",
      "    learn_time_ms: 3329.114\n",
      "    load_throughput: 23050.908\n",
      "    load_time_ms: 43.382\n",
      "    sample_throughput: 52.272\n",
      "    sample_time_ms: 19130.679\n",
      "    update_time_ms: 7.182\n",
      "  timestamp: 1631884719\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 278000\n",
      "  training_iteration: 278\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   278</td><td style=\"text-align: right;\">         6383.32</td><td style=\"text-align: right;\">278000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 279000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-18-59\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 280\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15204648971557622\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.590017549196879\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006035456381887233\n",
      "          policy_loss: 0.0939078358726369\n",
      "          total_loss: 0.06912598158750269\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00020065097810907496\n",
      "    num_agent_steps_sampled: 279000\n",
      "    num_agent_steps_trained: 279000\n",
      "    num_steps_sampled: 279000\n",
      "    num_steps_trained: 279000\n",
      "  iterations_since_restore: 279\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.92758620689655\n",
      "    ram_util_percent: 55.23448275862069\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06591312390869318\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.481615667532644\n",
      "    mean_inference_ms: 2.4215546304064475\n",
      "    mean_raw_obs_processing_ms: 0.8848142247222216\n",
      "  time_since_restore: 6403.457087755203\n",
      "  time_this_iter_s: 20.13478708267212\n",
      "  time_total_s: 6403.457087755203\n",
      "  timers:\n",
      "    learn_throughput: 300.684\n",
      "    learn_time_ms: 3325.749\n",
      "    load_throughput: 22247.492\n",
      "    load_time_ms: 44.949\n",
      "    sample_throughput: 58.447\n",
      "    sample_time_ms: 17109.375\n",
      "    update_time_ms: 8.072\n",
      "  timestamp: 1631884739\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 279000\n",
      "  training_iteration: 279\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   279</td><td style=\"text-align: right;\">         6403.46</td><td style=\"text-align: right;\">279000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-19-19\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 281\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15204648971557622\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.514970670806037\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01295887925274677\n",
      "          policy_loss: 0.07565965863565603\n",
      "          total_loss: 0.05288413324289852\n",
      "          vf_explained_var: -0.20167717337608337\n",
      "          vf_loss: 0.00040382903986634723\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "  iterations_since_restore: 280\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.14482758620687\n",
      "    ram_util_percent: 55.3551724137931\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06592920466738766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.47318041151679\n",
      "    mean_inference_ms: 2.4219670226992283\n",
      "    mean_raw_obs_processing_ms: 0.8845867884413703\n",
      "  time_since_restore: 6423.860958099365\n",
      "  time_this_iter_s: 20.403870344161987\n",
      "  time_total_s: 6423.860958099365\n",
      "  timers:\n",
      "    learn_throughput: 300.598\n",
      "    learn_time_ms: 3326.697\n",
      "    load_throughput: 22216.382\n",
      "    load_time_ms: 45.012\n",
      "    sample_throughput: 58.014\n",
      "    sample_time_ms: 17237.308\n",
      "    update_time_ms: 8.112\n",
      "  timestamp: 1631884759\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 280\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   280</td><td style=\"text-align: right;\">         6423.86</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 281000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-19-43\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 282\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15204648971557622\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.653016376495361\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005141748840291803\n",
      "          policy_loss: 0.04774459033376641\n",
      "          total_loss: 0.02204261819521586\n",
      "          vf_explained_var: -0.907717227935791\n",
      "          vf_loss: 4.640608339185645e-05\n",
      "    num_agent_steps_sampled: 281000\n",
      "    num_agent_steps_trained: 281000\n",
      "    num_steps_sampled: 281000\n",
      "    num_steps_trained: 281000\n",
      "  iterations_since_restore: 281\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.41212121212122\n",
      "    ram_util_percent: 55.2939393939394\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06594489902341082\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.464948361604353\n",
      "    mean_inference_ms: 2.422370434567159\n",
      "    mean_raw_obs_processing_ms: 0.8843735497864813\n",
      "  time_since_restore: 6446.921450138092\n",
      "  time_this_iter_s: 23.060492038726807\n",
      "  time_total_s: 6446.921450138092\n",
      "  timers:\n",
      "    learn_throughput: 300.57\n",
      "    learn_time_ms: 3327.011\n",
      "    load_throughput: 23126.404\n",
      "    load_time_ms: 43.241\n",
      "    sample_throughput: 56.678\n",
      "    sample_time_ms: 17643.395\n",
      "    update_time_ms: 8.149\n",
      "  timestamp: 1631884783\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 281000\n",
      "  training_iteration: 281\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   281</td><td style=\"text-align: right;\">         6446.92</td><td style=\"text-align: right;\">281000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 282000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-20-05\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 283\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15204648971557622\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.628738843070136\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005978218634318347\n",
      "          policy_loss: 0.02432834202837613\n",
      "          total_loss: -0.0009620924376779132\n",
      "          vf_explained_var: -0.95623779296875\n",
      "          vf_loss: 8.798712709297736e-05\n",
      "    num_agent_steps_sampled: 282000\n",
      "    num_agent_steps_trained: 282000\n",
      "    num_steps_sampled: 282000\n",
      "    num_steps_trained: 282000\n",
      "  iterations_since_restore: 282\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.38387096774194\n",
      "    ram_util_percent: 55.32903225806452\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06596009501409025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.456843269408424\n",
      "    mean_inference_ms: 2.4227735912674246\n",
      "    mean_raw_obs_processing_ms: 0.8841727856836392\n",
      "  time_since_restore: 6469.092414140701\n",
      "  time_this_iter_s: 22.170964002609253\n",
      "  time_total_s: 6469.092414140701\n",
      "  timers:\n",
      "    learn_throughput: 299.37\n",
      "    learn_time_ms: 3340.345\n",
      "    load_throughput: 24119.251\n",
      "    load_time_ms: 41.461\n",
      "    sample_throughput: 56.277\n",
      "    sample_time_ms: 17769.141\n",
      "    update_time_ms: 8.168\n",
      "  timestamp: 1631884805\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 282000\n",
      "  training_iteration: 282\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   282</td><td style=\"text-align: right;\">         6469.09</td><td style=\"text-align: right;\">282000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 283000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-20-25\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 284\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15204648971557622\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6835966454611886\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00681183663113365\n",
      "          policy_loss: 0.15913677844736313\n",
      "          total_loss: 0.1333863417307536\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 4.9813299119705336e-05\n",
      "    num_agent_steps_sampled: 283000\n",
      "    num_agent_steps_trained: 283000\n",
      "    num_steps_sampled: 283000\n",
      "    num_steps_trained: 283000\n",
      "  iterations_since_restore: 283\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.92413793103448\n",
      "    ram_util_percent: 55.389655172413796\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06597502937411184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.448746374743187\n",
      "    mean_inference_ms: 2.4231715156743956\n",
      "    mean_raw_obs_processing_ms: 0.8839856922229689\n",
      "  time_since_restore: 6489.186651945114\n",
      "  time_this_iter_s: 20.094237804412842\n",
      "  time_total_s: 6489.186651945114\n",
      "  timers:\n",
      "    learn_throughput: 298.09\n",
      "    learn_time_ms: 3354.687\n",
      "    load_throughput: 22870.628\n",
      "    load_time_ms: 43.724\n",
      "    sample_throughput: 57.747\n",
      "    sample_time_ms: 17316.806\n",
      "    update_time_ms: 8.219\n",
      "  timestamp: 1631884825\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 283000\n",
      "  training_iteration: 283\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   283</td><td style=\"text-align: right;\">         6489.19</td><td style=\"text-align: right;\">283000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 284000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-20-45\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 285\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15204648971557622\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6861200041241116\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008850301769182866\n",
      "          policy_loss: 0.12586577178703415\n",
      "          total_loss: 0.1003912650876575\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 4.1035588846069814e-05\n",
      "    num_agent_steps_sampled: 284000\n",
      "    num_agent_steps_trained: 284000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "  iterations_since_restore: 284\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.38571428571429\n",
      "    ram_util_percent: 55.464285714285715\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06598989694667826\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.440651004436244\n",
      "    mean_inference_ms: 2.4235672697292716\n",
      "    mean_raw_obs_processing_ms: 0.8838103072916994\n",
      "  time_since_restore: 6508.990305900574\n",
      "  time_this_iter_s: 19.803653955459595\n",
      "  time_total_s: 6508.990305900574\n",
      "  timers:\n",
      "    learn_throughput: 298.538\n",
      "    learn_time_ms: 3349.661\n",
      "    load_throughput: 22641.562\n",
      "    load_time_ms: 44.167\n",
      "    sample_throughput: 57.566\n",
      "    sample_time_ms: 17371.402\n",
      "    update_time_ms: 8.39\n",
      "  timestamp: 1631884845\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 284\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   284</td><td style=\"text-align: right;\">         6508.99</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 285000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-21-09\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 286\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15204648971557622\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.594762987560696\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012465199660029657\n",
      "          policy_loss: 0.0809033066034317\n",
      "          total_loss: 0.05719075567192501\n",
      "          vf_explained_var: -0.458179771900177\n",
      "          vf_loss: 0.00033978787113382066\n",
      "    num_agent_steps_sampled: 285000\n",
      "    num_agent_steps_trained: 285000\n",
      "    num_steps_sampled: 285000\n",
      "    num_steps_trained: 285000\n",
      "  iterations_since_restore: 285\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.28235294117648\n",
      "    ram_util_percent: 55.317647058823525\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06600473618213261\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.43263101040598\n",
      "    mean_inference_ms: 2.4239602562353078\n",
      "    mean_raw_obs_processing_ms: 0.8836484769425814\n",
      "  time_since_restore: 6532.818780183792\n",
      "  time_this_iter_s: 23.828474283218384\n",
      "  time_total_s: 6532.818780183792\n",
      "  timers:\n",
      "    learn_throughput: 298.511\n",
      "    learn_time_ms: 3349.959\n",
      "    load_throughput: 23592.466\n",
      "    load_time_ms: 42.386\n",
      "    sample_throughput: 56.099\n",
      "    sample_time_ms: 17825.786\n",
      "    update_time_ms: 8.282\n",
      "  timestamp: 1631884869\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 285000\n",
      "  training_iteration: 285\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   285</td><td style=\"text-align: right;\">         6532.82</td><td style=\"text-align: right;\">285000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 286000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-21-31\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 287\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15204648971557622\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6342938449647693\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00742433108662074\n",
      "          policy_loss: 0.097702813314067\n",
      "          total_loss: 0.07259404645818802\n",
      "          vf_explained_var: -0.9233048558235168\n",
      "          vf_loss: 0.00010532662353297281\n",
      "    num_agent_steps_sampled: 286000\n",
      "    num_agent_steps_trained: 286000\n",
      "    num_steps_sampled: 286000\n",
      "    num_steps_trained: 286000\n",
      "  iterations_since_restore: 286\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.534375\n",
      "    ram_util_percent: 55.3125\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06601936954139279\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.4247375067486\n",
      "    mean_inference_ms: 2.424348684667587\n",
      "    mean_raw_obs_processing_ms: 0.8835007923048961\n",
      "  time_since_restore: 6554.993449211121\n",
      "  time_this_iter_s: 22.17466902732849\n",
      "  time_total_s: 6554.993449211121\n",
      "  timers:\n",
      "    learn_throughput: 300.26\n",
      "    learn_time_ms: 3330.443\n",
      "    load_throughput: 23364.995\n",
      "    load_time_ms: 42.799\n",
      "    sample_throughput: 55.542\n",
      "    sample_time_ms: 18004.309\n",
      "    update_time_ms: 8.439\n",
      "  timestamp: 1631884891\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 286000\n",
      "  training_iteration: 286\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   286</td><td style=\"text-align: right;\">         6554.99</td><td style=\"text-align: right;\">286000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 287000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-21-52\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 288\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15204648971557622\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6728565163082547\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008199312571388628\n",
      "          policy_loss: 0.07098161060776975\n",
      "          total_loss: 0.04563160137169891\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00013187835401266864\n",
      "    num_agent_steps_sampled: 287000\n",
      "    num_agent_steps_trained: 287000\n",
      "    num_steps_sampled: 287000\n",
      "    num_steps_trained: 287000\n",
      "  iterations_since_restore: 287\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.15\n",
      "    ram_util_percent: 55.476666666666674\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06603365352598846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.416898172061433\n",
      "    mean_inference_ms: 2.424734265932427\n",
      "    mean_raw_obs_processing_ms: 0.883365095578502\n",
      "  time_since_restore: 6576.015362977982\n",
      "  time_this_iter_s: 21.021913766860962\n",
      "  time_total_s: 6576.015362977982\n",
      "  timers:\n",
      "    learn_throughput: 302.331\n",
      "    learn_time_ms: 3307.633\n",
      "    load_throughput: 23845.661\n",
      "    load_time_ms: 41.936\n",
      "    sample_throughput: 55.249\n",
      "    sample_time_ms: 18099.771\n",
      "    update_time_ms: 7.537\n",
      "  timestamp: 1631884912\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 287000\n",
      "  training_iteration: 287\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   287</td><td style=\"text-align: right;\">         6576.02</td><td style=\"text-align: right;\">287000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-22-12\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 289\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15204648971557622\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.71205964618259\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006346682099491316\n",
      "          policy_loss: 0.1056672726240423\n",
      "          total_loss: 0.07954907003376219\n",
      "          vf_explained_var: -0.9774124026298523\n",
      "          vf_loss: 3.74023309228101e-05\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_agent_steps_trained: 288000\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "  iterations_since_restore: 288\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.95862068965516\n",
      "    ram_util_percent: 55.40689655172414\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06604782638147152\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.409085728561784\n",
      "    mean_inference_ms: 2.425116387758488\n",
      "    mean_raw_obs_processing_ms: 0.8832418939922588\n",
      "  time_since_restore: 6596.480478286743\n",
      "  time_this_iter_s: 20.465115308761597\n",
      "  time_total_s: 6596.480478286743\n",
      "  timers:\n",
      "    learn_throughput: 302.192\n",
      "    learn_time_ms: 3309.15\n",
      "    load_throughput: 24581.411\n",
      "    load_time_ms: 40.681\n",
      "    sample_throughput: 55.711\n",
      "    sample_time_ms: 17949.729\n",
      "    update_time_ms: 7.581\n",
      "  timestamp: 1631884932\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 288\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   288</td><td style=\"text-align: right;\">         6596.48</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 289000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-22-33\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 290\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15204648971557622\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7392754289839\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.002583901194258968\n",
      "          policy_loss: -0.0015251671093412572\n",
      "          total_loss: -0.028494014114969306\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 3.1033457606907986e-05\n",
      "    num_agent_steps_sampled: 289000\n",
      "    num_agent_steps_trained: 289000\n",
      "    num_steps_sampled: 289000\n",
      "    num_steps_trained: 289000\n",
      "  iterations_since_restore: 289\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.5206896551724\n",
      "    ram_util_percent: 55.317241379310346\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06606180603238376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.401294453161643\n",
      "    mean_inference_ms: 2.4254884963231422\n",
      "    mean_raw_obs_processing_ms: 0.883130590954051\n",
      "  time_since_restore: 6616.623641014099\n",
      "  time_this_iter_s: 20.143162727355957\n",
      "  time_total_s: 6616.623641014099\n",
      "  timers:\n",
      "    learn_throughput: 301.807\n",
      "    learn_time_ms: 3313.377\n",
      "    load_throughput: 24972.309\n",
      "    load_time_ms: 40.044\n",
      "    sample_throughput: 55.714\n",
      "    sample_time_ms: 17948.903\n",
      "    update_time_ms: 6.251\n",
      "  timestamp: 1631884953\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 289000\n",
      "  training_iteration: 289\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   289</td><td style=\"text-align: right;\">         6616.62</td><td style=\"text-align: right;\">289000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 290000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-22-54\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 291\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07602324485778811\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5808987591001724\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013379837952902603\n",
      "          policy_loss: 0.08939649338523546\n",
      "          total_loss: 0.06504130644930733\n",
      "          vf_explained_var: -0.9134762287139893\n",
      "          vf_loss: 0.00043662014407649014\n",
      "    num_agent_steps_sampled: 290000\n",
      "    num_agent_steps_trained: 290000\n",
      "    num_steps_sampled: 290000\n",
      "    num_steps_trained: 290000\n",
      "  iterations_since_restore: 290\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.96000000000001\n",
      "    ram_util_percent: 55.306666666666665\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06607559962837714\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.393545120591387\n",
      "    mean_inference_ms: 2.425854437279916\n",
      "    mean_raw_obs_processing_ms: 0.8830327873236898\n",
      "  time_since_restore: 6637.801804304123\n",
      "  time_this_iter_s: 21.178163290023804\n",
      "  time_total_s: 6637.801804304123\n",
      "  timers:\n",
      "    learn_throughput: 303.179\n",
      "    learn_time_ms: 3298.379\n",
      "    load_throughput: 25055.52\n",
      "    load_time_ms: 39.911\n",
      "    sample_throughput: 55.43\n",
      "    sample_time_ms: 18040.89\n",
      "    update_time_ms: 6.657\n",
      "  timestamp: 1631884974\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 290000\n",
      "  training_iteration: 290\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   290</td><td style=\"text-align: right;\">          6637.8</td><td style=\"text-align: right;\">290000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 291000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-23-14\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 292\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07602324485778811\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0300462113486395\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021526053129853572\n",
      "          policy_loss: -0.051824366839395626\n",
      "          total_loss: -0.06783039818207423\n",
      "          vf_explained_var: -0.36844465136528015\n",
      "          vf_loss: 0.002657949284184724\n",
      "    num_agent_steps_sampled: 291000\n",
      "    num_agent_steps_trained: 291000\n",
      "    num_steps_sampled: 291000\n",
      "    num_steps_trained: 291000\n",
      "  iterations_since_restore: 291\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.86999999999998\n",
      "    ram_util_percent: 55.19666666666667\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06608926839392595\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.385810731520966\n",
      "    mean_inference_ms: 2.426215899882385\n",
      "    mean_raw_obs_processing_ms: 0.8829472169579161\n",
      "  time_since_restore: 6658.23334145546\n",
      "  time_this_iter_s: 20.43153715133667\n",
      "  time_total_s: 6658.23334145546\n",
      "  timers:\n",
      "    learn_throughput: 305.776\n",
      "    learn_time_ms: 3270.366\n",
      "    load_throughput: 22930.031\n",
      "    load_time_ms: 43.611\n",
      "    sample_throughput: 56.17\n",
      "    sample_time_ms: 17802.951\n",
      "    update_time_ms: 6.483\n",
      "  timestamp: 1631884994\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 291000\n",
      "  training_iteration: 291\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   291</td><td style=\"text-align: right;\">         6658.23</td><td style=\"text-align: right;\">291000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 292000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-23-36\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 293\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11403486728668213\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1903636693954467\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01747640852145622\n",
      "          policy_loss: -0.05006816662434074\n",
      "          total_loss: -0.0689697943524354\n",
      "          vf_explained_var: -0.5702649354934692\n",
      "          vf_loss: 0.0010090894567029965\n",
      "    num_agent_steps_sampled: 292000\n",
      "    num_agent_steps_trained: 292000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "  iterations_since_restore: 292\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.2\n",
      "    ram_util_percent: 55.30967741935484\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06610281398867654\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.378071911491862\n",
      "    mean_inference_ms: 2.4265704863521904\n",
      "    mean_raw_obs_processing_ms: 0.8828720041892154\n",
      "  time_since_restore: 6679.983457326889\n",
      "  time_this_iter_s: 21.750115871429443\n",
      "  time_total_s: 6679.983457326889\n",
      "  timers:\n",
      "    learn_throughput: 307.64\n",
      "    learn_time_ms: 3250.549\n",
      "    load_throughput: 21999.398\n",
      "    load_time_ms: 45.456\n",
      "    sample_throughput: 56.246\n",
      "    sample_time_ms: 17779.012\n",
      "    update_time_ms: 6.291\n",
      "  timestamp: 1631885016\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 292\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   292</td><td style=\"text-align: right;\">         6679.98</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 293000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-23-58\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 294\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11403486728668213\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.715079519483778\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005825567838672856\n",
      "          policy_loss: 0.054059695452451706\n",
      "          total_loss: 0.027669125960932836\n",
      "          vf_explained_var: -0.7730311155319214\n",
      "          vf_loss: 9.590674304086457e-05\n",
      "    num_agent_steps_sampled: 293000\n",
      "    num_agent_steps_trained: 293000\n",
      "    num_steps_sampled: 293000\n",
      "    num_steps_trained: 293000\n",
      "  iterations_since_restore: 293\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.69\n",
      "    ram_util_percent: 55.370000000000005\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06611586542134279\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.370408358484056\n",
      "    mean_inference_ms: 2.426920710861878\n",
      "    mean_raw_obs_processing_ms: 0.8828089266635818\n",
      "  time_since_restore: 6701.535575866699\n",
      "  time_this_iter_s: 21.55211853981018\n",
      "  time_total_s: 6701.535575866699\n",
      "  timers:\n",
      "    learn_throughput: 309.162\n",
      "    learn_time_ms: 3234.551\n",
      "    load_throughput: 23046.931\n",
      "    load_time_ms: 43.39\n",
      "    sample_throughput: 55.734\n",
      "    sample_time_ms: 17942.229\n",
      "    update_time_ms: 6.34\n",
      "  timestamp: 1631885038\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 293000\n",
      "  training_iteration: 293\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   293</td><td style=\"text-align: right;\">         6701.54</td><td style=\"text-align: right;\">293000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 294000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-24-18\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 295\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11403486728668213\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.740007151497735\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0059119788618384045\n",
      "          policy_loss: 0.04569174028519127\n",
      "          total_loss: 0.01905200283250047\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 8.616146703085784e-05\n",
      "    num_agent_steps_sampled: 294000\n",
      "    num_agent_steps_trained: 294000\n",
      "    num_steps_sampled: 294000\n",
      "    num_steps_trained: 294000\n",
      "  iterations_since_restore: 294\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.22666666666667\n",
      "    ram_util_percent: 55.31333333333334\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06612865568682286\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.36281657403621\n",
      "    mean_inference_ms: 2.42726543622804\n",
      "    mean_raw_obs_processing_ms: 0.8827578367685741\n",
      "  time_since_restore: 6721.97500038147\n",
      "  time_this_iter_s: 20.439424514770508\n",
      "  time_total_s: 6721.97500038147\n",
      "  timers:\n",
      "    learn_throughput: 308.522\n",
      "    learn_time_ms: 3241.261\n",
      "    load_throughput: 24384.113\n",
      "    load_time_ms: 41.01\n",
      "    sample_throughput: 55.551\n",
      "    sample_time_ms: 18001.573\n",
      "    update_time_ms: 6.329\n",
      "  timestamp: 1631885058\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 294000\n",
      "  training_iteration: 294\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   294</td><td style=\"text-align: right;\">         6721.98</td><td style=\"text-align: right;\">294000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 295000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-24-38\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 296\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11403486728668213\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7596044884787667\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004616245810422213\n",
      "          policy_loss: 0.08974302700824208\n",
      "          total_loss: 0.0626988043801652\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 2.5408403391540763e-05\n",
      "    num_agent_steps_sampled: 295000\n",
      "    num_agent_steps_trained: 295000\n",
      "    num_steps_sampled: 295000\n",
      "    num_steps_trained: 295000\n",
      "  iterations_since_restore: 295\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.65\n",
      "    ram_util_percent: 55.38571428571429\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06614125918207767\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.355238781640956\n",
      "    mean_inference_ms: 2.4276052389940124\n",
      "    mean_raw_obs_processing_ms: 0.8827171119345565\n",
      "  time_since_restore: 6741.840939760208\n",
      "  time_this_iter_s: 19.865939378738403\n",
      "  time_total_s: 6741.840939760208\n",
      "  timers:\n",
      "    learn_throughput: 308.698\n",
      "    learn_time_ms: 3239.412\n",
      "    load_throughput: 23935.515\n",
      "    load_time_ms: 41.779\n",
      "    sample_throughput: 56.806\n",
      "    sample_time_ms: 17603.854\n",
      "    update_time_ms: 8.505\n",
      "  timestamp: 1631885078\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 295000\n",
      "  training_iteration: 295\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   295</td><td style=\"text-align: right;\">         6741.84</td><td style=\"text-align: right;\">295000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-24-58\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 297\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.05701743364334107\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7227751440472074\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006479527670285368\n",
      "          policy_loss: -0.0519945389435937\n",
      "          total_loss: -0.07873268184355564\n",
      "          vf_explained_var: -0.713198184967041\n",
      "          vf_loss: 0.0001201620956433342\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "  iterations_since_restore: 296\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.91785714285713\n",
      "    ram_util_percent: 55.267857142857146\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0661537659225142\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.347521755356855\n",
      "    mean_inference_ms: 2.4279400437448033\n",
      "    mean_raw_obs_processing_ms: 0.8826888313933514\n",
      "  time_since_restore: 6761.608772039413\n",
      "  time_this_iter_s: 19.767832279205322\n",
      "  time_total_s: 6761.608772039413\n",
      "  timers:\n",
      "    learn_throughput: 307.782\n",
      "    learn_time_ms: 3249.055\n",
      "    load_throughput: 23892.484\n",
      "    load_time_ms: 41.854\n",
      "    sample_throughput: 57.624\n",
      "    sample_time_ms: 17353.936\n",
      "    update_time_ms: 8.263\n",
      "  timestamp: 1631885098\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 296\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   296</td><td style=\"text-align: right;\">         6761.61</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 297000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-25-18\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 298\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.05701743364334107\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.449505032433404\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.025076790889276872\n",
      "          policy_loss: 0.01081434248222245\n",
      "          total_loss: -0.011442748664153946\n",
      "          vf_explained_var: -0.025228386744856834\n",
      "          vf_loss: 0.000808143576917549\n",
      "    num_agent_steps_sampled: 297000\n",
      "    num_agent_steps_trained: 297000\n",
      "    num_steps_sampled: 297000\n",
      "    num_steps_trained: 297000\n",
      "  iterations_since_restore: 297\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.98620689655174\n",
      "    ram_util_percent: 55.44827586206896\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06616613738206362\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.339803899075445\n",
      "    mean_inference_ms: 2.4282684821381917\n",
      "    mean_raw_obs_processing_ms: 0.8826714594947851\n",
      "  time_since_restore: 6781.831339836121\n",
      "  time_this_iter_s: 20.222567796707153\n",
      "  time_total_s: 6781.831339836121\n",
      "  timers:\n",
      "    learn_throughput: 308.39\n",
      "    learn_time_ms: 3242.649\n",
      "    load_throughput: 23155.564\n",
      "    load_time_ms: 43.186\n",
      "    sample_throughput: 57.879\n",
      "    sample_time_ms: 17277.318\n",
      "    update_time_ms: 9.157\n",
      "  timestamp: 1631885118\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 297000\n",
      "  training_iteration: 297\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   297</td><td style=\"text-align: right;\">         6781.83</td><td style=\"text-align: right;\">297000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 298000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-25-38\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 299\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0855261504650116\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5872764481438533\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01653148897503319\n",
      "          policy_loss: 0.024194898006195825\n",
      "          total_loss: -3.3324030745360584e-05\n",
      "          vf_explained_var: -0.823795735836029\n",
      "          vf_loss: 0.00023066772506960358\n",
      "    num_agent_steps_sampled: 298000\n",
      "    num_agent_steps_trained: 298000\n",
      "    num_steps_sampled: 298000\n",
      "    num_steps_trained: 298000\n",
      "  iterations_since_restore: 298\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.42857142857143\n",
      "    ram_util_percent: 55.435714285714276\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06617850382959704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.332092585038826\n",
      "    mean_inference_ms: 2.428592740722399\n",
      "    mean_raw_obs_processing_ms: 0.8826643294453472\n",
      "  time_since_restore: 6801.411475419998\n",
      "  time_this_iter_s: 19.580135583877563\n",
      "  time_total_s: 6801.411475419998\n",
      "  timers:\n",
      "    learn_throughput: 306.444\n",
      "    learn_time_ms: 3263.238\n",
      "    load_throughput: 22456.958\n",
      "    load_time_ms: 44.53\n",
      "    sample_throughput: 58.251\n",
      "    sample_time_ms: 17166.96\n",
      "    update_time_ms: 9.079\n",
      "  timestamp: 1631885138\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 298000\n",
      "  training_iteration: 298\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   298</td><td style=\"text-align: right;\">         6801.41</td><td style=\"text-align: right;\">298000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 299000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-26-20\n",
      "  done: false\n",
      "  episode_len_mean: 994.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 300\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0855261504650116\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.943097103966607\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02565695260900607\n",
      "          policy_loss: -0.03890473345915477\n",
      "          total_loss: -0.054285597883992724\n",
      "          vf_explained_var: -0.21230025589466095\n",
      "          vf_loss: 0.0018557662176640912\n",
      "    num_agent_steps_sampled: 299000\n",
      "    num_agent_steps_trained: 299000\n",
      "    num_steps_sampled: 299000\n",
      "    num_steps_trained: 299000\n",
      "  iterations_since_restore: 299\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.64098360655737\n",
      "    ram_util_percent: 55.367213114754094\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06619088656175769\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.32447326731168\n",
      "    mean_inference_ms: 2.4289181275486533\n",
      "    mean_raw_obs_processing_ms: 0.8832949772627526\n",
      "  time_since_restore: 6843.71879529953\n",
      "  time_this_iter_s: 42.30731987953186\n",
      "  time_total_s: 6843.71879529953\n",
      "  timers:\n",
      "    learn_throughput: 305.041\n",
      "    learn_time_ms: 3278.248\n",
      "    load_throughput: 22346.643\n",
      "    load_time_ms: 44.749\n",
      "    sample_throughput: 51.633\n",
      "    sample_time_ms: 19367.452\n",
      "    update_time_ms: 9.261\n",
      "  timestamp: 1631885180\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 299000\n",
      "  training_iteration: 299\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         6843.72</td><td style=\"text-align: right;\">299000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-26-41\n",
      "  done: false\n",
      "  episode_len_mean: 994.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 301\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12828922569751744\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5828995757632787\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01222180818445694\n",
      "          policy_loss: -0.05976082806785901\n",
      "          total_loss: -0.0830717012596627\n",
      "          vf_explained_var: -0.9296801090240479\n",
      "          vf_loss: 0.0009501971827679275\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_agent_steps_trained: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "  iterations_since_restore: 300\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.94333333333336\n",
      "    ram_util_percent: 55.5\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06620300371210124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.31688345763121\n",
      "    mean_inference_ms: 2.4292388555448268\n",
      "    mean_raw_obs_processing_ms: 0.8839330615872656\n",
      "  time_since_restore: 6864.9570343494415\n",
      "  time_this_iter_s: 21.2382390499115\n",
      "  time_total_s: 6864.9570343494415\n",
      "  timers:\n",
      "    learn_throughput: 305.06\n",
      "    learn_time_ms: 3278.039\n",
      "    load_throughput: 22137.304\n",
      "    load_time_ms: 45.173\n",
      "    sample_throughput: 51.617\n",
      "    sample_time_ms: 19373.522\n",
      "    update_time_ms: 8.921\n",
      "  timestamp: 1631885201\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 300\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   300</td><td style=\"text-align: right;\">         6864.96</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 301000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-27-02\n",
      "  done: false\n",
      "  episode_len_mean: 994.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 302\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12828922569751744\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5723404407501222\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015406273359853352\n",
      "          policy_loss: -0.014544805884361267\n",
      "          total_loss: -0.03799937263958984\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0002923779955987508\n",
      "    num_agent_steps_sampled: 301000\n",
      "    num_agent_steps_trained: 301000\n",
      "    num_steps_sampled: 301000\n",
      "    num_steps_trained: 301000\n",
      "  iterations_since_restore: 301\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.04482758620689\n",
      "    ram_util_percent: 55.47241379310345\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06621509571876422\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.309300350043983\n",
      "    mean_inference_ms: 2.4295573266044364\n",
      "    mean_raw_obs_processing_ms: 0.8845794939968725\n",
      "  time_since_restore: 6885.482703447342\n",
      "  time_this_iter_s: 20.52566909790039\n",
      "  time_total_s: 6885.482703447342\n",
      "  timers:\n",
      "    learn_throughput: 304.478\n",
      "    learn_time_ms: 3284.304\n",
      "    load_throughput: 22195.831\n",
      "    load_time_ms: 45.054\n",
      "    sample_throughput: 51.61\n",
      "    sample_time_ms: 19376.102\n",
      "    update_time_ms: 9.392\n",
      "  timestamp: 1631885222\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 301000\n",
      "  training_iteration: 301\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   301</td><td style=\"text-align: right;\">         6885.48</td><td style=\"text-align: right;\">301000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 302000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-27-21\n",
      "  done: false\n",
      "  episode_len_mean: 994.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 303\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12828922569751744\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.465876163376702\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013190686082480084\n",
      "          policy_loss: -0.12211132724252012\n",
      "          total_loss: -0.1448359102010727\n",
      "          vf_explained_var: -0.8919652104377747\n",
      "          vf_loss: 0.00024195426053160595\n",
      "    num_agent_steps_sampled: 302000\n",
      "    num_agent_steps_trained: 302000\n",
      "    num_steps_sampled: 302000\n",
      "    num_steps_trained: 302000\n",
      "  iterations_since_restore: 302\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.25357142857142\n",
      "    ram_util_percent: 55.68214285714286\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06622694474095996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.30170412781756\n",
      "    mean_inference_ms: 2.429868823462488\n",
      "    mean_raw_obs_processing_ms: 0.8852333188836313\n",
      "  time_since_restore: 6904.71777009964\n",
      "  time_this_iter_s: 19.235066652297974\n",
      "  time_total_s: 6904.71777009964\n",
      "  timers:\n",
      "    learn_throughput: 301.377\n",
      "    learn_time_ms: 3318.104\n",
      "    load_throughput: 22691.293\n",
      "    load_time_ms: 44.07\n",
      "    sample_throughput: 52.379\n",
      "    sample_time_ms: 19091.688\n",
      "    update_time_ms: 9.543\n",
      "  timestamp: 1631885241\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 302000\n",
      "  training_iteration: 302\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   302</td><td style=\"text-align: right;\">         6904.72</td><td style=\"text-align: right;\">302000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 303000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-27-41\n",
      "  done: false\n",
      "  episode_len_mean: 994.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 304\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12828922569751744\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4633743074205188\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012553596910081764\n",
      "          policy_loss: -0.021862055154310334\n",
      "          total_loss: -0.0442400753705038\n",
      "          vf_explained_var: -0.952746570110321\n",
      "          vf_loss: 0.000645231101791271\n",
      "    num_agent_steps_sampled: 303000\n",
      "    num_agent_steps_trained: 303000\n",
      "    num_steps_sampled: 303000\n",
      "    num_steps_trained: 303000\n",
      "  iterations_since_restore: 303\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.39655172413795\n",
      "    ram_util_percent: 55.603448275862085\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06623856831620857\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.294094845921776\n",
      "    mean_inference_ms: 2.4301747019807403\n",
      "    mean_raw_obs_processing_ms: 0.8858931732155686\n",
      "  time_since_restore: 6924.9159944057465\n",
      "  time_this_iter_s: 20.198224306106567\n",
      "  time_total_s: 6924.9159944057465\n",
      "  timers:\n",
      "    learn_throughput: 300.225\n",
      "    learn_time_ms: 3330.84\n",
      "    load_throughput: 22962.62\n",
      "    load_time_ms: 43.549\n",
      "    sample_throughput: 52.786\n",
      "    sample_time_ms: 18944.303\n",
      "    update_time_ms: 9.78\n",
      "  timestamp: 1631885261\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 303000\n",
      "  training_iteration: 303\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   303</td><td style=\"text-align: right;\">         6924.92</td><td style=\"text-align: right;\">303000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-28-01\n",
      "  done: false\n",
      "  episode_len_mean: 994.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 305\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12828922569751744\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4431792497634888\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015462125781565087\n",
      "          policy_loss: 0.06915298985938231\n",
      "          total_loss: 0.0474073226046231\n",
      "          vf_explained_var: -0.6804192662239075\n",
      "          vf_loss: 0.0007024998766913389\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_agent_steps_trained: 304000\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "  iterations_since_restore: 304\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.54285714285713\n",
      "    ram_util_percent: 55.66428571428571\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.066250183141183\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.28648430083093\n",
      "    mean_inference_ms: 2.430474514494473\n",
      "    mean_raw_obs_processing_ms: 0.88656080742725\n",
      "  time_since_restore: 6944.974132537842\n",
      "  time_this_iter_s: 20.058138132095337\n",
      "  time_total_s: 6944.974132537842\n",
      "  timers:\n",
      "    learn_throughput: 299.038\n",
      "    learn_time_ms: 3344.053\n",
      "    load_throughput: 21869.378\n",
      "    load_time_ms: 45.726\n",
      "    sample_throughput: 52.937\n",
      "    sample_time_ms: 18890.5\n",
      "    update_time_ms: 9.935\n",
      "  timestamp: 1631885281\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 304\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   304</td><td style=\"text-align: right;\">         6944.97</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 305000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-28-22\n",
      "  done: false\n",
      "  episode_len_mean: 994.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 306\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12828922569751744\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5267819934421114\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0178893573896566\n",
      "          policy_loss: -0.06795270920006766\n",
      "          total_loss: -0.0905139504517946\n",
      "          vf_explained_var: -0.581429123878479\n",
      "          vf_loss: 0.0004115661172868891\n",
      "    num_agent_steps_sampled: 305000\n",
      "    num_agent_steps_trained: 305000\n",
      "    num_steps_sampled: 305000\n",
      "    num_steps_trained: 305000\n",
      "  iterations_since_restore: 305\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.83103448275861\n",
      "    ram_util_percent: 55.66551724137932\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06626178620356382\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.27876342384935\n",
      "    mean_inference_ms: 2.430771340754182\n",
      "    mean_raw_obs_processing_ms: 0.8872364186658924\n",
      "  time_since_restore: 6965.487051725388\n",
      "  time_this_iter_s: 20.512919187545776\n",
      "  time_total_s: 6965.487051725388\n",
      "  timers:\n",
      "    learn_throughput: 299.027\n",
      "    learn_time_ms: 3344.174\n",
      "    load_throughput: 22584.87\n",
      "    load_time_ms: 44.277\n",
      "    sample_throughput: 52.745\n",
      "    sample_time_ms: 18959.196\n",
      "    update_time_ms: 7.747\n",
      "  timestamp: 1631885302\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 305000\n",
      "  training_iteration: 305\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   305</td><td style=\"text-align: right;\">         6965.49</td><td style=\"text-align: right;\">305000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 306000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-28-43\n",
      "  done: false\n",
      "  episode_len_mean: 994.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 307\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12828922569751744\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.505867849455939\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017343864268813725\n",
      "          policy_loss: -0.0458839846154054\n",
      "          total_loss: -0.06821206888804833\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0005055652472543039\n",
      "    num_agent_steps_sampled: 306000\n",
      "    num_agent_steps_trained: 306000\n",
      "    num_steps_sampled: 306000\n",
      "    num_steps_trained: 306000\n",
      "  iterations_since_restore: 306\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.25806451612904\n",
      "    ram_util_percent: 55.745161290322585\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06627320859910248\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.271003321981375\n",
      "    mean_inference_ms: 2.4310614112458073\n",
      "    mean_raw_obs_processing_ms: 0.887921132761607\n",
      "  time_since_restore: 6986.515008449554\n",
      "  time_this_iter_s: 21.02795672416687\n",
      "  time_total_s: 6986.515008449554\n",
      "  timers:\n",
      "    learn_throughput: 297.733\n",
      "    learn_time_ms: 3358.717\n",
      "    load_throughput: 22657.682\n",
      "    load_time_ms: 44.135\n",
      "    sample_throughput: 52.438\n",
      "    sample_time_ms: 19069.967\n",
      "    update_time_ms: 8.021\n",
      "  timestamp: 1631885323\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 306000\n",
      "  training_iteration: 306\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   306</td><td style=\"text-align: right;\">         6986.52</td><td style=\"text-align: right;\">306000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 307000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-29-04\n",
      "  done: false\n",
      "  episode_len_mean: 994.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 308\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12828922569751744\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.499177736706204\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01354604856810224\n",
      "          policy_loss: -0.024357867903179592\n",
      "          total_loss: -0.04721443373709917\n",
      "          vf_explained_var: -0.9994920492172241\n",
      "          vf_loss: 0.000397397769201133\n",
      "    num_agent_steps_sampled: 307000\n",
      "    num_agent_steps_trained: 307000\n",
      "    num_steps_sampled: 307000\n",
      "    num_steps_trained: 307000\n",
      "  iterations_since_restore: 307\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.33666666666666\n",
      "    ram_util_percent: 55.74999999999999\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06628457153990955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.263272671704748\n",
      "    mean_inference_ms: 2.4313488694782\n",
      "    mean_raw_obs_processing_ms: 0.8886129914308663\n",
      "  time_since_restore: 7007.589211940765\n",
      "  time_this_iter_s: 21.074203491210938\n",
      "  time_total_s: 7007.589211940765\n",
      "  timers:\n",
      "    learn_throughput: 296.959\n",
      "    learn_time_ms: 3367.472\n",
      "    load_throughput: 23084.108\n",
      "    load_time_ms: 43.32\n",
      "    sample_throughput: 52.229\n",
      "    sample_time_ms: 19146.627\n",
      "    update_time_ms: 9.033\n",
      "  timestamp: 1631885344\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 307000\n",
      "  training_iteration: 307\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   307</td><td style=\"text-align: right;\">         7007.59</td><td style=\"text-align: right;\">307000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 308000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-29-24\n",
      "  done: false\n",
      "  episode_len_mean: 994.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 309\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12828922569751744\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.495782306459215\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03506498531670215\n",
      "          policy_loss: -0.03693333088109891\n",
      "          total_loss: -0.056966881391902764\n",
      "          vf_explained_var: -0.6741402745246887\n",
      "          vf_loss: 0.0004258098427574926\n",
      "    num_agent_steps_sampled: 308000\n",
      "    num_agent_steps_trained: 308000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "  iterations_since_restore: 308\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.68928571428572\n",
      "    ram_util_percent: 55.8142857142857\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06629588111324942\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.255548554693213\n",
      "    mean_inference_ms: 2.4316309801337677\n",
      "    mean_raw_obs_processing_ms: 0.8893118333334141\n",
      "  time_since_restore: 7027.512650251389\n",
      "  time_this_iter_s: 19.92343831062317\n",
      "  time_total_s: 7027.512650251389\n",
      "  timers:\n",
      "    learn_throughput: 297.637\n",
      "    learn_time_ms: 3359.801\n",
      "    load_throughput: 23105.956\n",
      "    load_time_ms: 43.279\n",
      "    sample_throughput: 52.113\n",
      "    sample_time_ms: 19188.908\n",
      "    update_time_ms: 8.926\n",
      "  timestamp: 1631885364\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 308\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   308</td><td style=\"text-align: right;\">         7027.51</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 309000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-29-45\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 310\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19243383854627605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5108295891020034\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015809574851891578\n",
      "          policy_loss: 0.05791384068628152\n",
      "          total_loss: 0.03679774660203192\n",
      "          vf_explained_var: -0.4413624703884125\n",
      "          vf_loss: 0.0009499044674157631\n",
      "    num_agent_steps_sampled: 309000\n",
      "    num_agent_steps_trained: 309000\n",
      "    num_steps_sampled: 309000\n",
      "    num_steps_trained: 309000\n",
      "  iterations_since_restore: 309\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.71333333333332\n",
      "    ram_util_percent: 55.63333333333332\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06630720315225465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.24769887610122\n",
      "    mean_inference_ms: 2.431908525215545\n",
      "    mean_raw_obs_processing_ms: 0.8890138434009549\n",
      "  time_since_restore: 7048.717900276184\n",
      "  time_this_iter_s: 21.205250024795532\n",
      "  time_total_s: 7048.717900276184\n",
      "  timers:\n",
      "    learn_throughput: 299.24\n",
      "    learn_time_ms: 3341.802\n",
      "    load_throughput: 23170.416\n",
      "    load_time_ms: 43.158\n",
      "    sample_throughput: 58.495\n",
      "    sample_time_ms: 17095.568\n",
      "    update_time_ms: 10.144\n",
      "  timestamp: 1631885385\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 309000\n",
      "  training_iteration: 309\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   309</td><td style=\"text-align: right;\">         7048.72</td><td style=\"text-align: right;\">309000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 310000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-30-06\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 311\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19243383854627605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5390938149558173\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012153583631710083\n",
      "          policy_loss: 0.024798952539761863\n",
      "          total_loss: 0.0022855339778794182\n",
      "          vf_explained_var: -0.4724244475364685\n",
      "          vf_loss: 0.0005387588354399971\n",
      "    num_agent_steps_sampled: 310000\n",
      "    num_agent_steps_trained: 310000\n",
      "    num_steps_sampled: 310000\n",
      "    num_steps_trained: 310000\n",
      "  iterations_since_restore: 310\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.05666666666667\n",
      "    ram_util_percent: 55.76333333333334\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06631855653455254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.239898315893633\n",
      "    mean_inference_ms: 2.4321818039450025\n",
      "    mean_raw_obs_processing_ms: 0.8887274691282857\n",
      "  time_since_restore: 7069.368768930435\n",
      "  time_this_iter_s: 20.6508686542511\n",
      "  time_total_s: 7069.368768930435\n",
      "  timers:\n",
      "    learn_throughput: 297.808\n",
      "    learn_time_ms: 3357.873\n",
      "    load_throughput: 23619.183\n",
      "    load_time_ms: 42.338\n",
      "    sample_throughput: 58.749\n",
      "    sample_time_ms: 17021.574\n",
      "    update_time_ms: 10.102\n",
      "  timestamp: 1631885406\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 310000\n",
      "  training_iteration: 310\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   310</td><td style=\"text-align: right;\">         7069.37</td><td style=\"text-align: right;\">310000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 311000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-30-27\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 312\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19243383854627605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2847437143325806\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.04750924064854833\n",
      "          policy_loss: -0.04836229748196072\n",
      "          total_loss: -0.049563030733002555\n",
      "          vf_explained_var: 0.08580955117940903\n",
      "          vf_loss: 0.002504315571988829\n",
      "    num_agent_steps_sampled: 311000\n",
      "    num_agent_steps_trained: 311000\n",
      "    num_steps_sampled: 311000\n",
      "    num_steps_trained: 311000\n",
      "  iterations_since_restore: 311\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.87333333333332\n",
      "    ram_util_percent: 55.803333333333335\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06632978829843285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.23221220069165\n",
      "    mean_inference_ms: 2.432454525123552\n",
      "    mean_raw_obs_processing_ms: 0.8884514465254741\n",
      "  time_since_restore: 7090.721147060394\n",
      "  time_this_iter_s: 21.352378129959106\n",
      "  time_total_s: 7090.721147060394\n",
      "  timers:\n",
      "    learn_throughput: 297.77\n",
      "    learn_time_ms: 3358.296\n",
      "    load_throughput: 24667.53\n",
      "    load_time_ms: 40.539\n",
      "    sample_throughput: 58.459\n",
      "    sample_time_ms: 17106.14\n",
      "    update_time_ms: 9.751\n",
      "  timestamp: 1631885427\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 311000\n",
      "  training_iteration: 311\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   311</td><td style=\"text-align: right;\">         7090.72</td><td style=\"text-align: right;\">311000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-30-48\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 313\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4916642904281616\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005707322127863663\n",
      "          policy_loss: -0.09620169376333555\n",
      "          total_loss: -0.11913233912653393\n",
      "          vf_explained_var: -0.5865264534950256\n",
      "          vf_loss: 0.0003385753644882546\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "  iterations_since_restore: 312\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.08333333333333\n",
      "    ram_util_percent: 55.77333333333333\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06634065219148039\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.224586227244107\n",
      "    mean_inference_ms: 2.4327281115169384\n",
      "    mean_raw_obs_processing_ms: 0.8881874940827372\n",
      "  time_since_restore: 7111.203065156937\n",
      "  time_this_iter_s: 20.48191809654236\n",
      "  time_total_s: 7111.203065156937\n",
      "  timers:\n",
      "    learn_throughput: 299.78\n",
      "    learn_time_ms: 3335.774\n",
      "    load_throughput: 24891.199\n",
      "    load_time_ms: 40.175\n",
      "    sample_throughput: 57.959\n",
      "    sample_time_ms: 17253.51\n",
      "    update_time_ms: 9.872\n",
      "  timestamp: 1631885448\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 312\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 69.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   312</td><td style=\"text-align: right;\">          7111.2</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 313000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-31-09\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 314\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5537684784995185\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007723044732857827\n",
      "          policy_loss: -0.04003762768374549\n",
      "          total_loss: -0.06311881169676781\n",
      "          vf_explained_var: -0.656319797039032\n",
      "          vf_loss: 0.00022723653888129372\n",
      "    num_agent_steps_sampled: 313000\n",
      "    num_agent_steps_trained: 313000\n",
      "    num_steps_sampled: 313000\n",
      "    num_steps_trained: 313000\n",
      "  iterations_since_restore: 313\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.97999999999999\n",
      "    ram_util_percent: 55.67333333333332\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06635144749375861\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.216957471181036\n",
      "    mean_inference_ms: 2.4330003575149037\n",
      "    mean_raw_obs_processing_ms: 0.8879357928191076\n",
      "  time_since_restore: 7132.297426223755\n",
      "  time_this_iter_s: 21.094361066818237\n",
      "  time_total_s: 7132.297426223755\n",
      "  timers:\n",
      "    learn_throughput: 300.12\n",
      "    learn_time_ms: 3331.998\n",
      "    load_throughput: 24930.139\n",
      "    load_time_ms: 40.112\n",
      "    sample_throughput: 57.651\n",
      "    sample_time_ms: 17345.739\n",
      "    update_time_ms: 10.684\n",
      "  timestamp: 1631885469\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 313000\n",
      "  training_iteration: 313\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   313</td><td style=\"text-align: right;\">          7132.3</td><td style=\"text-align: right;\">313000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 314000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-31-33\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 315\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5364712105857\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00706910255157175\n",
      "          policy_loss: -0.08178039809895886\n",
      "          total_loss: -0.10473292635546791\n",
      "          vf_explained_var: -0.7417970299720764\n",
      "          vf_loss: 0.0003716798840792358\n",
      "    num_agent_steps_sampled: 314000\n",
      "    num_agent_steps_trained: 314000\n",
      "    num_steps_sampled: 314000\n",
      "    num_steps_trained: 314000\n",
      "  iterations_since_restore: 314\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.62058823529412\n",
      "    ram_util_percent: 55.85\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06636202747776528\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.209496630637602\n",
      "    mean_inference_ms: 2.4332725550189496\n",
      "    mean_raw_obs_processing_ms: 0.8876955379784245\n",
      "  time_since_restore: 7155.868448019028\n",
      "  time_this_iter_s: 23.571021795272827\n",
      "  time_total_s: 7155.868448019028\n",
      "  timers:\n",
      "    learn_throughput: 301.051\n",
      "    learn_time_ms: 3321.701\n",
      "    load_throughput: 25094.21\n",
      "    load_time_ms: 39.85\n",
      "    sample_throughput: 56.473\n",
      "    sample_time_ms: 17707.532\n",
      "    update_time_ms: 10.719\n",
      "  timestamp: 1631885493\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 314000\n",
      "  training_iteration: 314\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   314</td><td style=\"text-align: right;\">         7155.87</td><td style=\"text-align: right;\">314000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 315000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-31-51\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 316\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6301744408077665\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015018740838336776\n",
      "          policy_loss: -0.0678845244149367\n",
      "          total_loss: -0.08955935332924128\n",
      "          vf_explained_var: 0.596545398235321\n",
      "          vf_loss: 0.0002917445962035951\n",
      "    num_agent_steps_sampled: 315000\n",
      "    num_agent_steps_trained: 315000\n",
      "    num_steps_sampled: 315000\n",
      "    num_steps_trained: 315000\n",
      "  iterations_since_restore: 315\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.23846153846154\n",
      "    ram_util_percent: 55.76538461538462\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06637223390616394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.202036070023752\n",
      "    mean_inference_ms: 2.4335423315692046\n",
      "    mean_raw_obs_processing_ms: 0.8874654527794106\n",
      "  time_since_restore: 7174.4768924713135\n",
      "  time_this_iter_s: 18.608444452285767\n",
      "  time_total_s: 7174.4768924713135\n",
      "  timers:\n",
      "    learn_throughput: 303.126\n",
      "    learn_time_ms: 3298.958\n",
      "    load_throughput: 23711.958\n",
      "    load_time_ms: 42.173\n",
      "    sample_throughput: 57.024\n",
      "    sample_time_ms: 17536.461\n",
      "    update_time_ms: 10.689\n",
      "  timestamp: 1631885511\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 315000\n",
      "  training_iteration: 315\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   315</td><td style=\"text-align: right;\">         7174.48</td><td style=\"text-align: right;\">315000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 316000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-32-11\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 317\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5903414567311605\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013666468243457332\n",
      "          policy_loss: -0.03220473610692554\n",
      "          total_loss: -0.053694354101187655\n",
      "          vf_explained_var: -0.4365336000919342\n",
      "          vf_loss: 0.0004689619866743063\n",
      "    num_agent_steps_sampled: 316000\n",
      "    num_agent_steps_trained: 316000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "  iterations_since_restore: 316\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.65714285714286\n",
      "    ram_util_percent: 55.76785714285715\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06638243750387772\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.194656072413235\n",
      "    mean_inference_ms: 2.433814110283187\n",
      "    mean_raw_obs_processing_ms: 0.8872462665965729\n",
      "  time_since_restore: 7193.977028608322\n",
      "  time_this_iter_s: 19.500136137008667\n",
      "  time_total_s: 7193.977028608322\n",
      "  timers:\n",
      "    learn_throughput: 305.108\n",
      "    learn_time_ms: 3277.525\n",
      "    load_throughput: 23628.178\n",
      "    load_time_ms: 42.322\n",
      "    sample_throughput: 57.459\n",
      "    sample_time_ms: 17403.805\n",
      "    update_time_ms: 10.977\n",
      "  timestamp: 1631885531\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 316\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   316</td><td style=\"text-align: right;\">         7193.98</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 317000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-32-31\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 318\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6392770104938084\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014180509863822325\n",
      "          policy_loss: 0.04551607188251283\n",
      "          total_loss: 0.023548009991645812\n",
      "          vf_explained_var: 0.13509918749332428\n",
      "          vf_loss: 0.00033149601658806206\n",
      "    num_agent_steps_sampled: 317000\n",
      "    num_agent_steps_trained: 317000\n",
      "    num_steps_sampled: 317000\n",
      "    num_steps_trained: 317000\n",
      "  iterations_since_restore: 317\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.60357142857143\n",
      "    ram_util_percent: 55.800000000000004\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06639254739476091\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.18725323045804\n",
      "    mean_inference_ms: 2.4340867349968693\n",
      "    mean_raw_obs_processing_ms: 0.887038022077326\n",
      "  time_since_restore: 7213.601871728897\n",
      "  time_this_iter_s: 19.62484312057495\n",
      "  time_total_s: 7213.601871728897\n",
      "  timers:\n",
      "    learn_throughput: 304.211\n",
      "    learn_time_ms: 3287.19\n",
      "    load_throughput: 23890.796\n",
      "    load_time_ms: 41.857\n",
      "    sample_throughput: 57.966\n",
      "    sample_time_ms: 17251.371\n",
      "    update_time_ms: 9.295\n",
      "  timestamp: 1631885551\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 317000\n",
      "  training_iteration: 317\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   317</td><td style=\"text-align: right;\">          7213.6</td><td style=\"text-align: right;\">317000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 318000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-32-51\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 319\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.627832285563151\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011714868604749708\n",
      "          policy_loss: 0.01712141020430459\n",
      "          total_loss: -0.005386201292276382\n",
      "          vf_explained_var: -0.6045064926147461\n",
      "          vf_loss: 0.0003892079620426456\n",
      "    num_agent_steps_sampled: 318000\n",
      "    num_agent_steps_trained: 318000\n",
      "    num_steps_sampled: 318000\n",
      "    num_steps_trained: 318000\n",
      "  iterations_since_restore: 318\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.49642857142858\n",
      "    ram_util_percent: 55.84642857142858\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06640259062698468\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.179931197004567\n",
      "    mean_inference_ms: 2.4343558059536696\n",
      "    mean_raw_obs_processing_ms: 0.8868392520662496\n",
      "  time_since_restore: 7233.535268306732\n",
      "  time_this_iter_s: 19.933396577835083\n",
      "  time_total_s: 7233.535268306732\n",
      "  timers:\n",
      "    learn_throughput: 305.461\n",
      "    learn_time_ms: 3273.735\n",
      "    load_throughput: 23911.28\n",
      "    load_time_ms: 41.821\n",
      "    sample_throughput: 57.924\n",
      "    sample_time_ms: 17264.067\n",
      "    update_time_ms: 10.336\n",
      "  timestamp: 1631885571\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 318000\n",
      "  training_iteration: 318\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   318</td><td style=\"text-align: right;\">         7233.54</td><td style=\"text-align: right;\">318000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 319000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-33-09\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 320\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.649402101834615\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011255105400806344\n",
      "          policy_loss: -0.01251573268738058\n",
      "          total_loss: -0.03542601902607\n",
      "          vf_explained_var: 0.03595118969678879\n",
      "          vf_loss: 0.0003349406571487634\n",
      "    num_agent_steps_sampled: 319000\n",
      "    num_agent_steps_trained: 319000\n",
      "    num_steps_sampled: 319000\n",
      "    num_steps_trained: 319000\n",
      "  iterations_since_restore: 319\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.83703703703704\n",
      "    ram_util_percent: 55.951851851851856\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06641249803369542\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.172570271448848\n",
      "    mean_inference_ms: 2.4346213532677017\n",
      "    mean_raw_obs_processing_ms: 0.8866493484631991\n",
      "  time_since_restore: 7252.315553188324\n",
      "  time_this_iter_s: 18.780284881591797\n",
      "  time_total_s: 7252.315553188324\n",
      "  timers:\n",
      "    learn_throughput: 305.177\n",
      "    learn_time_ms: 3276.787\n",
      "    load_throughput: 23848.21\n",
      "    load_time_ms: 41.932\n",
      "    sample_throughput: 58.755\n",
      "    sample_time_ms: 17019.781\n",
      "    update_time_ms: 9.299\n",
      "  timestamp: 1631885589\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 319000\n",
      "  training_iteration: 319\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   319</td><td style=\"text-align: right;\">         7252.32</td><td style=\"text-align: right;\">319000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-33-29\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 321\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.647917111714681\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009798290413640389\n",
      "          policy_loss: 0.128355705510411\n",
      "          total_loss: 0.10495235657112466\n",
      "          vf_explained_var: 0.690070629119873\n",
      "          vf_loss: 0.00024753660576000885\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_agent_steps_trained: 320000\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "  iterations_since_restore: 320\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.04642857142856\n",
      "    ram_util_percent: 56.00357142857141\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06642238160009997\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.16524009857041\n",
      "    mean_inference_ms: 2.4348868597388122\n",
      "    mean_raw_obs_processing_ms: 0.8864690456331772\n",
      "  time_since_restore: 7271.70529294014\n",
      "  time_this_iter_s: 19.389739751815796\n",
      "  time_total_s: 7271.70529294014\n",
      "  timers:\n",
      "    learn_throughput: 305.444\n",
      "    learn_time_ms: 3273.923\n",
      "    load_throughput: 23623.374\n",
      "    load_time_ms: 42.331\n",
      "    sample_throughput: 59.184\n",
      "    sample_time_ms: 16896.343\n",
      "    update_time_ms: 9.302\n",
      "  timestamp: 1631885609\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 320\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   320</td><td style=\"text-align: right;\">         7271.71</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 321000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-33-49\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 322\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6822238948610093\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013639886780067122\n",
      "          policy_loss: 0.014979426604178217\n",
      "          total_loss: -0.007476299131910006\n",
      "          vf_explained_var: -0.09404674917459488\n",
      "          vf_loss: 0.00042935083380749955\n",
      "    num_agent_steps_sampled: 321000\n",
      "    num_agent_steps_trained: 321000\n",
      "    num_steps_sampled: 321000\n",
      "    num_steps_trained: 321000\n",
      "  iterations_since_restore: 321\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.9793103448276\n",
      "    ram_util_percent: 56.00344827586208\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06643214799066853\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.158015415913205\n",
      "    mean_inference_ms: 2.4351496312268033\n",
      "    mean_raw_obs_processing_ms: 0.8862982468857887\n",
      "  time_since_restore: 7291.906312704086\n",
      "  time_this_iter_s: 20.201019763946533\n",
      "  time_total_s: 7291.906312704086\n",
      "  timers:\n",
      "    learn_throughput: 303.222\n",
      "    learn_time_ms: 3297.918\n",
      "    load_throughput: 23588.578\n",
      "    load_time_ms: 42.393\n",
      "    sample_throughput: 59.678\n",
      "    sample_time_ms: 16756.586\n",
      "    update_time_ms: 9.53\n",
      "  timestamp: 1631885629\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 321000\n",
      "  training_iteration: 321\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   321</td><td style=\"text-align: right;\">         7291.91</td><td style=\"text-align: right;\">321000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 322000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-34-09\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 323\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.664602420065138\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006705674179633863\n",
      "          policy_loss: -0.04995034212867419\n",
      "          total_loss: -0.07441381609274281\n",
      "          vf_explained_var: -0.2908651530742645\n",
      "          vf_loss: 0.0002469519293703747\n",
      "    num_agent_steps_sampled: 322000\n",
      "    num_agent_steps_trained: 322000\n",
      "    num_steps_sampled: 322000\n",
      "    num_steps_trained: 322000\n",
      "  iterations_since_restore: 322\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.8892857142857\n",
      "    ram_util_percent: 55.96071428571428\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06644180305912711\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.150824475610023\n",
      "    mean_inference_ms: 2.4354091003533407\n",
      "    mean_raw_obs_processing_ms: 0.8861368350064427\n",
      "  time_since_restore: 7311.8289613723755\n",
      "  time_this_iter_s: 19.922648668289185\n",
      "  time_total_s: 7311.8289613723755\n",
      "  timers:\n",
      "    learn_throughput: 301.715\n",
      "    learn_time_ms: 3314.385\n",
      "    load_throughput: 23348.243\n",
      "    load_time_ms: 42.83\n",
      "    sample_throughput: 59.938\n",
      "    sample_time_ms: 16683.975\n",
      "    update_time_ms: 9.394\n",
      "  timestamp: 1631885649\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 322000\n",
      "  training_iteration: 322\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   322</td><td style=\"text-align: right;\">         7311.83</td><td style=\"text-align: right;\">322000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 323000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-34-27\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 324\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6984516196780737\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008099874828295286\n",
      "          policy_loss: -0.054858312548862566\n",
      "          total_loss: -0.07925954167213704\n",
      "          vf_explained_var: -0.6145387887954712\n",
      "          vf_loss: 0.0002452529770178242\n",
      "    num_agent_steps_sampled: 323000\n",
      "    num_agent_steps_trained: 323000\n",
      "    num_steps_sampled: 323000\n",
      "    num_steps_trained: 323000\n",
      "  iterations_since_restore: 323\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.43846153846154\n",
      "    ram_util_percent: 55.876923076923084\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06645133180570846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.143604763956816\n",
      "    mean_inference_ms: 2.43566243959066\n",
      "    mean_raw_obs_processing_ms: 0.8859838970941132\n",
      "  time_since_restore: 7329.735572099686\n",
      "  time_this_iter_s: 17.90661072731018\n",
      "  time_total_s: 7329.735572099686\n",
      "  timers:\n",
      "    learn_throughput: 301.994\n",
      "    learn_time_ms: 3311.321\n",
      "    load_throughput: 22817.886\n",
      "    load_time_ms: 43.825\n",
      "    sample_throughput: 61.093\n",
      "    sample_time_ms: 16368.363\n",
      "    update_time_ms: 8.696\n",
      "  timestamp: 1631885667\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 323000\n",
      "  training_iteration: 323\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   323</td><td style=\"text-align: right;\">         7329.74</td><td style=\"text-align: right;\">323000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 324000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-34-45\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 325\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7145500156614517\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00916484364463014\n",
      "          policy_loss: -0.0075305701181706455\n",
      "          total_loss: -0.0317483983726965\n",
      "          vf_explained_var: -0.6218150854110718\n",
      "          vf_loss: 0.00028223175506557\n",
      "    num_agent_steps_sampled: 324000\n",
      "    num_agent_steps_trained: 324000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "  iterations_since_restore: 324\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.16153846153847\n",
      "    ram_util_percent: 55.800000000000004\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06646074730962309\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.136247785929417\n",
      "    mean_inference_ms: 2.435910299934585\n",
      "    mean_raw_obs_processing_ms: 0.8858398884049768\n",
      "  time_since_restore: 7348.210520029068\n",
      "  time_this_iter_s: 18.474947929382324\n",
      "  time_total_s: 7348.210520029068\n",
      "  timers:\n",
      "    learn_throughput: 302.523\n",
      "    learn_time_ms: 3305.537\n",
      "    load_throughput: 23828.809\n",
      "    load_time_ms: 41.966\n",
      "    sample_throughput: 63.026\n",
      "    sample_time_ms: 15866.396\n",
      "    update_time_ms: 8.773\n",
      "  timestamp: 1631885685\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 324\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   324</td><td style=\"text-align: right;\">         7348.21</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 325000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-35-04\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 326\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.670909153090583\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0060912616393631915\n",
      "          policy_loss: -0.009594168343270818\n",
      "          total_loss: -0.03388900289105044\n",
      "          vf_explained_var: -0.5336843132972717\n",
      "          vf_loss: 0.000656010420379971\n",
      "    num_agent_steps_sampled: 325000\n",
      "    num_agent_steps_trained: 325000\n",
      "    num_steps_sampled: 325000\n",
      "    num_steps_trained: 325000\n",
      "  iterations_since_restore: 325\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.0\n",
      "    ram_util_percent: 55.85357142857145\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06647012147920332\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.128870806206795\n",
      "    mean_inference_ms: 2.436154301436949\n",
      "    mean_raw_obs_processing_ms: 0.8857040450498992\n",
      "  time_since_restore: 7367.254451036453\n",
      "  time_this_iter_s: 19.043931007385254\n",
      "  time_total_s: 7367.254451036453\n",
      "  timers:\n",
      "    learn_throughput: 300.47\n",
      "    learn_time_ms: 3328.118\n",
      "    load_throughput: 23387.391\n",
      "    load_time_ms: 42.758\n",
      "    sample_throughput: 62.942\n",
      "    sample_time_ms: 15887.744\n",
      "    update_time_ms: 8.793\n",
      "  timestamp: 1631885704\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 325000\n",
      "  training_iteration: 325\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   325</td><td style=\"text-align: right;\">         7367.25</td><td style=\"text-align: right;\">325000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 326000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-35-24\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 327\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7275604910320705\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007098065238040855\n",
      "          policy_loss: -0.08070705276396539\n",
      "          total_loss: -0.10584342761172189\n",
      "          vf_explained_var: -0.5284174084663391\n",
      "          vf_loss: 9.036700936727963e-05\n",
      "    num_agent_steps_sampled: 326000\n",
      "    num_agent_steps_trained: 326000\n",
      "    num_steps_sampled: 326000\n",
      "    num_steps_trained: 326000\n",
      "  iterations_since_restore: 326\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.02592592592593\n",
      "    ram_util_percent: 56.088888888888896\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06647942231276895\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.12154252705686\n",
      "    mean_inference_ms: 2.4363951940389432\n",
      "    mean_raw_obs_processing_ms: 0.8855750211163097\n",
      "  time_since_restore: 7386.343120574951\n",
      "  time_this_iter_s: 19.088669538497925\n",
      "  time_total_s: 7386.343120574951\n",
      "  timers:\n",
      "    learn_throughput: 298.169\n",
      "    learn_time_ms: 3353.802\n",
      "    load_throughput: 25275.479\n",
      "    load_time_ms: 39.564\n",
      "    sample_throughput: 63.188\n",
      "    sample_time_ms: 15825.691\n",
      "    update_time_ms: 8.611\n",
      "  timestamp: 1631885724\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 326000\n",
      "  training_iteration: 326\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   326</td><td style=\"text-align: right;\">         7386.34</td><td style=\"text-align: right;\">326000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 327000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-35-41\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 328\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6853039026260377\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010833132522216557\n",
      "          policy_loss: -0.07705482666691145\n",
      "          total_loss: -0.10067113836606344\n",
      "          vf_explained_var: -0.36802974343299866\n",
      "          vf_loss: 0.00010973540863435321\n",
      "    num_agent_steps_sampled: 327000\n",
      "    num_agent_steps_trained: 327000\n",
      "    num_steps_sampled: 327000\n",
      "    num_steps_trained: 327000\n",
      "  iterations_since_restore: 327\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.196\n",
      "    ram_util_percent: 56.071999999999996\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06648858494430732\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.114096470811006\n",
      "    mean_inference_ms: 2.436627630112277\n",
      "    mean_raw_obs_processing_ms: 0.8854547442431769\n",
      "  time_since_restore: 7403.733864068985\n",
      "  time_this_iter_s: 17.390743494033813\n",
      "  time_total_s: 7403.733864068985\n",
      "  timers:\n",
      "    learn_throughput: 300.894\n",
      "    learn_time_ms: 3323.427\n",
      "    load_throughput: 24657.857\n",
      "    load_time_ms: 40.555\n",
      "    sample_throughput: 63.971\n",
      "    sample_time_ms: 15632.055\n",
      "    update_time_ms: 8.552\n",
      "  timestamp: 1631885741\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 327000\n",
      "  training_iteration: 327\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   327</td><td style=\"text-align: right;\">         7403.73</td><td style=\"text-align: right;\">327000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-36-00\n",
      "  done: false\n",
      "  episode_len_mean: 995.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 329\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.676465378867255\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010095163316262696\n",
      "          policy_loss: -0.03114379263586468\n",
      "          total_loss: -0.05480850860476494\n",
      "          vf_explained_var: 0.16705097258090973\n",
      "          vf_loss: 0.00018596170450564388\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "  iterations_since_restore: 328\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.49999999999999\n",
      "    ram_util_percent: 55.89230769230768\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06649766762705918\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.106576866267986\n",
      "    mean_inference_ms: 2.436856007117863\n",
      "    mean_raw_obs_processing_ms: 0.8853440270068408\n",
      "  time_since_restore: 7422.526319742203\n",
      "  time_this_iter_s: 18.792455673217773\n",
      "  time_total_s: 7422.526319742203\n",
      "  timers:\n",
      "    learn_throughput: 300.071\n",
      "    learn_time_ms: 3332.543\n",
      "    load_throughput: 24750.705\n",
      "    load_time_ms: 40.403\n",
      "    sample_throughput: 64.473\n",
      "    sample_time_ms: 15510.279\n",
      "    update_time_ms: 7.679\n",
      "  timestamp: 1631885760\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 328\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   328</td><td style=\"text-align: right;\">         7422.53</td><td style=\"text-align: right;\">328000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 329000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-36-36\n",
      "  done: false\n",
      "  episode_len_mean: 994.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 330\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.721941203541226\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0080743008743707\n",
      "          policy_loss: -0.03015322627292739\n",
      "          total_loss: -0.05484855638609992\n",
      "          vf_explained_var: -0.126287043094635\n",
      "          vf_loss: 0.0001934258487431685\n",
      "    num_agent_steps_sampled: 329000\n",
      "    num_agent_steps_trained: 329000\n",
      "    num_steps_sampled: 329000\n",
      "    num_steps_trained: 329000\n",
      "  iterations_since_restore: 329\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.42115384615384\n",
      "    ram_util_percent: 56.01538461538462\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06650668872856333\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.099036996914403\n",
      "    mean_inference_ms: 2.437078808266375\n",
      "    mean_raw_obs_processing_ms: 0.8857874398724109\n",
      "  time_since_restore: 7458.67424082756\n",
      "  time_this_iter_s: 36.147921085357666\n",
      "  time_total_s: 7458.67424082756\n",
      "  timers:\n",
      "    learn_throughput: 299.296\n",
      "    learn_time_ms: 3341.172\n",
      "    load_throughput: 24438.372\n",
      "    load_time_ms: 40.919\n",
      "    sample_throughput: 58.016\n",
      "    sample_time_ms: 17236.691\n",
      "    update_time_ms: 8.416\n",
      "  timestamp: 1631885796\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 329000\n",
      "  training_iteration: 329\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   329</td><td style=\"text-align: right;\">         7458.67</td><td style=\"text-align: right;\">329000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 330000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-36-57\n",
      "  done: false\n",
      "  episode_len_mean: 994.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 331\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6860768265194364\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008128431784233668\n",
      "          policy_loss: -0.05347922829290231\n",
      "          total_loss: -0.07776551035543283\n",
      "          vf_explained_var: -0.2141050398349762\n",
      "          vf_loss: 0.00022820530513298663\n",
      "    num_agent_steps_sampled: 330000\n",
      "    num_agent_steps_trained: 330000\n",
      "    num_steps_sampled: 330000\n",
      "    num_steps_trained: 330000\n",
      "  iterations_since_restore: 330\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.21666666666667\n",
      "    ram_util_percent: 55.97\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06651557369700827\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.09158038867762\n",
      "    mean_inference_ms: 2.4373009241386394\n",
      "    mean_raw_obs_processing_ms: 0.8862374628767941\n",
      "  time_since_restore: 7479.785315513611\n",
      "  time_this_iter_s: 21.111074686050415\n",
      "  time_total_s: 7479.785315513611\n",
      "  timers:\n",
      "    learn_throughput: 298.818\n",
      "    learn_time_ms: 3346.514\n",
      "    load_throughput: 24342.14\n",
      "    load_time_ms: 41.081\n",
      "    sample_throughput: 57.464\n",
      "    sample_time_ms: 17402.258\n",
      "    update_time_ms: 8.912\n",
      "  timestamp: 1631885817\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 330000\n",
      "  training_iteration: 330\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   330</td><td style=\"text-align: right;\">         7479.79</td><td style=\"text-align: right;\">330000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 331000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-37-18\n",
      "  done: false\n",
      "  episode_len_mean: 994.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 332\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6899021201663547\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013832345930857097\n",
      "          policy_loss: -0.0643574368622568\n",
      "          total_loss: -0.08668933854334884\n",
      "          vf_explained_var: -0.4183999300003052\n",
      "          vf_loss: 0.0005744021594687688\n",
      "    num_agent_steps_sampled: 331000\n",
      "    num_agent_steps_trained: 331000\n",
      "    num_steps_sampled: 331000\n",
      "    num_steps_trained: 331000\n",
      "  iterations_since_restore: 331\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.83103448275861\n",
      "    ram_util_percent: 56.048275862068955\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06652430824277805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.08404925018133\n",
      "    mean_inference_ms: 2.437517931063404\n",
      "    mean_raw_obs_processing_ms: 0.886694393662747\n",
      "  time_since_restore: 7500.03226852417\n",
      "  time_this_iter_s: 20.246953010559082\n",
      "  time_total_s: 7500.03226852417\n",
      "  timers:\n",
      "    learn_throughput: 299.484\n",
      "    learn_time_ms: 3339.08\n",
      "    load_throughput: 23934.053\n",
      "    load_time_ms: 41.781\n",
      "    sample_throughput: 57.425\n",
      "    sample_time_ms: 17414.055\n",
      "    update_time_ms: 8.786\n",
      "  timestamp: 1631885838\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 331000\n",
      "  training_iteration: 331\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   331</td><td style=\"text-align: right;\">         7500.03</td><td style=\"text-align: right;\">331000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 332000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-37-36\n",
      "  done: false\n",
      "  episode_len_mean: 994.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 333\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7192030005984837\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006572312706226195\n",
      "          policy_loss: -0.026911605811781352\n",
      "          total_loss: -0.05192607587410344\n",
      "          vf_explained_var: -0.4737413227558136\n",
      "          vf_loss: 0.00028045787253682243\n",
      "    num_agent_steps_sampled: 332000\n",
      "    num_agent_steps_trained: 332000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "  iterations_since_restore: 332\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.77307692307694\n",
      "    ram_util_percent: 56.14615384615385\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06653287145419297\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.07653644769661\n",
      "    mean_inference_ms: 2.4377331991550095\n",
      "    mean_raw_obs_processing_ms: 0.8871581783308501\n",
      "  time_since_restore: 7518.222243309021\n",
      "  time_this_iter_s: 18.189974784851074\n",
      "  time_total_s: 7518.222243309021\n",
      "  timers:\n",
      "    learn_throughput: 301.288\n",
      "    learn_time_ms: 3319.079\n",
      "    load_throughput: 23246.169\n",
      "    load_time_ms: 43.018\n",
      "    sample_throughput: 57.938\n",
      "    sample_time_ms: 17259.763\n",
      "    update_time_ms: 8.648\n",
      "  timestamp: 1631885856\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 332\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   332</td><td style=\"text-align: right;\">         7518.22</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 333000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-37-54\n",
      "  done: false\n",
      "  episode_len_mean: 994.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 334\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7130660984251236\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008168578612684187\n",
      "          policy_loss: 0.0004251049210627874\n",
      "          total_loss: -0.02404476830528842\n",
      "          vf_explained_var: -0.13306017220020294\n",
      "          vf_loss: 0.00030292009290254404\n",
      "    num_agent_steps_sampled: 333000\n",
      "    num_agent_steps_trained: 333000\n",
      "    num_steps_sampled: 333000\n",
      "    num_steps_trained: 333000\n",
      "  iterations_since_restore: 333\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.42962962962963\n",
      "    ram_util_percent: 56.099999999999994\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06654152539667753\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.069082655491187\n",
      "    mean_inference_ms: 2.4379492770556723\n",
      "    mean_raw_obs_processing_ms: 0.8876294074112491\n",
      "  time_since_restore: 7536.683943748474\n",
      "  time_this_iter_s: 18.461700439453125\n",
      "  time_total_s: 7536.683943748474\n",
      "  timers:\n",
      "    learn_throughput: 300.142\n",
      "    learn_time_ms: 3331.751\n",
      "    load_throughput: 22602.773\n",
      "    load_time_ms: 44.242\n",
      "    sample_throughput: 57.798\n",
      "    sample_time_ms: 17301.508\n",
      "    update_time_ms: 8.554\n",
      "  timestamp: 1631885874\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 333000\n",
      "  training_iteration: 333\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   333</td><td style=\"text-align: right;\">         7536.68</td><td style=\"text-align: right;\">333000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 334000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-38-15\n",
      "  done: false\n",
      "  episode_len_mean: 994.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 335\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7187144464916653\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005068223096763644\n",
      "          policy_loss: -0.017105157756143147\n",
      "          total_loss: -0.04236310794949531\n",
      "          vf_explained_var: -0.4819315969944\n",
      "          vf_loss: 0.00046624623533817714\n",
      "    num_agent_steps_sampled: 334000\n",
      "    num_agent_steps_trained: 334000\n",
      "    num_steps_sampled: 334000\n",
      "    num_steps_trained: 334000\n",
      "  iterations_since_restore: 334\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.99310344827585\n",
      "    ram_util_percent: 56.14482758620689\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06655026411401042\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.061715145241404\n",
      "    mean_inference_ms: 2.4381657147805025\n",
      "    mean_raw_obs_processing_ms: 0.8881071638221367\n",
      "  time_since_restore: 7557.463244199753\n",
      "  time_this_iter_s: 20.779300451278687\n",
      "  time_total_s: 7557.463244199753\n",
      "  timers:\n",
      "    learn_throughput: 298.241\n",
      "    learn_time_ms: 3352.992\n",
      "    load_throughput: 21558.438\n",
      "    load_time_ms: 46.386\n",
      "    sample_throughput: 57.114\n",
      "    sample_time_ms: 17508.694\n",
      "    update_time_ms: 8.334\n",
      "  timestamp: 1631885895\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 334000\n",
      "  training_iteration: 334\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   334</td><td style=\"text-align: right;\">         7557.46</td><td style=\"text-align: right;\">334000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 335000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-38-33\n",
      "  done: false\n",
      "  episode_len_mean: 994.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 336\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6532060384750364\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00913983088617999\n",
      "          policy_loss: -0.07659812370936075\n",
      "          total_loss: -0.10009994871086544\n",
      "          vf_explained_var: -0.42734771966934204\n",
      "          vf_loss: 0.00039201513593272667\n",
      "    num_agent_steps_sampled: 335000\n",
      "    num_agent_steps_trained: 335000\n",
      "    num_steps_sampled: 335000\n",
      "    num_steps_trained: 335000\n",
      "  iterations_since_restore: 335\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.33461538461538\n",
      "    ram_util_percent: 56.21153846153846\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06655887488321156\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.05433286285746\n",
      "    mean_inference_ms: 2.4383789996273073\n",
      "    mean_raw_obs_processing_ms: 0.888591706386009\n",
      "  time_since_restore: 7575.730330705643\n",
      "  time_this_iter_s: 18.267086505889893\n",
      "  time_total_s: 7575.730330705643\n",
      "  timers:\n",
      "    learn_throughput: 299.227\n",
      "    learn_time_ms: 3341.947\n",
      "    load_throughput: 22607.549\n",
      "    load_time_ms: 44.233\n",
      "    sample_throughput: 57.326\n",
      "    sample_time_ms: 17444.013\n",
      "    update_time_ms: 8.342\n",
      "  timestamp: 1631885913\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 335000\n",
      "  training_iteration: 335\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   335</td><td style=\"text-align: right;\">         7575.73</td><td style=\"text-align: right;\">335000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-38-54\n",
      "  done: false\n",
      "  episode_len_mean: 994.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 337\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6525537941190933\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007584276056918239\n",
      "          policy_loss: -0.042454076806704204\n",
      "          total_loss: -0.0659783790508906\n",
      "          vf_explained_var: -0.7123852968215942\n",
      "          vf_loss: 0.0008120273328434753\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_agent_steps_trained: 336000\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "  iterations_since_restore: 336\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.92333333333333\n",
      "    ram_util_percent: 56.22333333333334\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06656742060512535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.04699148139595\n",
      "    mean_inference_ms: 2.4385865967592504\n",
      "    mean_raw_obs_processing_ms: 0.8890826809094544\n",
      "  time_since_restore: 7596.7214069366455\n",
      "  time_this_iter_s: 20.991076231002808\n",
      "  time_total_s: 7596.7214069366455\n",
      "  timers:\n",
      "    learn_throughput: 301.292\n",
      "    learn_time_ms: 3319.039\n",
      "    load_throughput: 21841.659\n",
      "    load_time_ms: 45.784\n",
      "    sample_throughput: 56.639\n",
      "    sample_time_ms: 17655.769\n",
      "    update_time_ms: 8.157\n",
      "  timestamp: 1631885934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 336\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   336</td><td style=\"text-align: right;\">         7596.72</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 337000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-39-13\n",
      "  done: false\n",
      "  episode_len_mean: 994.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 338\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.74482438299391\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006225130138213271\n",
      "          policy_loss: -0.004856618607623709\n",
      "          total_loss: -0.030186659263239966\n",
      "          vf_explained_var: -0.552081823348999\n",
      "          vf_loss: 0.00032131355809623426\n",
      "    num_agent_steps_sampled: 337000\n",
      "    num_agent_steps_trained: 337000\n",
      "    num_steps_sampled: 337000\n",
      "    num_steps_trained: 337000\n",
      "  iterations_since_restore: 337\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.49259259259259\n",
      "    ram_util_percent: 56.25185185185185\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06657584228711685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.039711295875893\n",
      "    mean_inference_ms: 2.4387897115678547\n",
      "    mean_raw_obs_processing_ms: 0.8895803614499591\n",
      "  time_since_restore: 7615.512777805328\n",
      "  time_this_iter_s: 18.79137086868286\n",
      "  time_total_s: 7615.512777805328\n",
      "  timers:\n",
      "    learn_throughput: 299.399\n",
      "    learn_time_ms: 3340.021\n",
      "    load_throughput: 21740.526\n",
      "    load_time_ms: 45.997\n",
      "    sample_throughput: 56.265\n",
      "    sample_time_ms: 17772.988\n",
      "    update_time_ms: 8.723\n",
      "  timestamp: 1631885953\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 337000\n",
      "  training_iteration: 337\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   337</td><td style=\"text-align: right;\">         7615.51</td><td style=\"text-align: right;\">337000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 338000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-39-32\n",
      "  done: false\n",
      "  episode_len_mean: 994.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 339\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6095751418007747\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008488776476751742\n",
      "          policy_loss: -0.04109926610771153\n",
      "          total_loss: -0.06429805560037494\n",
      "          vf_explained_var: -0.15739214420318604\n",
      "          vf_loss: 0.0004466714235907906\n",
      "    num_agent_steps_sampled: 338000\n",
      "    num_agent_steps_trained: 338000\n",
      "    num_steps_sampled: 338000\n",
      "    num_steps_trained: 338000\n",
      "  iterations_since_restore: 338\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.99285714285715\n",
      "    ram_util_percent: 56.11785714285714\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06658430441979246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.03241405542095\n",
      "    mean_inference_ms: 2.4389926976601455\n",
      "    mean_raw_obs_processing_ms: 0.8893225285478276\n",
      "  time_since_restore: 7634.739299297333\n",
      "  time_this_iter_s: 19.226521492004395\n",
      "  time_total_s: 7634.739299297333\n",
      "  timers:\n",
      "    learn_throughput: 300.427\n",
      "    learn_time_ms: 3328.599\n",
      "    load_throughput: 22136.474\n",
      "    load_time_ms: 45.174\n",
      "    sample_throughput: 56.088\n",
      "    sample_time_ms: 17829.034\n",
      "    update_time_ms: 8.624\n",
      "  timestamp: 1631885972\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 338000\n",
      "  training_iteration: 338\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   338</td><td style=\"text-align: right;\">         7634.74</td><td style=\"text-align: right;\">338000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 339000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-39-54\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 340\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6967735211054484\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006022063673556449\n",
      "          policy_loss: -0.05559521358874109\n",
      "          total_loss: -0.08056984610027737\n",
      "          vf_explained_var: -0.4697314500808716\n",
      "          vf_loss: 0.00025483121989711637\n",
      "    num_agent_steps_sampled: 339000\n",
      "    num_agent_steps_trained: 339000\n",
      "    num_steps_sampled: 339000\n",
      "    num_steps_trained: 339000\n",
      "  iterations_since_restore: 339\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.44333333333333\n",
      "    ram_util_percent: 56.23666666666667\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06659275000021343\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.02509430927863\n",
      "    mean_inference_ms: 2.4391954500371837\n",
      "    mean_raw_obs_processing_ms: 0.8890452678399758\n",
      "  time_since_restore: 7655.885385751724\n",
      "  time_this_iter_s: 21.14608645439148\n",
      "  time_total_s: 7655.885385751724\n",
      "  timers:\n",
      "    learn_throughput: 300.711\n",
      "    learn_time_ms: 3325.455\n",
      "    load_throughput: 23305.521\n",
      "    load_time_ms: 42.908\n",
      "    sample_throughput: 61.217\n",
      "    sample_time_ms: 16335.315\n",
      "    update_time_ms: 7.482\n",
      "  timestamp: 1631885994\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 339000\n",
      "  training_iteration: 339\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   339</td><td style=\"text-align: right;\">         7655.89</td><td style=\"text-align: right;\">339000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 340000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-40-14\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 341\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7266912698745727\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007102061331480137\n",
      "          policy_loss: -0.012686932273209096\n",
      "          total_loss: -0.03760014039774736\n",
      "          vf_explained_var: -0.4610620141029358\n",
      "          vf_loss: 0.0003036857978966307\n",
      "    num_agent_steps_sampled: 340000\n",
      "    num_agent_steps_trained: 340000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "  iterations_since_restore: 340\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.77931034482758\n",
      "    ram_util_percent: 56.18275862068965\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06660109153018688\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.017787902274996\n",
      "    mean_inference_ms: 2.439395233265031\n",
      "    mean_raw_obs_processing_ms: 0.888777340307048\n",
      "  time_since_restore: 7675.995978116989\n",
      "  time_this_iter_s: 20.110592365264893\n",
      "  time_total_s: 7675.995978116989\n",
      "  timers:\n",
      "    learn_throughput: 302.527\n",
      "    learn_time_ms: 3305.49\n",
      "    load_throughput: 23787.28\n",
      "    load_time_ms: 42.039\n",
      "    sample_throughput: 61.511\n",
      "    sample_time_ms: 16257.181\n",
      "    update_time_ms: 6.829\n",
      "  timestamp: 1631886014\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 340\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   340</td><td style=\"text-align: right;\">            7676</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 341000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-40-32\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 342\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6579215314653184\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008395834049997969\n",
      "          policy_loss: -0.05904081579711702\n",
      "          total_loss: -0.0829032953414652\n",
      "          vf_explained_var: -0.48606160283088684\n",
      "          vf_loss: 0.00029327121384186387\n",
      "    num_agent_steps_sampled: 341000\n",
      "    num_agent_steps_trained: 341000\n",
      "    num_steps_sampled: 341000\n",
      "    num_steps_trained: 341000\n",
      "  iterations_since_restore: 341\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.56153846153846\n",
      "    ram_util_percent: 56.17307692307692\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06660942115126531\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.010411556123792\n",
      "    mean_inference_ms: 2.4395941876877503\n",
      "    mean_raw_obs_processing_ms: 0.8885189331663613\n",
      "  time_since_restore: 7694.345447301865\n",
      "  time_this_iter_s: 18.34946918487549\n",
      "  time_total_s: 7694.345447301865\n",
      "  timers:\n",
      "    learn_throughput: 301.75\n",
      "    learn_time_ms: 3314.001\n",
      "    load_throughput: 23309.757\n",
      "    load_time_ms: 42.9\n",
      "    sample_throughput: 62.275\n",
      "    sample_time_ms: 16057.927\n",
      "    update_time_ms: 6.859\n",
      "  timestamp: 1631886032\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 341000\n",
      "  training_iteration: 341\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   341</td><td style=\"text-align: right;\">         7694.35</td><td style=\"text-align: right;\">341000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 342000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-40-52\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 343\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1969785922103457\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0068758533798056395\n",
      "          policy_loss: 0.0857872744401296\n",
      "          total_loss: 0.0735799522863494\n",
      "          vf_explained_var: -0.464179664850235\n",
      "          vf_loss: 0.007777745729112616\n",
      "    num_agent_steps_sampled: 342000\n",
      "    num_agent_steps_trained: 342000\n",
      "    num_steps_sampled: 342000\n",
      "    num_steps_trained: 342000\n",
      "  iterations_since_restore: 342\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.63103448275861\n",
      "    ram_util_percent: 56.141379310344824\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06661769783576249\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.002989857100047\n",
      "    mean_inference_ms: 2.439789248548053\n",
      "    mean_raw_obs_processing_ms: 0.8882715179256847\n",
      "  time_since_restore: 7714.512392759323\n",
      "  time_this_iter_s: 20.166945457458496\n",
      "  time_total_s: 7714.512392759323\n",
      "  timers:\n",
      "    learn_throughput: 299.874\n",
      "    learn_time_ms: 3334.738\n",
      "    load_throughput: 23348.867\n",
      "    load_time_ms: 42.829\n",
      "    sample_throughput: 61.596\n",
      "    sample_time_ms: 16234.796\n",
      "    update_time_ms: 6.973\n",
      "  timestamp: 1631886052\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 342000\n",
      "  training_iteration: 342\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   342</td><td style=\"text-align: right;\">         7714.51</td><td style=\"text-align: right;\">342000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 343000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-41-13\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 344\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5027475449774\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010281908209682555\n",
      "          policy_loss: 0.1568133345908589\n",
      "          total_loss: 0.1356921030415429\n",
      "          vf_explained_var: -0.5464722514152527\n",
      "          vf_loss: 0.0009383633782476863\n",
      "    num_agent_steps_sampled: 343000\n",
      "    num_agent_steps_trained: 343000\n",
      "    num_steps_sampled: 343000\n",
      "    num_steps_trained: 343000\n",
      "  iterations_since_restore: 343\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.56551724137931\n",
      "    ram_util_percent: 56.134482758620685\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06662584768554368\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.995669526661871\n",
      "    mean_inference_ms: 2.4399853572128642\n",
      "    mean_raw_obs_processing_ms: 0.8880336755570083\n",
      "  time_since_restore: 7735.467301368713\n",
      "  time_this_iter_s: 20.95490860939026\n",
      "  time_total_s: 7735.467301368713\n",
      "  timers:\n",
      "    learn_throughput: 299.265\n",
      "    learn_time_ms: 3341.52\n",
      "    load_throughput: 23102.889\n",
      "    load_time_ms: 43.285\n",
      "    sample_throughput: 60.694\n",
      "    sample_time_ms: 16476.185\n",
      "    update_time_ms: 7.03\n",
      "  timestamp: 1631886073\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 343000\n",
      "  training_iteration: 343\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   343</td><td style=\"text-align: right;\">         7735.47</td><td style=\"text-align: right;\">343000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-41-34\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 345\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.524704894754622\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0092665881465344\n",
      "          policy_loss: -0.0686433658003807\n",
      "          total_loss: -0.09087883217467202\n",
      "          vf_explained_var: -0.37660691142082214\n",
      "          vf_loss: 0.00033677785647038644\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_agent_steps_trained: 344000\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "  iterations_since_restore: 344\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.33333333333333\n",
      "    ram_util_percent: 56.30333333333333\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06663373033888091\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.988427660832063\n",
      "    mean_inference_ms: 2.440184381764869\n",
      "    mean_raw_obs_processing_ms: 0.887805211035022\n",
      "  time_since_restore: 7755.802508115768\n",
      "  time_this_iter_s: 20.335206747055054\n",
      "  time_total_s: 7755.802508115768\n",
      "  timers:\n",
      "    learn_throughput: 299.353\n",
      "    learn_time_ms: 3340.533\n",
      "    load_throughput: 24003.443\n",
      "    load_time_ms: 41.661\n",
      "    sample_throughput: 60.849\n",
      "    sample_time_ms: 16434.167\n",
      "    update_time_ms: 6.929\n",
      "  timestamp: 1631886094\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 344\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   344</td><td style=\"text-align: right;\">          7755.8</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 345000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-41-52\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 346\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.429302243391673\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011110326919637595\n",
      "          policy_loss: -0.09010871069298851\n",
      "          total_loss: -0.11078161117103365\n",
      "          vf_explained_var: -0.2532065212726593\n",
      "          vf_loss: 0.00041312296760022746\n",
      "    num_agent_steps_sampled: 345000\n",
      "    num_agent_steps_trained: 345000\n",
      "    num_steps_sampled: 345000\n",
      "    num_steps_trained: 345000\n",
      "  iterations_since_restore: 345\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.53076923076922\n",
      "    ram_util_percent: 56.28076923076923\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06664145593864813\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.98109114213454\n",
      "    mean_inference_ms: 2.44038248369488\n",
      "    mean_raw_obs_processing_ms: 0.8875844223228679\n",
      "  time_since_restore: 7774.065911769867\n",
      "  time_this_iter_s: 18.26340365409851\n",
      "  time_total_s: 7774.065911769867\n",
      "  timers:\n",
      "    learn_throughput: 299.168\n",
      "    learn_time_ms: 3342.602\n",
      "    load_throughput: 23827.441\n",
      "    load_time_ms: 41.968\n",
      "    sample_throughput: 60.863\n",
      "    sample_time_ms: 16430.293\n",
      "    update_time_ms: 7.562\n",
      "  timestamp: 1631886112\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 345000\n",
      "  training_iteration: 345\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   345</td><td style=\"text-align: right;\">         7774.07</td><td style=\"text-align: right;\">345000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 346000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-42-11\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 347\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6006974299748737\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006052110022783649\n",
      "          policy_loss: -0.042362586905558906\n",
      "          total_loss: -0.06636353203405937\n",
      "          vf_explained_var: -0.8779569864273071\n",
      "          vf_loss: 0.00025908021408819576\n",
      "    num_agent_steps_sampled: 346000\n",
      "    num_agent_steps_trained: 346000\n",
      "    num_steps_sampled: 346000\n",
      "    num_steps_trained: 346000\n",
      "  iterations_since_restore: 346\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.36296296296297\n",
      "    ram_util_percent: 56.088888888888896\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06664930342070229\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.97384964750677\n",
      "    mean_inference_ms: 2.4405838196229936\n",
      "    mean_raw_obs_processing_ms: 0.8873717124637928\n",
      "  time_since_restore: 7793.08993268013\n",
      "  time_this_iter_s: 19.02402091026306\n",
      "  time_total_s: 7793.08993268013\n",
      "  timers:\n",
      "    learn_throughput: 300.365\n",
      "    learn_time_ms: 3329.278\n",
      "    load_throughput: 23542.674\n",
      "    load_time_ms: 42.476\n",
      "    sample_throughput: 61.551\n",
      "    sample_time_ms: 16246.7\n",
      "    update_time_ms: 7.273\n",
      "  timestamp: 1631886131\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 346000\n",
      "  training_iteration: 346\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   346</td><td style=\"text-align: right;\">         7793.09</td><td style=\"text-align: right;\">346000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 347000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-42-31\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 348\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5803742912080554\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01045361353186001\n",
      "          policy_loss: -0.02661841654529174\n",
      "          total_loss: -0.04907266307208273\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00033205220495599656\n",
      "    num_agent_steps_sampled: 347000\n",
      "    num_agent_steps_trained: 347000\n",
      "    num_steps_sampled: 347000\n",
      "    num_steps_trained: 347000\n",
      "  iterations_since_restore: 347\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.11071428571428\n",
      "    ram_util_percent: 56.10357142857142\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0666573552442062\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.966775633980504\n",
      "    mean_inference_ms: 2.440791941405301\n",
      "    mean_raw_obs_processing_ms: 0.8871681859001519\n",
      "  time_since_restore: 7813.122152090073\n",
      "  time_this_iter_s: 20.032219409942627\n",
      "  time_total_s: 7813.122152090073\n",
      "  timers:\n",
      "    learn_throughput: 300.87\n",
      "    learn_time_ms: 3323.69\n",
      "    load_throughput: 23564.379\n",
      "    load_time_ms: 42.437\n",
      "    sample_throughput: 61.06\n",
      "    sample_time_ms: 16377.43\n",
      "    update_time_ms: 6.94\n",
      "  timestamp: 1631886151\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 347000\n",
      "  training_iteration: 347\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   347</td><td style=\"text-align: right;\">         7813.12</td><td style=\"text-align: right;\">347000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 348000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-42-50\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 349\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.424767699506548\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007997640834481718\n",
      "          policy_loss: -0.011387708245052232\n",
      "          total_loss: -0.03317649604545699\n",
      "          vf_explained_var: -0.34963351488113403\n",
      "          vf_loss: 0.00015036388613225425\n",
      "    num_agent_steps_sampled: 348000\n",
      "    num_agent_steps_trained: 348000\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "  iterations_since_restore: 348\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.31111111111112\n",
      "    ram_util_percent: 56.04444444444444\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06666546542864232\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.95976057083707\n",
      "    mean_inference_ms: 2.4410017682266836\n",
      "    mean_raw_obs_processing_ms: 0.8869733992959682\n",
      "  time_since_restore: 7831.826570034027\n",
      "  time_this_iter_s: 18.704417943954468\n",
      "  time_total_s: 7831.826570034027\n",
      "  timers:\n",
      "    learn_throughput: 299.878\n",
      "    learn_time_ms: 3334.694\n",
      "    load_throughput: 25068.428\n",
      "    load_time_ms: 39.891\n",
      "    sample_throughput: 61.286\n",
      "    sample_time_ms: 16316.81\n",
      "    update_time_ms: 6.921\n",
      "  timestamp: 1631886170\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 348\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   348</td><td style=\"text-align: right;\">         7831.83</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 349000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-43-09\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 350\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.409401423401303\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006837070894781761\n",
      "          policy_loss: -0.07096812008983559\n",
      "          total_loss: -0.09288744141037265\n",
      "          vf_explained_var: -0.31013253331184387\n",
      "          vf_loss: 0.00020116848465679343\n",
      "    num_agent_steps_sampled: 349000\n",
      "    num_agent_steps_trained: 349000\n",
      "    num_steps_sampled: 349000\n",
      "    num_steps_trained: 349000\n",
      "  iterations_since_restore: 349\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.1962962962963\n",
      "    ram_util_percent: 56.05185185185184\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0666735949622547\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.952813841716884\n",
      "    mean_inference_ms: 2.4412147467693406\n",
      "    mean_raw_obs_processing_ms: 0.8867850364178551\n",
      "  time_since_restore: 7850.648437261581\n",
      "  time_this_iter_s: 18.82186722755432\n",
      "  time_total_s: 7850.648437261581\n",
      "  timers:\n",
      "    learn_throughput: 299.352\n",
      "    learn_time_ms: 3340.552\n",
      "    load_throughput: 24695.096\n",
      "    load_time_ms: 40.494\n",
      "    sample_throughput: 62.26\n",
      "    sample_time_ms: 16061.693\n",
      "    update_time_ms: 7.209\n",
      "  timestamp: 1631886189\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 349000\n",
      "  training_iteration: 349\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   349</td><td style=\"text-align: right;\">         7850.65</td><td style=\"text-align: right;\">349000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 350000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-43-28\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 351\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4264811555544537\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003183575550155086\n",
      "          policy_loss: -0.004329053602284855\n",
      "          total_loss: -0.027491595823731688\n",
      "          vf_explained_var: -0.571775496006012\n",
      "          vf_loss: 0.0001833285928114492\n",
      "    num_agent_steps_sampled: 350000\n",
      "    num_agent_steps_trained: 350000\n",
      "    num_steps_sampled: 350000\n",
      "    num_steps_trained: 350000\n",
      "  iterations_since_restore: 350\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.67857142857143\n",
      "    ram_util_percent: 55.96428571428572\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06668151741845174\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.945857391838594\n",
      "    mean_inference_ms: 2.441427299994922\n",
      "    mean_raw_obs_processing_ms: 0.8866039500100559\n",
      "  time_since_restore: 7869.906117916107\n",
      "  time_this_iter_s: 19.257680654525757\n",
      "  time_total_s: 7869.906117916107\n",
      "  timers:\n",
      "    learn_throughput: 297.684\n",
      "    learn_time_ms: 3359.272\n",
      "    load_throughput: 24384.099\n",
      "    load_time_ms: 41.01\n",
      "    sample_throughput: 62.676\n",
      "    sample_time_ms: 15955.039\n",
      "    update_time_ms: 7.887\n",
      "  timestamp: 1631886208\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 350000\n",
      "  training_iteration: 350\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   350</td><td style=\"text-align: right;\">         7869.91</td><td style=\"text-align: right;\">350000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 351000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-43-47\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 352\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14432537890970706\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3928215424219768\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01016050058314099\n",
      "          policy_loss: -0.007361174250642459\n",
      "          total_loss: -0.02966132230228848\n",
      "          vf_explained_var: -0.12918515503406525\n",
      "          vf_loss: 0.00016164853215438295\n",
      "    num_agent_steps_sampled: 351000\n",
      "    num_agent_steps_trained: 351000\n",
      "    num_steps_sampled: 351000\n",
      "    num_steps_trained: 351000\n",
      "  iterations_since_restore: 351\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.5925925925926\n",
      "    ram_util_percent: 56.01851851851852\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06668946005411232\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.938935893547685\n",
      "    mean_inference_ms: 2.441641417545497\n",
      "    mean_raw_obs_processing_ms: 0.8864295159153258\n",
      "  time_since_restore: 7888.854191064835\n",
      "  time_this_iter_s: 18.948073148727417\n",
      "  time_total_s: 7888.854191064835\n",
      "  timers:\n",
      "    learn_throughput: 300.14\n",
      "    learn_time_ms: 3331.78\n",
      "    load_throughput: 24322.194\n",
      "    load_time_ms: 41.115\n",
      "    sample_throughput: 62.335\n",
      "    sample_time_ms: 16042.385\n",
      "    update_time_ms: 7.913\n",
      "  timestamp: 1631886227\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 351000\n",
      "  training_iteration: 351\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   351</td><td style=\"text-align: right;\">         7888.85</td><td style=\"text-align: right;\">351000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 352000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-44-09\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 353\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14432537890970706\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2516479684246913\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013637727958336576\n",
      "          policy_loss: -0.1650662715236346\n",
      "          total_loss: -0.18551249926288924\n",
      "          vf_explained_var: -0.18262560665607452\n",
      "          vf_loss: 0.00010198090511595283\n",
      "    num_agent_steps_sampled: 352000\n",
      "    num_agent_steps_trained: 352000\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "  iterations_since_restore: 352\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.02\n",
      "    ram_util_percent: 56.14666666666667\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06669743615718604\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.93216005207867\n",
      "    mean_inference_ms: 2.4418586292354707\n",
      "    mean_raw_obs_processing_ms: 0.8862608653516323\n",
      "  time_since_restore: 7910.292239665985\n",
      "  time_this_iter_s: 21.438048601150513\n",
      "  time_total_s: 7910.292239665985\n",
      "  timers:\n",
      "    learn_throughput: 301.586\n",
      "    learn_time_ms: 3315.802\n",
      "    load_throughput: 25339.062\n",
      "    load_time_ms: 39.465\n",
      "    sample_throughput: 61.778\n",
      "    sample_time_ms: 16186.97\n",
      "    update_time_ms: 8.039\n",
      "  timestamp: 1631886249\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 352\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   352</td><td style=\"text-align: right;\">         7910.29</td><td style=\"text-align: right;\">352000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 353000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-44-28\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 354\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14432537890970706\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6691034343507556\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010753496375288505\n",
      "          policy_loss: -0.021820128046804003\n",
      "          total_loss: -0.04679673516915904\n",
      "          vf_explained_var: -0.829899787902832\n",
      "          vf_loss: 0.00016242640797877863\n",
      "    num_agent_steps_sampled: 353000\n",
      "    num_agent_steps_trained: 353000\n",
      "    num_steps_sampled: 353000\n",
      "    num_steps_trained: 353000\n",
      "  iterations_since_restore: 353\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.31379310344828\n",
      "    ram_util_percent: 56.282758620689656\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06670538007993074\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.92542955233061\n",
      "    mean_inference_ms: 2.44207496453145\n",
      "    mean_raw_obs_processing_ms: 0.8860993879926992\n",
      "  time_since_restore: 7930.05837726593\n",
      "  time_this_iter_s: 19.76613759994507\n",
      "  time_total_s: 7930.05837726593\n",
      "  timers:\n",
      "    learn_throughput: 302.603\n",
      "    learn_time_ms: 3304.661\n",
      "    load_throughput: 25876.773\n",
      "    load_time_ms: 38.645\n",
      "    sample_throughput: 62.184\n",
      "    sample_time_ms: 16081.329\n",
      "    update_time_ms: 7.549\n",
      "  timestamp: 1631886268\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 353000\n",
      "  training_iteration: 353\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   353</td><td style=\"text-align: right;\">         7930.06</td><td style=\"text-align: right;\">353000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 354000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-44-47\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 355\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14432537890970706\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.442146784729428\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.022506357961372497\n",
      "          policy_loss: -0.09260935046606594\n",
      "          total_loss: -0.11360132710801231\n",
      "          vf_explained_var: 0.012914324179291725\n",
      "          vf_loss: 0.00018125386047813664\n",
      "    num_agent_steps_sampled: 354000\n",
      "    num_agent_steps_trained: 354000\n",
      "    num_steps_sampled: 354000\n",
      "    num_steps_trained: 354000\n",
      "  iterations_since_restore: 354\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.75384615384615\n",
      "    ram_util_percent: 56.18076923076924\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06671330797941892\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.918671769832175\n",
      "    mean_inference_ms: 2.442288664742054\n",
      "    mean_raw_obs_processing_ms: 0.8859448282894283\n",
      "  time_since_restore: 7948.81977891922\n",
      "  time_this_iter_s: 18.761401653289795\n",
      "  time_total_s: 7948.81977891922\n",
      "  timers:\n",
      "    learn_throughput: 303.105\n",
      "    learn_time_ms: 3299.184\n",
      "    load_throughput: 24620.066\n",
      "    load_time_ms: 40.617\n",
      "    sample_throughput: 62.785\n",
      "    sample_time_ms: 15927.35\n",
      "    update_time_ms: 7.769\n",
      "  timestamp: 1631886287\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 354000\n",
      "  training_iteration: 354\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   354</td><td style=\"text-align: right;\">         7948.82</td><td style=\"text-align: right;\">354000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 355000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-45-06\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 356\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2164880683645607\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5479406012429133\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00820466397147028\n",
      "          policy_loss: -0.033725285799139075\n",
      "          total_loss: -0.05724827365742789\n",
      "          vf_explained_var: -0.30727967619895935\n",
      "          vf_loss: 0.00018020605675196243\n",
      "    num_agent_steps_sampled: 355000\n",
      "    num_agent_steps_trained: 355000\n",
      "    num_steps_sampled: 355000\n",
      "    num_steps_trained: 355000\n",
      "  iterations_since_restore: 355\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.87037037037038\n",
      "    ram_util_percent: 56.10000000000001\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0667211204556657\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.911880961231597\n",
      "    mean_inference_ms: 2.4425002429687934\n",
      "    mean_raw_obs_processing_ms: 0.8857974886746254\n",
      "  time_since_restore: 7967.525327682495\n",
      "  time_this_iter_s: 18.705548763275146\n",
      "  time_total_s: 7967.525327682495\n",
      "  timers:\n",
      "    learn_throughput: 301.953\n",
      "    learn_time_ms: 3311.779\n",
      "    load_throughput: 24500.77\n",
      "    load_time_ms: 40.815\n",
      "    sample_throughput: 62.658\n",
      "    sample_time_ms: 15959.776\n",
      "    update_time_ms: 7.156\n",
      "  timestamp: 1631886306\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 355000\n",
      "  training_iteration: 355\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   355</td><td style=\"text-align: right;\">         7967.53</td><td style=\"text-align: right;\">355000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 356000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-45-28\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 357\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2164880683645607\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5904988765716555\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00814372348868291\n",
      "          policy_loss: -0.011529728439119128\n",
      "          total_loss: -0.035502453106972906\n",
      "          vf_explained_var: -0.1817127913236618\n",
      "          vf_loss: 0.0001692431780712569\n",
      "    num_agent_steps_sampled: 356000\n",
      "    num_agent_steps_trained: 356000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "  iterations_since_restore: 356\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.89354838709679\n",
      "    ram_util_percent: 56.035483870967724\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06672880654260153\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.905148137530198\n",
      "    mean_inference_ms: 2.4427121640894374\n",
      "    mean_raw_obs_processing_ms: 0.8856538470088603\n",
      "  time_since_restore: 7989.372864484787\n",
      "  time_this_iter_s: 21.84753680229187\n",
      "  time_total_s: 7989.372864484787\n",
      "  timers:\n",
      "    learn_throughput: 300.974\n",
      "    learn_time_ms: 3322.545\n",
      "    load_throughput: 24839.65\n",
      "    load_time_ms: 40.258\n",
      "    sample_throughput: 61.608\n",
      "    sample_time_ms: 16231.619\n",
      "    update_time_ms: 7.24\n",
      "  timestamp: 1631886328\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 356\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   356</td><td style=\"text-align: right;\">         7989.37</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 357000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-45-46\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 358\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2164880683645607\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6028334935506185\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009718088618194306\n",
      "          policy_loss: 0.002227604513367017\n",
      "          total_loss: -0.02154703280991978\n",
      "          vf_explained_var: -0.5414928197860718\n",
      "          vf_loss: 0.0001498463695093556\n",
      "    num_agent_steps_sampled: 357000\n",
      "    num_agent_steps_trained: 357000\n",
      "    num_steps_sampled: 357000\n",
      "    num_steps_trained: 357000\n",
      "  iterations_since_restore: 357\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.45384615384616\n",
      "    ram_util_percent: 56.03846153846154\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06673658727269109\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.898450297392596\n",
      "    mean_inference_ms: 2.4429203404363187\n",
      "    mean_raw_obs_processing_ms: 0.885517038725163\n",
      "  time_since_restore: 8007.645374774933\n",
      "  time_this_iter_s: 18.272510290145874\n",
      "  time_total_s: 8007.645374774933\n",
      "  timers:\n",
      "    learn_throughput: 300.047\n",
      "    learn_time_ms: 3332.815\n",
      "    load_throughput: 26240.115\n",
      "    load_time_ms: 38.11\n",
      "    sample_throughput: 62.317\n",
      "    sample_time_ms: 16046.946\n",
      "    update_time_ms: 7.115\n",
      "  timestamp: 1631886346\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 357000\n",
      "  training_iteration: 357\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   357</td><td style=\"text-align: right;\">         8007.65</td><td style=\"text-align: right;\">357000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 358000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-46-07\n",
      "  done: false\n",
      "  episode_len_mean: 995.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 359\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2164880683645607\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4946287194887797\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006631969990679268\n",
      "          policy_loss: 0.20513532848821747\n",
      "          total_loss: 0.18179127929939165\n",
      "          vf_explained_var: 0.24851621687412262\n",
      "          vf_loss: 0.00016649667195957186\n",
      "    num_agent_steps_sampled: 358000\n",
      "    num_agent_steps_trained: 358000\n",
      "    num_steps_sampled: 358000\n",
      "    num_steps_trained: 358000\n",
      "  iterations_since_restore: 358\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.60666666666667\n",
      "    ram_util_percent: 55.919999999999995\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06674437855032528\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.89162728387529\n",
      "    mean_inference_ms: 2.443128586297624\n",
      "    mean_raw_obs_processing_ms: 0.8853867779393838\n",
      "  time_since_restore: 8028.197555303574\n",
      "  time_this_iter_s: 20.552180528640747\n",
      "  time_total_s: 8028.197555303574\n",
      "  timers:\n",
      "    learn_throughput: 299.111\n",
      "    learn_time_ms: 3343.236\n",
      "    load_throughput: 24000.064\n",
      "    load_time_ms: 41.667\n",
      "    sample_throughput: 61.664\n",
      "    sample_time_ms: 16216.9\n",
      "    update_time_ms: 7.264\n",
      "  timestamp: 1631886367\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 358000\n",
      "  training_iteration: 358\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   358</td><td style=\"text-align: right;\">          8028.2</td><td style=\"text-align: right;\">358000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 359000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-46-44\n",
      "  done: false\n",
      "  episode_len_mean: 994.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 360\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2164880683645607\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2289300349023606\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012169338283106453\n",
      "          policy_loss: -0.10791561073727078\n",
      "          total_loss: -0.12699826773669984\n",
      "          vf_explained_var: -0.006891065277159214\n",
      "          vf_loss: 0.0005721295322776617\n",
      "    num_agent_steps_sampled: 359000\n",
      "    num_agent_steps_trained: 359000\n",
      "    num_steps_sampled: 359000\n",
      "    num_steps_trained: 359000\n",
      "  iterations_since_restore: 359\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.91698113207548\n",
      "    ram_util_percent: 56.0509433962264\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06675222487368512\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.884816751574933\n",
      "    mean_inference_ms: 2.443335021578039\n",
      "    mean_raw_obs_processing_ms: 0.8857468239398862\n",
      "  time_since_restore: 8065.816974639893\n",
      "  time_this_iter_s: 37.61941933631897\n",
      "  time_total_s: 8065.816974639893\n",
      "  timers:\n",
      "    learn_throughput: 301.351\n",
      "    learn_time_ms: 3318.393\n",
      "    load_throughput: 23392.934\n",
      "    load_time_ms: 42.748\n",
      "    sample_throughput: 55.136\n",
      "    sample_time_ms: 18136.924\n",
      "    update_time_ms: 7.144\n",
      "  timestamp: 1631886404\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 359000\n",
      "  training_iteration: 359\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   359</td><td style=\"text-align: right;\">         8065.82</td><td style=\"text-align: right;\">359000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-47-05\n",
      "  done: false\n",
      "  episode_len_mean: 994.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 361\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2164880683645607\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.479541603724162\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012160532714472438\n",
      "          policy_loss: 0.018200669023725722\n",
      "          total_loss: -0.0036669424838489954\n",
      "          vf_explained_var: -0.4719142019748688\n",
      "          vf_loss: 0.00029519534239928665\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "  iterations_since_restore: 360\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.52666666666664\n",
      "    ram_util_percent: 56.183333333333344\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06675982454132096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.878134332127651\n",
      "    mean_inference_ms: 2.443543401712512\n",
      "    mean_raw_obs_processing_ms: 0.8861134686641069\n",
      "  time_since_restore: 8086.820284366608\n",
      "  time_this_iter_s: 21.003309726715088\n",
      "  time_total_s: 8086.820284366608\n",
      "  timers:\n",
      "    learn_throughput: 303.159\n",
      "    learn_time_ms: 3298.601\n",
      "    load_throughput: 24359.076\n",
      "    load_time_ms: 41.052\n",
      "    sample_throughput: 54.541\n",
      "    sample_time_ms: 18334.693\n",
      "    update_time_ms: 6.825\n",
      "  timestamp: 1631886425\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 360\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   360</td><td style=\"text-align: right;\">         8086.82</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 361000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-47-25\n",
      "  done: false\n",
      "  episode_len_mean: 994.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 362\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2164880683645607\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2422058277659946\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.032095341449972475\n",
      "          policy_loss: -0.0003589140044318305\n",
      "          total_loss: -0.015599201122919718\n",
      "          vf_explained_var: 0.29046642780303955\n",
      "          vf_loss: 0.00023351311572494322\n",
      "    num_agent_steps_sampled: 361000\n",
      "    num_agent_steps_trained: 361000\n",
      "    num_steps_sampled: 361000\n",
      "    num_steps_trained: 361000\n",
      "  iterations_since_restore: 361\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.89310344827587\n",
      "    ram_util_percent: 56.16896551724137\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06676732952143744\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.871552347770933\n",
      "    mean_inference_ms: 2.443753572544\n",
      "    mean_raw_obs_processing_ms: 0.8864847072068248\n",
      "  time_since_restore: 8106.571154594421\n",
      "  time_this_iter_s: 19.75087022781372\n",
      "  time_total_s: 8106.571154594421\n",
      "  timers:\n",
      "    learn_throughput: 302.184\n",
      "    learn_time_ms: 3309.244\n",
      "    load_throughput: 24595.047\n",
      "    load_time_ms: 40.659\n",
      "    sample_throughput: 54.335\n",
      "    sample_time_ms: 18404.236\n",
      "    update_time_ms: 6.813\n",
      "  timestamp: 1631886445\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 361000\n",
      "  training_iteration: 361\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   361</td><td style=\"text-align: right;\">         8106.57</td><td style=\"text-align: right;\">361000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 362000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-47-44\n",
      "  done: false\n",
      "  episode_len_mean: 994.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 363\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3247321025468408\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.712560118569268\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007307432627861113\n",
      "          policy_loss: -0.014730172355969746\n",
      "          total_loss: -0.03916866381963094\n",
      "          vf_explained_var: -0.9600992798805237\n",
      "          vf_loss: 0.0003141505580768151\n",
      "    num_agent_steps_sampled: 362000\n",
      "    num_agent_steps_trained: 362000\n",
      "    num_steps_sampled: 362000\n",
      "    num_steps_trained: 362000\n",
      "  iterations_since_restore: 362\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.82962962962964\n",
      "    ram_util_percent: 56.166666666666664\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06677478520517442\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.864928489543406\n",
      "    mean_inference_ms: 2.443961872065843\n",
      "    mean_raw_obs_processing_ms: 0.8868625370556975\n",
      "  time_since_restore: 8125.772198915482\n",
      "  time_this_iter_s: 19.20104432106018\n",
      "  time_total_s: 8125.772198915482\n",
      "  timers:\n",
      "    learn_throughput: 301.022\n",
      "    learn_time_ms: 3322.022\n",
      "    load_throughput: 23453.092\n",
      "    load_time_ms: 42.638\n",
      "    sample_throughput: 55.049\n",
      "    sample_time_ms: 18165.516\n",
      "    update_time_ms: 7.045\n",
      "  timestamp: 1631886464\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 362000\n",
      "  training_iteration: 362\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   362</td><td style=\"text-align: right;\">         8125.77</td><td style=\"text-align: right;\">362000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 363000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-48-05\n",
      "  done: false\n",
      "  episode_len_mean: 994.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 364\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3247321025468408\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7046281311247085\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009669853967214455\n",
      "          policy_loss: -0.046685966854501104\n",
      "          total_loss: -0.07029772903252807\n",
      "          vf_explained_var: -0.07758232951164246\n",
      "          vf_loss: 0.000294408303357664\n",
      "    num_agent_steps_sampled: 363000\n",
      "    num_agent_steps_trained: 363000\n",
      "    num_steps_sampled: 363000\n",
      "    num_steps_trained: 363000\n",
      "  iterations_since_restore: 363\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.70666666666668\n",
      "    ram_util_percent: 56.23666666666667\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06678226046873595\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.858393393062249\n",
      "    mean_inference_ms: 2.444170132924606\n",
      "    mean_raw_obs_processing_ms: 0.8872461475511855\n",
      "  time_since_restore: 8146.787481546402\n",
      "  time_this_iter_s: 21.01528263092041\n",
      "  time_total_s: 8146.787481546402\n",
      "  timers:\n",
      "    learn_throughput: 301.064\n",
      "    learn_time_ms: 3321.55\n",
      "    load_throughput: 23192.773\n",
      "    load_time_ms: 43.117\n",
      "    sample_throughput: 54.674\n",
      "    sample_time_ms: 18290.08\n",
      "    update_time_ms: 7.196\n",
      "  timestamp: 1631886485\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 363000\n",
      "  training_iteration: 363\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   363</td><td style=\"text-align: right;\">         8146.79</td><td style=\"text-align: right;\">363000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 364000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-48-26\n",
      "  done: false\n",
      "  episode_len_mean: 994.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 365\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3247321025468408\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.662181838353475\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009337135613887406\n",
      "          policy_loss: -0.042644434173901874\n",
      "          total_loss: -0.06600863629331191\n",
      "          vf_explained_var: -0.8159119486808777\n",
      "          vf_loss: 0.00022555000286956784\n",
      "    num_agent_steps_sampled: 364000\n",
      "    num_agent_steps_trained: 364000\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "  iterations_since_restore: 364\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.40689655172413\n",
      "    ram_util_percent: 56.372413793103455\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06678973607571453\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.851902513598208\n",
      "    mean_inference_ms: 2.4443774708180444\n",
      "    mean_raw_obs_processing_ms: 0.8876347680852212\n",
      "  time_since_restore: 8167.160704374313\n",
      "  time_this_iter_s: 20.373222827911377\n",
      "  time_total_s: 8167.160704374313\n",
      "  timers:\n",
      "    learn_throughput: 300.913\n",
      "    learn_time_ms: 3323.217\n",
      "    load_throughput: 23972.465\n",
      "    load_time_ms: 41.715\n",
      "    sample_throughput: 54.196\n",
      "    sample_time_ms: 18451.54\n",
      "    update_time_ms: 7.018\n",
      "  timestamp: 1631886506\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 364\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   364</td><td style=\"text-align: right;\">         8167.16</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 365000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-48-44\n",
      "  done: false\n",
      "  episode_len_mean: 994.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 366\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3247321025468408\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.585230827331543\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016188980909258098\n",
      "          policy_loss: -0.0006983224716451433\n",
      "          total_loss: -0.02113165503574742\n",
      "          vf_explained_var: -0.8551611304283142\n",
      "          vf_loss: 0.00016189483487020576\n",
      "    num_agent_steps_sampled: 365000\n",
      "    num_agent_steps_trained: 365000\n",
      "    num_steps_sampled: 365000\n",
      "    num_steps_trained: 365000\n",
      "  iterations_since_restore: 365\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.50384615384615\n",
      "    ram_util_percent: 56.28846153846154\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06679734811040028\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.845474244620563\n",
      "    mean_inference_ms: 2.4445835584016056\n",
      "    mean_raw_obs_processing_ms: 0.8880279033714261\n",
      "  time_since_restore: 8185.498564958572\n",
      "  time_this_iter_s: 18.337860584259033\n",
      "  time_total_s: 8185.498564958572\n",
      "  timers:\n",
      "    learn_throughput: 301.405\n",
      "    learn_time_ms: 3317.791\n",
      "    load_throughput: 24254.893\n",
      "    load_time_ms: 41.229\n",
      "    sample_throughput: 54.286\n",
      "    sample_time_ms: 18420.964\n",
      "    update_time_ms: 6.968\n",
      "  timestamp: 1631886524\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 365000\n",
      "  training_iteration: 365\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   365</td><td style=\"text-align: right;\">          8185.5</td><td style=\"text-align: right;\">365000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 366000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-49-03\n",
      "  done: false\n",
      "  episode_len_mean: 994.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 367\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3247321025468408\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.705186790890164\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005934364896149052\n",
      "          policy_loss: 0.008653082119094001\n",
      "          total_loss: -0.016348461227284537\n",
      "          vf_explained_var: -0.3664303123950958\n",
      "          vf_loss: 0.0001232445274935243\n",
      "    num_agent_steps_sampled: 366000\n",
      "    num_agent_steps_trained: 366000\n",
      "    num_steps_sampled: 366000\n",
      "    num_steps_trained: 366000\n",
      "  iterations_since_restore: 366\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.21851851851852\n",
      "    ram_util_percent: 56.39259259259259\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0668049882303636\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.839092025421126\n",
      "    mean_inference_ms: 2.4447858875276878\n",
      "    mean_raw_obs_processing_ms: 0.8884253732478697\n",
      "  time_since_restore: 8204.095926761627\n",
      "  time_this_iter_s: 18.59736180305481\n",
      "  time_total_s: 8204.095926761627\n",
      "  timers:\n",
      "    learn_throughput: 301.315\n",
      "    learn_time_ms: 3318.785\n",
      "    load_throughput: 24458.68\n",
      "    load_time_ms: 40.885\n",
      "    sample_throughput: 55.263\n",
      "    sample_time_ms: 18095.428\n",
      "    update_time_ms: 7.006\n",
      "  timestamp: 1631886543\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 366000\n",
      "  training_iteration: 366\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   366</td><td style=\"text-align: right;\">          8204.1</td><td style=\"text-align: right;\">366000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 367000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-49-22\n",
      "  done: false\n",
      "  episode_len_mean: 994.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 368\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3247321025468408\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7252588987350466\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0048783795375248375\n",
      "          policy_loss: -0.011173952536450493\n",
      "          total_loss: -0.036769012227240536\n",
      "          vf_explained_var: -0.26505523920059204\n",
      "          vf_loss: 7.336366434679399e-05\n",
      "    num_agent_steps_sampled: 367000\n",
      "    num_agent_steps_trained: 367000\n",
      "    num_steps_sampled: 367000\n",
      "    num_steps_trained: 367000\n",
      "  iterations_since_restore: 367\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.83333333333333\n",
      "    ram_util_percent: 56.44814814814814\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06681254416069665\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.832642068283148\n",
      "    mean_inference_ms: 2.444983906228425\n",
      "    mean_raw_obs_processing_ms: 0.8888280842420524\n",
      "  time_since_restore: 8222.705383062363\n",
      "  time_this_iter_s: 18.609456300735474\n",
      "  time_total_s: 8222.705383062363\n",
      "  timers:\n",
      "    learn_throughput: 302.224\n",
      "    learn_time_ms: 3308.805\n",
      "    load_throughput: 23949.852\n",
      "    load_time_ms: 41.754\n",
      "    sample_throughput: 55.128\n",
      "    sample_time_ms: 18139.661\n",
      "    update_time_ms: 6.845\n",
      "  timestamp: 1631886562\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 367000\n",
      "  training_iteration: 367\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   367</td><td style=\"text-align: right;\">         8222.71</td><td style=\"text-align: right;\">367000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 368000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-49-42\n",
      "  done: false\n",
      "  episode_len_mean: 994.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 369\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.647808986239963\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012044445800842614\n",
      "          policy_loss: -0.03584382811354266\n",
      "          total_loss: -0.06013608167154921\n",
      "          vf_explained_var: -0.49362343549728394\n",
      "          vf_loss: 0.00023022979780257122\n",
      "    num_agent_steps_sampled: 368000\n",
      "    num_agent_steps_trained: 368000\n",
      "    num_steps_sampled: 368000\n",
      "    num_steps_trained: 368000\n",
      "  iterations_since_restore: 368\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.31379310344829\n",
      "    ram_util_percent: 56.31379310344826\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06682008783200767\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.82631293473212\n",
      "    mean_inference_ms: 2.4451801464461997\n",
      "    mean_raw_obs_processing_ms: 0.8892360583655102\n",
      "  time_since_restore: 8243.524510860443\n",
      "  time_this_iter_s: 20.819127798080444\n",
      "  time_total_s: 8243.524510860443\n",
      "  timers:\n",
      "    learn_throughput: 301.894\n",
      "    learn_time_ms: 3312.419\n",
      "    load_throughput: 24954.569\n",
      "    load_time_ms: 40.073\n",
      "    sample_throughput: 55.051\n",
      "    sample_time_ms: 18165.021\n",
      "    update_time_ms: 6.742\n",
      "  timestamp: 1631886582\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 368000\n",
      "  training_iteration: 368\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   368</td><td style=\"text-align: right;\">         8243.52</td><td style=\"text-align: right;\">368000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 369000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-50-02\n",
      "  done: false\n",
      "  episode_len_mean: 996.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 370\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6707877582973905\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013165805046734785\n",
      "          policy_loss: -0.024240666793452367\n",
      "          total_loss: -0.04871015925374296\n",
      "          vf_explained_var: -0.4830853044986725\n",
      "          vf_loss: 0.00010070523317659131\n",
      "    num_agent_steps_sampled: 369000\n",
      "    num_agent_steps_trained: 369000\n",
      "    num_steps_sampled: 369000\n",
      "    num_steps_trained: 369000\n",
      "  iterations_since_restore: 369\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.48214285714286\n",
      "    ram_util_percent: 56.37857142857143\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06682770030719355\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.820047286747881\n",
      "    mean_inference_ms: 2.445373329079465\n",
      "    mean_raw_obs_processing_ms: 0.8888439835622602\n",
      "  time_since_restore: 8262.682503700256\n",
      "  time_this_iter_s: 19.157992839813232\n",
      "  time_total_s: 8262.682503700256\n",
      "  timers:\n",
      "    learn_throughput: 299.279\n",
      "    learn_time_ms: 3341.367\n",
      "    load_throughput: 24793.163\n",
      "    load_time_ms: 40.334\n",
      "    sample_throughput: 61.39\n",
      "    sample_time_ms: 16289.176\n",
      "    update_time_ms: 6.897\n",
      "  timestamp: 1631886602\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 369000\n",
      "  training_iteration: 369\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   369</td><td style=\"text-align: right;\">         8262.68</td><td style=\"text-align: right;\">369000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 370000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-50-20\n",
      "  done: false\n",
      "  episode_len_mean: 996.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 371\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.690021006266276\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012996325890493408\n",
      "          policy_loss: -0.04223383907228708\n",
      "          total_loss: -0.06691924296319485\n",
      "          vf_explained_var: -0.3254469633102417\n",
      "          vf_loss: 0.00010464186301659438\n",
      "    num_agent_steps_sampled: 370000\n",
      "    num_agent_steps_trained: 370000\n",
      "    num_steps_sampled: 370000\n",
      "    num_steps_trained: 370000\n",
      "  iterations_since_restore: 370\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.78\n",
      "    ram_util_percent: 56.464\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06683514164559362\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.81380652706156\n",
      "    mean_inference_ms: 2.445563636835073\n",
      "    mean_raw_obs_processing_ms: 0.8884591173153585\n",
      "  time_since_restore: 8280.612253427505\n",
      "  time_this_iter_s: 17.929749727249146\n",
      "  time_total_s: 8280.612253427505\n",
      "  timers:\n",
      "    learn_throughput: 299.058\n",
      "    learn_time_ms: 3343.83\n",
      "    load_throughput: 24631.951\n",
      "    load_time_ms: 40.598\n",
      "    sample_throughput: 62.581\n",
      "    sample_time_ms: 15979.361\n",
      "    update_time_ms: 6.766\n",
      "  timestamp: 1631886620\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 370000\n",
      "  training_iteration: 370\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   370</td><td style=\"text-align: right;\">         8280.61</td><td style=\"text-align: right;\">370000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 371000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-50-39\n",
      "  done: false\n",
      "  episode_len_mean: 996.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 372\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.652987922562493\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011565029635795731\n",
      "          policy_loss: -0.04825569440921148\n",
      "          total_loss: -0.0722326650387711\n",
      "          vf_explained_var: 0.43440574407577515\n",
      "          vf_loss: 0.000675144433580499\n",
      "    num_agent_steps_sampled: 371000\n",
      "    num_agent_steps_trained: 371000\n",
      "    num_steps_sampled: 371000\n",
      "    num_steps_trained: 371000\n",
      "  iterations_since_restore: 371\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.99999999999999\n",
      "    ram_util_percent: 56.54285714285714\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06684280250030886\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.807620081335534\n",
      "    mean_inference_ms: 2.4457519443752243\n",
      "    mean_raw_obs_processing_ms: 0.8880825157821285\n",
      "  time_since_restore: 8299.766925573349\n",
      "  time_this_iter_s: 19.154672145843506\n",
      "  time_total_s: 8299.766925573349\n",
      "  timers:\n",
      "    learn_throughput: 297.897\n",
      "    learn_time_ms: 3356.864\n",
      "    load_throughput: 24761.343\n",
      "    load_time_ms: 40.386\n",
      "    sample_throughput: 62.863\n",
      "    sample_time_ms: 15907.55\n",
      "    update_time_ms: 6.614\n",
      "  timestamp: 1631886639\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 371000\n",
      "  training_iteration: 371\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   371</td><td style=\"text-align: right;\">         8299.77</td><td style=\"text-align: right;\">371000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 372000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-50-57\n",
      "  done: false\n",
      "  episode_len_mean: 996.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 373\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.75353172355228\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01141613707364743\n",
      "          policy_loss: -0.041853930200967525\n",
      "          total_loss: -0.06743457843032148\n",
      "          vf_explained_var: -0.7750107049942017\n",
      "          vf_loss: 0.00010107831073279764\n",
      "    num_agent_steps_sampled: 372000\n",
      "    num_agent_steps_trained: 372000\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "  iterations_since_restore: 372\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.71923076923078\n",
      "    ram_util_percent: 56.423076923076934\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06685038455295958\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.801401314368924\n",
      "    mean_inference_ms: 2.4459370186151803\n",
      "    mean_raw_obs_processing_ms: 0.8877114240304949\n",
      "  time_since_restore: 8318.170865058899\n",
      "  time_this_iter_s: 18.403939485549927\n",
      "  time_total_s: 8318.170865058899\n",
      "  timers:\n",
      "    learn_throughput: 298.121\n",
      "    learn_time_ms: 3354.34\n",
      "    load_throughput: 25971.325\n",
      "    load_time_ms: 38.504\n",
      "    sample_throughput: 63.162\n",
      "    sample_time_ms: 15832.241\n",
      "    update_time_ms: 6.561\n",
      "  timestamp: 1631886657\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 372\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   372</td><td style=\"text-align: right;\">         8318.17</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 373000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-51-17\n",
      "  done: false\n",
      "  episode_len_mean: 996.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 374\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.697941139009264\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01697473513881549\n",
      "          policy_loss: -0.06888369988236162\n",
      "          total_loss: -0.09301718067791727\n",
      "          vf_explained_var: -0.7201606035232544\n",
      "          vf_loss: 8.981157428327909e-05\n",
      "    num_agent_steps_sampled: 373000\n",
      "    num_agent_steps_trained: 373000\n",
      "    num_steps_sampled: 373000\n",
      "    num_steps_trained: 373000\n",
      "  iterations_since_restore: 373\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.43571428571428\n",
      "    ram_util_percent: 56.407142857142844\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06685798440176872\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.795037641393085\n",
      "    mean_inference_ms: 2.4461216721944363\n",
      "    mean_raw_obs_processing_ms: 0.887347384881813\n",
      "  time_since_restore: 8337.450021982193\n",
      "  time_this_iter_s: 19.279156923294067\n",
      "  time_total_s: 8337.450021982193\n",
      "  timers:\n",
      "    learn_throughput: 298.967\n",
      "    learn_time_ms: 3344.852\n",
      "    load_throughput: 26381.925\n",
      "    load_time_ms: 37.905\n",
      "    sample_throughput: 63.821\n",
      "    sample_time_ms: 15668.926\n",
      "    update_time_ms: 6.52\n",
      "  timestamp: 1631886677\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 373000\n",
      "  training_iteration: 373\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   373</td><td style=\"text-align: right;\">         8337.45</td><td style=\"text-align: right;\">373000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 374000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-51-35\n",
      "  done: false\n",
      "  episode_len_mean: 996.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 375\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.784498315387302\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009601020981590797\n",
      "          policy_loss: 0.02966654553181595\n",
      "          total_loss: 0.003451869636774063\n",
      "          vf_explained_var: -0.2974758446216583\n",
      "          vf_loss: 7.142631421730685e-05\n",
      "    num_agent_steps_sampled: 374000\n",
      "    num_agent_steps_trained: 374000\n",
      "    num_steps_sampled: 374000\n",
      "    num_steps_trained: 374000\n",
      "  iterations_since_restore: 374\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.73461538461538\n",
      "    ram_util_percent: 56.61153846153846\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0668657222392545\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.788696610698567\n",
      "    mean_inference_ms: 2.4463074269328273\n",
      "    mean_raw_obs_processing_ms: 0.8869904379268709\n",
      "  time_since_restore: 8355.895020008087\n",
      "  time_this_iter_s: 18.444998025894165\n",
      "  time_total_s: 8355.895020008087\n",
      "  timers:\n",
      "    learn_throughput: 300.674\n",
      "    learn_time_ms: 3325.856\n",
      "    load_throughput: 25628.563\n",
      "    load_time_ms: 39.019\n",
      "    sample_throughput: 64.542\n",
      "    sample_time_ms: 15493.709\n",
      "    update_time_ms: 6.725\n",
      "  timestamp: 1631886695\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 374000\n",
      "  training_iteration: 374\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   374</td><td style=\"text-align: right;\">          8355.9</td><td style=\"text-align: right;\">374000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 375000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-51-53\n",
      "  done: false\n",
      "  episode_len_mean: 996.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 376\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7810833136240642\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009270449126179702\n",
      "          policy_loss: -0.007881391296784083\n",
      "          total_loss: -0.034119709953665735\n",
      "          vf_explained_var: -0.470388799905777\n",
      "          vf_loss: 6.730874747012826e-05\n",
      "    num_agent_steps_sampled: 375000\n",
      "    num_agent_steps_trained: 375000\n",
      "    num_steps_sampled: 375000\n",
      "    num_steps_trained: 375000\n",
      "  iterations_since_restore: 375\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.27307692307691\n",
      "    ram_util_percent: 56.49230769230768\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06687315736748979\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.782370891508528\n",
      "    mean_inference_ms: 2.446492181775272\n",
      "    mean_raw_obs_processing_ms: 0.886641500251086\n",
      "  time_since_restore: 8374.065539836884\n",
      "  time_this_iter_s: 18.170519828796387\n",
      "  time_total_s: 8374.065539836884\n",
      "  timers:\n",
      "    learn_throughput: 300.037\n",
      "    learn_time_ms: 3332.926\n",
      "    load_throughput: 24653.77\n",
      "    load_time_ms: 40.562\n",
      "    sample_throughput: 64.652\n",
      "    sample_time_ms: 15467.36\n",
      "    update_time_ms: 6.844\n",
      "  timestamp: 1631886713\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 375000\n",
      "  training_iteration: 375\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 70.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   375</td><td style=\"text-align: right;\">         8374.07</td><td style=\"text-align: right;\">375000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-52-12\n",
      "  done: false\n",
      "  episode_len_mean: 996.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 377\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7727836767832437\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01086257715956774\n",
      "          policy_loss: -0.014404447707864974\n",
      "          total_loss: -0.04024603772494528\n",
      "          vf_explained_var: -0.9079561233520508\n",
      "          vf_loss: 0.00012253463002909686\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_agent_steps_trained: 376000\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 376000\n",
      "  iterations_since_restore: 376\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.78148148148148\n",
      "    ram_util_percent: 56.418518518518525\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06688066326253163\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.776056241494912\n",
      "    mean_inference_ms: 2.4466761064275193\n",
      "    mean_raw_obs_processing_ms: 0.8863002255325262\n",
      "  time_since_restore: 8392.944192171097\n",
      "  time_this_iter_s: 18.878652334213257\n",
      "  time_total_s: 8392.944192171097\n",
      "  timers:\n",
      "    learn_throughput: 299.817\n",
      "    learn_time_ms: 3335.372\n",
      "    load_throughput: 23479.993\n",
      "    load_time_ms: 42.589\n",
      "    sample_throughput: 64.554\n",
      "    sample_time_ms: 15490.8\n",
      "    update_time_ms: 6.727\n",
      "  timestamp: 1631886732\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 376\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   376</td><td style=\"text-align: right;\">         8392.94</td><td style=\"text-align: right;\">376000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 377000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-52-31\n",
      "  done: false\n",
      "  episode_len_mean: 996.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 378\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7737542284859553\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009789917970932807\n",
      "          policy_loss: 0.04262277086575826\n",
      "          total_loss: 0.01654117065999243\n",
      "          vf_explained_var: -0.7528213262557983\n",
      "          vf_loss: 6.639112608455535e-05\n",
      "    num_agent_steps_sampled: 377000\n",
      "    num_agent_steps_trained: 377000\n",
      "    num_steps_sampled: 377000\n",
      "    num_steps_trained: 377000\n",
      "  iterations_since_restore: 377\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.2925925925926\n",
      "    ram_util_percent: 56.45925925925926\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06688820566851007\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.769744292169442\n",
      "    mean_inference_ms: 2.4468571904621346\n",
      "    mean_raw_obs_processing_ms: 0.8859659469946402\n",
      "  time_since_restore: 8411.688728094101\n",
      "  time_this_iter_s: 18.74453592300415\n",
      "  time_total_s: 8411.688728094101\n",
      "  timers:\n",
      "    learn_throughput: 300.57\n",
      "    learn_time_ms: 3327.01\n",
      "    load_throughput: 22855.536\n",
      "    load_time_ms: 43.753\n",
      "    sample_throughput: 64.543\n",
      "    sample_time_ms: 15493.546\n",
      "    update_time_ms: 24.82\n",
      "  timestamp: 1631886751\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 377000\n",
      "  training_iteration: 377\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   377</td><td style=\"text-align: right;\">         8411.69</td><td style=\"text-align: right;\">377000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 378000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-52-52\n",
      "  done: false\n",
      "  episode_len_mean: 996.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 379\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.725517921977573\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010376800391661374\n",
      "          policy_loss: -0.04143803904039992\n",
      "          total_loss: -0.06683690558291144\n",
      "          vf_explained_var: -0.9870806932449341\n",
      "          vf_loss: 0.00017147261488490687\n",
      "    num_agent_steps_sampled: 378000\n",
      "    num_agent_steps_trained: 378000\n",
      "    num_steps_sampled: 378000\n",
      "    num_steps_trained: 378000\n",
      "  iterations_since_restore: 378\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.64482758620689\n",
      "    ram_util_percent: 56.44137931034483\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06689570353633388\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.763417363765068\n",
      "    mean_inference_ms: 2.4470360331240197\n",
      "    mean_raw_obs_processing_ms: 0.8856387683347208\n",
      "  time_since_restore: 8432.38692522049\n",
      "  time_this_iter_s: 20.69819712638855\n",
      "  time_total_s: 8432.38692522049\n",
      "  timers:\n",
      "    learn_throughput: 301.713\n",
      "    learn_time_ms: 3314.407\n",
      "    load_throughput: 22086.374\n",
      "    load_time_ms: 45.277\n",
      "    sample_throughput: 64.548\n",
      "    sample_time_ms: 15492.454\n",
      "    update_time_ms: 25.066\n",
      "  timestamp: 1631886772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 378000\n",
      "  training_iteration: 378\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   378</td><td style=\"text-align: right;\">         8432.39</td><td style=\"text-align: right;\">378000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 379000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-53-10\n",
      "  done: false\n",
      "  episode_len_mean: 996.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 380\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.67585735850864\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010265976067808325\n",
      "          policy_loss: -0.011453885957598687\n",
      "          total_loss: -0.036347503587603566\n",
      "          vf_explained_var: -0.7000203728675842\n",
      "          vf_loss: 0.000198111284488631\n",
      "    num_agent_steps_sampled: 379000\n",
      "    num_agent_steps_trained: 379000\n",
      "    num_steps_sampled: 379000\n",
      "    num_steps_trained: 379000\n",
      "  iterations_since_restore: 379\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.98076923076923\n",
      "    ram_util_percent: 56.69615384615384\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06690312397146954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.757082737548394\n",
      "    mean_inference_ms: 2.4472113479136324\n",
      "    mean_raw_obs_processing_ms: 0.8853180075853431\n",
      "  time_since_restore: 8450.791496276855\n",
      "  time_this_iter_s: 18.404571056365967\n",
      "  time_total_s: 8450.791496276855\n",
      "  timers:\n",
      "    learn_throughput: 302.233\n",
      "    learn_time_ms: 3308.709\n",
      "    load_throughput: 22338.657\n",
      "    load_time_ms: 44.765\n",
      "    sample_throughput: 64.834\n",
      "    sample_time_ms: 15423.98\n",
      "    update_time_ms: 24.88\n",
      "  timestamp: 1631886790\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 379000\n",
      "  training_iteration: 379\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   379</td><td style=\"text-align: right;\">         8450.79</td><td style=\"text-align: right;\">379000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 380000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-53-30\n",
      "  done: false\n",
      "  episode_len_mean: 996.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 381\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5779058853785197\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011731256070111318\n",
      "          policy_loss: -0.0858241147465176\n",
      "          total_loss: -0.10960051446325249\n",
      "          vf_explained_var: -0.380015105009079\n",
      "          vf_loss: 9.790238064953075e-05\n",
      "    num_agent_steps_sampled: 380000\n",
      "    num_agent_steps_trained: 380000\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "  iterations_since_restore: 380\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.8793103448276\n",
      "    ram_util_percent: 56.4448275862069\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06691067666047566\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.750774042400597\n",
      "    mean_inference_ms: 2.4473857402597328\n",
      "    mean_raw_obs_processing_ms: 0.8850046855771058\n",
      "  time_since_restore: 8470.82804608345\n",
      "  time_this_iter_s: 20.03654980659485\n",
      "  time_total_s: 8470.82804608345\n",
      "  timers:\n",
      "    learn_throughput: 300.82\n",
      "    learn_time_ms: 3324.248\n",
      "    load_throughput: 21806.899\n",
      "    load_time_ms: 45.857\n",
      "    sample_throughput: 64.029\n",
      "    sample_time_ms: 15617.846\n",
      "    update_time_ms: 24.723\n",
      "  timestamp: 1631886810\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 380\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   380</td><td style=\"text-align: right;\">         8470.83</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 381000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-53-48\n",
      "  done: false\n",
      "  episode_len_mean: 996.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 382\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6416123946507772\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013689721995357099\n",
      "          policy_loss: -0.043507836386561395\n",
      "          total_loss: -0.06758540795288152\n",
      "          vf_explained_var: -0.7980751991271973\n",
      "          vf_loss: 0.00011580470503152659\n",
      "    num_agent_steps_sampled: 381000\n",
      "    num_agent_steps_trained: 381000\n",
      "    num_steps_sampled: 381000\n",
      "    num_steps_trained: 381000\n",
      "  iterations_since_restore: 381\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.00769230769231\n",
      "    ram_util_percent: 56.46538461538462\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0669181769780285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.744347082218615\n",
      "    mean_inference_ms: 2.447559934360667\n",
      "    mean_raw_obs_processing_ms: 0.8846990192036889\n",
      "  time_since_restore: 8489.107627391815\n",
      "  time_this_iter_s: 18.279581308364868\n",
      "  time_total_s: 8489.107627391815\n",
      "  timers:\n",
      "    learn_throughput: 300.575\n",
      "    learn_time_ms: 3326.955\n",
      "    load_throughput: 21756.269\n",
      "    load_time_ms: 45.964\n",
      "    sample_throughput: 64.403\n",
      "    sample_time_ms: 15527.268\n",
      "    update_time_ms: 24.746\n",
      "  timestamp: 1631886828\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 381000\n",
      "  training_iteration: 381\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   381</td><td style=\"text-align: right;\">         8489.11</td><td style=\"text-align: right;\">381000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 382000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-54-06\n",
      "  done: false\n",
      "  episode_len_mean: 996.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 383\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6501853307088217\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007833863500544899\n",
      "          policy_loss: -0.032793698728912406\n",
      "          total_loss: -0.05791006382140848\n",
      "          vf_explained_var: -0.829461932182312\n",
      "          vf_loss: 0.00011353441736395729\n",
      "    num_agent_steps_sampled: 382000\n",
      "    num_agent_steps_trained: 382000\n",
      "    num_steps_sampled: 382000\n",
      "    num_steps_trained: 382000\n",
      "  iterations_since_restore: 382\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.74230769230769\n",
      "    ram_util_percent: 56.51153846153847\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06692565453161622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.737828999655356\n",
      "    mean_inference_ms: 2.447729600338486\n",
      "    mean_raw_obs_processing_ms: 0.8844006941446015\n",
      "  time_since_restore: 8506.996807098389\n",
      "  time_this_iter_s: 17.889179706573486\n",
      "  time_total_s: 8506.996807098389\n",
      "  timers:\n",
      "    learn_throughput: 301.192\n",
      "    learn_time_ms: 3320.137\n",
      "    load_throughput: 20955.855\n",
      "    load_time_ms: 47.719\n",
      "    sample_throughput: 64.606\n",
      "    sample_time_ms: 15478.499\n",
      "    update_time_ms: 27.171\n",
      "  timestamp: 1631886846\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 382000\n",
      "  training_iteration: 382\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   382</td><td style=\"text-align: right;\">            8507</td><td style=\"text-align: right;\">382000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 383000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-54-25\n",
      "  done: false\n",
      "  episode_len_mean: 996.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 384\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7035409132639567\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011438571610927381\n",
      "          policy_loss: -0.055441390722990036\n",
      "          total_loss: -0.08052002665483289\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 9.953871093683927e-05\n",
      "    num_agent_steps_sampled: 383000\n",
      "    num_agent_steps_trained: 383000\n",
      "    num_steps_sampled: 383000\n",
      "    num_steps_trained: 383000\n",
      "  iterations_since_restore: 383\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.28148148148148\n",
      "    ram_util_percent: 56.407407407407405\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0669330770253123\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.73131652216251\n",
      "    mean_inference_ms: 2.4478970749414874\n",
      "    mean_raw_obs_processing_ms: 0.8841093596160448\n",
      "  time_since_restore: 8525.767654895782\n",
      "  time_this_iter_s: 18.7708477973938\n",
      "  time_total_s: 8525.767654895782\n",
      "  timers:\n",
      "    learn_throughput: 300.307\n",
      "    learn_time_ms: 3329.928\n",
      "    load_throughput: 21300.117\n",
      "    load_time_ms: 46.948\n",
      "    sample_throughput: 64.858\n",
      "    sample_time_ms: 15418.416\n",
      "    update_time_ms: 27.235\n",
      "  timestamp: 1631886865\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 383000\n",
      "  training_iteration: 383\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   383</td><td style=\"text-align: right;\">         8525.77</td><td style=\"text-align: right;\">383000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 384000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-54-46\n",
      "  done: false\n",
      "  episode_len_mean: 996.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 385\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7213875267240737\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0066227682505054335\n",
      "          policy_loss: -0.011037457154856788\n",
      "          total_loss: -0.03707976730333434\n",
      "          vf_explained_var: -0.5579561591148376\n",
      "          vf_loss: 9.625232984641721e-05\n",
      "    num_agent_steps_sampled: 384000\n",
      "    num_agent_steps_trained: 384000\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "  iterations_since_restore: 384\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.66551724137932\n",
      "    ram_util_percent: 56.493103448275875\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0669403925426329\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.724854744582625\n",
      "    mean_inference_ms: 2.448063366973403\n",
      "    mean_raw_obs_processing_ms: 0.8838245495219417\n",
      "  time_since_restore: 8546.211728334427\n",
      "  time_this_iter_s: 20.44407343864441\n",
      "  time_total_s: 8546.211728334427\n",
      "  timers:\n",
      "    learn_throughput: 298.494\n",
      "    learn_time_ms: 3350.148\n",
      "    load_throughput: 21904.716\n",
      "    load_time_ms: 45.652\n",
      "    sample_throughput: 64.108\n",
      "    sample_time_ms: 15598.715\n",
      "    update_time_ms: 27.802\n",
      "  timestamp: 1631886886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 384\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   384</td><td style=\"text-align: right;\">         8546.21</td><td style=\"text-align: right;\">384000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 385000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-55-05\n",
      "  done: false\n",
      "  episode_len_mean: 996.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 386\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.719818788104587\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010057569851299932\n",
      "          policy_loss: -0.012420377994163168\n",
      "          total_loss: -0.037908492195937366\n",
      "          vf_explained_var: -0.9925962686538696\n",
      "          vf_loss: 7.706543567312312e-05\n",
      "    num_agent_steps_sampled: 385000\n",
      "    num_agent_steps_trained: 385000\n",
      "    num_steps_sampled: 385000\n",
      "    num_steps_trained: 385000\n",
      "  iterations_since_restore: 385\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.29629629629629\n",
      "    ram_util_percent: 56.59629629629629\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06694761516959011\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.718277814487596\n",
      "    mean_inference_ms: 2.448226387802637\n",
      "    mean_raw_obs_processing_ms: 0.8835460288752551\n",
      "  time_since_restore: 8565.244885921478\n",
      "  time_this_iter_s: 19.03315758705139\n",
      "  time_total_s: 8565.244885921478\n",
      "  timers:\n",
      "    learn_throughput: 300.312\n",
      "    learn_time_ms: 3329.869\n",
      "    load_throughput: 22019.69\n",
      "    load_time_ms: 45.414\n",
      "    sample_throughput: 63.674\n",
      "    sample_time_ms: 15704.921\n",
      "    update_time_ms: 28.414\n",
      "  timestamp: 1631886905\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 385000\n",
      "  training_iteration: 385\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   385</td><td style=\"text-align: right;\">         8565.24</td><td style=\"text-align: right;\">385000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 386000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-55-23\n",
      "  done: false\n",
      "  episode_len_mean: 996.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 387\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7080723312166004\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009908345930965566\n",
      "          policy_loss: -0.009525803269611464\n",
      "          total_loss: -0.034934577097495395\n",
      "          vf_explained_var: -0.501922070980072\n",
      "          vf_loss: 6.316931588925298e-05\n",
      "    num_agent_steps_sampled: 386000\n",
      "    num_agent_steps_trained: 386000\n",
      "    num_steps_sampled: 386000\n",
      "    num_steps_trained: 386000\n",
      "  iterations_since_restore: 386\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.36923076923077\n",
      "    ram_util_percent: 56.54230769230771\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06695485640360081\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.711606639618381\n",
      "    mean_inference_ms: 2.448388228219417\n",
      "    mean_raw_obs_processing_ms: 0.8832758115426924\n",
      "  time_since_restore: 8583.343375205994\n",
      "  time_this_iter_s: 18.09848928451538\n",
      "  time_total_s: 8583.343375205994\n",
      "  timers:\n",
      "    learn_throughput: 299.321\n",
      "    learn_time_ms: 3340.89\n",
      "    load_throughput: 22064.577\n",
      "    load_time_ms: 45.322\n",
      "    sample_throughput: 64.035\n",
      "    sample_time_ms: 15616.43\n",
      "    update_time_ms: 28.446\n",
      "  timestamp: 1631886923\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 386000\n",
      "  training_iteration: 386\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   386</td><td style=\"text-align: right;\">         8583.34</td><td style=\"text-align: right;\">386000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 387000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-55-42\n",
      "  done: false\n",
      "  episode_len_mean: 996.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 388\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.709765746858385\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013104676164743347\n",
      "          policy_loss: -0.019436824942628543\n",
      "          total_loss: -0.044302680881487\n",
      "          vf_explained_var: -0.9933132529258728\n",
      "          vf_loss: 0.0001040467769497708\n",
      "    num_agent_steps_sampled: 387000\n",
      "    num_agent_steps_trained: 387000\n",
      "    num_steps_sampled: 387000\n",
      "    num_steps_trained: 387000\n",
      "  iterations_since_restore: 387\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.52962962962962\n",
      "    ram_util_percent: 56.50370370370371\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06696203908134744\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.704924957335532\n",
      "    mean_inference_ms: 2.4485463571004673\n",
      "    mean_raw_obs_processing_ms: 0.8830126471924806\n",
      "  time_since_restore: 8602.70693397522\n",
      "  time_this_iter_s: 19.363558769226074\n",
      "  time_total_s: 8602.70693397522\n",
      "  timers:\n",
      "    learn_throughput: 299.634\n",
      "    learn_time_ms: 3337.407\n",
      "    load_throughput: 21924.547\n",
      "    load_time_ms: 45.611\n",
      "    sample_throughput: 63.7\n",
      "    sample_time_ms: 15698.597\n",
      "    update_time_ms: 10.566\n",
      "  timestamp: 1631886942\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 387000\n",
      "  training_iteration: 387\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   387</td><td style=\"text-align: right;\">         8602.71</td><td style=\"text-align: right;\">387000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 388000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-56-02\n",
      "  done: false\n",
      "  episode_len_mean: 996.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 389\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.678366017341614\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00995627543734214\n",
      "          policy_loss: -0.05295260029120578\n",
      "          total_loss: -0.07788545481550196\n",
      "          vf_explained_var: -0.6591284275054932\n",
      "          vf_loss: 0.00023424463718887031\n",
      "    num_agent_steps_sampled: 388000\n",
      "    num_agent_steps_trained: 388000\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "  iterations_since_restore: 388\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.47857142857141\n",
      "    ram_util_percent: 56.64642857142856\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06696913515648252\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.698254787792985\n",
      "    mean_inference_ms: 2.448702025317246\n",
      "    mean_raw_obs_processing_ms: 0.8827560751805761\n",
      "  time_since_restore: 8622.153028726578\n",
      "  time_this_iter_s: 19.446094751358032\n",
      "  time_total_s: 8622.153028726578\n",
      "  timers:\n",
      "    learn_throughput: 300.289\n",
      "    learn_time_ms: 3330.122\n",
      "    load_throughput: 23326.428\n",
      "    load_time_ms: 42.87\n",
      "    sample_throughput: 64.171\n",
      "    sample_time_ms: 15583.291\n",
      "    update_time_ms: 10.446\n",
      "  timestamp: 1631886962\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 388\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   388</td><td style=\"text-align: right;\">         8622.15</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 389000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-56-38\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 390\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6535605404112075\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008091704007040809\n",
      "          policy_loss: -0.05256472267210484\n",
      "          total_loss: -0.07755394716643625\n",
      "          vf_explained_var: -0.9668577313423157\n",
      "          vf_loss: 0.00023256526422604413\n",
      "    num_agent_steps_sampled: 389000\n",
      "    num_agent_steps_trained: 389000\n",
      "    num_steps_sampled: 389000\n",
      "    num_steps_trained: 389000\n",
      "  iterations_since_restore: 389\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.85094339622641\n",
      "    ram_util_percent: 56.69056603773586\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06697619390456434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.691565232417702\n",
      "    mean_inference_ms: 2.4488556766576526\n",
      "    mean_raw_obs_processing_ms: 0.8829892634048092\n",
      "  time_since_restore: 8658.747980833054\n",
      "  time_this_iter_s: 36.59495210647583\n",
      "  time_total_s: 8658.747980833054\n",
      "  timers:\n",
      "    learn_throughput: 302.699\n",
      "    learn_time_ms: 3303.616\n",
      "    load_throughput: 23148.868\n",
      "    load_time_ms: 43.199\n",
      "    sample_throughput: 57.378\n",
      "    sample_time_ms: 17428.434\n",
      "    update_time_ms: 10.472\n",
      "  timestamp: 1631886998\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 389000\n",
      "  training_iteration: 389\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   389</td><td style=\"text-align: right;\">         8658.75</td><td style=\"text-align: right;\">389000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 390000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-56-58\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 391\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.584041889508565\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011833135124723245\n",
      "          policy_loss: -0.07449960075318814\n",
      "          total_loss: -0.09818411104174124\n",
      "          vf_explained_var: -0.636766791343689\n",
      "          vf_loss: 0.0002346088052743451\n",
      "    num_agent_steps_sampled: 390000\n",
      "    num_agent_steps_trained: 390000\n",
      "    num_steps_sampled: 390000\n",
      "    num_steps_trained: 390000\n",
      "  iterations_since_restore: 390\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.15714285714284\n",
      "    ram_util_percent: 56.660714285714285\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.066983155909795\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.684894811326467\n",
      "    mean_inference_ms: 2.449006651361025\n",
      "    mean_raw_obs_processing_ms: 0.8832263313396373\n",
      "  time_since_restore: 8678.700009346008\n",
      "  time_this_iter_s: 19.952028512954712\n",
      "  time_total_s: 8678.700009346008\n",
      "  timers:\n",
      "    learn_throughput: 302.489\n",
      "    learn_time_ms: 3305.908\n",
      "    load_throughput: 22971.965\n",
      "    load_time_ms: 43.531\n",
      "    sample_throughput: 57.412\n",
      "    sample_time_ms: 17417.942\n",
      "    update_time_ms: 10.415\n",
      "  timestamp: 1631887018\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 390000\n",
      "  training_iteration: 390\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   390</td><td style=\"text-align: right;\">          8678.7</td><td style=\"text-align: right;\">390000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 391000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-57-18\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 392\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6792087343004014\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008853459976117447\n",
      "          policy_loss: -0.049507653361393344\n",
      "          total_loss: -0.07470779204741121\n",
      "          vf_explained_var: -0.394969642162323\n",
      "          vf_loss: 0.0001544482614816742\n",
      "    num_agent_steps_sampled: 391000\n",
      "    num_agent_steps_trained: 391000\n",
      "    num_steps_sampled: 391000\n",
      "    num_steps_trained: 391000\n",
      "  iterations_since_restore: 391\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.49285714285713\n",
      "    ram_util_percent: 56.78214285714286\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06699007157236\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.678225457311678\n",
      "    mean_inference_ms: 2.4491555061676755\n",
      "    mean_raw_obs_processing_ms: 0.8834676937391276\n",
      "  time_since_restore: 8698.005050420761\n",
      "  time_this_iter_s: 19.305041074752808\n",
      "  time_total_s: 8698.005050420761\n",
      "  timers:\n",
      "    learn_throughput: 304.072\n",
      "    learn_time_ms: 3288.693\n",
      "    load_throughput: 22652.481\n",
      "    load_time_ms: 44.145\n",
      "    sample_throughput: 57.021\n",
      "    sample_time_ms: 17537.274\n",
      "    update_time_ms: 10.437\n",
      "  timestamp: 1631887038\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 391000\n",
      "  training_iteration: 391\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   391</td><td style=\"text-align: right;\">         8698.01</td><td style=\"text-align: right;\">391000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-57-36\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 393\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.660081158743964\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010442366704553762\n",
      "          policy_loss: -0.03184106925295459\n",
      "          total_loss: -0.05660949266619152\n",
      "          vf_explained_var: -0.3474540412425995\n",
      "          vf_loss: 0.00013690399711473826\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "  iterations_since_restore: 392\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.264\n",
      "    ram_util_percent: 56.775999999999996\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06699695357458726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.671480742336247\n",
      "    mean_inference_ms: 2.4493005764116957\n",
      "    mean_raw_obs_processing_ms: 0.8837134157582313\n",
      "  time_since_restore: 8715.948829889297\n",
      "  time_this_iter_s: 17.943779468536377\n",
      "  time_total_s: 8715.948829889297\n",
      "  timers:\n",
      "    learn_throughput: 304.973\n",
      "    learn_time_ms: 3278.982\n",
      "    load_throughput: 22508.417\n",
      "    load_time_ms: 44.428\n",
      "    sample_throughput: 56.964\n",
      "    sample_time_ms: 17554.885\n",
      "    update_time_ms: 7.754\n",
      "  timestamp: 1631887056\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 392\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   392</td><td style=\"text-align: right;\">         8715.95</td><td style=\"text-align: right;\">392000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 393000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-57-53\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 394\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.572627125846015\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01254817604774967\n",
      "          policy_loss: -0.0779642259909047\n",
      "          total_loss: -0.10149676493472523\n",
      "          vf_explained_var: -0.1312384307384491\n",
      "          vf_loss: 0.00015633087622417304\n",
      "    num_agent_steps_sampled: 393000\n",
      "    num_agent_steps_trained: 393000\n",
      "    num_steps_sampled: 393000\n",
      "    num_steps_trained: 393000\n",
      "  iterations_since_restore: 393\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.404\n",
      "    ram_util_percent: 56.816\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06700376991790503\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.66466029083064\n",
      "    mean_inference_ms: 2.4494421675494666\n",
      "    mean_raw_obs_processing_ms: 0.8839631781991547\n",
      "  time_since_restore: 8733.472739458084\n",
      "  time_this_iter_s: 17.52390956878662\n",
      "  time_total_s: 8733.472739458084\n",
      "  timers:\n",
      "    learn_throughput: 306.723\n",
      "    learn_time_ms: 3260.272\n",
      "    load_throughput: 21887.124\n",
      "    load_time_ms: 45.689\n",
      "    sample_throughput: 57.314\n",
      "    sample_time_ms: 17447.764\n",
      "    update_time_ms: 7.709\n",
      "  timestamp: 1631887073\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 393000\n",
      "  training_iteration: 393\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   393</td><td style=\"text-align: right;\">         8733.47</td><td style=\"text-align: right;\">393000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 394000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-58-12\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 395\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.598134591844347\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00680764804241893\n",
      "          policy_loss: -0.04152444822506772\n",
      "          total_loss: -0.06614842249287499\n",
      "          vf_explained_var: -0.7298527359962463\n",
      "          vf_loss: 0.00025204361646602696\n",
      "    num_agent_steps_sampled: 394000\n",
      "    num_agent_steps_trained: 394000\n",
      "    num_steps_sampled: 394000\n",
      "    num_steps_trained: 394000\n",
      "  iterations_since_restore: 394\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.45555555555555\n",
      "    ram_util_percent: 56.77407407407407\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06701058847702043\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.657826802097079\n",
      "    mean_inference_ms: 2.4495824669920307\n",
      "    mean_raw_obs_processing_ms: 0.884217158825685\n",
      "  time_since_restore: 8752.09922671318\n",
      "  time_this_iter_s: 18.626487255096436\n",
      "  time_total_s: 8752.09922671318\n",
      "  timers:\n",
      "    learn_throughput: 308.219\n",
      "    learn_time_ms: 3244.45\n",
      "    load_throughput: 21148.245\n",
      "    load_time_ms: 47.285\n",
      "    sample_throughput: 57.868\n",
      "    sample_time_ms: 17280.639\n",
      "    update_time_ms: 7.021\n",
      "  timestamp: 1631887092\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 394000\n",
      "  training_iteration: 394\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   394</td><td style=\"text-align: right;\">          8752.1</td><td style=\"text-align: right;\">394000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 395000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-58-30\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 396\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.593301592932807\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006699314550097175\n",
      "          policy_loss: -0.03776931319799688\n",
      "          total_loss: -0.06240100860595703\n",
      "          vf_explained_var: -0.43097105622291565\n",
      "          vf_loss: 0.0002135785672635393\n",
      "    num_agent_steps_sampled: 395000\n",
      "    num_agent_steps_trained: 395000\n",
      "    num_steps_sampled: 395000\n",
      "    num_steps_trained: 395000\n",
      "  iterations_since_restore: 395\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.55000000000001\n",
      "    ram_util_percent: 56.78846153846154\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06701743070247984\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.650990122392148\n",
      "    mean_inference_ms: 2.4497210856766993\n",
      "    mean_raw_obs_processing_ms: 0.8844763835979618\n",
      "  time_since_restore: 8770.156086921692\n",
      "  time_this_iter_s: 18.056860208511353\n",
      "  time_total_s: 8770.156086921692\n",
      "  timers:\n",
      "    learn_throughput: 308.208\n",
      "    learn_time_ms: 3244.558\n",
      "    load_throughput: 20885.305\n",
      "    load_time_ms: 47.881\n",
      "    sample_throughput: 58.196\n",
      "    sample_time_ms: 17183.254\n",
      "    update_time_ms: 6.568\n",
      "  timestamp: 1631887110\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 395000\n",
      "  training_iteration: 395\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   395</td><td style=\"text-align: right;\">         8770.16</td><td style=\"text-align: right;\">395000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 396000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-58-48\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 397\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5471455679999457\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014019232189958488\n",
      "          policy_loss: -0.0033809554245736863\n",
      "          total_loss: -0.026470103363196055\n",
      "          vf_explained_var: -0.8172590732574463\n",
      "          vf_loss: 0.0001060596768588261\n",
      "    num_agent_steps_sampled: 396000\n",
      "    num_agent_steps_trained: 396000\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "  iterations_since_restore: 396\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.4076923076923\n",
      "    ram_util_percent: 56.89615384615385\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06702429043164522\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.644157537564467\n",
      "    mean_inference_ms: 2.44985857246764\n",
      "    mean_raw_obs_processing_ms: 0.8847389599547416\n",
      "  time_since_restore: 8788.500838279724\n",
      "  time_this_iter_s: 18.344751358032227\n",
      "  time_total_s: 8788.500838279724\n",
      "  timers:\n",
      "    learn_throughput: 309.028\n",
      "    learn_time_ms: 3235.956\n",
      "    load_throughput: 20862.814\n",
      "    load_time_ms: 47.932\n",
      "    sample_throughput: 58.086\n",
      "    sample_time_ms: 17215.878\n",
      "    update_time_ms: 6.702\n",
      "  timestamp: 1631887128\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 396\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   396</td><td style=\"text-align: right;\">          8788.5</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 397000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-59-08\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 398\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.471708775891198\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008100454469026946\n",
      "          policy_loss: -0.0688040307826466\n",
      "          total_loss: -0.09196700462036662\n",
      "          vf_explained_var: -0.09301232546567917\n",
      "          vf_loss: 0.00023887799247353946\n",
      "    num_agent_steps_sampled: 397000\n",
      "    num_agent_steps_trained: 397000\n",
      "    num_steps_sampled: 397000\n",
      "    num_steps_trained: 397000\n",
      "  iterations_since_restore: 397\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.71071428571429\n",
      "    ram_util_percent: 56.975\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.067031057397647\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.637350976498162\n",
      "    mean_inference_ms: 2.4499931900605887\n",
      "    mean_raw_obs_processing_ms: 0.8850064607364558\n",
      "  time_since_restore: 8808.484169006348\n",
      "  time_this_iter_s: 19.983330726623535\n",
      "  time_total_s: 8808.484169006348\n",
      "  timers:\n",
      "    learn_throughput: 306.698\n",
      "    learn_time_ms: 3260.531\n",
      "    load_throughput: 21318.706\n",
      "    load_time_ms: 46.907\n",
      "    sample_throughput: 57.954\n",
      "    sample_time_ms: 17255.025\n",
      "    update_time_ms: 6.566\n",
      "  timestamp: 1631887148\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 397000\n",
      "  training_iteration: 397\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   397</td><td style=\"text-align: right;\">         8808.48</td><td style=\"text-align: right;\">397000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 398000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-59-29\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 399\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4103651377889843\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014558951652462307\n",
      "          policy_loss: -0.06424767110082838\n",
      "          total_loss: -0.0857564616534445\n",
      "          vf_explained_var: -0.1191609799861908\n",
      "          vf_loss: 0.0002309836169255656\n",
      "    num_agent_steps_sampled: 398000\n",
      "    num_agent_steps_trained: 398000\n",
      "    num_steps_sampled: 398000\n",
      "    num_steps_trained: 398000\n",
      "  iterations_since_restore: 398\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.39000000000001\n",
      "    ram_util_percent: 56.99000000000001\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06703769173319075\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.630621547441997\n",
      "    mean_inference_ms: 2.450125478671605\n",
      "    mean_raw_obs_processing_ms: 0.8852783738937782\n",
      "  time_since_restore: 8829.23012471199\n",
      "  time_this_iter_s: 20.7459557056427\n",
      "  time_total_s: 8829.23012471199\n",
      "  timers:\n",
      "    learn_throughput: 307.115\n",
      "    learn_time_ms: 3256.106\n",
      "    load_throughput: 20982.221\n",
      "    load_time_ms: 47.659\n",
      "    sample_throughput: 57.509\n",
      "    sample_time_ms: 17388.666\n",
      "    update_time_ms: 6.532\n",
      "  timestamp: 1631887169\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 398000\n",
      "  training_iteration: 398\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   398</td><td style=\"text-align: right;\">         8829.23</td><td style=\"text-align: right;\">398000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 399000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_13-59-49\n",
      "  done: false\n",
      "  episode_len_mean: 996.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 400\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6052537891599865\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009301511545731481\n",
      "          policy_loss: -0.05671023776133855\n",
      "          total_loss: -0.08109013732108805\n",
      "          vf_explained_var: -0.2832314968109131\n",
      "          vf_loss: 0.00016238796242002234\n",
      "    num_agent_steps_sampled: 399000\n",
      "    num_agent_steps_trained: 399000\n",
      "    num_steps_sampled: 399000\n",
      "    num_steps_trained: 399000\n",
      "  iterations_since_restore: 399\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.75714285714287\n",
      "    ram_util_percent: 56.83214285714285\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0670441002489159\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.623801802724726\n",
      "    mean_inference_ms: 2.4502503461588216\n",
      "    mean_raw_obs_processing_ms: 0.8849264118781106\n",
      "  time_since_restore: 8848.685195207596\n",
      "  time_this_iter_s: 19.45507049560547\n",
      "  time_total_s: 8848.685195207596\n",
      "  timers:\n",
      "    learn_throughput: 304.474\n",
      "    learn_time_ms: 3284.357\n",
      "    load_throughput: 20883.34\n",
      "    load_time_ms: 47.885\n",
      "    sample_throughput: 63.913\n",
      "    sample_time_ms: 15646.161\n",
      "    update_time_ms: 6.518\n",
      "  timestamp: 1631887189\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 399000\n",
      "  training_iteration: 399\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">         8848.69</td><td style=\"text-align: right;\">399000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-00-08\n",
      "  done: false\n",
      "  episode_len_mean: 996.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 401\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6612859964370728\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0061401988924113604\n",
      "          policy_loss: -0.05334654073748324\n",
      "          total_loss: -0.07886511058443123\n",
      "          vf_explained_var: 0.07603204250335693\n",
      "          vf_loss: 9.732741496640706e-05\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_agent_steps_trained: 400000\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "  iterations_since_restore: 400\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.27142857142859\n",
      "    ram_util_percent: 56.83928571428573\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06705065384394172\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.61695768401062\n",
      "    mean_inference_ms: 2.450375323713006\n",
      "    mean_raw_obs_processing_ms: 0.8845818981522544\n",
      "  time_since_restore: 8868.03721666336\n",
      "  time_this_iter_s: 19.35202145576477\n",
      "  time_total_s: 8868.03721666336\n",
      "  timers:\n",
      "    learn_throughput: 306.436\n",
      "    learn_time_ms: 3263.328\n",
      "    load_throughput: 20666.096\n",
      "    load_time_ms: 48.388\n",
      "    sample_throughput: 64.076\n",
      "    sample_time_ms: 15606.463\n",
      "    update_time_ms: 6.561\n",
      "  timestamp: 1631887208\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 400\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   400</td><td style=\"text-align: right;\">         8868.04</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 401000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-00-27\n",
      "  done: false\n",
      "  episode_len_mean: 996.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 402\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.340549793508318\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009155103756542597\n",
      "          policy_loss: -0.051580087095499036\n",
      "          total_loss: -0.07317910773886574\n",
      "          vf_explained_var: -0.30738502740859985\n",
      "          vf_loss: 0.0003200001724609239\n",
      "    num_agent_steps_sampled: 401000\n",
      "    num_agent_steps_trained: 401000\n",
      "    num_steps_sampled: 401000\n",
      "    num_steps_trained: 401000\n",
      "  iterations_since_restore: 401\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.0423076923077\n",
      "    ram_util_percent: 56.95769230769231\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06705706311440784\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.610100762661585\n",
      "    mean_inference_ms: 2.4504988144508437\n",
      "    mean_raw_obs_processing_ms: 0.884243670038318\n",
      "  time_since_restore: 8886.87459897995\n",
      "  time_this_iter_s: 18.837382316589355\n",
      "  time_total_s: 8886.87459897995\n",
      "  timers:\n",
      "    learn_throughput: 306.098\n",
      "    learn_time_ms: 3266.923\n",
      "    load_throughput: 20732.7\n",
      "    load_time_ms: 48.233\n",
      "    sample_throughput: 64.283\n",
      "    sample_time_ms: 15556.331\n",
      "    update_time_ms: 6.542\n",
      "  timestamp: 1631887227\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 401000\n",
      "  training_iteration: 401\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   401</td><td style=\"text-align: right;\">         8886.87</td><td style=\"text-align: right;\">401000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 402000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-00-47\n",
      "  done: false\n",
      "  episode_len_mean: 996.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 403\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.308161211013794\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00744609671228276\n",
      "          policy_loss: -0.01780758553908931\n",
      "          total_loss: -0.039316436234447694\n",
      "          vf_explained_var: -0.5371439456939697\n",
      "          vf_loss: 0.00036377303561291937\n",
      "    num_agent_steps_sampled: 402000\n",
      "    num_agent_steps_trained: 402000\n",
      "    num_steps_sampled: 402000\n",
      "    num_steps_trained: 402000\n",
      "  iterations_since_restore: 402\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.21428571428571\n",
      "    ram_util_percent: 56.9\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06706352093923872\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.603296594643242\n",
      "    mean_inference_ms: 2.450621054905189\n",
      "    mean_raw_obs_processing_ms: 0.8839121685637907\n",
      "  time_since_restore: 8906.496416330338\n",
      "  time_this_iter_s: 19.621817350387573\n",
      "  time_total_s: 8906.496416330338\n",
      "  timers:\n",
      "    learn_throughput: 303.763\n",
      "    learn_time_ms: 3292.036\n",
      "    load_throughput: 20906.115\n",
      "    load_time_ms: 47.833\n",
      "    sample_throughput: 63.699\n",
      "    sample_time_ms: 15698.91\n",
      "    update_time_ms: 7.118\n",
      "  timestamp: 1631887247\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 402000\n",
      "  training_iteration: 402\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   402</td><td style=\"text-align: right;\">          8906.5</td><td style=\"text-align: right;\">402000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 403000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-01-07\n",
      "  done: false\n",
      "  episode_len_mean: 996.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 404\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2067686120669046\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013513848188058751\n",
      "          policy_loss: -0.06531418959299723\n",
      "          total_loss: -0.08406128303872215\n",
      "          vf_explained_var: 0.014394700527191162\n",
      "          vf_loss: 0.0011264018834380193\n",
      "    num_agent_steps_sampled: 403000\n",
      "    num_agent_steps_trained: 403000\n",
      "    num_steps_sampled: 403000\n",
      "    num_steps_trained: 403000\n",
      "  iterations_since_restore: 403\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.97586206896553\n",
      "    ram_util_percent: 56.91379310344828\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06706997802620741\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.596521404017802\n",
      "    mean_inference_ms: 2.450743578529816\n",
      "    mean_raw_obs_processing_ms: 0.8835865660452634\n",
      "  time_since_restore: 8926.478785276413\n",
      "  time_this_iter_s: 19.98236894607544\n",
      "  time_total_s: 8926.478785276413\n",
      "  timers:\n",
      "    learn_throughput: 302.579\n",
      "    learn_time_ms: 3304.925\n",
      "    load_throughput: 20914.82\n",
      "    load_time_ms: 47.813\n",
      "    sample_throughput: 62.768\n",
      "    sample_time_ms: 15931.616\n",
      "    update_time_ms: 7.414\n",
      "  timestamp: 1631887267\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 403000\n",
      "  training_iteration: 403\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   403</td><td style=\"text-align: right;\">         8926.48</td><td style=\"text-align: right;\">403000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 404000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-01-26\n",
      "  done: false\n",
      "  episode_len_mean: 996.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 405\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5116917265786065\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009768870881078603\n",
      "          policy_loss: -0.0032839493619071115\n",
      "          total_loss: -0.01610993891954422\n",
      "          vf_explained_var: 0.34729915857315063\n",
      "          vf_loss: 0.0007047960268285655\n",
      "    num_agent_steps_sampled: 404000\n",
      "    num_agent_steps_trained: 404000\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "  iterations_since_restore: 404\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.83214285714287\n",
      "    ram_util_percent: 57.00714285714285\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06707636129274523\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.58977530736396\n",
      "    mean_inference_ms: 2.450865958234766\n",
      "    mean_raw_obs_processing_ms: 0.883267242690053\n",
      "  time_since_restore: 8946.079854726791\n",
      "  time_this_iter_s: 19.601069450378418\n",
      "  time_total_s: 8946.079854726791\n",
      "  timers:\n",
      "    learn_throughput: 303.735\n",
      "    learn_time_ms: 3292.34\n",
      "    load_throughput: 21050.744\n",
      "    load_time_ms: 47.504\n",
      "    sample_throughput: 62.338\n",
      "    sample_time_ms: 16041.672\n",
      "    update_time_ms: 7.923\n",
      "  timestamp: 1631887286\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 404\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   404</td><td style=\"text-align: right;\">         8946.08</td><td style=\"text-align: right;\">404000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 405000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-01-45\n",
      "  done: false\n",
      "  episode_len_mean: 996.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 406\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.323415860864851\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006546312075292823\n",
      "          policy_loss: -0.02952905659460359\n",
      "          total_loss: -0.05142295103934076\n",
      "          vf_explained_var: -0.14986583590507507\n",
      "          vf_loss: 0.0002773640908180318\n",
      "    num_agent_steps_sampled: 405000\n",
      "    num_agent_steps_trained: 405000\n",
      "    num_steps_sampled: 405000\n",
      "    num_steps_trained: 405000\n",
      "  iterations_since_restore: 405\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.12592592592591\n",
      "    ram_util_percent: 56.88888888888888\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0670827129081348\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.583019898597385\n",
      "    mean_inference_ms: 2.4509863653170765\n",
      "    mean_raw_obs_processing_ms: 0.8829542632425599\n",
      "  time_since_restore: 8964.915919303894\n",
      "  time_this_iter_s: 18.83606457710266\n",
      "  time_total_s: 8964.915919303894\n",
      "  timers:\n",
      "    learn_throughput: 302.745\n",
      "    learn_time_ms: 3303.112\n",
      "    load_throughput: 22892.811\n",
      "    load_time_ms: 43.682\n",
      "    sample_throughput: 62.073\n",
      "    sample_time_ms: 16110.118\n",
      "    update_time_ms: 8.76\n",
      "  timestamp: 1631887305\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 405000\n",
      "  training_iteration: 405\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   405</td><td style=\"text-align: right;\">         8964.92</td><td style=\"text-align: right;\">405000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 406000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-02-06\n",
      "  done: false\n",
      "  episode_len_mean: 996.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 407\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8547712153858609\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010511258935038232\n",
      "          policy_loss: -0.04830743256542418\n",
      "          total_loss: -0.06483452725741598\n",
      "          vf_explained_var: -0.24330079555511475\n",
      "          vf_loss: 0.0003139473570980853\n",
      "    num_agent_steps_sampled: 406000\n",
      "    num_agent_steps_trained: 406000\n",
      "    num_steps_sampled: 406000\n",
      "    num_steps_trained: 406000\n",
      "  iterations_since_restore: 406\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.3241379310345\n",
      "    ram_util_percent: 56.879310344827594\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06708910190372146\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.576281782787053\n",
      "    mean_inference_ms: 2.451107485022027\n",
      "    mean_raw_obs_processing_ms: 0.8826446256150423\n",
      "  time_since_restore: 8985.343649148941\n",
      "  time_this_iter_s: 20.427729845046997\n",
      "  time_total_s: 8985.343649148941\n",
      "  timers:\n",
      "    learn_throughput: 302.713\n",
      "    learn_time_ms: 3303.459\n",
      "    load_throughput: 23031.896\n",
      "    load_time_ms: 43.418\n",
      "    sample_throughput: 61.28\n",
      "    sample_time_ms: 16318.56\n",
      "    update_time_ms: 8.651\n",
      "  timestamp: 1631887326\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 406000\n",
      "  training_iteration: 406\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   406</td><td style=\"text-align: right;\">         8985.34</td><td style=\"text-align: right;\">406000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 407000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-02-25\n",
      "  done: false\n",
      "  episode_len_mean: 996.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 408\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.236898548073239\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005339140282348372\n",
      "          policy_loss: -0.018838611053716807\n",
      "          total_loss: -0.03992495392966602\n",
      "          vf_explained_var: -0.43648430705070496\n",
      "          vf_loss: 0.0004157457831145924\n",
      "    num_agent_steps_sampled: 407000\n",
      "    num_agent_steps_trained: 407000\n",
      "    num_steps_sampled: 407000\n",
      "    num_steps_trained: 407000\n",
      "  iterations_since_restore: 407\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.23214285714286\n",
      "    ram_util_percent: 56.98928571428571\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06709548443547261\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.56953034367213\n",
      "    mean_inference_ms: 2.451226334193835\n",
      "    mean_raw_obs_processing_ms: 0.8823405922912535\n",
      "  time_since_restore: 9004.78639626503\n",
      "  time_this_iter_s: 19.442747116088867\n",
      "  time_total_s: 9004.78639626503\n",
      "  timers:\n",
      "    learn_throughput: 302.855\n",
      "    learn_time_ms: 3301.914\n",
      "    load_throughput: 22620.608\n",
      "    load_time_ms: 44.207\n",
      "    sample_throughput: 61.481\n",
      "    sample_time_ms: 16265.272\n",
      "    update_time_ms: 8.816\n",
      "  timestamp: 1631887345\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 407000\n",
      "  training_iteration: 407\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   407</td><td style=\"text-align: right;\">         9004.79</td><td style=\"text-align: right;\">407000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-02-45\n",
      "  done: false\n",
      "  episode_len_mean: 996.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 409\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3562538438373144\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010391130764861442\n",
      "          policy_loss: -0.016813630983233453\n",
      "          total_loss: -0.03837730296783977\n",
      "          vf_explained_var: -0.024343809112906456\n",
      "          vf_loss: 0.00031169769544147935\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_agent_steps_trained: 408000\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "  iterations_since_restore: 408\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.82142857142856\n",
      "    ram_util_percent: 56.84285714285715\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06710178523380111\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.562817168172948\n",
      "    mean_inference_ms: 2.451344313830557\n",
      "    mean_raw_obs_processing_ms: 0.882043223553155\n",
      "  time_since_restore: 9024.705878973007\n",
      "  time_this_iter_s: 19.919482707977295\n",
      "  time_total_s: 9024.705878973007\n",
      "  timers:\n",
      "    learn_throughput: 303.024\n",
      "    learn_time_ms: 3300.068\n",
      "    load_throughput: 22562.795\n",
      "    load_time_ms: 44.321\n",
      "    sample_throughput: 61.788\n",
      "    sample_time_ms: 16184.431\n",
      "    update_time_ms: 8.894\n",
      "  timestamp: 1631887365\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 408\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   408</td><td style=\"text-align: right;\">         9024.71</td><td style=\"text-align: right;\">408000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 409000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-03-06\n",
      "  done: false\n",
      "  episode_len_mean: 996.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 410\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2625971449746025\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00995651955977561\n",
      "          policy_loss: -0.010546170081943274\n",
      "          total_loss: -0.03138864677813318\n",
      "          vf_explained_var: -0.8370319604873657\n",
      "          vf_loss: 0.00016689649739937015\n",
      "    num_agent_steps_sampled: 409000\n",
      "    num_agent_steps_trained: 409000\n",
      "    num_steps_sampled: 409000\n",
      "    num_steps_trained: 409000\n",
      "  iterations_since_restore: 409\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.64000000000001\n",
      "    ram_util_percent: 56.78666666666665\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06710804524591098\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.556106326895417\n",
      "    mean_inference_ms: 2.451463284144744\n",
      "    mean_raw_obs_processing_ms: 0.8817519767432668\n",
      "  time_since_restore: 9045.353086233139\n",
      "  time_this_iter_s: 20.647207260131836\n",
      "  time_total_s: 9045.353086233139\n",
      "  timers:\n",
      "    learn_throughput: 302.099\n",
      "    learn_time_ms: 3310.17\n",
      "    load_throughput: 22939.65\n",
      "    load_time_ms: 43.593\n",
      "    sample_throughput: 61.372\n",
      "    sample_time_ms: 16294.088\n",
      "    update_time_ms: 9.153\n",
      "  timestamp: 1631887386\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 409000\n",
      "  training_iteration: 409\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   409</td><td style=\"text-align: right;\">         9045.35</td><td style=\"text-align: right;\">409000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 410000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-03-26\n",
      "  done: false\n",
      "  episode_len_mean: 996.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 411\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3171555726064577\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011687767496425907\n",
      "          policy_loss: -0.03114007959763209\n",
      "          total_loss: -0.0423307484222783\n",
      "          vf_explained_var: -0.09535645693540573\n",
      "          vf_loss: 8.319288196061179e-05\n",
      "    num_agent_steps_sampled: 410000\n",
      "    num_agent_steps_trained: 410000\n",
      "    num_steps_sampled: 410000\n",
      "    num_steps_trained: 410000\n",
      "  iterations_since_restore: 410\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.33571428571427\n",
      "    ram_util_percent: 56.75357142857142\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06711417005976243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.549401896264362\n",
      "    mean_inference_ms: 2.4515822946433055\n",
      "    mean_raw_obs_processing_ms: 0.8814683255274813\n",
      "  time_since_restore: 9065.13218832016\n",
      "  time_this_iter_s: 19.779102087020874\n",
      "  time_total_s: 9065.13218832016\n",
      "  timers:\n",
      "    learn_throughput: 302.114\n",
      "    learn_time_ms: 3310.014\n",
      "    load_throughput: 23247.754\n",
      "    load_time_ms: 43.015\n",
      "    sample_throughput: 61.262\n",
      "    sample_time_ms: 16323.457\n",
      "    update_time_ms: 9.655\n",
      "  timestamp: 1631887406\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 410000\n",
      "  training_iteration: 410\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   410</td><td style=\"text-align: right;\">         9065.13</td><td style=\"text-align: right;\">410000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 411000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-03-44\n",
      "  done: false\n",
      "  episode_len_mean: 996.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 412\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1623660512734204\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.169094361199273\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021160498713781693\n",
      "          policy_loss: 0.0073917287919256424\n",
      "          total_loss: -0.010488005313608382\n",
      "          vf_explained_var: -0.03621051087975502\n",
      "          vf_loss: 0.00037546501911391614\n",
      "    num_agent_steps_sampled: 411000\n",
      "    num_agent_steps_trained: 411000\n",
      "    num_steps_sampled: 411000\n",
      "    num_steps_trained: 411000\n",
      "  iterations_since_restore: 411\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.65384615384615\n",
      "    ram_util_percent: 56.98846153846154\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06712029089805296\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.542647813723404\n",
      "    mean_inference_ms: 2.4517007438800493\n",
      "    mean_raw_obs_processing_ms: 0.8811913979577065\n",
      "  time_since_restore: 9083.576307535172\n",
      "  time_this_iter_s: 18.444119215011597\n",
      "  time_total_s: 9083.576307535172\n",
      "  timers:\n",
      "    learn_throughput: 303.286\n",
      "    learn_time_ms: 3297.219\n",
      "    load_throughput: 23313.657\n",
      "    load_time_ms: 42.893\n",
      "    sample_throughput: 61.361\n",
      "    sample_time_ms: 16296.938\n",
      "    update_time_ms: 9.624\n",
      "  timestamp: 1631887424\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 411000\n",
      "  training_iteration: 411\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   411</td><td style=\"text-align: right;\">         9083.58</td><td style=\"text-align: right;\">411000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 412000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-04-03\n",
      "  done: false\n",
      "  episode_len_mean: 996.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 413\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.629051036304898\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00573230410499532\n",
      "          policy_loss: 0.006175234499904845\n",
      "          total_loss: -0.01861179553800159\n",
      "          vf_explained_var: -0.19004510343074799\n",
      "          vf_loss: 0.00010738369618366051\n",
      "    num_agent_steps_sampled: 412000\n",
      "    num_agent_steps_trained: 412000\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "  iterations_since_restore: 412\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.9\n",
      "    ram_util_percent: 56.87407407407407\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06712630361121467\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.535883103921933\n",
      "    mean_inference_ms: 2.4518145745822517\n",
      "    mean_raw_obs_processing_ms: 0.8809193145624752\n",
      "  time_since_restore: 9102.335450172424\n",
      "  time_this_iter_s: 18.759142637252808\n",
      "  time_total_s: 9102.335450172424\n",
      "  timers:\n",
      "    learn_throughput: 304.312\n",
      "    learn_time_ms: 3286.101\n",
      "    load_throughput: 23084.413\n",
      "    load_time_ms: 43.319\n",
      "    sample_throughput: 61.648\n",
      "    sample_time_ms: 16221.243\n",
      "    update_time_ms: 9.169\n",
      "  timestamp: 1631887443\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 412\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   412</td><td style=\"text-align: right;\">         9102.34</td><td style=\"text-align: right;\">412000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 413000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-04-22\n",
      "  done: false\n",
      "  episode_len_mean: 996.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 414\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.698948947588603\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009781457161016426\n",
      "          policy_loss: -0.03041883541478051\n",
      "          total_loss: -0.054959981754008264\n",
      "          vf_explained_var: -0.7206754684448242\n",
      "          vf_loss: 6.607919685974492e-05\n",
      "    num_agent_steps_sampled: 413000\n",
      "    num_agent_steps_trained: 413000\n",
      "    num_steps_sampled: 413000\n",
      "    num_steps_trained: 413000\n",
      "  iterations_since_restore: 413\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.09629629629629\n",
      "    ram_util_percent: 56.95555555555556\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06713222837669576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.529086265101384\n",
      "    mean_inference_ms: 2.45192362507558\n",
      "    mean_raw_obs_processing_ms: 0.880651571182285\n",
      "  time_since_restore: 9121.055481433868\n",
      "  time_this_iter_s: 18.720031261444092\n",
      "  time_total_s: 9121.055481433868\n",
      "  timers:\n",
      "    learn_throughput: 302.329\n",
      "    learn_time_ms: 3307.653\n",
      "    load_throughput: 23049.907\n",
      "    load_time_ms: 43.384\n",
      "    sample_throughput: 62.213\n",
      "    sample_time_ms: 16073.717\n",
      "    update_time_ms: 8.892\n",
      "  timestamp: 1631887462\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 413000\n",
      "  training_iteration: 413\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   413</td><td style=\"text-align: right;\">         9121.06</td><td style=\"text-align: right;\">413000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 414000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-04-40\n",
      "  done: false\n",
      "  episode_len_mean: 996.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 415\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4880574160152014\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006319304414000962\n",
      "          policy_loss: -0.020789297266552844\n",
      "          total_loss: -0.0439835872915056\n",
      "          vf_explained_var: -0.360118567943573\n",
      "          vf_loss: 0.00014722232183430605\n",
      "    num_agent_steps_sampled: 414000\n",
      "    num_agent_steps_trained: 414000\n",
      "    num_steps_sampled: 414000\n",
      "    num_steps_trained: 414000\n",
      "  iterations_since_restore: 414\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.83461538461539\n",
      "    ram_util_percent: 56.94230769230769\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06713805193895216\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.52216059325391\n",
      "    mean_inference_ms: 2.4520282619902845\n",
      "    mean_raw_obs_processing_ms: 0.880389327472955\n",
      "  time_since_restore: 9139.076680898666\n",
      "  time_this_iter_s: 18.021199464797974\n",
      "  time_total_s: 9139.076680898666\n",
      "  timers:\n",
      "    learn_throughput: 299.521\n",
      "    learn_time_ms: 3338.661\n",
      "    load_throughput: 23281.369\n",
      "    load_time_ms: 42.953\n",
      "    sample_throughput: 62.95\n",
      "    sample_time_ms: 15885.73\n",
      "    update_time_ms: 8.371\n",
      "  timestamp: 1631887480\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 414000\n",
      "  training_iteration: 414\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   414</td><td style=\"text-align: right;\">         9139.08</td><td style=\"text-align: right;\">414000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 415000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-04-58\n",
      "  done: false\n",
      "  episode_len_mean: 996.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 416\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.351855683326721\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014094839710330771\n",
      "          policy_loss: -0.043871132532755534\n",
      "          total_loss: -0.06345158725873463\n",
      "          vf_explained_var: -0.23679937422275543\n",
      "          vf_loss: 0.0005053153441230684\n",
      "    num_agent_steps_sampled: 415000\n",
      "    num_agent_steps_trained: 415000\n",
      "    num_steps_sampled: 415000\n",
      "    num_steps_trained: 415000\n",
      "  iterations_since_restore: 415\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.21538461538462\n",
      "    ram_util_percent: 57.10000000000001\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06714390357831396\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.515277449079546\n",
      "    mean_inference_ms: 2.452130529707814\n",
      "    mean_raw_obs_processing_ms: 0.8801328248472992\n",
      "  time_since_restore: 9157.801449537277\n",
      "  time_this_iter_s: 18.72476863861084\n",
      "  time_total_s: 9157.801449537277\n",
      "  timers:\n",
      "    learn_throughput: 299.489\n",
      "    learn_time_ms: 3339.017\n",
      "    load_throughput: 21809.802\n",
      "    load_time_ms: 45.851\n",
      "    sample_throughput: 62.995\n",
      "    sample_time_ms: 15874.189\n",
      "    update_time_ms: 7.605\n",
      "  timestamp: 1631887498\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 415000\n",
      "  training_iteration: 415\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   415</td><td style=\"text-align: right;\">          9157.8</td><td style=\"text-align: right;\">415000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 416000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-05-17\n",
      "  done: false\n",
      "  episode_len_mean: 996.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 417\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.436581860648261\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00884643609044365\n",
      "          policy_loss: -0.020057313475343917\n",
      "          total_loss: -0.041981271193880176\n",
      "          vf_explained_var: -0.22541865706443787\n",
      "          vf_loss: 0.0002873189154570355\n",
      "    num_agent_steps_sampled: 416000\n",
      "    num_agent_steps_trained: 416000\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "  iterations_since_restore: 416\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.62962962962963\n",
      "    ram_util_percent: 56.93703703703704\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06714967089238404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.508409343430237\n",
      "    mean_inference_ms: 2.4522305710284416\n",
      "    mean_raw_obs_processing_ms: 0.8798827956298594\n",
      "  time_since_restore: 9176.313587665558\n",
      "  time_this_iter_s: 18.51213812828064\n",
      "  time_total_s: 9176.313587665558\n",
      "  timers:\n",
      "    learn_throughput: 300.679\n",
      "    learn_time_ms: 3325.802\n",
      "    load_throughput: 21780.751\n",
      "    load_time_ms: 45.912\n",
      "    sample_throughput: 63.711\n",
      "    sample_time_ms: 15695.927\n",
      "    update_time_ms: 7.689\n",
      "  timestamp: 1631887517\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 416\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   416</td><td style=\"text-align: right;\">         9176.31</td><td style=\"text-align: right;\">416000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 417000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-05-36\n",
      "  done: false\n",
      "  episode_len_mean: 996.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 418\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5723947339587743\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015329179864761806\n",
      "          policy_loss: -0.06524330758386188\n",
      "          total_loss: -0.08675920094052951\n",
      "          vf_explained_var: 0.5537768602371216\n",
      "          vf_loss: 0.00047464694231014924\n",
      "    num_agent_steps_sampled: 417000\n",
      "    num_agent_steps_trained: 417000\n",
      "    num_steps_sampled: 417000\n",
      "    num_steps_trained: 417000\n",
      "  iterations_since_restore: 417\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.18571428571428\n",
      "    ram_util_percent: 57.021428571428565\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06715535021443281\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.501578881779793\n",
      "    mean_inference_ms: 2.452325781894349\n",
      "    mean_raw_obs_processing_ms: 0.8796373813802575\n",
      "  time_since_restore: 9195.767755508423\n",
      "  time_this_iter_s: 19.45416784286499\n",
      "  time_total_s: 9195.767755508423\n",
      "  timers:\n",
      "    learn_throughput: 300.502\n",
      "    learn_time_ms: 3327.768\n",
      "    load_throughput: 21627.872\n",
      "    load_time_ms: 46.237\n",
      "    sample_throughput: 63.715\n",
      "    sample_time_ms: 15694.965\n",
      "    update_time_ms: 7.496\n",
      "  timestamp: 1631887536\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 417000\n",
      "  training_iteration: 417\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   417</td><td style=\"text-align: right;\">         9195.77</td><td style=\"text-align: right;\">417000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 418000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-05-55\n",
      "  done: false\n",
      "  episode_len_mean: 996.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 419\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5825632333755495\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006260418244428105\n",
      "          policy_loss: -0.013313289359211921\n",
      "          total_loss: -0.03753188124133481\n",
      "          vf_explained_var: -0.654242217540741\n",
      "          vf_loss: 8.232124258837657e-05\n",
      "    num_agent_steps_sampled: 418000\n",
      "    num_agent_steps_trained: 418000\n",
      "    num_steps_sampled: 418000\n",
      "    num_steps_trained: 418000\n",
      "  iterations_since_restore: 418\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.58461538461539\n",
      "    ram_util_percent: 56.9423076923077\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06716100416646958\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.494744598043823\n",
      "    mean_inference_ms: 2.4524199951719377\n",
      "    mean_raw_obs_processing_ms: 0.8793974193220713\n",
      "  time_since_restore: 9214.164813518524\n",
      "  time_this_iter_s: 18.39705801010132\n",
      "  time_total_s: 9214.164813518524\n",
      "  timers:\n",
      "    learn_throughput: 299.764\n",
      "    learn_time_ms: 3335.953\n",
      "    load_throughput: 21423.513\n",
      "    load_time_ms: 46.678\n",
      "    sample_throughput: 64.373\n",
      "    sample_time_ms: 15534.352\n",
      "    update_time_ms: 7.327\n",
      "  timestamp: 1631887555\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 418000\n",
      "  training_iteration: 418\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   418</td><td style=\"text-align: right;\">         9214.16</td><td style=\"text-align: right;\">418000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 419000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-06-31\n",
      "  done: false\n",
      "  episode_len_mean: 995.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 420\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4183794418970743\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01022095959359108\n",
      "          policy_loss: -0.04359382877333297\n",
      "          total_loss: -0.06509484963284598\n",
      "          vf_explained_var: -0.6650058627128601\n",
      "          vf_loss: 0.00019346714153066892\n",
      "    num_agent_steps_sampled: 419000\n",
      "    num_agent_steps_trained: 419000\n",
      "    num_steps_sampled: 419000\n",
      "    num_steps_trained: 419000\n",
      "  iterations_since_restore: 419\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.23076923076924\n",
      "    ram_util_percent: 56.95\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.067166460938335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.487959520299789\n",
      "    mean_inference_ms: 2.4525106984143625\n",
      "    mean_raw_obs_processing_ms: 0.879569131732624\n",
      "  time_since_restore: 9250.230546236038\n",
      "  time_this_iter_s: 36.06573271751404\n",
      "  time_total_s: 9250.230546236038\n",
      "  timers:\n",
      "    learn_throughput: 301.069\n",
      "    learn_time_ms: 3321.501\n",
      "    load_throughput: 20825.295\n",
      "    load_time_ms: 48.019\n",
      "    sample_throughput: 58.515\n",
      "    sample_time_ms: 17089.534\n",
      "    update_time_ms: 6.943\n",
      "  timestamp: 1631887591\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 419000\n",
      "  training_iteration: 419\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   419</td><td style=\"text-align: right;\">         9250.23</td><td style=\"text-align: right;\">419000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 420000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-06-54\n",
      "  done: false\n",
      "  episode_len_mean: 995.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 421\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4375331746207345\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007601621032219659\n",
      "          policy_loss: -0.04247588620831569\n",
      "          total_loss: -0.06470583308902052\n",
      "          vf_explained_var: -0.617312490940094\n",
      "          vf_loss: 0.0002940145967575821\n",
      "    num_agent_steps_sampled: 420000\n",
      "    num_agent_steps_trained: 420000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 420\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.14687500000001\n",
      "    ram_util_percent: 57.043749999999996\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06717184389575902\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.481285019927515\n",
      "    mean_inference_ms: 2.452599655090955\n",
      "    mean_raw_obs_processing_ms: 0.8797450263834783\n",
      "  time_since_restore: 9272.785785198212\n",
      "  time_this_iter_s: 22.555238962173462\n",
      "  time_total_s: 9272.785785198212\n",
      "  timers:\n",
      "    learn_throughput: 299.022\n",
      "    learn_time_ms: 3344.239\n",
      "    load_throughput: 20646.31\n",
      "    load_time_ms: 48.435\n",
      "    sample_throughput: 57.618\n",
      "    sample_time_ms: 17355.78\n",
      "    update_time_ms: 7.764\n",
      "  timestamp: 1631887614\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 420\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   420</td><td style=\"text-align: right;\">         9272.79</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 421000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-07-12\n",
      "  done: false\n",
      "  episode_len_mean: 995.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 422\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4658088551627264\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00838473377780831\n",
      "          policy_loss: -0.03192607584512896\n",
      "          total_loss: -0.054142975359637704\n",
      "          vf_explained_var: -0.6950350999832153\n",
      "          vf_loss: 0.0003990967648936324\n",
      "    num_agent_steps_sampled: 421000\n",
      "    num_agent_steps_trained: 421000\n",
      "    num_steps_sampled: 421000\n",
      "    num_steps_trained: 421000\n",
      "  iterations_since_restore: 421\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.38461538461539\n",
      "    ram_util_percent: 57.10000000000001\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06717746559584174\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.474604763708996\n",
      "    mean_inference_ms: 2.4526878848397895\n",
      "    mean_raw_obs_processing_ms: 0.8799256030016924\n",
      "  time_since_restore: 9291.430753469467\n",
      "  time_this_iter_s: 18.644968271255493\n",
      "  time_total_s: 9291.430753469467\n",
      "  timers:\n",
      "    learn_throughput: 297.133\n",
      "    learn_time_ms: 3365.491\n",
      "    load_throughput: 20519.205\n",
      "    load_time_ms: 48.735\n",
      "    sample_throughput: 57.623\n",
      "    sample_time_ms: 17354.126\n",
      "    update_time_ms: 7.768\n",
      "  timestamp: 1631887632\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 421000\n",
      "  training_iteration: 421\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   421</td><td style=\"text-align: right;\">         9291.43</td><td style=\"text-align: right;\">421000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 422000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-07-31\n",
      "  done: false\n",
      "  episode_len_mean: 995.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 423\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.548492606480916\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0147651876584016\n",
      "          policy_loss: -0.033222812010596195\n",
      "          total_loss: -0.054925679456856516\n",
      "          vf_explained_var: -0.2710510492324829\n",
      "          vf_loss: 0.0001860127738761245\n",
      "    num_agent_steps_sampled: 422000\n",
      "    num_agent_steps_trained: 422000\n",
      "    num_steps_sampled: 422000\n",
      "    num_steps_trained: 422000\n",
      "  iterations_since_restore: 422\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.98888888888888\n",
      "    ram_util_percent: 57.11851851851852\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06718306216463539\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.467938373926156\n",
      "    mean_inference_ms: 2.4527737706170756\n",
      "    mean_raw_obs_processing_ms: 0.8801100359976926\n",
      "  time_since_restore: 9310.299003362656\n",
      "  time_this_iter_s: 18.868249893188477\n",
      "  time_total_s: 9310.299003362656\n",
      "  timers:\n",
      "    learn_throughput: 298.055\n",
      "    learn_time_ms: 3355.086\n",
      "    load_throughput: 20689.624\n",
      "    load_time_ms: 48.333\n",
      "    sample_throughput: 57.55\n",
      "    sample_time_ms: 17376.12\n",
      "    update_time_ms: 7.988\n",
      "  timestamp: 1631887651\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 422000\n",
      "  training_iteration: 422\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   422</td><td style=\"text-align: right;\">          9310.3</td><td style=\"text-align: right;\">422000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 423000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-07-50\n",
      "  done: false\n",
      "  episode_len_mean: 995.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 424\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.460719754960802\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015321531171422325\n",
      "          policy_loss: -0.033168145600292416\n",
      "          total_loss: -0.05372587293386459\n",
      "          vf_explained_var: -0.9421509504318237\n",
      "          vf_loss: 0.0003179275213268637\n",
      "    num_agent_steps_sampled: 423000\n",
      "    num_agent_steps_trained: 423000\n",
      "    num_steps_sampled: 423000\n",
      "    num_steps_trained: 423000\n",
      "  iterations_since_restore: 423\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.79259259259258\n",
      "    ram_util_percent: 57.22592592592592\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06718868347496508\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.461324205887438\n",
      "    mean_inference_ms: 2.4528602667775075\n",
      "    mean_raw_obs_processing_ms: 0.8802992458711565\n",
      "  time_since_restore: 9328.695310592651\n",
      "  time_this_iter_s: 18.396307229995728\n",
      "  time_total_s: 9328.695310592651\n",
      "  timers:\n",
      "    learn_throughput: 298.76\n",
      "    learn_time_ms: 3347.173\n",
      "    load_throughput: 21183.14\n",
      "    load_time_ms: 47.207\n",
      "    sample_throughput: 57.628\n",
      "    sample_time_ms: 17352.815\n",
      "    update_time_ms: 7.994\n",
      "  timestamp: 1631887670\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 423000\n",
      "  training_iteration: 423\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   423</td><td style=\"text-align: right;\">          9328.7</td><td style=\"text-align: right;\">423000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-08-08\n",
      "  done: false\n",
      "  episode_len_mean: 995.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 425\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.604417575730218\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010627427630359815\n",
      "          policy_loss: -0.023573455752597915\n",
      "          total_loss: -0.046912547532055114\n",
      "          vf_explained_var: -0.6157037019729614\n",
      "          vf_loss: 0.00011678171217934302\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_agent_steps_trained: 424000\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "  iterations_since_restore: 424\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.02307692307691\n",
      "    ram_util_percent: 57.29230769230769\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06719428862811602\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.4547461775289\n",
      "    mean_inference_ms: 2.4529455671393063\n",
      "    mean_raw_obs_processing_ms: 0.8804928176230753\n",
      "  time_since_restore: 9347.10654091835\n",
      "  time_this_iter_s: 18.411230325698853\n",
      "  time_total_s: 9347.10654091835\n",
      "  timers:\n",
      "    learn_throughput: 299.255\n",
      "    learn_time_ms: 3341.633\n",
      "    load_throughput: 21462.134\n",
      "    load_time_ms: 46.594\n",
      "    sample_throughput: 57.478\n",
      "    sample_time_ms: 17398.016\n",
      "    update_time_ms: 8.08\n",
      "  timestamp: 1631887688\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 424\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   424</td><td style=\"text-align: right;\">         9347.11</td><td style=\"text-align: right;\">424000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 425000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-08-26\n",
      "  done: false\n",
      "  episode_len_mean: 995.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 426\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.668295163578457\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0070645424338256334\n",
      "          policy_loss: -0.04269654254118602\n",
      "          total_loss: -0.06756193422608905\n",
      "          vf_explained_var: -0.5838393568992615\n",
      "          vf_loss: 9.69958968628109e-05\n",
      "    num_agent_steps_sampled: 425000\n",
      "    num_agent_steps_trained: 425000\n",
      "    num_steps_sampled: 425000\n",
      "    num_steps_trained: 425000\n",
      "  iterations_since_restore: 425\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.62692307692308\n",
      "    ram_util_percent: 57.20769230769231\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06719985915291414\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.448186166733565\n",
      "    mean_inference_ms: 2.4530292385255743\n",
      "    mean_raw_obs_processing_ms: 0.8806911051737053\n",
      "  time_since_restore: 9365.37881731987\n",
      "  time_this_iter_s: 18.272276401519775\n",
      "  time_total_s: 9365.37881731987\n",
      "  timers:\n",
      "    learn_throughput: 299.389\n",
      "    learn_time_ms: 3340.139\n",
      "    load_throughput: 21301.891\n",
      "    load_time_ms: 46.944\n",
      "    sample_throughput: 57.624\n",
      "    sample_time_ms: 17353.746\n",
      "    update_time_ms: 7.962\n",
      "  timestamp: 1631887706\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 425000\n",
      "  training_iteration: 425\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   425</td><td style=\"text-align: right;\">         9365.38</td><td style=\"text-align: right;\">425000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 426000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-08-46\n",
      "  done: false\n",
      "  episode_len_mean: 995.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 427\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.612310806910197\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005384739293910456\n",
      "          policy_loss: -0.05296218459390932\n",
      "          total_loss: -0.07766881183617645\n",
      "          vf_explained_var: -0.6645883321762085\n",
      "          vf_loss: 0.00010503139272562596\n",
      "    num_agent_steps_sampled: 426000\n",
      "    num_agent_steps_trained: 426000\n",
      "    num_steps_sampled: 426000\n",
      "    num_steps_trained: 426000\n",
      "  iterations_since_restore: 426\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.13928571428572\n",
      "    ram_util_percent: 57.23571428571428\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0672054387741315\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.441671095339334\n",
      "    mean_inference_ms: 2.45311259857768\n",
      "    mean_raw_obs_processing_ms: 0.8808946009971079\n",
      "  time_since_restore: 9384.73608326912\n",
      "  time_this_iter_s: 19.357265949249268\n",
      "  time_total_s: 9384.73608326912\n",
      "  timers:\n",
      "    learn_throughput: 297.689\n",
      "    learn_time_ms: 3359.205\n",
      "    load_throughput: 20847.177\n",
      "    load_time_ms: 47.968\n",
      "    sample_throughput: 57.41\n",
      "    sample_time_ms: 17418.485\n",
      "    update_time_ms: 7.727\n",
      "  timestamp: 1631887726\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 426000\n",
      "  training_iteration: 426\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   426</td><td style=\"text-align: right;\">         9384.74</td><td style=\"text-align: right;\">426000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 427000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-09-05\n",
      "  done: false\n",
      "  episode_len_mean: 995.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 428\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.528361678123474\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00830214183673661\n",
      "          policy_loss: -0.03972700557981928\n",
      "          total_loss: -0.06289188474830654\n",
      "          vf_explained_var: -0.45438483357429504\n",
      "          vf_loss: 9.675657020630347e-05\n",
      "    num_agent_steps_sampled: 427000\n",
      "    num_agent_steps_trained: 427000\n",
      "    num_steps_sampled: 427000\n",
      "    num_steps_trained: 427000\n",
      "  iterations_since_restore: 427\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.30769230769232\n",
      "    ram_util_percent: 57.25\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06721100760772018\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.43522799309469\n",
      "    mean_inference_ms: 2.4531975133808417\n",
      "    mean_raw_obs_processing_ms: 0.881102809315412\n",
      "  time_since_restore: 9403.498374938965\n",
      "  time_this_iter_s: 18.76229166984558\n",
      "  time_total_s: 9403.498374938965\n",
      "  timers:\n",
      "    learn_throughput: 298.616\n",
      "    learn_time_ms: 3348.786\n",
      "    load_throughput: 20906.251\n",
      "    load_time_ms: 47.833\n",
      "    sample_throughput: 57.604\n",
      "    sample_time_ms: 17359.816\n",
      "    update_time_ms: 7.725\n",
      "  timestamp: 1631887745\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 427000\n",
      "  training_iteration: 427\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   427</td><td style=\"text-align: right;\">          9403.5</td><td style=\"text-align: right;\">427000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 428000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-09-23\n",
      "  done: false\n",
      "  episode_len_mean: 995.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 429\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.409044689602322\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010679662165003812\n",
      "          policy_loss: -0.02120408525483476\n",
      "          total_loss: -0.04258890264771051\n",
      "          vf_explained_var: -0.42985567450523376\n",
      "          vf_loss: 0.00010460694789015987\n",
      "    num_agent_steps_sampled: 428000\n",
      "    num_agent_steps_trained: 428000\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "  iterations_since_restore: 428\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.10370370370372\n",
      "    ram_util_percent: 57.140740740740746\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06721670371612144\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.428821404023452\n",
      "    mean_inference_ms: 2.4532831048805073\n",
      "    mean_raw_obs_processing_ms: 0.8813147363532824\n",
      "  time_since_restore: 9422.104041576385\n",
      "  time_this_iter_s: 18.605666637420654\n",
      "  time_total_s: 9422.104041576385\n",
      "  timers:\n",
      "    learn_throughput: 299.408\n",
      "    learn_time_ms: 3339.922\n",
      "    load_throughput: 20612.167\n",
      "    load_time_ms: 48.515\n",
      "    sample_throughput: 57.51\n",
      "    sample_time_ms: 17388.385\n",
      "    update_time_ms: 7.899\n",
      "  timestamp: 1631887763\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 428\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   428</td><td style=\"text-align: right;\">          9422.1</td><td style=\"text-align: right;\">428000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 429000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-09-44\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 430\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4516500896877713\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006696872704786945\n",
      "          policy_loss: -0.025416988879442215\n",
      "          total_loss: -0.04813739282803403\n",
      "          vf_explained_var: -0.8618009686470032\n",
      "          vf_loss: 0.00016507965982428787\n",
      "    num_agent_steps_sampled: 429000\n",
      "    num_agent_steps_trained: 429000\n",
      "    num_steps_sampled: 429000\n",
      "    num_steps_trained: 429000\n",
      "  iterations_since_restore: 429\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.24000000000001\n",
      "    ram_util_percent: 57.19666666666667\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06722244723337616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.422517751565511\n",
      "    mean_inference_ms: 2.4533708944117834\n",
      "    mean_raw_obs_processing_ms: 0.8809853171598799\n",
      "  time_since_restore: 9443.081392765045\n",
      "  time_this_iter_s: 20.977351188659668\n",
      "  time_total_s: 9443.081392765045\n",
      "  timers:\n",
      "    learn_throughput: 298.721\n",
      "    learn_time_ms: 3347.602\n",
      "    load_throughput: 20875.514\n",
      "    load_time_ms: 47.903\n",
      "    sample_throughput: 63.005\n",
      "    sample_time_ms: 15871.847\n",
      "    update_time_ms: 8.192\n",
      "  timestamp: 1631887784\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 429000\n",
      "  training_iteration: 429\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   429</td><td style=\"text-align: right;\">         9443.08</td><td style=\"text-align: right;\">429000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 430000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-10-03\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 431\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4466636419296264\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00559351122822128\n",
      "          policy_loss: -0.04614826742973593\n",
      "          total_loss: -0.06885803573661381\n",
      "          vf_explained_var: -0.3378874957561493\n",
      "          vf_loss: 0.0003945734954287319\n",
      "    num_agent_steps_sampled: 430000\n",
      "    num_agent_steps_trained: 430000\n",
      "    num_steps_sampled: 430000\n",
      "    num_steps_trained: 430000\n",
      "  iterations_since_restore: 430\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.41111111111113\n",
      "    ram_util_percent: 57.17407407407408\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0672281877232433\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.416182478031567\n",
      "    mean_inference_ms: 2.4534562508313957\n",
      "    mean_raw_obs_processing_ms: 0.880661330664202\n",
      "  time_since_restore: 9461.709237337112\n",
      "  time_this_iter_s: 18.62784457206726\n",
      "  time_total_s: 9461.709237337112\n",
      "  timers:\n",
      "    learn_throughput: 299.205\n",
      "    learn_time_ms: 3342.189\n",
      "    load_throughput: 20984.467\n",
      "    load_time_ms: 47.654\n",
      "    sample_throughput: 64.57\n",
      "    sample_time_ms: 15486.969\n",
      "    update_time_ms: 6.918\n",
      "  timestamp: 1631887803\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 430000\n",
      "  training_iteration: 430\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   430</td><td style=\"text-align: right;\">         9461.71</td><td style=\"text-align: right;\">430000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 431000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-10-22\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 432\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5294286410013833\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006717035952657508\n",
      "          policy_loss: -0.05386041891243723\n",
      "          total_loss: -0.07733995529512565\n",
      "          vf_explained_var: -0.12416709959506989\n",
      "          vf_loss: 0.00017882059686245258\n",
      "    num_agent_steps_sampled: 431000\n",
      "    num_agent_steps_trained: 431000\n",
      "    num_steps_sampled: 431000\n",
      "    num_steps_trained: 431000\n",
      "  iterations_since_restore: 431\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.67692307692309\n",
      "    ram_util_percent: 57.130769230769225\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06723392656714737\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.409837790535256\n",
      "    mean_inference_ms: 2.453542503749099\n",
      "    mean_raw_obs_processing_ms: 0.8803418314979278\n",
      "  time_since_restore: 9480.399086475372\n",
      "  time_this_iter_s: 18.689849138259888\n",
      "  time_total_s: 9480.399086475372\n",
      "  timers:\n",
      "    learn_throughput: 299.626\n",
      "    learn_time_ms: 3337.49\n",
      "    load_throughput: 21028.802\n",
      "    load_time_ms: 47.554\n",
      "    sample_throughput: 64.53\n",
      "    sample_time_ms: 15496.578\n",
      "    update_time_ms: 6.84\n",
      "  timestamp: 1631887822\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 431000\n",
      "  training_iteration: 431\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   431</td><td style=\"text-align: right;\">          9480.4</td><td style=\"text-align: right;\">431000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 432000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-10-41\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 433\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3469099521636965\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011378298842912906\n",
      "          policy_loss: -0.07526256897383266\n",
      "          total_loss: -0.0956122811883688\n",
      "          vf_explained_var: 0.050974417477846146\n",
      "          vf_loss: 0.0003482157581301079\n",
      "    num_agent_steps_sampled: 432000\n",
      "    num_agent_steps_trained: 432000\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "  iterations_since_restore: 432\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.5\n",
      "    ram_util_percent: 57.253571428571426\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06723960257772195\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.403563355940957\n",
      "    mean_inference_ms: 2.453627107947981\n",
      "    mean_raw_obs_processing_ms: 0.8800270335124522\n",
      "  time_since_restore: 9499.95408296585\n",
      "  time_this_iter_s: 19.554996490478516\n",
      "  time_total_s: 9499.95408296585\n",
      "  timers:\n",
      "    learn_throughput: 298.541\n",
      "    learn_time_ms: 3349.619\n",
      "    load_throughput: 20856.341\n",
      "    load_time_ms: 47.947\n",
      "    sample_throughput: 64.3\n",
      "    sample_time_ms: 15552.033\n",
      "    update_time_ms: 6.649\n",
      "  timestamp: 1631887841\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 432\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   432</td><td style=\"text-align: right;\">         9499.95</td><td style=\"text-align: right;\">432000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 433000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-11-00\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 434\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5709939850701224\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01035874243539877\n",
      "          policy_loss: -0.06142423641350534\n",
      "          total_loss: -0.084306979427735\n",
      "          vf_explained_var: -0.6432350277900696\n",
      "          vf_loss: 0.00030433207255353207\n",
      "    num_agent_steps_sampled: 433000\n",
      "    num_agent_steps_trained: 433000\n",
      "    num_steps_sampled: 433000\n",
      "    num_steps_trained: 433000\n",
      "  iterations_since_restore: 433\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.74074074074075\n",
      "    ram_util_percent: 57.24444444444445\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06724514797841591\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.397331325672056\n",
      "    mean_inference_ms: 2.4537091914854896\n",
      "    mean_raw_obs_processing_ms: 0.8797176538977252\n",
      "  time_since_restore: 9518.333161115646\n",
      "  time_this_iter_s: 18.379078149795532\n",
      "  time_total_s: 9518.333161115646\n",
      "  timers:\n",
      "    learn_throughput: 299.929\n",
      "    learn_time_ms: 3334.128\n",
      "    load_throughput: 20413.01\n",
      "    load_time_ms: 48.988\n",
      "    sample_throughput: 64.253\n",
      "    sample_time_ms: 15563.577\n",
      "    update_time_ms: 7.289\n",
      "  timestamp: 1631887860\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 433000\n",
      "  training_iteration: 433\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   433</td><td style=\"text-align: right;\">         9518.33</td><td style=\"text-align: right;\">433000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 434000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-11-18\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 435\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8977992508146497\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009752157216930771\n",
      "          policy_loss: -0.04619228865744339\n",
      "          total_loss: -0.06181051391694281\n",
      "          vf_explained_var: 0.5132461786270142\n",
      "          vf_loss: 0.0009846363987890071\n",
      "    num_agent_steps_sampled: 434000\n",
      "    num_agent_steps_trained: 434000\n",
      "    num_steps_sampled: 434000\n",
      "    num_steps_trained: 434000\n",
      "  iterations_since_restore: 434\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.57307692307693\n",
      "    ram_util_percent: 57.30769230769231\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06725064144625986\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.391082439688295\n",
      "    mean_inference_ms: 2.453789928109978\n",
      "    mean_raw_obs_processing_ms: 0.8794139998780531\n",
      "  time_since_restore: 9537.164150714874\n",
      "  time_this_iter_s: 18.830989599227905\n",
      "  time_total_s: 9537.164150714874\n",
      "  timers:\n",
      "    learn_throughput: 300.818\n",
      "    learn_time_ms: 3324.264\n",
      "    load_throughput: 20947.912\n",
      "    load_time_ms: 47.737\n",
      "    sample_throughput: 64.033\n",
      "    sample_time_ms: 15616.926\n",
      "    update_time_ms: 7.111\n",
      "  timestamp: 1631887878\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 434000\n",
      "  training_iteration: 434\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   434</td><td style=\"text-align: right;\">         9537.16</td><td style=\"text-align: right;\">434000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 435000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-11-39\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 436\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.999037414126926\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00915373186221322\n",
      "          policy_loss: -0.047739315778017044\n",
      "          total_loss: -0.0640147269393007\n",
      "          vf_explained_var: 0.10560929775238037\n",
      "          vf_loss: 0.0014855806626858086\n",
      "    num_agent_steps_sampled: 435000\n",
      "    num_agent_steps_trained: 435000\n",
      "    num_steps_sampled: 435000\n",
      "    num_steps_trained: 435000\n",
      "  iterations_since_restore: 435\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.31\n",
      "    ram_util_percent: 57.24\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06725619127224317\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.384927296133235\n",
      "    mean_inference_ms: 2.453872990935852\n",
      "    mean_raw_obs_processing_ms: 0.8791155511162079\n",
      "  time_since_restore: 9557.809698581696\n",
      "  time_this_iter_s: 20.64554786682129\n",
      "  time_total_s: 9557.809698581696\n",
      "  timers:\n",
      "    learn_throughput: 302.51\n",
      "    learn_time_ms: 3305.68\n",
      "    load_throughput: 20825.233\n",
      "    load_time_ms: 48.019\n",
      "    sample_throughput: 63.005\n",
      "    sample_time_ms: 15871.676\n",
      "    update_time_ms: 7.638\n",
      "  timestamp: 1631887899\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 435000\n",
      "  training_iteration: 435\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   435</td><td style=\"text-align: right;\">         9557.81</td><td style=\"text-align: right;\">435000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 436000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-11-58\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 437\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6312927511003283\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003727065020496016\n",
      "          policy_loss: -0.03063903757267528\n",
      "          total_loss: -0.05584150882851746\n",
      "          vf_explained_var: -0.38748428225517273\n",
      "          vf_loss: 0.00020273458276278688\n",
      "    num_agent_steps_sampled: 436000\n",
      "    num_agent_steps_trained: 436000\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "  iterations_since_restore: 436\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.01923076923076\n",
      "    ram_util_percent: 57.23076923076923\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06726159537167839\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.378733446294142\n",
      "    mean_inference_ms: 2.453956035750605\n",
      "    mean_raw_obs_processing_ms: 0.8788217197617172\n",
      "  time_since_restore: 9576.278401613235\n",
      "  time_this_iter_s: 18.468703031539917\n",
      "  time_total_s: 9576.278401613235\n",
      "  timers:\n",
      "    learn_throughput: 302.921\n",
      "    learn_time_ms: 3301.187\n",
      "    load_throughput: 21231.768\n",
      "    load_time_ms: 47.099\n",
      "    sample_throughput: 63.339\n",
      "    sample_time_ms: 15787.944\n",
      "    update_time_ms: 7.821\n",
      "  timestamp: 1631887918\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 436\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   436</td><td style=\"text-align: right;\">         9576.28</td><td style=\"text-align: right;\">436000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 437000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-12-19\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 438\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12177453845506533\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.529413339826796\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010446505428593333\n",
      "          policy_loss: -0.01797972485009167\n",
      "          total_loss: -0.04174121926642126\n",
      "          vf_explained_var: -0.3380849063396454\n",
      "          vf_loss: 0.00026052093749563535\n",
      "    num_agent_steps_sampled: 437000\n",
      "    num_agent_steps_trained: 437000\n",
      "    num_steps_sampled: 437000\n",
      "    num_steps_trained: 437000\n",
      "  iterations_since_restore: 437\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.8741935483871\n",
      "    ram_util_percent: 57.13870967741936\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06726704032925777\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.372636658816225\n",
      "    mean_inference_ms: 2.454041149386511\n",
      "    mean_raw_obs_processing_ms: 0.8785328771941308\n",
      "  time_since_restore: 9597.72530055046\n",
      "  time_this_iter_s: 21.446898937225342\n",
      "  time_total_s: 9597.72530055046\n",
      "  timers:\n",
      "    learn_throughput: 303.529\n",
      "    learn_time_ms: 3294.574\n",
      "    load_throughput: 21192.858\n",
      "    load_time_ms: 47.186\n",
      "    sample_throughput: 62.255\n",
      "    sample_time_ms: 16062.935\n",
      "    update_time_ms: 7.791\n",
      "  timestamp: 1631887939\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 437000\n",
      "  training_iteration: 437\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   437</td><td style=\"text-align: right;\">         9597.73</td><td style=\"text-align: right;\">437000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 438000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-12-39\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 439\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12177453845506533\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.581742432382372\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01136596089841504\n",
      "          policy_loss: -0.00768283245464166\n",
      "          total_loss: -0.03192990066276656\n",
      "          vf_explained_var: -0.8456012010574341\n",
      "          vf_loss: 0.00018627341929055143\n",
      "    num_agent_steps_sampled: 438000\n",
      "    num_agent_steps_trained: 438000\n",
      "    num_steps_sampled: 438000\n",
      "    num_steps_trained: 438000\n",
      "  iterations_since_restore: 438\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.30357142857143\n",
      "    ram_util_percent: 57.27857142857142\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06727239097929429\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.366573740972887\n",
      "    mean_inference_ms: 2.4541250822312124\n",
      "    mean_raw_obs_processing_ms: 0.8782492964983877\n",
      "  time_since_restore: 9617.32316160202\n",
      "  time_this_iter_s: 19.59786105155945\n",
      "  time_total_s: 9617.32316160202\n",
      "  timers:\n",
      "    learn_throughput: 300.763\n",
      "    learn_time_ms: 3324.872\n",
      "    load_throughput: 20738.338\n",
      "    load_time_ms: 48.22\n",
      "    sample_throughput: 61.992\n",
      "    sample_time_ms: 16131.224\n",
      "    update_time_ms: 7.65\n",
      "  timestamp: 1631887959\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 438000\n",
      "  training_iteration: 438\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   438</td><td style=\"text-align: right;\">         9617.32</td><td style=\"text-align: right;\">438000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 439000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-12-57\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 440\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12177453845506533\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.670542565981547\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010353470433576244\n",
      "          policy_loss: -0.025753865300470757\n",
      "          total_loss: -0.05101320385519001\n",
      "          vf_explained_var: -0.6775668263435364\n",
      "          vf_loss: 0.00018529702534174754\n",
      "    num_agent_steps_sampled: 439000\n",
      "    num_agent_steps_trained: 439000\n",
      "    num_steps_sampled: 439000\n",
      "    num_steps_trained: 439000\n",
      "  iterations_since_restore: 439\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.92692307692307\n",
      "    ram_util_percent: 57.23076923076923\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06727777829371581\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.360459266217704\n",
      "    mean_inference_ms: 2.4542091586575636\n",
      "    mean_raw_obs_processing_ms: 0.8779709305079811\n",
      "  time_since_restore: 9635.534936904907\n",
      "  time_this_iter_s: 18.211775302886963\n",
      "  time_total_s: 9635.534936904907\n",
      "  timers:\n",
      "    learn_throughput: 300.353\n",
      "    learn_time_ms: 3329.42\n",
      "    load_throughput: 20707.009\n",
      "    load_time_ms: 48.293\n",
      "    sample_throughput: 63.088\n",
      "    sample_time_ms: 15850.971\n",
      "    update_time_ms: 7.366\n",
      "  timestamp: 1631887977\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 439000\n",
      "  training_iteration: 439\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   439</td><td style=\"text-align: right;\">         9635.53</td><td style=\"text-align: right;\">439000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-13-15\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 441\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12177453845506533\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6189249647988215\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010135591903527658\n",
      "          policy_loss: -0.03402284965333011\n",
      "          total_loss: -0.05871575638237927\n",
      "          vf_explained_var: -0.7053928375244141\n",
      "          vf_loss: 0.0002620820764807124\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_agent_steps_trained: 440000\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "  iterations_since_restore: 440\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.92692307692307\n",
      "    ram_util_percent: 57.25384615384615\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06728314665729047\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.35432767216856\n",
      "    mean_inference_ms: 2.454292503420747\n",
      "    mean_raw_obs_processing_ms: 0.8776976541447481\n",
      "  time_since_restore: 9653.795929908752\n",
      "  time_this_iter_s: 18.260993003845215\n",
      "  time_total_s: 9653.795929908752\n",
      "  timers:\n",
      "    learn_throughput: 301.462\n",
      "    learn_time_ms: 3317.17\n",
      "    load_throughput: 20854.692\n",
      "    load_time_ms: 47.951\n",
      "    sample_throughput: 63.186\n",
      "    sample_time_ms: 15826.187\n",
      "    update_time_ms: 7.839\n",
      "  timestamp: 1631887995\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 440\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   440</td><td style=\"text-align: right;\">          9653.8</td><td style=\"text-align: right;\">440000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 441000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-13-35\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 442\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12177453845506533\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.623038819101122\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007491108362922184\n",
      "          policy_loss: -0.03452975373301241\n",
      "          total_loss: -0.05948935478097862\n",
      "          vf_explained_var: -0.6232406497001648\n",
      "          vf_loss: 0.00035856102314129305\n",
      "    num_agent_steps_sampled: 441000\n",
      "    num_agent_steps_trained: 441000\n",
      "    num_steps_sampled: 441000\n",
      "    num_steps_trained: 441000\n",
      "  iterations_since_restore: 441\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.47500000000001\n",
      "    ram_util_percent: 57.342857142857156\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06728866752865487\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.348266915658035\n",
      "    mean_inference_ms: 2.454376197499348\n",
      "    mean_raw_obs_processing_ms: 0.8774286454463429\n",
      "  time_since_restore: 9673.516629219055\n",
      "  time_this_iter_s: 19.720699310302734\n",
      "  time_total_s: 9673.516629219055\n",
      "  timers:\n",
      "    learn_throughput: 301.064\n",
      "    learn_time_ms: 3321.558\n",
      "    load_throughput: 20944.889\n",
      "    load_time_ms: 47.744\n",
      "    sample_throughput: 62.795\n",
      "    sample_time_ms: 15924.869\n",
      "    update_time_ms: 8.071\n",
      "  timestamp: 1631888015\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 441000\n",
      "  training_iteration: 441\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   441</td><td style=\"text-align: right;\">         9673.52</td><td style=\"text-align: right;\">441000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 442000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-13-55\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 443\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12177453845506533\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4896509766578676\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019618463331451952\n",
      "          policy_loss: -0.03781465498937501\n",
      "          total_loss: -0.05961268800828192\n",
      "          vf_explained_var: 0.26291152834892273\n",
      "          vf_loss: 0.0007094477353929404\n",
      "    num_agent_steps_sampled: 442000\n",
      "    num_agent_steps_trained: 442000\n",
      "    num_steps_sampled: 442000\n",
      "    num_steps_trained: 442000\n",
      "  iterations_since_restore: 442\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.85172413793104\n",
      "    ram_util_percent: 57.286206896551725\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06729419626930143\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.342234763089179\n",
      "    mean_inference_ms: 2.454460350241806\n",
      "    mean_raw_obs_processing_ms: 0.8771627792206008\n",
      "  time_since_restore: 9693.816770076752\n",
      "  time_this_iter_s: 20.300140857696533\n",
      "  time_total_s: 9693.816770076752\n",
      "  timers:\n",
      "    learn_throughput: 300.21\n",
      "    learn_time_ms: 3331.003\n",
      "    load_throughput: 20968.595\n",
      "    load_time_ms: 47.69\n",
      "    sample_throughput: 62.534\n",
      "    sample_time_ms: 15991.315\n",
      "    update_time_ms: 7.763\n",
      "  timestamp: 1631888035\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 442000\n",
      "  training_iteration: 442\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   442</td><td style=\"text-align: right;\">         9693.82</td><td style=\"text-align: right;\">442000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 443000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-14-14\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 444\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12177453845506533\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7032640006807114\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008528385002155787\n",
      "          policy_loss: -0.005972886499431398\n",
      "          total_loss: -0.03184189461171627\n",
      "          vf_explained_var: -0.36428478360176086\n",
      "          vf_loss: 0.0001250890224431108\n",
      "    num_agent_steps_sampled: 443000\n",
      "    num_agent_steps_trained: 443000\n",
      "    num_steps_sampled: 443000\n",
      "    num_steps_trained: 443000\n",
      "  iterations_since_restore: 443\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.37777777777777\n",
      "    ram_util_percent: 57.28148148148148\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06729965897042409\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.33617668071575\n",
      "    mean_inference_ms: 2.4545422073230068\n",
      "    mean_raw_obs_processing_ms: 0.8769010372729861\n",
      "  time_since_restore: 9712.478007793427\n",
      "  time_this_iter_s: 18.661237716674805\n",
      "  time_total_s: 9712.478007793427\n",
      "  timers:\n",
      "    learn_throughput: 298.999\n",
      "    learn_time_ms: 3344.494\n",
      "    load_throughput: 21987.139\n",
      "    load_time_ms: 45.481\n",
      "    sample_throughput: 62.467\n",
      "    sample_time_ms: 16008.381\n",
      "    update_time_ms: 7.574\n",
      "  timestamp: 1631888054\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 443000\n",
      "  training_iteration: 443\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   443</td><td style=\"text-align: right;\">         9712.48</td><td style=\"text-align: right;\">443000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 444000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-14-38\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 445\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12177453845506533\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5519860876931086\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012584415043301276\n",
      "          policy_loss: -0.05751279890537262\n",
      "          total_loss: -0.08114249921507305\n",
      "          vf_explained_var: -0.6914742588996887\n",
      "          vf_loss: 0.0003576990498155889\n",
      "    num_agent_steps_sampled: 444000\n",
      "    num_agent_steps_trained: 444000\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "  iterations_since_restore: 444\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.90294117647058\n",
      "    ram_util_percent: 57.305882352941175\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06730508401762579\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.33022543025354\n",
      "    mean_inference_ms: 2.4546224681023743\n",
      "    mean_raw_obs_processing_ms: 0.8766434746836574\n",
      "  time_since_restore: 9736.19045495987\n",
      "  time_this_iter_s: 23.71244716644287\n",
      "  time_total_s: 9736.19045495987\n",
      "  timers:\n",
      "    learn_throughput: 299.498\n",
      "    learn_time_ms: 3338.925\n",
      "    load_throughput: 20911.254\n",
      "    load_time_ms: 47.821\n",
      "    sample_throughput: 60.609\n",
      "    sample_time_ms: 16499.259\n",
      "    update_time_ms: 7.908\n",
      "  timestamp: 1631888078\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 444\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   444</td><td style=\"text-align: right;\">         9736.19</td><td style=\"text-align: right;\">444000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 445000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-15-00\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 446\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12177453845506533\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7126666327317557\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015707821770307717\n",
      "          policy_loss: 0.10149090356296964\n",
      "          total_loss: 0.08672370066245397\n",
      "          vf_explained_var: 0.21496105194091797\n",
      "          vf_loss: 0.00044664785172143534\n",
      "    num_agent_steps_sampled: 445000\n",
      "    num_agent_steps_trained: 445000\n",
      "    num_steps_sampled: 445000\n",
      "    num_steps_trained: 445000\n",
      "  iterations_since_restore: 445\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.49354838709678\n",
      "    ram_util_percent: 57.20645161290322\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06731055695649549\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.324396947166868\n",
      "    mean_inference_ms: 2.4547035459521647\n",
      "    mean_raw_obs_processing_ms: 0.876391137974013\n",
      "  time_since_restore: 9758.31527709961\n",
      "  time_this_iter_s: 22.12482213973999\n",
      "  time_total_s: 9758.31527709961\n",
      "  timers:\n",
      "    learn_throughput: 298.437\n",
      "    learn_time_ms: 3350.795\n",
      "    load_throughput: 20987.281\n",
      "    load_time_ms: 47.648\n",
      "    sample_throughput: 60.108\n",
      "    sample_time_ms: 16636.739\n",
      "    update_time_ms: 7.261\n",
      "  timestamp: 1631888100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 445000\n",
      "  training_iteration: 445\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   445</td><td style=\"text-align: right;\">         9758.32</td><td style=\"text-align: right;\">445000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 446000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-15-20\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 447\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12177453845506533\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9807809975412156\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016009484737873503\n",
      "          policy_loss: -0.04622587660948436\n",
      "          total_loss: -0.06318163091523779\n",
      "          vf_explained_var: -0.2661104202270508\n",
      "          vf_loss: 0.0009025027257141321\n",
      "    num_agent_steps_sampled: 446000\n",
      "    num_agent_steps_trained: 446000\n",
      "    num_steps_sampled: 446000\n",
      "    num_steps_trained: 446000\n",
      "  iterations_since_restore: 446\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.89999999999999\n",
      "    ram_util_percent: 57.28620689655173\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06731594234187178\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.318624075521225\n",
      "    mean_inference_ms: 2.454782637246914\n",
      "    mean_raw_obs_processing_ms: 0.8761442833914505\n",
      "  time_since_restore: 9778.691479682922\n",
      "  time_this_iter_s: 20.37620258331299\n",
      "  time_total_s: 9778.691479682922\n",
      "  timers:\n",
      "    learn_throughput: 297.617\n",
      "    learn_time_ms: 3360.024\n",
      "    load_throughput: 21282.694\n",
      "    load_time_ms: 46.987\n",
      "    sample_throughput: 59.457\n",
      "    sample_time_ms: 16818.844\n",
      "    update_time_ms: 7.273\n",
      "  timestamp: 1631888120\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 446000\n",
      "  training_iteration: 446\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   446</td><td style=\"text-align: right;\">         9778.69</td><td style=\"text-align: right;\">446000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 447000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-15-39\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 448\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12177453845506533\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.641496131155226\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012552680992655576\n",
      "          policy_loss: 0.005699213304453426\n",
      "          total_loss: -0.019037325763040118\n",
      "          vf_explained_var: -0.1110762357711792\n",
      "          vf_loss: 0.00014982701149569443\n",
      "    num_agent_steps_sampled: 447000\n",
      "    num_agent_steps_trained: 447000\n",
      "    num_steps_sampled: 447000\n",
      "    num_steps_trained: 447000\n",
      "  iterations_since_restore: 447\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.78076923076922\n",
      "    ram_util_percent: 57.369230769230775\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06732116631981447\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.31283885353532\n",
      "    mean_inference_ms: 2.4548576215415583\n",
      "    mean_raw_obs_processing_ms: 0.8759020922191723\n",
      "  time_since_restore: 9796.883710622787\n",
      "  time_this_iter_s: 18.192230939865112\n",
      "  time_total_s: 9796.883710622787\n",
      "  timers:\n",
      "    learn_throughput: 297.133\n",
      "    learn_time_ms: 3365.498\n",
      "    load_throughput: 22318.782\n",
      "    load_time_ms: 44.805\n",
      "    sample_throughput: 60.647\n",
      "    sample_time_ms: 16488.851\n",
      "    update_time_ms: 7.55\n",
      "  timestamp: 1631888139\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 447000\n",
      "  training_iteration: 447\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   447</td><td style=\"text-align: right;\">         9796.88</td><td style=\"text-align: right;\">447000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 448000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-15-59\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 449\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12177453845506533\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.453835588031345\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0056670713586582065\n",
      "          policy_loss: -0.05021398237182034\n",
      "          total_loss: -0.06343801431357861\n",
      "          vf_explained_var: -0.9985901117324829\n",
      "          vf_loss: 0.0006242212498263042\n",
      "    num_agent_steps_sampled: 448000\n",
      "    num_agent_steps_trained: 448000\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "  iterations_since_restore: 448\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.98000000000002\n",
      "    ram_util_percent: 57.290000000000006\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06732637567197793\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.307124610463942\n",
      "    mean_inference_ms: 2.454933096246371\n",
      "    mean_raw_obs_processing_ms: 0.8756647681988059\n",
      "  time_since_restore: 9817.321182489395\n",
      "  time_this_iter_s: 20.437471866607666\n",
      "  time_total_s: 9817.321182489395\n",
      "  timers:\n",
      "    learn_throughput: 299.176\n",
      "    learn_time_ms: 3342.509\n",
      "    load_throughput: 22220.748\n",
      "    load_time_ms: 45.003\n",
      "    sample_throughput: 60.259\n",
      "    sample_time_ms: 16594.981\n",
      "    update_time_ms: 8.011\n",
      "  timestamp: 1631888159\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 448\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   448</td><td style=\"text-align: right;\">         9817.32</td><td style=\"text-align: right;\">448000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 449000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-16-40\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 450\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12177453845506533\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6810022089216443\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00602185953986021\n",
      "          policy_loss: -0.025127790909674434\n",
      "          total_loss: -0.05105205361420909\n",
      "          vf_explained_var: -0.8718673586845398\n",
      "          vf_loss: 0.00015245164686853563\n",
      "    num_agent_steps_sampled: 449000\n",
      "    num_agent_steps_trained: 449000\n",
      "    num_steps_sampled: 449000\n",
      "    num_steps_trained: 449000\n",
      "  iterations_since_restore: 449\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.68275862068964\n",
      "    ram_util_percent: 57.38448275862069\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06733164466315715\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.301477102035205\n",
      "    mean_inference_ms: 2.4550074416656984\n",
      "    mean_raw_obs_processing_ms: 0.8759006934337363\n",
      "  time_since_restore: 9858.433383703232\n",
      "  time_this_iter_s: 41.11220121383667\n",
      "  time_total_s: 9858.433383703232\n",
      "  timers:\n",
      "    learn_throughput: 299.242\n",
      "    learn_time_ms: 3341.774\n",
      "    load_throughput: 22516.005\n",
      "    load_time_ms: 44.413\n",
      "    sample_throughput: 52.949\n",
      "    sample_time_ms: 18886.179\n",
      "    update_time_ms: 8.145\n",
      "  timestamp: 1631888200\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 449000\n",
      "  training_iteration: 449\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   449</td><td style=\"text-align: right;\">         9858.43</td><td style=\"text-align: right;\">449000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 450000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-17-00\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 451\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12177453845506533\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3230859683619605\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019914145932902055\n",
      "          policy_loss: -0.03422067765560415\n",
      "          total_loss: -0.0433813056598107\n",
      "          vf_explained_var: 0.05400201305747032\n",
      "          vf_loss: 0.0016451960377809073\n",
      "    num_agent_steps_sampled: 450000\n",
      "    num_agent_steps_trained: 450000\n",
      "    num_steps_sampled: 450000\n",
      "    num_steps_trained: 450000\n",
      "  iterations_since_restore: 450\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.37241379310343\n",
      "    ram_util_percent: 57.3448275862069\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06733680658911291\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.295880103317407\n",
      "    mean_inference_ms: 2.455079205144921\n",
      "    mean_raw_obs_processing_ms: 0.876140052269688\n",
      "  time_since_restore: 9878.379511594772\n",
      "  time_this_iter_s: 19.946127891540527\n",
      "  time_total_s: 9878.379511594772\n",
      "  timers:\n",
      "    learn_throughput: 298.745\n",
      "    learn_time_ms: 3347.334\n",
      "    load_throughput: 23118.411\n",
      "    load_time_ms: 43.256\n",
      "    sample_throughput: 52.491\n",
      "    sample_time_ms: 19050.852\n",
      "    update_time_ms: 7.769\n",
      "  timestamp: 1631888220\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 450000\n",
      "  training_iteration: 450\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   450</td><td style=\"text-align: right;\">         9878.38</td><td style=\"text-align: right;\">450000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 451000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-17-19\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 452\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12177453845506533\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1945432504018147\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007256555330985432\n",
      "          policy_loss: -0.02721389815625217\n",
      "          total_loss: -0.047862661717873484\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00041300401221633365\n",
      "    num_agent_steps_sampled: 451000\n",
      "    num_agent_steps_trained: 451000\n",
      "    num_steps_sampled: 451000\n",
      "    num_steps_trained: 451000\n",
      "  iterations_since_restore: 451\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.04999999999998\n",
      "    ram_util_percent: 57.38076923076923\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06734189702291977\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.290297721581247\n",
      "    mean_inference_ms: 2.4551497742852857\n",
      "    mean_raw_obs_processing_ms: 0.876383295416951\n",
      "  time_since_restore: 9896.648303508759\n",
      "  time_this_iter_s: 18.268791913986206\n",
      "  time_total_s: 9896.648303508759\n",
      "  timers:\n",
      "    learn_throughput: 300.179\n",
      "    learn_time_ms: 3331.342\n",
      "    load_throughput: 23096.21\n",
      "    load_time_ms: 43.297\n",
      "    sample_throughput: 52.849\n",
      "    sample_time_ms: 18921.778\n",
      "    update_time_ms: 7.514\n",
      "  timestamp: 1631888239\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 451000\n",
      "  training_iteration: 451\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   451</td><td style=\"text-align: right;\">         9896.65</td><td style=\"text-align: right;\">451000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 452000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-17-39\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 453\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12177453845506533\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8973057707150778\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017014958277370776\n",
      "          policy_loss: -0.03471099951614936\n",
      "          total_loss: -0.05124086058802075\n",
      "          vf_explained_var: -0.5929005742073059\n",
      "          vf_loss: 0.00037120742286636717\n",
      "    num_agent_steps_sampled: 452000\n",
      "    num_agent_steps_trained: 452000\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "  iterations_since_restore: 452\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.37586206896552\n",
      "    ram_util_percent: 57.47241379310343\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06734702390884933\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.284698712150652\n",
      "    mean_inference_ms: 2.4552183105185215\n",
      "    mean_raw_obs_processing_ms: 0.8766313353542499\n",
      "  time_since_restore: 9916.801041603088\n",
      "  time_this_iter_s: 20.152738094329834\n",
      "  time_total_s: 9916.801041603088\n",
      "  timers:\n",
      "    learn_throughput: 300.77\n",
      "    learn_time_ms: 3324.802\n",
      "    load_throughput: 24076.373\n",
      "    load_time_ms: 41.534\n",
      "    sample_throughput: 52.868\n",
      "    sample_time_ms: 18915.114\n",
      "    update_time_ms: 7.67\n",
      "  timestamp: 1631888259\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 452\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   452</td><td style=\"text-align: right;\">          9916.8</td><td style=\"text-align: right;\">452000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 453000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-17-57\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 454\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12177453845506533\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.666109636094835\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010957735236550864\n",
      "          policy_loss: -0.04502872170673476\n",
      "          total_loss: -0.07021971952377094\n",
      "          vf_explained_var: -0.6955724358558655\n",
      "          vf_loss: 0.00013572681752533654\n",
      "    num_agent_steps_sampled: 453000\n",
      "    num_agent_steps_trained: 453000\n",
      "    num_steps_sampled: 453000\n",
      "    num_steps_trained: 453000\n",
      "  iterations_since_restore: 453\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.18\n",
      "    ram_util_percent: 57.416\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06735218500061428\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.279084213372904\n",
      "    mean_inference_ms: 2.4552869046737067\n",
      "    mean_raw_obs_processing_ms: 0.8768831332303761\n",
      "  time_since_restore: 9934.719299793243\n",
      "  time_this_iter_s: 17.91825819015503\n",
      "  time_total_s: 9934.719299793243\n",
      "  timers:\n",
      "    learn_throughput: 301.714\n",
      "    learn_time_ms: 3314.401\n",
      "    load_throughput: 23605.252\n",
      "    load_time_ms: 42.363\n",
      "    sample_throughput: 53.046\n",
      "    sample_time_ms: 18851.51\n",
      "    update_time_ms: 7.111\n",
      "  timestamp: 1631888277\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 453000\n",
      "  training_iteration: 453\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   453</td><td style=\"text-align: right;\">         9934.72</td><td style=\"text-align: right;\">453000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 454000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-18-15\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 455\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12177453845506533\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3672903971539603\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008295371435336405\n",
      "          policy_loss: -0.06342636404765976\n",
      "          total_loss: -0.07435700086255868\n",
      "          vf_explained_var: -0.5956247448921204\n",
      "          vf_loss: 0.0017321014149729308\n",
      "    num_agent_steps_sampled: 454000\n",
      "    num_agent_steps_trained: 454000\n",
      "    num_steps_sampled: 454000\n",
      "    num_steps_trained: 454000\n",
      "  iterations_since_restore: 454\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.6851851851852\n",
      "    ram_util_percent: 57.45555555555556\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0673573758691893\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.273498506643298\n",
      "    mean_inference_ms: 2.4553563163578485\n",
      "    mean_raw_obs_processing_ms: 0.8771387824422954\n",
      "  time_since_restore: 9953.413687467575\n",
      "  time_this_iter_s: 18.694387674331665\n",
      "  time_total_s: 9953.413687467575\n",
      "  timers:\n",
      "    learn_throughput: 300.322\n",
      "    learn_time_ms: 3329.761\n",
      "    load_throughput: 24779.087\n",
      "    load_time_ms: 40.357\n",
      "    sample_throughput: 54.535\n",
      "    sample_time_ms: 18336.73\n",
      "    update_time_ms: 6.765\n",
      "  timestamp: 1631888295\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 454000\n",
      "  training_iteration: 454\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   454</td><td style=\"text-align: right;\">         9953.41</td><td style=\"text-align: right;\">454000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 455000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-18-34\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 456\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12177453845506533\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7377027140723333\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011405081690274146\n",
      "          policy_loss: -0.020884392534693083\n",
      "          total_loss: -0.046681597497728135\n",
      "          vf_explained_var: -0.6272464990615845\n",
      "          vf_loss: 0.0001909725888279152\n",
      "    num_agent_steps_sampled: 455000\n",
      "    num_agent_steps_trained: 455000\n",
      "    num_steps_sampled: 455000\n",
      "    num_steps_trained: 455000\n",
      "  iterations_since_restore: 455\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.99615384615385\n",
      "    ram_util_percent: 57.58461538461539\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06736261514910798\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.267937354371078\n",
      "    mean_inference_ms: 2.4554255966806964\n",
      "    mean_raw_obs_processing_ms: 0.8773982580109636\n",
      "  time_since_restore: 9971.870916843414\n",
      "  time_this_iter_s: 18.457229375839233\n",
      "  time_total_s: 9971.870916843414\n",
      "  timers:\n",
      "    learn_throughput: 298.367\n",
      "    learn_time_ms: 3351.577\n",
      "    load_throughput: 25602.094\n",
      "    load_time_ms: 39.059\n",
      "    sample_throughput: 55.713\n",
      "    sample_time_ms: 17949.231\n",
      "    update_time_ms: 6.939\n",
      "  timestamp: 1631888314\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 455000\n",
      "  training_iteration: 455\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   455</td><td style=\"text-align: right;\">         9971.87</td><td style=\"text-align: right;\">455000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 456000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-18-52\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 457\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12177453845506533\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.578715385331048\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010406557975682289\n",
      "          policy_loss: -0.031809034074346224\n",
      "          total_loss: -0.056227660158442126\n",
      "          vf_explained_var: -0.627547562122345\n",
      "          vf_loss: 0.00010127219134372151\n",
      "    num_agent_steps_sampled: 456000\n",
      "    num_agent_steps_trained: 456000\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "  iterations_since_restore: 456\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.52692307692308\n",
      "    ram_util_percent: 57.6076923076923\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06736774309027697\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.262310784389033\n",
      "    mean_inference_ms: 2.4554932201243904\n",
      "    mean_raw_obs_processing_ms: 0.8776607923115405\n",
      "  time_since_restore: 9990.07043838501\n",
      "  time_this_iter_s: 18.19952154159546\n",
      "  time_total_s: 9990.07043838501\n",
      "  timers:\n",
      "    learn_throughput: 300.516\n",
      "    learn_time_ms: 3327.612\n",
      "    load_throughput: 25151.513\n",
      "    load_time_ms: 39.759\n",
      "    sample_throughput: 56.322\n",
      "    sample_time_ms: 17755.07\n",
      "    update_time_ms: 6.891\n",
      "  timestamp: 1631888332\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 456\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   456</td><td style=\"text-align: right;\">         9990.07</td><td style=\"text-align: right;\">456000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 457000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-19-13\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 458\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12177453845506533\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3583771381320225\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.002525306581841723\n",
      "          policy_loss: 0.054665571658147706\n",
      "          total_loss: 0.05167645453992817\n",
      "          vf_explained_var: 0.22384963929653168\n",
      "          vf_loss: 0.00028713428643338074\n",
      "    num_agent_steps_sampled: 457000\n",
      "    num_agent_steps_trained: 457000\n",
      "    num_steps_sampled: 457000\n",
      "    num_steps_trained: 457000\n",
      "  iterations_since_restore: 457\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.43\n",
      "    ram_util_percent: 57.586666666666666\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06737269015160231\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.256772609357133\n",
      "    mean_inference_ms: 2.4555605464353736\n",
      "    mean_raw_obs_processing_ms: 0.877926732986682\n",
      "  time_since_restore: 10010.55519413948\n",
      "  time_this_iter_s: 20.484755754470825\n",
      "  time_total_s: 10010.55519413948\n",
      "  timers:\n",
      "    learn_throughput: 302.108\n",
      "    learn_time_ms: 3310.07\n",
      "    load_throughput: 24006.603\n",
      "    load_time_ms: 41.655\n",
      "    sample_throughput: 55.551\n",
      "    sample_time_ms: 18001.448\n",
      "    update_time_ms: 6.499\n",
      "  timestamp: 1631888353\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 457000\n",
      "  training_iteration: 457\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   457</td><td style=\"text-align: right;\">         10010.6</td><td style=\"text-align: right;\">457000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 458000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-19-32\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 459\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06088726922753267\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.368317405382792\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007158488564135645\n",
      "          policy_loss: -0.04748345087799761\n",
      "          total_loss: -0.06976218827896648\n",
      "          vf_explained_var: -0.38782835006713867\n",
      "          vf_loss: 0.0009685721797268343\n",
      "    num_agent_steps_sampled: 458000\n",
      "    num_agent_steps_trained: 458000\n",
      "    num_steps_sampled: 458000\n",
      "    num_steps_trained: 458000\n",
      "  iterations_since_restore: 458\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.39629629629628\n",
      "    ram_util_percent: 57.492592592592594\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06737763260548903\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.25121854989571\n",
      "    mean_inference_ms: 2.4556261515823596\n",
      "    mean_raw_obs_processing_ms: 0.8781972045485317\n",
      "  time_since_restore: 10029.36610007286\n",
      "  time_this_iter_s: 18.810905933380127\n",
      "  time_total_s: 10029.36610007286\n",
      "  timers:\n",
      "    learn_throughput: 302.459\n",
      "    learn_time_ms: 3306.235\n",
      "    load_throughput: 24083.741\n",
      "    load_time_ms: 41.522\n",
      "    sample_throughput: 56.095\n",
      "    sample_time_ms: 17826.751\n",
      "    update_time_ms: 6.067\n",
      "  timestamp: 1631888372\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 458000\n",
      "  training_iteration: 458\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   458</td><td style=\"text-align: right;\">         10029.4</td><td style=\"text-align: right;\">458000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 459000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-19-52\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 460\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06088726922753267\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9946177636583646\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005172814076711339\n",
      "          policy_loss: -0.0597515350414647\n",
      "          total_loss: -0.06915121682816082\n",
      "          vf_explained_var: 0.021541954949498177\n",
      "          vf_loss: 0.00023153882882777705\n",
      "    num_agent_steps_sampled: 459000\n",
      "    num_agent_steps_trained: 459000\n",
      "    num_steps_sampled: 459000\n",
      "    num_steps_trained: 459000\n",
      "  iterations_since_restore: 459\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.61785714285715\n",
      "    ram_util_percent: 57.582142857142856\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06738256353666579\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.24567171364858\n",
      "    mean_inference_ms: 2.4556923088214857\n",
      "    mean_raw_obs_processing_ms: 0.8779873563828913\n",
      "  time_since_restore: 10049.318764686584\n",
      "  time_this_iter_s: 19.952664613723755\n",
      "  time_total_s: 10049.318764686584\n",
      "  timers:\n",
      "    learn_throughput: 302.939\n",
      "    learn_time_ms: 3300.99\n",
      "    load_throughput: 24652.814\n",
      "    load_time_ms: 40.563\n",
      "    sample_throughput: 63.625\n",
      "    sample_time_ms: 15717.042\n",
      "    update_time_ms: 6.004\n",
      "  timestamp: 1631888392\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 459000\n",
      "  training_iteration: 459\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   459</td><td style=\"text-align: right;\">         10049.3</td><td style=\"text-align: right;\">459000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 460000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-20-12\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 461\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06088726922753267\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.530983734130859\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010089601518784224\n",
      "          policy_loss: -0.07294931775993771\n",
      "          total_loss: -0.09735613018274307\n",
      "          vf_explained_var: -0.29255056381225586\n",
      "          vf_loss: 0.0002886964658854494\n",
      "    num_agent_steps_sampled: 460000\n",
      "    num_agent_steps_trained: 460000\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "  iterations_since_restore: 460\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.40344827586208\n",
      "    ram_util_percent: 57.593103448275855\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06738748416623515\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.24012793982831\n",
      "    mean_inference_ms: 2.455756625067502\n",
      "    mean_raw_obs_processing_ms: 0.8777822664720871\n",
      "  time_since_restore: 10069.835763692856\n",
      "  time_this_iter_s: 20.516999006271362\n",
      "  time_total_s: 10069.835763692856\n",
      "  timers:\n",
      "    learn_throughput: 302.183\n",
      "    learn_time_ms: 3309.248\n",
      "    load_throughput: 23834.062\n",
      "    load_time_ms: 41.957\n",
      "    sample_throughput: 63.435\n",
      "    sample_time_ms: 15764.283\n",
      "    update_time_ms: 6.051\n",
      "  timestamp: 1631888412\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 460\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   460</td><td style=\"text-align: right;\">         10069.8</td><td style=\"text-align: right;\">460000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 461000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-20-30\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 462\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06088726922753267\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.072739793194665\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01276087996523611\n",
      "          policy_loss: -0.09400202946530448\n",
      "          total_loss: -0.11369005226426654\n",
      "          vf_explained_var: 0.024511927738785744\n",
      "          vf_loss: 0.000262398849306717\n",
      "    num_agent_steps_sampled: 461000\n",
      "    num_agent_steps_trained: 461000\n",
      "    num_steps_sampled: 461000\n",
      "    num_steps_trained: 461000\n",
      "  iterations_since_restore: 461\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.8576923076923\n",
      "    ram_util_percent: 57.56153846153846\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06739228635768205\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.234567509444314\n",
      "    mean_inference_ms: 2.4558156726597344\n",
      "    mean_raw_obs_processing_ms: 0.8775812581401812\n",
      "  time_since_restore: 10087.55429315567\n",
      "  time_this_iter_s: 17.71852946281433\n",
      "  time_total_s: 10087.55429315567\n",
      "  timers:\n",
      "    learn_throughput: 300.886\n",
      "    learn_time_ms: 3323.52\n",
      "    load_throughput: 23777.072\n",
      "    load_time_ms: 42.057\n",
      "    sample_throughput: 63.718\n",
      "    sample_time_ms: 15694.222\n",
      "    update_time_ms: 6.644\n",
      "  timestamp: 1631888430\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 461000\n",
      "  training_iteration: 461\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   461</td><td style=\"text-align: right;\">         10087.6</td><td style=\"text-align: right;\">461000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 462000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-20-49\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 463\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06088726922753267\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2190097723570135\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010464712275196566\n",
      "          policy_loss: -0.1131413336429331\n",
      "          total_loss: -0.12390681041611565\n",
      "          vf_explained_var: -0.3122045397758484\n",
      "          vf_loss: 0.0007874499818879283\n",
      "    num_agent_steps_sampled: 462000\n",
      "    num_agent_steps_trained: 462000\n",
      "    num_steps_sampled: 462000\n",
      "    num_steps_trained: 462000\n",
      "  iterations_since_restore: 462\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.01851851851852\n",
      "    ram_util_percent: 57.425925925925924\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06739706577759379\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.229045374429932\n",
      "    mean_inference_ms: 2.4558752929726535\n",
      "    mean_raw_obs_processing_ms: 0.877384083741928\n",
      "  time_since_restore: 10106.995249032974\n",
      "  time_this_iter_s: 19.440955877304077\n",
      "  time_total_s: 10106.995249032974\n",
      "  timers:\n",
      "    learn_throughput: 302.433\n",
      "    learn_time_ms: 3306.514\n",
      "    load_throughput: 23985.873\n",
      "    load_time_ms: 41.691\n",
      "    sample_throughput: 63.939\n",
      "    sample_time_ms: 15639.965\n",
      "    update_time_ms: 6.807\n",
      "  timestamp: 1631888449\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 462000\n",
      "  training_iteration: 462\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   462</td><td style=\"text-align: right;\">           10107</td><td style=\"text-align: right;\">462000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 463000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-21-09\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 464\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06088726922753267\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2454647888739905\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009535792345572346\n",
      "          policy_loss: -0.059208126159177886\n",
      "          total_loss: -0.0702360356433524\n",
      "          vf_explained_var: -0.0027121801394969225\n",
      "          vf_loss: 0.0008461283516630324\n",
      "    num_agent_steps_sampled: 463000\n",
      "    num_agent_steps_trained: 463000\n",
      "    num_steps_sampled: 463000\n",
      "    num_steps_trained: 463000\n",
      "  iterations_since_restore: 463\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.01379310344828\n",
      "    ram_util_percent: 57.60344827586207\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06740186841813188\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.223514676667287\n",
      "    mean_inference_ms: 2.4559350358243828\n",
      "    mean_raw_obs_processing_ms: 0.877191186249335\n",
      "  time_since_restore: 10126.684580087662\n",
      "  time_this_iter_s: 19.6893310546875\n",
      "  time_total_s: 10126.684580087662\n",
      "  timers:\n",
      "    learn_throughput: 303.679\n",
      "    learn_time_ms: 3292.949\n",
      "    load_throughput: 23238.351\n",
      "    load_time_ms: 43.032\n",
      "    sample_throughput: 63.175\n",
      "    sample_time_ms: 15829.129\n",
      "    update_time_ms: 6.775\n",
      "  timestamp: 1631888469\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 463000\n",
      "  training_iteration: 463\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   463</td><td style=\"text-align: right;\">         10126.7</td><td style=\"text-align: right;\">463000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 464000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-21-31\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 465\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06088726922753267\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5514810469415452\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00885568915539433\n",
      "          policy_loss: -0.05593184228572581\n",
      "          total_loss: -0.07038914863434102\n",
      "          vf_explained_var: -0.5174252986907959\n",
      "          vf_loss: 0.0005183065123371004\n",
      "    num_agent_steps_sampled: 464000\n",
      "    num_agent_steps_trained: 464000\n",
      "    num_steps_sampled: 464000\n",
      "    num_steps_trained: 464000\n",
      "  iterations_since_restore: 464\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.2290322580645\n",
      "    ram_util_percent: 57.56451612903226\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06740672500078772\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.218024867407273\n",
      "    mean_inference_ms: 2.455997869658296\n",
      "    mean_raw_obs_processing_ms: 0.8770029723705478\n",
      "  time_since_restore: 10148.673860788345\n",
      "  time_this_iter_s: 21.989280700683594\n",
      "  time_total_s: 10148.673860788345\n",
      "  timers:\n",
      "    learn_throughput: 303.574\n",
      "    learn_time_ms: 3294.094\n",
      "    load_throughput: 23643.962\n",
      "    load_time_ms: 42.294\n",
      "    sample_throughput: 61.888\n",
      "    sample_time_ms: 16158.33\n",
      "    update_time_ms: 6.702\n",
      "  timestamp: 1631888491\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 464000\n",
      "  training_iteration: 464\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   464</td><td style=\"text-align: right;\">         10148.7</td><td style=\"text-align: right;\">464000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 465000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-21-53\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 466\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06088726922753267\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0655089881685047\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.030535868006721107\n",
      "          policy_loss: -0.022330492734909058\n",
      "          total_loss: -0.040634355942408246\n",
      "          vf_explained_var: -0.06506825238466263\n",
      "          vf_loss: 0.0004919813508523576\n",
      "    num_agent_steps_sampled: 465000\n",
      "    num_agent_steps_trained: 465000\n",
      "    num_steps_sampled: 465000\n",
      "    num_steps_trained: 465000\n",
      "  iterations_since_restore: 465\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.6741935483871\n",
      "    ram_util_percent: 57.60967741935483\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06741145228069442\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.212640039376954\n",
      "    mean_inference_ms: 2.4560604121561425\n",
      "    mean_raw_obs_processing_ms: 0.8768195990923462\n",
      "  time_since_restore: 10170.31291103363\n",
      "  time_this_iter_s: 21.639050245285034\n",
      "  time_total_s: 10170.31291103363\n",
      "  timers:\n",
      "    learn_throughput: 305.851\n",
      "    learn_time_ms: 3269.567\n",
      "    load_throughput: 23952.341\n",
      "    load_time_ms: 41.75\n",
      "    sample_throughput: 60.6\n",
      "    sample_time_ms: 16501.582\n",
      "    update_time_ms: 6.603\n",
      "  timestamp: 1631888513\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 465000\n",
      "  training_iteration: 465\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   465</td><td style=\"text-align: right;\">         10170.3</td><td style=\"text-align: right;\">465000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 466000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-22-15\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 467\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09133090384129905\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.396990028106504\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004559627034772538\n",
      "          policy_loss: -0.04386145277983612\n",
      "          total_loss: -0.04707411188218329\n",
      "          vf_explained_var: -0.25020837783813477\n",
      "          vf_loss: 0.0003408064585477809\n",
      "    num_agent_steps_sampled: 466000\n",
      "    num_agent_steps_trained: 466000\n",
      "    num_steps_sampled: 466000\n",
      "    num_steps_trained: 466000\n",
      "  iterations_since_restore: 466\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.7\n",
      "    ram_util_percent: 57.553125\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06741600440721572\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.207355888576382\n",
      "    mean_inference_ms: 2.456123029587786\n",
      "    mean_raw_obs_processing_ms: 0.8766417077909966\n",
      "  time_since_restore: 10192.48856973648\n",
      "  time_this_iter_s: 22.175658702850342\n",
      "  time_total_s: 10192.48856973648\n",
      "  timers:\n",
      "    learn_throughput: 305.796\n",
      "    learn_time_ms: 3270.152\n",
      "    load_throughput: 24272.326\n",
      "    load_time_ms: 41.199\n",
      "    sample_throughput: 59.174\n",
      "    sample_time_ms: 16899.19\n",
      "    update_time_ms: 6.482\n",
      "  timestamp: 1631888535\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 466000\n",
      "  training_iteration: 466\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   466</td><td style=\"text-align: right;\">         10192.5</td><td style=\"text-align: right;\">466000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 467000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-22-36\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 468\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.045665451920649525\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.122266868750254\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014914114673041764\n",
      "          policy_loss: -0.051011890938712494\n",
      "          total_loss: -0.07135279847619434\n",
      "          vf_explained_var: -0.5712050795555115\n",
      "          vf_loss: 0.00020070370678695326\n",
      "    num_agent_steps_sampled: 467000\n",
      "    num_agent_steps_trained: 467000\n",
      "    num_steps_sampled: 467000\n",
      "    num_steps_trained: 467000\n",
      "  iterations_since_restore: 467\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.5103448275862\n",
      "    ram_util_percent: 57.503448275862056\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06742059671734985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.2021456420762\n",
      "    mean_inference_ms: 2.456188736146538\n",
      "    mean_raw_obs_processing_ms: 0.8764688691761774\n",
      "  time_since_restore: 10213.336911439896\n",
      "  time_this_iter_s: 20.848341703414917\n",
      "  time_total_s: 10213.336911439896\n",
      "  timers:\n",
      "    learn_throughput: 304.781\n",
      "    learn_time_ms: 3281.04\n",
      "    load_throughput: 24975.773\n",
      "    load_time_ms: 40.039\n",
      "    sample_throughput: 59.082\n",
      "    sample_time_ms: 16925.571\n",
      "    update_time_ms: 6.652\n",
      "  timestamp: 1631888556\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 467000\n",
      "  training_iteration: 467\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   467</td><td style=\"text-align: right;\">         10213.3</td><td style=\"text-align: right;\">467000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 468000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-22-56\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 469\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.045665451920649525\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.802607364124722\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020954665900566265\n",
      "          policy_loss: 0.011495308950543404\n",
      "          total_loss: -0.004945362814598613\n",
      "          vf_explained_var: 0.31724846363067627\n",
      "          vf_loss: 0.0006284968276961283\n",
      "    num_agent_steps_sampled: 468000\n",
      "    num_agent_steps_trained: 468000\n",
      "    num_steps_sampled: 468000\n",
      "    num_steps_trained: 468000\n",
      "  iterations_since_restore: 468\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.16896551724139\n",
      "    ram_util_percent: 57.679310344827584\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06742523700159667\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.19693129450114\n",
      "    mean_inference_ms: 2.456255230618103\n",
      "    mean_raw_obs_processing_ms: 0.876300462379395\n",
      "  time_since_restore: 10233.274913549423\n",
      "  time_this_iter_s: 19.938002109527588\n",
      "  time_total_s: 10233.274913549423\n",
      "  timers:\n",
      "    learn_throughput: 302.333\n",
      "    learn_time_ms: 3307.615\n",
      "    load_throughput: 25282.503\n",
      "    load_time_ms: 39.553\n",
      "    sample_throughput: 58.724\n",
      "    sample_time_ms: 17028.685\n",
      "    update_time_ms: 6.899\n",
      "  timestamp: 1631888576\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 468000\n",
      "  training_iteration: 468\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   468</td><td style=\"text-align: right;\">         10233.3</td><td style=\"text-align: right;\">468000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 469000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-23-14\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 470\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06849817788097423\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0412432312965394\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012834691027710695\n",
      "          policy_loss: -0.03435459463960595\n",
      "          total_loss: -0.053108514348665876\n",
      "          vf_explained_var: 0.34287601709365845\n",
      "          vf_loss: 0.0007793619751080567\n",
      "    num_agent_steps_sampled: 469000\n",
      "    num_agent_steps_trained: 469000\n",
      "    num_steps_sampled: 469000\n",
      "    num_steps_trained: 469000\n",
      "  iterations_since_restore: 469\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.33846153846153\n",
      "    ram_util_percent: 57.6576923076923\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06742981869151252\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.191730652113984\n",
      "    mean_inference_ms: 2.456323810554495\n",
      "    mean_raw_obs_processing_ms: 0.8761361388138234\n",
      "  time_since_restore: 10251.78427863121\n",
      "  time_this_iter_s: 18.50936508178711\n",
      "  time_total_s: 10251.78427863121\n",
      "  timers:\n",
      "    learn_throughput: 303.41\n",
      "    learn_time_ms: 3295.868\n",
      "    load_throughput: 24367.595\n",
      "    load_time_ms: 41.038\n",
      "    sample_throughput: 59.191\n",
      "    sample_time_ms: 16894.585\n",
      "    update_time_ms: 6.955\n",
      "  timestamp: 1631888594\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 469000\n",
      "  training_iteration: 469\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   469</td><td style=\"text-align: right;\">         10251.8</td><td style=\"text-align: right;\">469000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 470000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-23-36\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 471\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06849817788097423\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4461164945529567\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.002786448891222943\n",
      "          policy_loss: -0.005990066710445616\n",
      "          total_loss: -0.006640911805960867\n",
      "          vf_explained_var: 0.5629656314849854\n",
      "          vf_loss: 0.0036194526717331934\n",
      "    num_agent_steps_sampled: 470000\n",
      "    num_agent_steps_trained: 470000\n",
      "    num_steps_sampled: 470000\n",
      "    num_steps_trained: 470000\n",
      "  iterations_since_restore: 470\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.99032258064514\n",
      "    ram_util_percent: 57.58064516129031\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06743438499859121\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.186636121144618\n",
      "    mean_inference_ms: 2.4563942627015667\n",
      "    mean_raw_obs_processing_ms: 0.8759765196979314\n",
      "  time_since_restore: 10273.456258058548\n",
      "  time_this_iter_s: 21.671979427337646\n",
      "  time_total_s: 10273.456258058548\n",
      "  timers:\n",
      "    learn_throughput: 303.885\n",
      "    learn_time_ms: 3290.72\n",
      "    load_throughput: 24128.464\n",
      "    load_time_ms: 41.445\n",
      "    sample_throughput: 58.774\n",
      "    sample_time_ms: 17014.398\n",
      "    update_time_ms: 7.139\n",
      "  timestamp: 1631888616\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 470000\n",
      "  training_iteration: 470\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   470</td><td style=\"text-align: right;\">         10273.5</td><td style=\"text-align: right;\">470000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 471000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-23-55\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 472\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.034249088940487116\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.305202117231157\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010389102108391807\n",
      "          policy_loss: -0.033354941631356876\n",
      "          total_loss: -0.05516926824218697\n",
      "          vf_explained_var: -0.6695531010627747\n",
      "          vf_loss: 0.0008818793098448117\n",
      "    num_agent_steps_sampled: 471000\n",
      "    num_agent_steps_trained: 471000\n",
      "    num_steps_sampled: 471000\n",
      "    num_steps_trained: 471000\n",
      "  iterations_since_restore: 471\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.95000000000002\n",
      "    ram_util_percent: 57.41428571428571\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06743869987042428\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.181576444840239\n",
      "    mean_inference_ms: 2.456464929786732\n",
      "    mean_raw_obs_processing_ms: 0.8758208360085878\n",
      "  time_since_restore: 10292.8786444664\n",
      "  time_this_iter_s: 19.422386407852173\n",
      "  time_total_s: 10292.8786444664\n",
      "  timers:\n",
      "    learn_throughput: 305.006\n",
      "    learn_time_ms: 3278.623\n",
      "    load_throughput: 24059.4\n",
      "    load_time_ms: 41.564\n",
      "    sample_throughput: 58.147\n",
      "    sample_time_ms: 17197.65\n",
      "    update_time_ms: 6.536\n",
      "  timestamp: 1631888635\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 471000\n",
      "  training_iteration: 471\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   471</td><td style=\"text-align: right;\">         10292.9</td><td style=\"text-align: right;\">471000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 472000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-24-17\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 473\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.034249088940487116\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7763199281361368\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.042346070427771054\n",
      "          policy_loss: 0.11399242298470603\n",
      "          total_loss: 0.10822935435507032\n",
      "          vf_explained_var: 0.6536979675292969\n",
      "          vf_loss: 0.0005498179815024034\n",
      "    num_agent_steps_sampled: 472000\n",
      "    num_agent_steps_trained: 472000\n",
      "    num_steps_sampled: 472000\n",
      "    num_steps_trained: 472000\n",
      "  iterations_since_restore: 472\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.10967741935484\n",
      "    ram_util_percent: 57.58064516129032\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06744305315299992\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.176611260155573\n",
      "    mean_inference_ms: 2.4565359731398253\n",
      "    mean_raw_obs_processing_ms: 0.8756699417972098\n",
      "  time_since_restore: 10314.392652988434\n",
      "  time_this_iter_s: 21.51400852203369\n",
      "  time_total_s: 10314.392652988434\n",
      "  timers:\n",
      "    learn_throughput: 303.899\n",
      "    learn_time_ms: 3290.565\n",
      "    load_throughput: 23223.49\n",
      "    load_time_ms: 43.06\n",
      "    sample_throughput: 57.498\n",
      "    sample_time_ms: 17391.971\n",
      "    update_time_ms: 6.228\n",
      "  timestamp: 1631888657\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 472000\n",
      "  training_iteration: 472\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   472</td><td style=\"text-align: right;\">         10314.4</td><td style=\"text-align: right;\">472000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 473000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-24-40\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 474\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.05137363341073069\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.540102587805854\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.027133606059722443\n",
      "          policy_loss: -0.006584450933668349\n",
      "          total_loss: -0.030234960714975993\n",
      "          vf_explained_var: -0.12332286685705185\n",
      "          vf_loss: 0.00035655863947441606\n",
      "    num_agent_steps_sampled: 473000\n",
      "    num_agent_steps_trained: 473000\n",
      "    num_steps_sampled: 473000\n",
      "    num_steps_trained: 473000\n",
      "  iterations_since_restore: 473\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.171875\n",
      "    ram_util_percent: 57.587500000000006\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06744748728597755\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.171740839894778\n",
      "    mean_inference_ms: 2.4566065348798447\n",
      "    mean_raw_obs_processing_ms: 0.8755224697027092\n",
      "  time_since_restore: 10336.99176621437\n",
      "  time_this_iter_s: 22.59911322593689\n",
      "  time_total_s: 10336.99176621437\n",
      "  timers:\n",
      "    learn_throughput: 302.879\n",
      "    learn_time_ms: 3301.648\n",
      "    load_throughput: 23733.937\n",
      "    load_time_ms: 42.134\n",
      "    sample_throughput: 56.585\n",
      "    sample_time_ms: 17672.662\n",
      "    update_time_ms: 6.575\n",
      "  timestamp: 1631888680\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 473000\n",
      "  training_iteration: 473\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   473</td><td style=\"text-align: right;\">           10337</td><td style=\"text-align: right;\">473000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 474000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-24-58\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 475\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07706045011609605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.537904821501838\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00955412340334411\n",
      "          policy_loss: 0.008886739363272984\n",
      "          total_loss: -0.015604344341490004\n",
      "          vf_explained_var: 0.07411803305149078\n",
      "          vf_loss: 0.00015171740023510615\n",
      "    num_agent_steps_sampled: 474000\n",
      "    num_agent_steps_trained: 474000\n",
      "    num_steps_sampled: 474000\n",
      "    num_steps_trained: 474000\n",
      "  iterations_since_restore: 474\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.41923076923077\n",
      "    ram_util_percent: 57.54615384615384\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06745180069619519\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.1668883840983\n",
      "    mean_inference_ms: 2.4566757438315885\n",
      "    mean_raw_obs_processing_ms: 0.8753793169235824\n",
      "  time_since_restore: 10355.010406017303\n",
      "  time_this_iter_s: 18.01863980293274\n",
      "  time_total_s: 10355.010406017303\n",
      "  timers:\n",
      "    learn_throughput: 303.822\n",
      "    learn_time_ms: 3291.401\n",
      "    load_throughput: 22565.611\n",
      "    load_time_ms: 44.315\n",
      "    sample_throughput: 57.86\n",
      "    sample_time_ms: 17282.985\n",
      "    update_time_ms: 7.23\n",
      "  timestamp: 1631888698\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 474000\n",
      "  training_iteration: 474\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   474</td><td style=\"text-align: right;\">           10355</td><td style=\"text-align: right;\">474000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 475000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-25-18\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 476\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07706045011609605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4117024766074286\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006826200314286288\n",
      "          policy_loss: -0.022615923939479723\n",
      "          total_loss: -0.0459994295405017\n",
      "          vf_explained_var: -0.4140413999557495\n",
      "          vf_loss: 0.00020748829688272964\n",
      "    num_agent_steps_sampled: 475000\n",
      "    num_agent_steps_trained: 475000\n",
      "    num_steps_sampled: 475000\n",
      "    num_steps_trained: 475000\n",
      "  iterations_since_restore: 475\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.68928571428572\n",
      "    ram_util_percent: 57.667857142857144\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06745619747082382\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.162103559350044\n",
      "    mean_inference_ms: 2.456746138323838\n",
      "    mean_raw_obs_processing_ms: 0.875240174526002\n",
      "  time_since_restore: 10374.86188864708\n",
      "  time_this_iter_s: 19.851482629776\n",
      "  time_total_s: 10374.86188864708\n",
      "  timers:\n",
      "    learn_throughput: 302.902\n",
      "    learn_time_ms: 3301.397\n",
      "    load_throughput: 23103.258\n",
      "    load_time_ms: 43.284\n",
      "    sample_throughput: 58.494\n",
      "    sample_time_ms: 17095.691\n",
      "    update_time_ms: 6.982\n",
      "  timestamp: 1631888718\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 475000\n",
      "  training_iteration: 475\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   475</td><td style=\"text-align: right;\">         10374.9</td><td style=\"text-align: right;\">475000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 476000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-25-36\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 477\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07706045011609605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.529960584640503\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01442217966016395\n",
      "          policy_loss: -0.017319711794455846\n",
      "          total_loss: -0.0412733793258667\n",
      "          vf_explained_var: -0.7000537514686584\n",
      "          vf_loss: 0.00023455858563971156\n",
      "    num_agent_steps_sampled: 476000\n",
      "    num_agent_steps_trained: 476000\n",
      "    num_steps_sampled: 476000\n",
      "    num_steps_trained: 476000\n",
      "  iterations_since_restore: 476\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.71481481481482\n",
      "    ram_util_percent: 57.529629629629625\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06746048590833052\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.157335216540808\n",
      "    mean_inference_ms: 2.4568160355527384\n",
      "    mean_raw_obs_processing_ms: 0.8751042028084091\n",
      "  time_since_restore: 10393.271821975708\n",
      "  time_this_iter_s: 18.40993332862854\n",
      "  time_total_s: 10393.271821975708\n",
      "  timers:\n",
      "    learn_throughput: 301.471\n",
      "    learn_time_ms: 3317.072\n",
      "    load_throughput: 24781.942\n",
      "    load_time_ms: 40.352\n",
      "    sample_throughput: 59.86\n",
      "    sample_time_ms: 16705.633\n",
      "    update_time_ms: 7.422\n",
      "  timestamp: 1631888736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 476000\n",
      "  training_iteration: 476\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   476</td><td style=\"text-align: right;\">         10393.3</td><td style=\"text-align: right;\">476000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 477000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-25-55\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 478\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07706045011609605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.608019550641378\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.026329895452219452\n",
      "          policy_loss: -0.030989828954140344\n",
      "          total_loss: -0.054838336093558204\n",
      "          vf_explained_var: -0.056962378323078156\n",
      "          vf_loss: 0.0002026959330186805\n",
      "    num_agent_steps_sampled: 477000\n",
      "    num_agent_steps_trained: 477000\n",
      "    num_steps_sampled: 477000\n",
      "    num_steps_trained: 477000\n",
      "  iterations_since_restore: 477\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.9888888888889\n",
      "    ram_util_percent: 57.474074074074075\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06746470248771033\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.152597046648273\n",
      "    mean_inference_ms: 2.4568861689060633\n",
      "    mean_raw_obs_processing_ms: 0.8749723077022\n",
      "  time_since_restore: 10412.244311332703\n",
      "  time_this_iter_s: 18.97248935699463\n",
      "  time_total_s: 10412.244311332703\n",
      "  timers:\n",
      "    learn_throughput: 300.121\n",
      "    learn_time_ms: 3331.984\n",
      "    load_throughput: 23826.21\n",
      "    load_time_ms: 41.971\n",
      "    sample_throughput: 60.601\n",
      "    sample_time_ms: 16501.387\n",
      "    update_time_ms: 7.414\n",
      "  timestamp: 1631888755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 477000\n",
      "  training_iteration: 477\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   477</td><td style=\"text-align: right;\">         10412.2</td><td style=\"text-align: right;\">477000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 478000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-26-35\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 480\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11559067517414401\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6306171152326794\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013846151509619986\n",
      "          policy_loss: -0.03371066740817494\n",
      "          total_loss: -0.05829522502091196\n",
      "          vf_explained_var: 0.15396815538406372\n",
      "          vf_loss: 0.00012112802272289022\n",
      "    num_agent_steps_sampled: 478000\n",
      "    num_agent_steps_trained: 478000\n",
      "    num_steps_sampled: 478000\n",
      "    num_steps_trained: 478000\n",
      "  iterations_since_restore: 478\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.54464285714286\n",
      "    ram_util_percent: 57.596428571428575\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06747303619577043\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.143172515415888\n",
      "    mean_inference_ms: 2.4570246178065624\n",
      "    mean_raw_obs_processing_ms: 0.8755885216887058\n",
      "  time_since_restore: 10451.902071475983\n",
      "  time_this_iter_s: 39.65776014328003\n",
      "  time_total_s: 10451.902071475983\n",
      "  timers:\n",
      "    learn_throughput: 300.721\n",
      "    learn_time_ms: 3325.346\n",
      "    load_throughput: 23720.849\n",
      "    load_time_ms: 42.157\n",
      "    sample_throughput: 54.113\n",
      "    sample_time_ms: 18479.787\n",
      "    update_time_ms: 7.293\n",
      "  timestamp: 1631888795\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 478000\n",
      "  training_iteration: 478\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   478</td><td style=\"text-align: right;\">         10451.9</td><td style=\"text-align: right;\">478000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 479000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-26-57\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 481\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11559067517414401\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.222937375969357\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009838743810675036\n",
      "          policy_loss: 0.040407797694206236\n",
      "          total_loss: 0.019806493446230887\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0004908008054169638\n",
      "    num_agent_steps_sampled: 479000\n",
      "    num_agent_steps_trained: 479000\n",
      "    num_steps_sampled: 479000\n",
      "    num_steps_trained: 479000\n",
      "  iterations_since_restore: 479\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.90625\n",
      "    ram_util_percent: 57.45625\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06747705101302734\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.138572543580405\n",
      "    mean_inference_ms: 2.4570920249329897\n",
      "    mean_raw_obs_processing_ms: 0.875907962867036\n",
      "  time_since_restore: 10474.028589487076\n",
      "  time_this_iter_s: 22.12651801109314\n",
      "  time_total_s: 10474.028589487076\n",
      "  timers:\n",
      "    learn_throughput: 302.127\n",
      "    learn_time_ms: 3309.865\n",
      "    load_throughput: 23785.283\n",
      "    load_time_ms: 42.043\n",
      "    sample_throughput: 53.03\n",
      "    sample_time_ms: 18857.101\n",
      "    update_time_ms: 7.3\n",
      "  timestamp: 1631888817\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 479000\n",
      "  training_iteration: 479\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   479</td><td style=\"text-align: right;\">           10474</td><td style=\"text-align: right;\">479000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 480000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-27-17\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 482\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11559067517414401\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7194816430409747\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010276735528798842\n",
      "          policy_loss: -0.026586795018778905\n",
      "          total_loss: -0.052460826685031256\n",
      "          vf_explained_var: -0.7933923602104187\n",
      "          vf_loss: 0.00013289139666691578\n",
      "    num_agent_steps_sampled: 480000\n",
      "    num_agent_steps_trained: 480000\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "  iterations_since_restore: 480\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.78571428571429\n",
      "    ram_util_percent: 57.58571428571429\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06748115215625373\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.134023429745675\n",
      "    mean_inference_ms: 2.457158458760583\n",
      "    mean_raw_obs_processing_ms: 0.8762293560588073\n",
      "  time_since_restore: 10493.811538934708\n",
      "  time_this_iter_s: 19.782949447631836\n",
      "  time_total_s: 10493.811538934708\n",
      "  timers:\n",
      "    learn_throughput: 301.475\n",
      "    learn_time_ms: 3317.024\n",
      "    load_throughput: 24988.079\n",
      "    load_time_ms: 40.019\n",
      "    sample_throughput: 53.582\n",
      "    sample_time_ms: 18663.059\n",
      "    update_time_ms: 7.302\n",
      "  timestamp: 1631888837\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 480\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   480</td><td style=\"text-align: right;\">         10493.8</td><td style=\"text-align: right;\">480000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 481000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-27-35\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 483\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11559067517414401\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.532224522696601\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009572884214486403\n",
      "          policy_loss: 0.07335307912694083\n",
      "          total_loss: 0.049358748189277116\n",
      "          vf_explained_var: -0.659435510635376\n",
      "          vf_loss: 0.00022137700000510248\n",
      "    num_agent_steps_sampled: 481000\n",
      "    num_agent_steps_trained: 481000\n",
      "    num_steps_sampled: 481000\n",
      "    num_steps_trained: 481000\n",
      "  iterations_since_restore: 481\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.23461538461538\n",
      "    ram_util_percent: 57.71538461538462\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06748519651119995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.129513883530755\n",
      "    mean_inference_ms: 2.4572246966747078\n",
      "    mean_raw_obs_processing_ms: 0.8765533461502857\n",
      "  time_since_restore: 10512.213130950928\n",
      "  time_this_iter_s: 18.401592016220093\n",
      "  time_total_s: 10512.213130950928\n",
      "  timers:\n",
      "    learn_throughput: 300.174\n",
      "    learn_time_ms: 3331.396\n",
      "    load_throughput: 26068.79\n",
      "    load_time_ms: 38.36\n",
      "    sample_throughput: 53.918\n",
      "    sample_time_ms: 18546.655\n",
      "    update_time_ms: 7.78\n",
      "  timestamp: 1631888855\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 481000\n",
      "  training_iteration: 481\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   481</td><td style=\"text-align: right;\">         10512.2</td><td style=\"text-align: right;\">481000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 482000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-27-54\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 484\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11559067517414401\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7705679125256006\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003458475189043379\n",
      "          policy_loss: -0.11276661819881863\n",
      "          total_loss: -0.14005896498759587\n",
      "          vf_explained_var: -0.8868134617805481\n",
      "          vf_loss: 1.3564649416113954e-05\n",
      "    num_agent_steps_sampled: 482000\n",
      "    num_agent_steps_trained: 482000\n",
      "    num_steps_sampled: 482000\n",
      "    num_steps_trained: 482000\n",
      "  iterations_since_restore: 482\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.78148148148149\n",
      "    ram_util_percent: 57.75185185185185\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06748915421014567\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.125031356452903\n",
      "    mean_inference_ms: 2.457287747702431\n",
      "    mean_raw_obs_processing_ms: 0.8768797731083944\n",
      "  time_since_restore: 10530.985517501831\n",
      "  time_this_iter_s: 18.77238655090332\n",
      "  time_total_s: 10530.985517501831\n",
      "  timers:\n",
      "    learn_throughput: 299.429\n",
      "    learn_time_ms: 3339.686\n",
      "    load_throughput: 25821.653\n",
      "    load_time_ms: 38.727\n",
      "    sample_throughput: 54.754\n",
      "    sample_time_ms: 18263.643\n",
      "    update_time_ms: 8.107\n",
      "  timestamp: 1631888874\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 482000\n",
      "  training_iteration: 482\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   482</td><td style=\"text-align: right;\">           10531</td><td style=\"text-align: right;\">482000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 483000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-28-13\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 485\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.057795337587072004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7303270472420587\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005018119422552757\n",
      "          policy_loss: -0.040701075482906565\n",
      "          total_loss: -0.06766454618838098\n",
      "          vf_explained_var: -0.6052854061126709\n",
      "          vf_loss: 4.9775112408193915e-05\n",
      "    num_agent_steps_sampled: 483000\n",
      "    num_agent_steps_trained: 483000\n",
      "    num_steps_sampled: 483000\n",
      "    num_steps_trained: 483000\n",
      "  iterations_since_restore: 483\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.52142857142859\n",
      "    ram_util_percent: 57.775\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06749302694545059\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.120550106069903\n",
      "    mean_inference_ms: 2.4573486503469115\n",
      "    mean_raw_obs_processing_ms: 0.8772084354641244\n",
      "  time_since_restore: 10550.384374141693\n",
      "  time_this_iter_s: 19.39885663986206\n",
      "  time_total_s: 10550.384374141693\n",
      "  timers:\n",
      "    learn_throughput: 300.557\n",
      "    learn_time_ms: 3327.161\n",
      "    load_throughput: 25084.47\n",
      "    load_time_ms: 39.865\n",
      "    sample_throughput: 55.696\n",
      "    sample_time_ms: 17954.503\n",
      "    update_time_ms: 8.577\n",
      "  timestamp: 1631888893\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 483000\n",
      "  training_iteration: 483\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   483</td><td style=\"text-align: right;\">         10550.4</td><td style=\"text-align: right;\">483000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 484000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-28-33\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 486\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.057795337587072004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6537790245480006\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004838647955655478\n",
      "          policy_loss: -0.12603062391281128\n",
      "          total_loss: -0.1522703405883577\n",
      "          vf_explained_var: -0.3105494976043701\n",
      "          vf_loss: 1.8421845735853517e-05\n",
      "    num_agent_steps_sampled: 484000\n",
      "    num_agent_steps_trained: 484000\n",
      "    num_steps_sampled: 484000\n",
      "    num_steps_trained: 484000\n",
      "  iterations_since_restore: 484\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.13571428571427\n",
      "    ram_util_percent: 57.714285714285715\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06749682267273466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.116093063131764\n",
      "    mean_inference_ms: 2.45740589418575\n",
      "    mean_raw_obs_processing_ms: 0.8775396189003857\n",
      "  time_since_restore: 10569.264950275421\n",
      "  time_this_iter_s: 18.880576133728027\n",
      "  time_total_s: 10569.264950275421\n",
      "  timers:\n",
      "    learn_throughput: 301.187\n",
      "    learn_time_ms: 3320.192\n",
      "    load_throughput: 25022.425\n",
      "    load_time_ms: 39.964\n",
      "    sample_throughput: 55.409\n",
      "    sample_time_ms: 18047.516\n",
      "    update_time_ms: 8.471\n",
      "  timestamp: 1631888913\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 484000\n",
      "  training_iteration: 484\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   484</td><td style=\"text-align: right;\">         10569.3</td><td style=\"text-align: right;\">484000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 485000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-28-52\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 487\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.028897668793536002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4633710357877945\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007538227922844953\n",
      "          policy_loss: -0.024493449843592113\n",
      "          total_loss: -0.04884784316851033\n",
      "          vf_explained_var: -0.7779942750930786\n",
      "          vf_loss: 6.147812873172775e-05\n",
      "    num_agent_steps_sampled: 485000\n",
      "    num_agent_steps_trained: 485000\n",
      "    num_steps_sampled: 485000\n",
      "    num_steps_trained: 485000\n",
      "  iterations_since_restore: 485\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.38461538461539\n",
      "    ram_util_percent: 57.696153846153834\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06750059465002468\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.111686289447926\n",
      "    mean_inference_ms: 2.4574607227173555\n",
      "    mean_raw_obs_processing_ms: 0.8778712985846385\n",
      "  time_since_restore: 10587.98128914833\n",
      "  time_this_iter_s: 18.716338872909546\n",
      "  time_total_s: 10587.98128914833\n",
      "  timers:\n",
      "    learn_throughput: 302.998\n",
      "    learn_time_ms: 3300.357\n",
      "    load_throughput: 23181.249\n",
      "    load_time_ms: 43.138\n",
      "    sample_throughput: 55.708\n",
      "    sample_time_ms: 17950.768\n",
      "    update_time_ms: 8.401\n",
      "  timestamp: 1631888932\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 485000\n",
      "  training_iteration: 485\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   485</td><td style=\"text-align: right;\">           10588</td><td style=\"text-align: right;\">485000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 486000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-29-13\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 488\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.028897668793536002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.659690613216824\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008674489410090401\n",
      "          policy_loss: -0.07953675190607706\n",
      "          total_loss: -0.10586324623889393\n",
      "          vf_explained_var: -0.642365574836731\n",
      "          vf_loss: 1.9739901952359814e-05\n",
      "    num_agent_steps_sampled: 486000\n",
      "    num_agent_steps_trained: 486000\n",
      "    num_steps_sampled: 486000\n",
      "    num_steps_trained: 486000\n",
      "  iterations_since_restore: 486\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.05333333333333\n",
      "    ram_util_percent: 57.68333333333332\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06750439235637394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.107320646652582\n",
      "    mean_inference_ms: 2.4575155309768175\n",
      "    mean_raw_obs_processing_ms: 0.8782057142318672\n",
      "  time_since_restore: 10608.958825588226\n",
      "  time_this_iter_s: 20.97753643989563\n",
      "  time_total_s: 10608.958825588226\n",
      "  timers:\n",
      "    learn_throughput: 303.549\n",
      "    learn_time_ms: 3294.357\n",
      "    load_throughput: 21668.229\n",
      "    load_time_ms: 46.151\n",
      "    sample_throughput: 54.911\n",
      "    sample_time_ms: 18211.322\n",
      "    update_time_ms: 7.949\n",
      "  timestamp: 1631888953\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 486000\n",
      "  training_iteration: 486\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   486</td><td style=\"text-align: right;\">           10609</td><td style=\"text-align: right;\">486000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 487000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-29-33\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 489\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.028897668793536002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6575949986775718\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005933227208472265\n",
      "          policy_loss: -0.08475043657753202\n",
      "          total_loss: -0.11113936818308301\n",
      "          vf_explained_var: -0.7554783225059509\n",
      "          vf_loss: 1.556075374840778e-05\n",
      "    num_agent_steps_sampled: 487000\n",
      "    num_agent_steps_trained: 487000\n",
      "    num_steps_sampled: 487000\n",
      "    num_steps_trained: 487000\n",
      "  iterations_since_restore: 487\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.23103448275863\n",
      "    ram_util_percent: 57.762068965517244\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06750819612747665\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.102991272728241\n",
      "    mean_inference_ms: 2.4575699282056434\n",
      "    mean_raw_obs_processing_ms: 0.8785427628774475\n",
      "  time_since_restore: 10629.338417053223\n",
      "  time_this_iter_s: 20.379591464996338\n",
      "  time_total_s: 10629.338417053223\n",
      "  timers:\n",
      "    learn_throughput: 304.83\n",
      "    learn_time_ms: 3280.514\n",
      "    load_throughput: 22844.333\n",
      "    load_time_ms: 43.775\n",
      "    sample_throughput: 54.442\n",
      "    sample_time_ms: 18368.292\n",
      "    update_time_ms: 8.119\n",
      "  timestamp: 1631888973\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 487000\n",
      "  training_iteration: 487\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   487</td><td style=\"text-align: right;\">         10629.3</td><td style=\"text-align: right;\">487000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 488000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-29-50\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 490\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.028897668793536002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6679013464185926\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007346755615161705\n",
      "          policy_loss: -0.08879726309743192\n",
      "          total_loss: -0.11524526675542196\n",
      "          vf_explained_var: -0.5384464263916016\n",
      "          vf_loss: 1.870533336740563e-05\n",
      "    num_agent_steps_sampled: 488000\n",
      "    num_agent_steps_trained: 488000\n",
      "    num_steps_sampled: 488000\n",
      "    num_steps_trained: 488000\n",
      "  iterations_since_restore: 488\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.78399999999999\n",
      "    ram_util_percent: 57.86\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06751200126736681\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.098677410904402\n",
      "    mean_inference_ms: 2.4576234434236524\n",
      "    mean_raw_obs_processing_ms: 0.87839939067531\n",
      "  time_since_restore: 10646.78003358841\n",
      "  time_this_iter_s: 17.441616535186768\n",
      "  time_total_s: 10646.78003358841\n",
      "  timers:\n",
      "    learn_throughput: 305.383\n",
      "    learn_time_ms: 3274.574\n",
      "    load_throughput: 23052.099\n",
      "    load_time_ms: 43.38\n",
      "    sample_throughput: 61.908\n",
      "    sample_time_ms: 16152.993\n",
      "    update_time_ms: 7.925\n",
      "  timestamp: 1631888990\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 488000\n",
      "  training_iteration: 488\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   488</td><td style=\"text-align: right;\">         10646.8</td><td style=\"text-align: right;\">488000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 489000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-30-10\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 491\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.028897668793536002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5624329646428428\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006035462830244005\n",
      "          policy_loss: -0.13362680342462327\n",
      "          total_loss: -0.15904437344935204\n",
      "          vf_explained_var: -0.5286230444908142\n",
      "          vf_loss: 3.234958097285319e-05\n",
      "    num_agent_steps_sampled: 489000\n",
      "    num_agent_steps_trained: 489000\n",
      "    num_steps_sampled: 489000\n",
      "    num_steps_trained: 489000\n",
      "  iterations_since_restore: 489\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.60714285714286\n",
      "    ram_util_percent: 57.79285714285715\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06751582108995756\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.094370091413346\n",
      "    mean_inference_ms: 2.457677741931738\n",
      "    mean_raw_obs_processing_ms: 0.8782612475167982\n",
      "  time_since_restore: 10666.065079927444\n",
      "  time_this_iter_s: 19.285046339035034\n",
      "  time_total_s: 10666.065079927444\n",
      "  timers:\n",
      "    learn_throughput: 304.751\n",
      "    learn_time_ms: 3281.37\n",
      "    load_throughput: 24017.917\n",
      "    load_time_ms: 41.636\n",
      "    sample_throughput: 63.038\n",
      "    sample_time_ms: 15863.47\n",
      "    update_time_ms: 7.808\n",
      "  timestamp: 1631889010\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 489000\n",
      "  training_iteration: 489\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   489</td><td style=\"text-align: right;\">         10666.1</td><td style=\"text-align: right;\">489000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 490000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-30-31\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 492\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.028897668793536002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.574409286181132\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005479410861412205\n",
      "          policy_loss: -0.10474177218145794\n",
      "          total_loss: -0.1302899176047908\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 3.760535637411522e-05\n",
      "    num_agent_steps_sampled: 490000\n",
      "    num_agent_steps_trained: 490000\n",
      "    num_steps_sampled: 490000\n",
      "    num_steps_trained: 490000\n",
      "  iterations_since_restore: 490\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.23333333333333\n",
      "    ram_util_percent: 57.926666666666655\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06751968325799046\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.0901068041241\n",
      "    mean_inference_ms: 2.4577333292573753\n",
      "    mean_raw_obs_processing_ms: 0.878128365192991\n",
      "  time_since_restore: 10686.872223615646\n",
      "  time_this_iter_s: 20.807143688201904\n",
      "  time_total_s: 10686.872223615646\n",
      "  timers:\n",
      "    learn_throughput: 304.255\n",
      "    learn_time_ms: 3286.722\n",
      "    load_throughput: 23094.887\n",
      "    load_time_ms: 43.3\n",
      "    sample_throughput: 62.657\n",
      "    sample_time_ms: 15959.957\n",
      "    update_time_ms: 7.355\n",
      "  timestamp: 1631889031\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 490000\n",
      "  training_iteration: 490\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   490</td><td style=\"text-align: right;\">         10686.9</td><td style=\"text-align: right;\">490000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 491000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-30-51\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 493\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.028897668793536002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3967163032955594\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009997724336196593\n",
      "          policy_loss: -0.06847777428726355\n",
      "          total_loss: -0.09212523508403037\n",
      "          vf_explained_var: -0.4756556749343872\n",
      "          vf_loss: 3.079117469850543e-05\n",
      "    num_agent_steps_sampled: 491000\n",
      "    num_agent_steps_trained: 491000\n",
      "    num_steps_sampled: 491000\n",
      "    num_steps_trained: 491000\n",
      "  iterations_since_restore: 491\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.11785714285715\n",
      "    ram_util_percent: 57.807142857142864\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06752356415057335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.085896172627638\n",
      "    mean_inference_ms: 2.457791227285558\n",
      "    mean_raw_obs_processing_ms: 0.8780088464288955\n",
      "  time_since_restore: 10706.793078184128\n",
      "  time_this_iter_s: 19.920854568481445\n",
      "  time_total_s: 10706.793078184128\n",
      "  timers:\n",
      "    learn_throughput: 304.872\n",
      "    learn_time_ms: 3280.068\n",
      "    load_throughput: 22683.709\n",
      "    load_time_ms: 44.085\n",
      "    sample_throughput: 62.039\n",
      "    sample_time_ms: 16118.764\n",
      "    update_time_ms: 6.912\n",
      "  timestamp: 1631889051\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 491000\n",
      "  training_iteration: 491\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   491</td><td style=\"text-align: right;\">         10706.8</td><td style=\"text-align: right;\">491000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 492000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-31-11\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 494\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.028897668793536002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.43504974577162\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014520948128394403\n",
      "          policy_loss: -0.14249040939741664\n",
      "          total_loss: -0.1663231227133009\n",
      "          vf_explained_var: -0.9905728697776794\n",
      "          vf_loss: 9.816317857864002e-05\n",
      "    num_agent_steps_sampled: 492000\n",
      "    num_agent_steps_trained: 492000\n",
      "    num_steps_sampled: 492000\n",
      "    num_steps_trained: 492000\n",
      "  iterations_since_restore: 492\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.22413793103448\n",
      "    ram_util_percent: 58.00689655172414\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06752748386914081\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.081762592227726\n",
      "    mean_inference_ms: 2.4578520397610895\n",
      "    mean_raw_obs_processing_ms: 0.8778940727475121\n",
      "  time_since_restore: 10727.034290075302\n",
      "  time_this_iter_s: 20.241211891174316\n",
      "  time_total_s: 10727.034290075302\n",
      "  timers:\n",
      "    learn_throughput: 306.777\n",
      "    learn_time_ms: 3259.7\n",
      "    load_throughput: 22550.446\n",
      "    load_time_ms: 44.345\n",
      "    sample_throughput: 61.413\n",
      "    sample_time_ms: 16283.32\n",
      "    update_time_ms: 6.585\n",
      "  timestamp: 1631889071\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 492000\n",
      "  training_iteration: 492\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   492</td><td style=\"text-align: right;\">           10727</td><td style=\"text-align: right;\">492000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 493000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-31-35\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 495\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.028897668793536002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2952616307470532\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010162124795809073\n",
      "          policy_loss: -0.08760170488514835\n",
      "          total_loss: -0.1100768692791462\n",
      "          vf_explained_var: -0.9758238196372986\n",
      "          vf_loss: 0.00018378891068601257\n",
      "    num_agent_steps_sampled: 493000\n",
      "    num_agent_steps_trained: 493000\n",
      "    num_steps_sampled: 493000\n",
      "    num_steps_trained: 493000\n",
      "  iterations_since_restore: 493\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.92941176470588\n",
      "    ram_util_percent: 57.84117647058823\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0675314766675303\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.077751863452493\n",
      "    mean_inference_ms: 2.457914561069025\n",
      "    mean_raw_obs_processing_ms: 0.8777839551303735\n",
      "  time_since_restore: 10750.910349845886\n",
      "  time_this_iter_s: 23.876059770584106\n",
      "  time_total_s: 10750.910349845886\n",
      "  timers:\n",
      "    learn_throughput: 304.444\n",
      "    learn_time_ms: 3284.679\n",
      "    load_throughput: 22612.656\n",
      "    load_time_ms: 44.223\n",
      "    sample_throughput: 59.855\n",
      "    sample_time_ms: 16706.989\n",
      "    update_time_ms: 5.826\n",
      "  timestamp: 1631889095\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 493000\n",
      "  training_iteration: 493\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   493</td><td style=\"text-align: right;\">         10750.9</td><td style=\"text-align: right;\">493000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 494000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-31-58\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 496\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.028897668793536002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3500713215933904\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010675736513986919\n",
      "          policy_loss: -0.07692220840189191\n",
      "          total_loss: -0.09984098896384239\n",
      "          vf_explained_var: -0.8298978805541992\n",
      "          vf_loss: 0.0002734283760623334\n",
      "    num_agent_steps_sampled: 494000\n",
      "    num_agent_steps_trained: 494000\n",
      "    num_steps_sampled: 494000\n",
      "    num_steps_trained: 494000\n",
      "  iterations_since_restore: 494\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.51818181818182\n",
      "    ram_util_percent: 57.92424242424242\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06753555553804355\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.073863025507169\n",
      "    mean_inference_ms: 2.4579773773790574\n",
      "    mean_raw_obs_processing_ms: 0.8776775714406106\n",
      "  time_since_restore: 10773.77414727211\n",
      "  time_this_iter_s: 22.863797426223755\n",
      "  time_total_s: 10773.77414727211\n",
      "  timers:\n",
      "    learn_throughput: 304.183\n",
      "    learn_time_ms: 3287.49\n",
      "    load_throughput: 22384.437\n",
      "    load_time_ms: 44.674\n",
      "    sample_throughput: 58.47\n",
      "    sample_time_ms: 17102.731\n",
      "    update_time_ms: 5.405\n",
      "  timestamp: 1631889118\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 494000\n",
      "  training_iteration: 494\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   494</td><td style=\"text-align: right;\">         10773.8</td><td style=\"text-align: right;\">494000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 495000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-32-22\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 497\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.028897668793536002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1240750352541604\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00987833212644623\n",
      "          policy_loss: -0.1602950721979141\n",
      "          total_loss: -0.181168339567052\n",
      "          vf_explained_var: -0.5964279174804688\n",
      "          vf_loss: 8.202235488473283e-05\n",
      "    num_agent_steps_sampled: 495000\n",
      "    num_agent_steps_trained: 495000\n",
      "    num_steps_sampled: 495000\n",
      "    num_steps_trained: 495000\n",
      "  iterations_since_restore: 495\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.56764705882352\n",
      "    ram_util_percent: 57.694117647058825\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06753960920844997\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.070108096663763\n",
      "    mean_inference_ms: 2.458041048467648\n",
      "    mean_raw_obs_processing_ms: 0.8775750819711524\n",
      "  time_since_restore: 10797.658643007278\n",
      "  time_this_iter_s: 23.884495735168457\n",
      "  time_total_s: 10797.658643007278\n",
      "  timers:\n",
      "    learn_throughput: 302.372\n",
      "    learn_time_ms: 3307.179\n",
      "    load_throughput: 23412.756\n",
      "    load_time_ms: 42.712\n",
      "    sample_throughput: 56.814\n",
      "    sample_time_ms: 17601.449\n",
      "    update_time_ms: 5.466\n",
      "  timestamp: 1631889142\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 495000\n",
      "  training_iteration: 495\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   495</td><td style=\"text-align: right;\">         10797.7</td><td style=\"text-align: right;\">495000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 496000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-32-43\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 498\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.028897668793536002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5814486066500346\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011007736970502396\n",
      "          policy_loss: -0.18446284814013375\n",
      "          total_loss: -0.1962224945425987\n",
      "          vf_explained_var: -0.8123257756233215\n",
      "          vf_loss: 0.0037367444210960657\n",
      "    num_agent_steps_sampled: 496000\n",
      "    num_agent_steps_trained: 496000\n",
      "    num_steps_sampled: 496000\n",
      "    num_steps_trained: 496000\n",
      "  iterations_since_restore: 496\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.55333333333334\n",
      "    ram_util_percent: 57.73999999999999\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06754372786017879\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.066400083383158\n",
      "    mean_inference_ms: 2.4581062743749134\n",
      "    mean_raw_obs_processing_ms: 0.8774762558755398\n",
      "  time_since_restore: 10819.136708974838\n",
      "  time_this_iter_s: 21.478065967559814\n",
      "  time_total_s: 10819.136708974838\n",
      "  timers:\n",
      "    learn_throughput: 303.367\n",
      "    learn_time_ms: 3296.337\n",
      "    load_throughput: 23279.573\n",
      "    load_time_ms: 42.956\n",
      "    sample_throughput: 56.647\n",
      "    sample_time_ms: 17653.317\n",
      "    update_time_ms: 5.705\n",
      "  timestamp: 1631889163\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 496000\n",
      "  training_iteration: 496\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   496</td><td style=\"text-align: right;\">         10819.1</td><td style=\"text-align: right;\">496000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 497000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-33-05\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 499\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.028897668793536002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.24283957423435318\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0013429703280526913\n",
      "          policy_loss: -0.09572512159744899\n",
      "          total_loss: -0.09709762020243538\n",
      "          vf_explained_var: -0.5334974527359009\n",
      "          vf_loss: 0.001017086977588772\n",
      "    num_agent_steps_sampled: 497000\n",
      "    num_agent_steps_trained: 497000\n",
      "    num_steps_sampled: 497000\n",
      "    num_steps_trained: 497000\n",
      "  iterations_since_restore: 497\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.3483870967742\n",
      "    ram_util_percent: 57.66451612903226\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.067548193971199\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.062709790964174\n",
      "    mean_inference_ms: 2.458172016141392\n",
      "    mean_raw_obs_processing_ms: 0.8773807867438604\n",
      "  time_since_restore: 10840.506703615189\n",
      "  time_this_iter_s: 21.369994640350342\n",
      "  time_total_s: 10840.506703615189\n",
      "  timers:\n",
      "    learn_throughput: 301.492\n",
      "    learn_time_ms: 3316.84\n",
      "    load_throughput: 22763.855\n",
      "    load_time_ms: 43.929\n",
      "    sample_throughput: 56.398\n",
      "    sample_time_ms: 17731.079\n",
      "    update_time_ms: 5.484\n",
      "  timestamp: 1631889185\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 497000\n",
      "  training_iteration: 497\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   497</td><td style=\"text-align: right;\">         10840.5</td><td style=\"text-align: right;\">497000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 498000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-33-25\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 500\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.014448834396768001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8756245930989583\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0265183360355597\n",
      "          policy_loss: -0.03877437436539266\n",
      "          total_loss: -0.056355342786345214\n",
      "          vf_explained_var: -0.9598748087882996\n",
      "          vf_loss: 0.0007921161420300551\n",
      "    num_agent_steps_sampled: 498000\n",
      "    num_agent_steps_trained: 498000\n",
      "    num_steps_sampled: 498000\n",
      "    num_steps_trained: 498000\n",
      "  iterations_since_restore: 498\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.64137931034483\n",
      "    ram_util_percent: 57.92413793103449\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06755272192996023\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.059052763092929\n",
      "    mean_inference_ms: 2.458241483156868\n",
      "    mean_raw_obs_processing_ms: 0.8772899092665563\n",
      "  time_since_restore: 10861.096311807632\n",
      "  time_this_iter_s: 20.589608192443848\n",
      "  time_total_s: 10861.096311807632\n",
      "  timers:\n",
      "    learn_throughput: 301.427\n",
      "    learn_time_ms: 3317.554\n",
      "    load_throughput: 23320.241\n",
      "    load_time_ms: 42.881\n",
      "    sample_throughput: 55.414\n",
      "    sample_time_ms: 18045.86\n",
      "    update_time_ms: 6.084\n",
      "  timestamp: 1631889205\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 498000\n",
      "  training_iteration: 498\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   498</td><td style=\"text-align: right;\">         10861.1</td><td style=\"text-align: right;\">498000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 499000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-33-49\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 501\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.021673251595152015\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.1123921562400129\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02409174821528434\n",
      "          policy_loss: -0.1626317759354909\n",
      "          total_loss: -0.16312926250199478\n",
      "          vf_explained_var: 0.3735807240009308\n",
      "          vf_loss: 0.00010429106647886025\n",
      "    num_agent_steps_sampled: 499000\n",
      "    num_agent_steps_trained: 499000\n",
      "    num_steps_sampled: 499000\n",
      "    num_steps_trained: 499000\n",
      "  iterations_since_restore: 499\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.91142857142856\n",
      "    ram_util_percent: 57.85142857142858\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0675571587174124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.055509836256842\n",
      "    mean_inference_ms: 2.45831071590706\n",
      "    mean_raw_obs_processing_ms: 0.8772018120281742\n",
      "  time_since_restore: 10885.246164560318\n",
      "  time_this_iter_s: 24.149852752685547\n",
      "  time_total_s: 10885.246164560318\n",
      "  timers:\n",
      "    learn_throughput: 300.853\n",
      "    learn_time_ms: 3323.884\n",
      "    load_throughput: 22423.689\n",
      "    load_time_ms: 44.596\n",
      "    sample_throughput: 53.983\n",
      "    sample_time_ms: 18524.471\n",
      "    update_time_ms: 6.364\n",
      "  timestamp: 1631889229\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 499000\n",
      "  training_iteration: 499\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         10885.2</td><td style=\"text-align: right;\">499000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 500000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-34-13\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 502\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03250987739272802\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2875504904323156\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02089970023622314\n",
      "          policy_loss: -0.16423510693841512\n",
      "          total_loss: -0.1859818811217944\n",
      "          vf_explained_var: -0.9391829371452332\n",
      "          vf_loss: 0.00044928059930195255\n",
      "    num_agent_steps_sampled: 500000\n",
      "    num_agent_steps_trained: 500000\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "  iterations_since_restore: 500\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.12058823529412\n",
      "    ram_util_percent: 57.79117647058823\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06756160352499663\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.052084654800215\n",
      "    mean_inference_ms: 2.4583803722402453\n",
      "    mean_raw_obs_processing_ms: 0.8771177464296247\n",
      "  time_since_restore: 10909.185211658478\n",
      "  time_this_iter_s: 23.93904709815979\n",
      "  time_total_s: 10909.185211658478\n",
      "  timers:\n",
      "    learn_throughput: 301.114\n",
      "    learn_time_ms: 3321.003\n",
      "    load_throughput: 22182.214\n",
      "    load_time_ms: 45.081\n",
      "    sample_throughput: 53.083\n",
      "    sample_time_ms: 18838.438\n",
      "    update_time_ms: 6.918\n",
      "  timestamp: 1631889253\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 500\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         10909.2</td><td style=\"text-align: right;\">500000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 501000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-34-36\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 503\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.048764816089092014\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0539753913879393\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015339988381818397\n",
      "          policy_loss: -0.062015817542042995\n",
      "          total_loss: -0.08107718692885504\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.000730333416908656\n",
      "    num_agent_steps_sampled: 501000\n",
      "    num_agent_steps_trained: 501000\n",
      "    num_steps_sampled: 501000\n",
      "    num_steps_trained: 501000\n",
      "  iterations_since_restore: 501\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.06969696969698\n",
      "    ram_util_percent: 57.83030303030303\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06756610137206048\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.048740009981692\n",
      "    mean_inference_ms: 2.458452635329481\n",
      "    mean_raw_obs_processing_ms: 0.8770379043323095\n",
      "  time_since_restore: 10932.025650024414\n",
      "  time_this_iter_s: 22.84043836593628\n",
      "  time_total_s: 10932.025650024414\n",
      "  timers:\n",
      "    learn_throughput: 301.56\n",
      "    learn_time_ms: 3316.091\n",
      "    load_throughput: 21849.089\n",
      "    load_time_ms: 45.768\n",
      "    sample_throughput: 52.263\n",
      "    sample_time_ms: 19134.166\n",
      "    update_time_ms: 7.248\n",
      "  timestamp: 1631889276\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 501000\n",
      "  training_iteration: 501\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   501</td><td style=\"text-align: right;\">           10932</td><td style=\"text-align: right;\">501000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 502000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-34-57\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 504\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.048764816089092014\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3727987421883476\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010604600789105727\n",
      "          policy_loss: -0.09837819350262483\n",
      "          total_loss: -0.12129114282627901\n",
      "          vf_explained_var: -0.9954457879066467\n",
      "          vf_loss: 0.0002979075215989724\n",
      "    num_agent_steps_sampled: 502000\n",
      "    num_agent_steps_trained: 502000\n",
      "    num_steps_sampled: 502000\n",
      "    num_steps_trained: 502000\n",
      "  iterations_since_restore: 502\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.10689655172413\n",
      "    ram_util_percent: 57.88275862068966\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0675706232592331\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.045414969052851\n",
      "    mean_inference_ms: 2.4585243559215666\n",
      "    mean_raw_obs_processing_ms: 0.8769622159773873\n",
      "  time_since_restore: 10952.372980594635\n",
      "  time_this_iter_s: 20.347330570220947\n",
      "  time_total_s: 10952.372980594635\n",
      "  timers:\n",
      "    learn_throughput: 300.266\n",
      "    learn_time_ms: 3330.385\n",
      "    load_throughput: 22060.655\n",
      "    load_time_ms: 45.33\n",
      "    sample_throughput: 52.264\n",
      "    sample_time_ms: 19133.759\n",
      "    update_time_ms: 7.216\n",
      "  timestamp: 1631889297\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 502000\n",
      "  training_iteration: 502\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   502</td><td style=\"text-align: right;\">         10952.4</td><td style=\"text-align: right;\">502000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 503000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-35-16\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 505\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.048764816089092014\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2688973016209073\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021660072181967395\n",
      "          policy_loss: -0.07573002295361625\n",
      "          total_loss: -0.09708800833258364\n",
      "          vf_explained_var: -0.9270698428153992\n",
      "          vf_loss: 0.00027473837237468817\n",
      "    num_agent_steps_sampled: 503000\n",
      "    num_agent_steps_trained: 503000\n",
      "    num_steps_sampled: 503000\n",
      "    num_steps_trained: 503000\n",
      "  iterations_since_restore: 503\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.65\n",
      "    ram_util_percent: 57.857142857142875\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06757502970357288\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.042105251726086\n",
      "    mean_inference_ms: 2.4585943365724434\n",
      "    mean_raw_obs_processing_ms: 0.8768898327415822\n",
      "  time_since_restore: 10972.091755867004\n",
      "  time_this_iter_s: 19.718775272369385\n",
      "  time_total_s: 10972.091755867004\n",
      "  timers:\n",
      "    learn_throughput: 301.379\n",
      "    learn_time_ms: 3318.079\n",
      "    load_throughput: 22150.222\n",
      "    load_time_ms: 45.146\n",
      "    sample_throughput: 53.392\n",
      "    sample_time_ms: 18729.449\n",
      "    update_time_ms: 7.676\n",
      "  timestamp: 1631889316\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 503000\n",
      "  training_iteration: 503\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   503</td><td style=\"text-align: right;\">         10972.1</td><td style=\"text-align: right;\">503000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 504000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-35-38\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 506\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07314722413363806\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.410827491018507\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013777060229978186\n",
      "          policy_loss: -0.047209196082419816\n",
      "          total_loss: -0.06999300896293587\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0003167087758710194\n",
      "    num_agent_steps_sampled: 504000\n",
      "    num_agent_steps_trained: 504000\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "  iterations_since_restore: 504\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.36774193548388\n",
      "    ram_util_percent: 57.82903225806452\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06757936703201536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.03887133717609\n",
      "    mean_inference_ms: 2.4586634299642074\n",
      "    mean_raw_obs_processing_ms: 0.8768206767624949\n",
      "  time_since_restore: 10993.745669603348\n",
      "  time_this_iter_s: 21.653913736343384\n",
      "  time_total_s: 10993.745669603348\n",
      "  timers:\n",
      "    learn_throughput: 300.458\n",
      "    learn_time_ms: 3328.253\n",
      "    load_throughput: 22456.309\n",
      "    load_time_ms: 44.531\n",
      "    sample_throughput: 53.769\n",
      "    sample_time_ms: 18597.951\n",
      "    update_time_ms: 7.958\n",
      "  timestamp: 1631889338\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 504\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   504</td><td style=\"text-align: right;\">         10993.7</td><td style=\"text-align: right;\">504000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 505000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-35-59\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 507\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07314722413363806\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.257451672024197\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015691791195648227\n",
      "          policy_loss: -0.039591306603203216\n",
      "          total_loss: -0.0607847751946085\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00023323815548792481\n",
      "    num_agent_steps_sampled: 505000\n",
      "    num_agent_steps_trained: 505000\n",
      "    num_steps_sampled: 505000\n",
      "    num_steps_trained: 505000\n",
      "  iterations_since_restore: 505\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.65517241379312\n",
      "    ram_util_percent: 57.94137931034483\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06758364799633947\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.035650060076573\n",
      "    mean_inference_ms: 2.458732813445498\n",
      "    mean_raw_obs_processing_ms: 0.8767549345742692\n",
      "  time_since_restore: 11014.398465394974\n",
      "  time_this_iter_s: 20.652795791625977\n",
      "  time_total_s: 11014.398465394974\n",
      "  timers:\n",
      "    learn_throughput: 301.563\n",
      "    learn_time_ms: 3316.061\n",
      "    load_throughput: 21394.718\n",
      "    load_time_ms: 46.741\n",
      "    sample_throughput: 54.729\n",
      "    sample_time_ms: 18271.796\n",
      "    update_time_ms: 8.057\n",
      "  timestamp: 1631889359\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 505000\n",
      "  training_iteration: 505\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   505</td><td style=\"text-align: right;\">         11014.4</td><td style=\"text-align: right;\">505000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 506000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-36-21\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 508\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07314722413363806\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9233611947960323\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013907272384975045\n",
      "          policy_loss: 0.0033873151573869916\n",
      "          total_loss: -0.014283562948306402\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0005454546092854192\n",
      "    num_agent_steps_sampled: 506000\n",
      "    num_agent_steps_trained: 506000\n",
      "    num_steps_sampled: 506000\n",
      "    num_steps_trained: 506000\n",
      "  iterations_since_restore: 506\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.73548387096774\n",
      "    ram_util_percent: 57.92258064516129\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06758792788537361\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.03247667895404\n",
      "    mean_inference_ms: 2.458803626799353\n",
      "    mean_raw_obs_processing_ms: 0.8766930944693624\n",
      "  time_since_restore: 11036.215586423874\n",
      "  time_this_iter_s: 21.817121028900146\n",
      "  time_total_s: 11036.215586423874\n",
      "  timers:\n",
      "    learn_throughput: 299.705\n",
      "    learn_time_ms: 3336.618\n",
      "    load_throughput: 21927.699\n",
      "    load_time_ms: 45.604\n",
      "    sample_throughput: 54.862\n",
      "    sample_time_ms: 18227.564\n",
      "    update_time_ms: 7.946\n",
      "  timestamp: 1631889381\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 506000\n",
      "  training_iteration: 506\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   506</td><td style=\"text-align: right;\">         11036.2</td><td style=\"text-align: right;\">506000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 507000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-36-43\n",
      "  done: false\n",
      "  episode_len_mean: 996.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 509\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07314722413363806\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2816174387931825\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014828978943188728\n",
      "          policy_loss: -0.008391682048224741\n",
      "          total_loss: -0.02972927795102199\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0003938780385295912\n",
      "    num_agent_steps_sampled: 507000\n",
      "    num_agent_steps_trained: 507000\n",
      "    num_steps_sampled: 507000\n",
      "    num_steps_trained: 507000\n",
      "  iterations_since_restore: 507\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.32727272727273\n",
      "    ram_util_percent: 57.7969696969697\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06759219147755906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.029367324235528\n",
      "    mean_inference_ms: 2.458875535135429\n",
      "    mean_raw_obs_processing_ms: 0.8766338260321949\n",
      "  time_since_restore: 11058.75840473175\n",
      "  time_this_iter_s: 22.542818307876587\n",
      "  time_total_s: 11058.75840473175\n",
      "  timers:\n",
      "    learn_throughput: 301.949\n",
      "    learn_time_ms: 3311.817\n",
      "    load_throughput: 21412.926\n",
      "    load_time_ms: 46.701\n",
      "    sample_throughput: 54.44\n",
      "    sample_time_ms: 18368.713\n",
      "    update_time_ms: 7.808\n",
      "  timestamp: 1631889403\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 507000\n",
      "  training_iteration: 507\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   507</td><td style=\"text-align: right;\">         11058.8</td><td style=\"text-align: right;\">507000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 508000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-37-21\n",
      "  done: false\n",
      "  episode_len_mean: 995.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 510\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07314722413363806\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8791213572025298\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02064562115323468\n",
      "          policy_loss: -0.02596113748020596\n",
      "          total_loss: -0.042469850844807096\n",
      "          vf_explained_var: -0.5232135653495789\n",
      "          vf_loss: 0.0007723312712591401\n",
      "    num_agent_steps_sampled: 508000\n",
      "    num_agent_steps_trained: 508000\n",
      "    num_steps_sampled: 508000\n",
      "    num_steps_trained: 508000\n",
      "  iterations_since_restore: 508\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.66226415094339\n",
      "    ram_util_percent: 57.93396226415095\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0675964465884217\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.026269716293795\n",
      "    mean_inference_ms: 2.45894585665974\n",
      "    mean_raw_obs_processing_ms: 0.8769153532157803\n",
      "  time_since_restore: 11096.305051326752\n",
      "  time_this_iter_s: 37.54664659500122\n",
      "  time_total_s: 11096.305051326752\n",
      "  timers:\n",
      "    learn_throughput: 303.835\n",
      "    learn_time_ms: 3291.26\n",
      "    load_throughput: 20756.164\n",
      "    load_time_ms: 48.178\n",
      "    sample_throughput: 49.791\n",
      "    sample_time_ms: 20083.82\n",
      "    update_time_ms: 7.542\n",
      "  timestamp: 1631889441\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 508000\n",
      "  training_iteration: 508\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   508</td><td style=\"text-align: right;\">         11096.3</td><td style=\"text-align: right;\">508000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 509000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-37-43\n",
      "  done: false\n",
      "  episode_len_mean: 995.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 511\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10972083620045707\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4790609147813587\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006438130755853777\n",
      "          policy_loss: -0.12469698049955898\n",
      "          total_loss: -0.148728389872445\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 5.280212327407854e-05\n",
      "    num_agent_steps_sampled: 509000\n",
      "    num_agent_steps_trained: 509000\n",
      "    num_steps_sampled: 509000\n",
      "    num_steps_trained: 509000\n",
      "  iterations_since_restore: 509\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.03125\n",
      "    ram_util_percent: 57.909375000000004\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06760067307455736\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.02323316376603\n",
      "    mean_inference_ms: 2.4590156110772123\n",
      "    mean_raw_obs_processing_ms: 0.8771982574678399\n",
      "  time_since_restore: 11118.370289564133\n",
      "  time_this_iter_s: 22.06523823738098\n",
      "  time_total_s: 11118.370289564133\n",
      "  timers:\n",
      "    learn_throughput: 303.764\n",
      "    learn_time_ms: 3292.032\n",
      "    load_throughput: 20797.26\n",
      "    load_time_ms: 48.083\n",
      "    sample_throughput: 50.332\n",
      "    sample_time_ms: 19868.139\n",
      "    update_time_ms: 8.22\n",
      "  timestamp: 1631889463\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 509000\n",
      "  training_iteration: 509\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 72.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   509</td><td style=\"text-align: right;\">         11118.4</td><td style=\"text-align: right;\">509000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 510000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-38-06\n",
      "  done: false\n",
      "  episode_len_mean: 995.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 512\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10972083620045707\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4895880222320557\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0049258923161618505\n",
      "          policy_loss: -0.07473957373036279\n",
      "          total_loss: -0.09902638809548484\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 6.859269940630636e-05\n",
      "    num_agent_steps_sampled: 510000\n",
      "    num_agent_steps_trained: 510000\n",
      "    num_steps_sampled: 510000\n",
      "    num_steps_trained: 510000\n",
      "  iterations_since_restore: 510\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.284375\n",
      "    ram_util_percent: 58.00625\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06760487614919239\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.020290335704269\n",
      "    mean_inference_ms: 2.459083888404075\n",
      "    mean_raw_obs_processing_ms: 0.8774840957036367\n",
      "  time_since_restore: 11141.07978606224\n",
      "  time_this_iter_s: 22.70949649810791\n",
      "  time_total_s: 11141.07978606224\n",
      "  timers:\n",
      "    learn_throughput: 303.663\n",
      "    learn_time_ms: 3293.122\n",
      "    load_throughput: 21312.228\n",
      "    load_time_ms: 46.921\n",
      "    sample_throughput: 50.642\n",
      "    sample_time_ms: 19746.606\n",
      "    update_time_ms: 7.746\n",
      "  timestamp: 1631889486\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 510000\n",
      "  training_iteration: 510\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   510</td><td style=\"text-align: right;\">         11141.1</td><td style=\"text-align: right;\">510000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 511000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-38-27\n",
      "  done: false\n",
      "  episode_len_mean: 995.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 513\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.054860418100228535\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.414458303981357\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013396531897963188\n",
      "          policy_loss: -0.07797566051077512\n",
      "          total_loss: -0.10100639752215809\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0003789059021073626\n",
      "    num_agent_steps_sampled: 511000\n",
      "    num_agent_steps_trained: 511000\n",
      "    num_steps_sampled: 511000\n",
      "    num_steps_trained: 511000\n",
      "  iterations_since_restore: 511\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.51333333333332\n",
      "    ram_util_percent: 58.21000000000001\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06760908968223825\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.017411259909954\n",
      "    mean_inference_ms: 2.45914961594348\n",
      "    mean_raw_obs_processing_ms: 0.8777731841897228\n",
      "  time_since_restore: 11162.157286643982\n",
      "  time_this_iter_s: 21.077500581741333\n",
      "  time_total_s: 11162.157286643982\n",
      "  timers:\n",
      "    learn_throughput: 302.118\n",
      "    learn_time_ms: 3309.964\n",
      "    load_throughput: 21409.275\n",
      "    load_time_ms: 46.709\n",
      "    sample_throughput: 51.139\n",
      "    sample_time_ms: 19554.376\n",
      "    update_time_ms: 7.516\n",
      "  timestamp: 1631889507\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 511000\n",
      "  training_iteration: 511\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   511</td><td style=\"text-align: right;\">         11162.2</td><td style=\"text-align: right;\">511000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 512000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-38-52\n",
      "  done: false\n",
      "  episode_len_mean: 995.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 514\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.054860418100228535\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.098707440164354\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015318832278247789\n",
      "          policy_loss: -0.0530853271484375\n",
      "          total_loss: -0.07188038358257877\n",
      "          vf_explained_var: -0.5699307322502136\n",
      "          vf_loss: 0.0013516180747602548\n",
      "    num_agent_steps_sampled: 512000\n",
      "    num_agent_steps_trained: 512000\n",
      "    num_steps_sampled: 512000\n",
      "    num_steps_trained: 512000\n",
      "  iterations_since_restore: 512\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.19166666666668\n",
      "    ram_util_percent: 58.15833333333333\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06761331902288015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.014673332794573\n",
      "    mean_inference_ms: 2.4592161951241835\n",
      "    mean_raw_obs_processing_ms: 0.8780655508107741\n",
      "  time_since_restore: 11187.068008184433\n",
      "  time_this_iter_s: 24.91072154045105\n",
      "  time_total_s: 11187.068008184433\n",
      "  timers:\n",
      "    learn_throughput: 302.095\n",
      "    learn_time_ms: 3310.216\n",
      "    load_throughput: 21324.667\n",
      "    load_time_ms: 46.894\n",
      "    sample_throughput: 49.976\n",
      "    sample_time_ms: 20009.571\n",
      "    update_time_ms: 7.815\n",
      "  timestamp: 1631889532\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 512000\n",
      "  training_iteration: 512\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   512</td><td style=\"text-align: right;\">         11187.1</td><td style=\"text-align: right;\">512000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 513000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-39-13\n",
      "  done: false\n",
      "  episode_len_mean: 995.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 515\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.054860418100228535\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8372570070955487\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03093460881979912\n",
      "          policy_loss: -0.17179959648185306\n",
      "          total_loss: -0.18054712288495567\n",
      "          vf_explained_var: -0.71117103099823\n",
      "          vf_loss: 0.00792796040075498\n",
      "    num_agent_steps_sampled: 513000\n",
      "    num_agent_steps_trained: 513000\n",
      "    num_steps_sampled: 513000\n",
      "    num_steps_trained: 513000\n",
      "  iterations_since_restore: 513\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.32666666666667\n",
      "    ram_util_percent: 58.22333333333333\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06761757054767976\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.012014380453634\n",
      "    mean_inference_ms: 2.4592842187401134\n",
      "    mean_raw_obs_processing_ms: 0.8783609878259506\n",
      "  time_since_restore: 11208.048490047455\n",
      "  time_this_iter_s: 20.98048186302185\n",
      "  time_total_s: 11208.048490047455\n",
      "  timers:\n",
      "    learn_throughput: 301.982\n",
      "    learn_time_ms: 3311.457\n",
      "    load_throughput: 21218.128\n",
      "    load_time_ms: 47.13\n",
      "    sample_throughput: 49.664\n",
      "    sample_time_ms: 20135.23\n",
      "    update_time_ms: 7.412\n",
      "  timestamp: 1631889553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 513000\n",
      "  training_iteration: 513\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   513</td><td style=\"text-align: right;\">           11208</td><td style=\"text-align: right;\">513000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 514000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-39-34\n",
      "  done: false\n",
      "  episode_len_mean: 995.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 516\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08229062715034278\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.523766507042779\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008941336024957992\n",
      "          policy_loss: -0.18787387824720805\n",
      "          total_loss: -0.2122943252325058\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 8.142728818509366e-05\n",
      "    num_agent_steps_sampled: 514000\n",
      "    num_agent_steps_trained: 514000\n",
      "    num_steps_sampled: 514000\n",
      "    num_steps_trained: 514000\n",
      "  iterations_since_restore: 514\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.71000000000001\n",
      "    ram_util_percent: 58.22999999999999\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06762178581494649\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.009423554259264\n",
      "    mean_inference_ms: 2.45935469221475\n",
      "    mean_raw_obs_processing_ms: 0.8786593735489872\n",
      "  time_since_restore: 11229.366125822067\n",
      "  time_this_iter_s: 21.317635774612427\n",
      "  time_total_s: 11229.366125822067\n",
      "  timers:\n",
      "    learn_throughput: 303.191\n",
      "    learn_time_ms: 3298.247\n",
      "    load_throughput: 21502.326\n",
      "    load_time_ms: 46.507\n",
      "    sample_throughput: 49.711\n",
      "    sample_time_ms: 20116.268\n",
      "    update_time_ms: 7.068\n",
      "  timestamp: 1631889574\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 514000\n",
      "  training_iteration: 514\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   514</td><td style=\"text-align: right;\">         11229.4</td><td style=\"text-align: right;\">514000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 515000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-39-56\n",
      "  done: false\n",
      "  episode_len_mean: 995.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 517\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08229062715034278\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.068241188261244\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01085640508429893\n",
      "          policy_loss: -0.26840681706865627\n",
      "          total_loss: -0.2879281679375304\n",
      "          vf_explained_var: -0.38491329550743103\n",
      "          vf_loss: 0.0002676834096746461\n",
      "    num_agent_steps_sampled: 515000\n",
      "    num_agent_steps_trained: 515000\n",
      "    num_steps_sampled: 515000\n",
      "    num_steps_trained: 515000\n",
      "  iterations_since_restore: 515\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.184375\n",
      "    ram_util_percent: 58.15625\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06762613104890813\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.006925988568195\n",
      "    mean_inference_ms: 2.4594251057949252\n",
      "    mean_raw_obs_processing_ms: 0.8789590996249085\n",
      "  time_since_restore: 11251.726903438568\n",
      "  time_this_iter_s: 22.360777616500854\n",
      "  time_total_s: 11251.726903438568\n",
      "  timers:\n",
      "    learn_throughput: 303.664\n",
      "    learn_time_ms: 3293.108\n",
      "    load_throughput: 21712.188\n",
      "    load_time_ms: 46.057\n",
      "    sample_throughput: 49.248\n",
      "    sample_time_ms: 20305.495\n",
      "    update_time_ms: 7.302\n",
      "  timestamp: 1631889596\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 515000\n",
      "  training_iteration: 515\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   515</td><td style=\"text-align: right;\">         11251.7</td><td style=\"text-align: right;\">515000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 516000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-40-19\n",
      "  done: false\n",
      "  episode_len_mean: 995.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 518\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08229062715034278\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1974939187367757\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011563463167984513\n",
      "          policy_loss: -0.18115492347213957\n",
      "          total_loss: -0.20171380821201537\n",
      "          vf_explained_var: -0.332089900970459\n",
      "          vf_loss: 0.0004644889357425402\n",
      "    num_agent_steps_sampled: 516000\n",
      "    num_agent_steps_trained: 516000\n",
      "    num_steps_sampled: 516000\n",
      "    num_steps_trained: 516000\n",
      "  iterations_since_restore: 516\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.16250000000001\n",
      "    ram_util_percent: 58.03125\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06763051025616383\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.00449672441775\n",
      "    mean_inference_ms: 2.459497145500378\n",
      "    mean_raw_obs_processing_ms: 0.8792619438953827\n",
      "  time_since_restore: 11274.007295131683\n",
      "  time_this_iter_s: 22.280391693115234\n",
      "  time_total_s: 11274.007295131683\n",
      "  timers:\n",
      "    learn_throughput: 304.008\n",
      "    learn_time_ms: 3289.392\n",
      "    load_throughput: 22463.248\n",
      "    load_time_ms: 44.517\n",
      "    sample_throughput: 48.961\n",
      "    sample_time_ms: 20424.253\n",
      "    update_time_ms: 7.418\n",
      "  timestamp: 1631889619\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 516000\n",
      "  training_iteration: 516\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   516</td><td style=\"text-align: right;\">           11274</td><td style=\"text-align: right;\">516000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 517000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-40-41\n",
      "  done: false\n",
      "  episode_len_mean: 995.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 519\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08229062715034278\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4779151333702933\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011497218053667603\n",
      "          policy_loss: -0.12234580053223504\n",
      "          total_loss: -0.14598980396986008\n",
      "          vf_explained_var: -0.8401803970336914\n",
      "          vf_loss: 0.00018903409630487052\n",
      "    num_agent_steps_sampled: 517000\n",
      "    num_agent_steps_trained: 517000\n",
      "    num_steps_sampled: 517000\n",
      "    num_steps_trained: 517000\n",
      "  iterations_since_restore: 517\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.69354838709677\n",
      "    ram_util_percent: 58.22580645161291\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06763492858382572\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.002148866306083\n",
      "    mean_inference_ms: 2.4595709413382276\n",
      "    mean_raw_obs_processing_ms: 0.8795673931421316\n",
      "  time_since_restore: 11295.901978731155\n",
      "  time_this_iter_s: 21.894683599472046\n",
      "  time_total_s: 11295.901978731155\n",
      "  timers:\n",
      "    learn_throughput: 302.216\n",
      "    learn_time_ms: 3308.896\n",
      "    load_throughput: 22560.841\n",
      "    load_time_ms: 44.325\n",
      "    sample_throughput: 49.165\n",
      "    sample_time_ms: 20339.785\n",
      "    update_time_ms: 7.571\n",
      "  timestamp: 1631889641\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 517000\n",
      "  training_iteration: 517\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   517</td><td style=\"text-align: right;\">         11295.9</td><td style=\"text-align: right;\">517000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 518000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-41-02\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 520\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08229062715034278\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.704946920606825\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.07124854204078235\n",
      "          policy_loss: -0.12227423335942957\n",
      "          total_loss: -0.1319421600550413\n",
      "          vf_explained_var: -0.07987243682146072\n",
      "          vf_loss: 0.0015184556028947959\n",
      "    num_agent_steps_sampled: 518000\n",
      "    num_agent_steps_trained: 518000\n",
      "    num_steps_sampled: 518000\n",
      "    num_steps_trained: 518000\n",
      "  iterations_since_restore: 518\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.53548387096774\n",
      "    ram_util_percent: 58.277419354838706\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06763938822766588\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.999866793789042\n",
      "    mean_inference_ms: 2.459646803815335\n",
      "    mean_raw_obs_processing_ms: 0.8794696079921022\n",
      "  time_since_restore: 11317.576539993286\n",
      "  time_this_iter_s: 21.674561262130737\n",
      "  time_total_s: 11317.576539993286\n",
      "  timers:\n",
      "    learn_throughput: 301.847\n",
      "    learn_time_ms: 3312.941\n",
      "    load_throughput: 22607.866\n",
      "    load_time_ms: 44.232\n",
      "    sample_throughput: 53.341\n",
      "    sample_time_ms: 18747.323\n",
      "    update_time_ms: 8.312\n",
      "  timestamp: 1631889662\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 518000\n",
      "  training_iteration: 518\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   518</td><td style=\"text-align: right;\">         11317.6</td><td style=\"text-align: right;\">518000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 519000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-41-25\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 521\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1234359407255142\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8973988455202845\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015577599081932809\n",
      "          policy_loss: -0.09629991501569748\n",
      "          total_loss: -0.10259836945268842\n",
      "          vf_explained_var: 0.16199597716331482\n",
      "          vf_loss: 0.0007526995249160488\n",
      "    num_agent_steps_sampled: 519000\n",
      "    num_agent_steps_trained: 519000\n",
      "    num_steps_sampled: 519000\n",
      "    num_steps_trained: 519000\n",
      "  iterations_since_restore: 519\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.02424242424242\n",
      "    ram_util_percent: 58.16969696969697\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06764388257826254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.99759680382102\n",
      "    mean_inference_ms: 2.4597228181284434\n",
      "    mean_raw_obs_processing_ms: 0.8793755656263031\n",
      "  time_since_restore: 11340.522794485092\n",
      "  time_this_iter_s: 22.94625449180603\n",
      "  time_total_s: 11340.522794485092\n",
      "  timers:\n",
      "    learn_throughput: 302.461\n",
      "    learn_time_ms: 3306.215\n",
      "    load_throughput: 22364.373\n",
      "    load_time_ms: 44.714\n",
      "    sample_throughput: 53.055\n",
      "    sample_time_ms: 18848.53\n",
      "    update_time_ms: 7.326\n",
      "  timestamp: 1631889685\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 519000\n",
      "  training_iteration: 519\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   519</td><td style=\"text-align: right;\">         11340.5</td><td style=\"text-align: right;\">519000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 520000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-41-46\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 522\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1234359407255142\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.195314470926921\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01324005730149417\n",
      "          policy_loss: -0.17414653673768044\n",
      "          total_loss: -0.1940800810439719\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0003853025142032291\n",
      "    num_agent_steps_sampled: 520000\n",
      "    num_agent_steps_trained: 520000\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "  iterations_since_restore: 520\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.99999999999999\n",
      "    ram_util_percent: 58.15333333333333\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06764813700337174\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.995390253228939\n",
      "    mean_inference_ms: 2.459800895740875\n",
      "    mean_raw_obs_processing_ms: 0.8792847943222851\n",
      "  time_since_restore: 11361.529355049133\n",
      "  time_this_iter_s: 21.006560564041138\n",
      "  time_total_s: 11361.529355049133\n",
      "  timers:\n",
      "    learn_throughput: 304.674\n",
      "    learn_time_ms: 3282.198\n",
      "    load_throughput: 22077.562\n",
      "    load_time_ms: 45.295\n",
      "    sample_throughput: 53.472\n",
      "    sample_time_ms: 18701.25\n",
      "    update_time_ms: 7.758\n",
      "  timestamp: 1631889706\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 520\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   520</td><td style=\"text-align: right;\">         11361.5</td><td style=\"text-align: right;\">520000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 521000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-42-08\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 523\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1234359407255142\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8175113452805414\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01860023109294894\n",
      "          policy_loss: -0.07597437041501204\n",
      "          total_loss: -0.08110989307363828\n",
      "          vf_explained_var: -0.015234099701046944\n",
      "          vf_loss: 0.0007436546587592198\n",
      "    num_agent_steps_sampled: 521000\n",
      "    num_agent_steps_trained: 521000\n",
      "    num_steps_sampled: 521000\n",
      "    num_steps_trained: 521000\n",
      "  iterations_since_restore: 521\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.00645161290323\n",
      "    ram_util_percent: 58.12903225806452\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06765242082160244\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.993245807639964\n",
      "    mean_inference_ms: 2.4598806513285827\n",
      "    mean_raw_obs_processing_ms: 0.8791976809425859\n",
      "  time_since_restore: 11382.897112369537\n",
      "  time_this_iter_s: 21.367757320404053\n",
      "  time_total_s: 11382.897112369537\n",
      "  timers:\n",
      "    learn_throughput: 306.841\n",
      "    learn_time_ms: 3259.015\n",
      "    load_throughput: 22638.763\n",
      "    load_time_ms: 44.172\n",
      "    sample_throughput: 53.323\n",
      "    sample_time_ms: 18753.578\n",
      "    update_time_ms: 7.714\n",
      "  timestamp: 1631889728\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 521000\n",
      "  training_iteration: 521\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   521</td><td style=\"text-align: right;\">         11382.9</td><td style=\"text-align: right;\">521000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 522000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-42-29\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 524\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1234359407255142\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4047685994042292\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014901117214772351\n",
      "          policy_loss: -0.1150808156364494\n",
      "          total_loss: -0.1264477574163013\n",
      "          vf_explained_var: -0.7836671471595764\n",
      "          vf_loss: 0.0008414103778906994\n",
      "    num_agent_steps_sampled: 522000\n",
      "    num_agent_steps_trained: 522000\n",
      "    num_steps_sampled: 522000\n",
      "    num_steps_trained: 522000\n",
      "  iterations_since_restore: 522\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.14333333333333\n",
      "    ram_util_percent: 58.169999999999995\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06765668949987137\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.991174521655102\n",
      "    mean_inference_ms: 2.459961346921194\n",
      "    mean_raw_obs_processing_ms: 0.879113323475096\n",
      "  time_since_restore: 11404.145976543427\n",
      "  time_this_iter_s: 21.24886417388916\n",
      "  time_total_s: 11404.145976543427\n",
      "  timers:\n",
      "    learn_throughput: 306.98\n",
      "    learn_time_ms: 3257.536\n",
      "    load_throughput: 22762.237\n",
      "    load_time_ms: 43.932\n",
      "    sample_throughput: 54.378\n",
      "    sample_time_ms: 18389.818\n",
      "    update_time_ms: 7.444\n",
      "  timestamp: 1631889749\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 522000\n",
      "  training_iteration: 522\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   522</td><td style=\"text-align: right;\">         11404.1</td><td style=\"text-align: right;\">522000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 523000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-42-51\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 525\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1234359407255142\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4792133543226456\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008815336229194868\n",
      "          policy_loss: -0.27544548941983116\n",
      "          total_loss: -0.2991116808520423\n",
      "          vf_explained_var: -0.7397075891494751\n",
      "          vf_loss: 3.7813520329008394e-05\n",
      "    num_agent_steps_sampled: 523000\n",
      "    num_agent_steps_trained: 523000\n",
      "    num_steps_sampled: 523000\n",
      "    num_steps_trained: 523000\n",
      "  iterations_since_restore: 523\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.66129032258064\n",
      "    ram_util_percent: 58.09677419354838\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06766097916244812\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.989174561168383\n",
      "    mean_inference_ms: 2.4600427210678295\n",
      "    mean_raw_obs_processing_ms: 0.8790318327248108\n",
      "  time_since_restore: 11425.266026973724\n",
      "  time_this_iter_s: 21.12005043029785\n",
      "  time_total_s: 11425.266026973724\n",
      "  timers:\n",
      "    learn_throughput: 307.865\n",
      "    learn_time_ms: 3248.175\n",
      "    load_throughput: 22794.04\n",
      "    load_time_ms: 43.871\n",
      "    sample_throughput: 54.31\n",
      "    sample_time_ms: 18412.981\n",
      "    update_time_ms: 7.609\n",
      "  timestamp: 1631889771\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 523000\n",
      "  training_iteration: 523\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   523</td><td style=\"text-align: right;\">         11425.3</td><td style=\"text-align: right;\">523000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 524000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-43-13\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 526\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1234359407255142\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2648269799020557\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014548087717880204\n",
      "          policy_loss: -0.16275349772638745\n",
      "          total_loss: -0.18338604552878274\n",
      "          vf_explained_var: -0.9773444533348083\n",
      "          vf_loss: 0.00021996668100554315\n",
      "    num_agent_steps_sampled: 524000\n",
      "    num_agent_steps_trained: 524000\n",
      "    num_steps_sampled: 524000\n",
      "    num_steps_trained: 524000\n",
      "  iterations_since_restore: 524\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.60645161290323\n",
      "    ram_util_percent: 58.19354838709678\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06766528039411185\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.987269195334829\n",
      "    mean_inference_ms: 2.460125452788211\n",
      "    mean_raw_obs_processing_ms: 0.8789530676528959\n",
      "  time_since_restore: 11447.471893787384\n",
      "  time_this_iter_s: 22.205866813659668\n",
      "  time_total_s: 11447.471893787384\n",
      "  timers:\n",
      "    learn_throughput: 306.195\n",
      "    learn_time_ms: 3265.892\n",
      "    load_throughput: 22038.966\n",
      "    load_time_ms: 45.374\n",
      "    sample_throughput: 54.105\n",
      "    sample_time_ms: 18482.622\n",
      "    update_time_ms: 7.618\n",
      "  timestamp: 1631889793\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 524000\n",
      "  training_iteration: 524\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   524</td><td style=\"text-align: right;\">         11447.5</td><td style=\"text-align: right;\">524000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 525000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-43-35\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 527\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1234359407255142\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1383634971247778\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02017972596666038\n",
      "          policy_loss: -0.1308349298934142\n",
      "          total_loss: -0.1385013859305117\n",
      "          vf_explained_var: -0.06490620225667953\n",
      "          vf_loss: 0.0012262752027406047\n",
      "    num_agent_steps_sampled: 525000\n",
      "    num_agent_steps_trained: 525000\n",
      "    num_steps_sampled: 525000\n",
      "    num_steps_trained: 525000\n",
      "  iterations_since_restore: 525\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.2875\n",
      "    ram_util_percent: 58.178125\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06766956782760639\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.985431587613279\n",
      "    mean_inference_ms: 2.4602079966727146\n",
      "    mean_raw_obs_processing_ms: 0.878876737856855\n",
      "  time_since_restore: 11469.726813316345\n",
      "  time_this_iter_s: 22.25491952896118\n",
      "  time_total_s: 11469.726813316345\n",
      "  timers:\n",
      "    learn_throughput: 303.579\n",
      "    learn_time_ms: 3294.035\n",
      "    load_throughput: 23179.161\n",
      "    load_time_ms: 43.142\n",
      "    sample_throughput: 54.213\n",
      "    sample_time_ms: 18445.747\n",
      "    update_time_ms: 7.516\n",
      "  timestamp: 1631889815\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 525000\n",
      "  training_iteration: 525\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   525</td><td style=\"text-align: right;\">         11469.7</td><td style=\"text-align: right;\">525000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 526000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-43-57\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 528\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18515391108827126\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3795015619860755\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00855152556486891\n",
      "          policy_loss: -0.01771056759688589\n",
      "          total_loss: -0.028734240473972425\n",
      "          vf_explained_var: -0.894822359085083\n",
      "          vf_loss: 0.0011879969815102717\n",
      "    num_agent_steps_sampled: 526000\n",
      "    num_agent_steps_trained: 526000\n",
      "    num_steps_sampled: 526000\n",
      "    num_steps_trained: 526000\n",
      "  iterations_since_restore: 526\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.71935483870966\n",
      "    ram_util_percent: 58.21612903225806\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06767382741412545\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.983666643894255\n",
      "    mean_inference_ms: 2.46029037382271\n",
      "    mean_raw_obs_processing_ms: 0.8788029050369002\n",
      "  time_since_restore: 11491.602744817734\n",
      "  time_this_iter_s: 21.87593150138855\n",
      "  time_total_s: 11491.602744817734\n",
      "  timers:\n",
      "    learn_throughput: 303.082\n",
      "    learn_time_ms: 3299.436\n",
      "    load_throughput: 21862.368\n",
      "    load_time_ms: 45.741\n",
      "    sample_throughput: 54.356\n",
      "    sample_time_ms: 18397.164\n",
      "    update_time_ms: 7.391\n",
      "  timestamp: 1631889837\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 526000\n",
      "  training_iteration: 526\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   526</td><td style=\"text-align: right;\">         11491.6</td><td style=\"text-align: right;\">526000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 527000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-44-20\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 529\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18515391108827126\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9290863275527954\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006387024646745942\n",
      "          policy_loss: -0.09421847992473179\n",
      "          total_loss: -0.10124279643512435\n",
      "          vf_explained_var: -0.9773922562599182\n",
      "          vf_loss: 0.0010839628670105917\n",
      "    num_agent_steps_sampled: 527000\n",
      "    num_agent_steps_trained: 527000\n",
      "    num_steps_sampled: 527000\n",
      "    num_steps_trained: 527000\n",
      "  iterations_since_restore: 527\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.98484848484848\n",
      "    ram_util_percent: 58.2030303030303\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06767794240931174\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.982002429858529\n",
      "    mean_inference_ms: 2.46037284580165\n",
      "    mean_raw_obs_processing_ms: 0.8787318410441283\n",
      "  time_since_restore: 11514.762367725372\n",
      "  time_this_iter_s: 23.15962290763855\n",
      "  time_total_s: 11514.762367725372\n",
      "  timers:\n",
      "    learn_throughput: 304.363\n",
      "    learn_time_ms: 3285.551\n",
      "    load_throughput: 22031.673\n",
      "    load_time_ms: 45.389\n",
      "    sample_throughput: 53.943\n",
      "    sample_time_ms: 18538.145\n",
      "    update_time_ms: 7.233\n",
      "  timestamp: 1631889860\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 527000\n",
      "  training_iteration: 527\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   527</td><td style=\"text-align: right;\">         11514.8</td><td style=\"text-align: right;\">527000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 528000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-44-41\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 530\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18515391108827126\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0623185250494216\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010004336372164675\n",
      "          policy_loss: -0.22488393800126183\n",
      "          total_loss: -0.2432866154445542\n",
      "          vf_explained_var: -0.9726977944374084\n",
      "          vf_loss: 0.0003681669875732041\n",
      "    num_agent_steps_sampled: 528000\n",
      "    num_agent_steps_trained: 528000\n",
      "    num_steps_sampled: 528000\n",
      "    num_steps_trained: 528000\n",
      "  iterations_since_restore: 528\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.32666666666664\n",
      "    ram_util_percent: 58.28000000000001\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06768200146341304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.980349266038486\n",
      "    mean_inference_ms: 2.460455909514275\n",
      "    mean_raw_obs_processing_ms: 0.878664185512024\n",
      "  time_since_restore: 11535.82237124443\n",
      "  time_this_iter_s: 21.060003519058228\n",
      "  time_total_s: 11535.82237124443\n",
      "  timers:\n",
      "    learn_throughput: 304.933\n",
      "    learn_time_ms: 3279.406\n",
      "    load_throughput: 21939.972\n",
      "    load_time_ms: 45.579\n",
      "    sample_throughput: 54.1\n",
      "    sample_time_ms: 18484.425\n",
      "    update_time_ms: 6.045\n",
      "  timestamp: 1631889881\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 528000\n",
      "  training_iteration: 528\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   528</td><td style=\"text-align: right;\">         11535.8</td><td style=\"text-align: right;\">528000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 529000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-45-03\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 531\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18515391108827126\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8973294019699096\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006386706152951089\n",
      "          policy_loss: -0.18379337506161797\n",
      "          total_loss: -0.20128959599468443\n",
      "          vf_explained_var: -0.913356602191925\n",
      "          vf_loss: 0.00029454916449241054\n",
      "    num_agent_steps_sampled: 529000\n",
      "    num_agent_steps_trained: 529000\n",
      "    num_steps_sampled: 529000\n",
      "    num_steps_trained: 529000\n",
      "  iterations_since_restore: 529\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.95161290322582\n",
      "    ram_util_percent: 58.23870967741935\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06768615202756215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.978760455467887\n",
      "    mean_inference_ms: 2.4605389480469064\n",
      "    mean_raw_obs_processing_ms: 0.8785996095944201\n",
      "  time_since_restore: 11557.032360315323\n",
      "  time_this_iter_s: 21.209989070892334\n",
      "  time_total_s: 11557.032360315323\n",
      "  timers:\n",
      "    learn_throughput: 303.671\n",
      "    learn_time_ms: 3293.04\n",
      "    load_throughput: 23207.351\n",
      "    load_time_ms: 43.09\n",
      "    sample_throughput: 54.651\n",
      "    sample_time_ms: 18297.92\n",
      "    update_time_ms: 6.79\n",
      "  timestamp: 1631889903\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 529000\n",
      "  training_iteration: 529\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   529</td><td style=\"text-align: right;\">           11557</td><td style=\"text-align: right;\">529000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 530000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-45-24\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 532\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18515391108827126\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4488715012868245\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005910731846372702\n",
      "          policy_loss: -0.20014284915394254\n",
      "          total_loss: -0.22351381066772674\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 2.335799504685888e-05\n",
      "    num_agent_steps_sampled: 530000\n",
      "    num_agent_steps_trained: 530000\n",
      "    num_steps_sampled: 530000\n",
      "    num_steps_trained: 530000\n",
      "  iterations_since_restore: 530\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.30666666666666\n",
      "    ram_util_percent: 58.206666666666656\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06769030363413285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.977234442883871\n",
      "    mean_inference_ms: 2.460620747990938\n",
      "    mean_raw_obs_processing_ms: 0.878538181262194\n",
      "  time_since_restore: 11578.129924297333\n",
      "  time_this_iter_s: 21.097563982009888\n",
      "  time_total_s: 11578.129924297333\n",
      "  timers:\n",
      "    learn_throughput: 303.207\n",
      "    learn_time_ms: 3298.079\n",
      "    load_throughput: 24147.245\n",
      "    load_time_ms: 41.413\n",
      "    sample_throughput: 54.632\n",
      "    sample_time_ms: 18304.326\n",
      "    update_time_ms: 6.284\n",
      "  timestamp: 1631889924\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 530000\n",
      "  training_iteration: 530\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   530</td><td style=\"text-align: right;\">         11578.1</td><td style=\"text-align: right;\">530000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 531000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-45-47\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 533\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18515391108827126\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0753524078263177\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00798942462980467\n",
      "          policy_loss: -0.08025503497984675\n",
      "          total_loss: -0.09929667914079296\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.000232606830185331\n",
      "    num_agent_steps_sampled: 531000\n",
      "    num_agent_steps_trained: 531000\n",
      "    num_steps_sampled: 531000\n",
      "    num_steps_trained: 531000\n",
      "  iterations_since_restore: 531\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.29999999999998\n",
      "    ram_util_percent: 58.21212121212121\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06769441693296159\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.975783773221053\n",
      "    mean_inference_ms: 2.4607044161835\n",
      "    mean_raw_obs_processing_ms: 0.8784799606583737\n",
      "  time_since_restore: 11601.286572217941\n",
      "  time_this_iter_s: 23.15664792060852\n",
      "  time_total_s: 11601.286572217941\n",
      "  timers:\n",
      "    learn_throughput: 301.096\n",
      "    learn_time_ms: 3321.2\n",
      "    load_throughput: 23187.798\n",
      "    load_time_ms: 43.126\n",
      "    sample_throughput: 54.173\n",
      "    sample_time_ms: 18459.51\n",
      "    update_time_ms: 6.356\n",
      "  timestamp: 1631889947\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 531000\n",
      "  training_iteration: 531\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   531</td><td style=\"text-align: right;\">         11601.3</td><td style=\"text-align: right;\">531000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 532000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-46-07\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 534\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18515391108827126\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.390712547302246\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009250104669687372\n",
      "          policy_loss: -0.11301558423373434\n",
      "          total_loss: -0.13516692477795814\n",
      "          vf_explained_var: -0.9830273985862732\n",
      "          vf_loss: 4.309134158878199e-05\n",
      "    num_agent_steps_sampled: 532000\n",
      "    num_agent_steps_trained: 532000\n",
      "    num_steps_sampled: 532000\n",
      "    num_steps_trained: 532000\n",
      "  iterations_since_restore: 532\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.74285714285715\n",
      "    ram_util_percent: 58.35714285714285\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06769848442959094\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.974381009949791\n",
      "    mean_inference_ms: 2.460786486438565\n",
      "    mean_raw_obs_processing_ms: 0.8784240483332656\n",
      "  time_since_restore: 11621.13203883171\n",
      "  time_this_iter_s: 19.84546661376953\n",
      "  time_total_s: 11621.13203883171\n",
      "  timers:\n",
      "    learn_throughput: 302.831\n",
      "    learn_time_ms: 3302.167\n",
      "    load_throughput: 24444.966\n",
      "    load_time_ms: 40.908\n",
      "    sample_throughput: 54.525\n",
      "    sample_time_ms: 18340.327\n",
      "    update_time_ms: 6.354\n",
      "  timestamp: 1631889967\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 532000\n",
      "  training_iteration: 532\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 73.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   532</td><td style=\"text-align: right;\">         11621.1</td><td style=\"text-align: right;\">532000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 533000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-46-25\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 535\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18515391108827126\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.885311077038447\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012338240892783622\n",
      "          policy_loss: -0.196664766387807\n",
      "          total_loss: -0.20232960118187798\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0009037997825847318\n",
      "    num_agent_steps_sampled: 533000\n",
      "    num_agent_steps_trained: 533000\n",
      "    num_steps_sampled: 533000\n",
      "    num_steps_trained: 533000\n",
      "  iterations_since_restore: 533\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.684615384615384\n",
      "    ram_util_percent: 57.46153846153846\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06770237836005545\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.972985851490012\n",
      "    mean_inference_ms: 2.460864370568661\n",
      "    mean_raw_obs_processing_ms: 0.8783705572730738\n",
      "  time_since_restore: 11639.375002622604\n",
      "  time_this_iter_s: 18.242963790893555\n",
      "  time_total_s: 11639.375002622604\n",
      "  timers:\n",
      "    learn_throughput: 302.776\n",
      "    learn_time_ms: 3302.772\n",
      "    load_throughput: 24583.86\n",
      "    load_time_ms: 40.677\n",
      "    sample_throughput: 55.394\n",
      "    sample_time_ms: 18052.652\n",
      "    update_time_ms: 6.112\n",
      "  timestamp: 1631889985\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 533000\n",
      "  training_iteration: 533\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 71.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   533</td><td style=\"text-align: right;\">         11639.4</td><td style=\"text-align: right;\">533000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 534000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-46-42\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 536\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18515391108827126\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0952178716659544\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009276583804162868\n",
      "          policy_loss: -0.1489173885020945\n",
      "          total_loss: -0.1679145872592926\n",
      "          vf_explained_var: -0.7203199863433838\n",
      "          vf_loss: 0.00023738427694802846\n",
      "    num_agent_steps_sampled: 534000\n",
      "    num_agent_steps_trained: 534000\n",
      "    num_steps_sampled: 534000\n",
      "    num_steps_trained: 534000\n",
      "  iterations_since_restore: 534\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.79599999999999\n",
      "    ram_util_percent: 43.216\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06770618508388444\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.97155623281318\n",
      "    mean_inference_ms: 2.460939199600531\n",
      "    mean_raw_obs_processing_ms: 0.8783197089365408\n",
      "  time_since_restore: 11656.358880996704\n",
      "  time_this_iter_s: 16.98387837409973\n",
      "  time_total_s: 11656.358880996704\n",
      "  timers:\n",
      "    learn_throughput: 317.825\n",
      "    learn_time_ms: 3146.386\n",
      "    load_throughput: 26508.964\n",
      "    load_time_ms: 37.723\n",
      "    sample_throughput: 56.529\n",
      "    sample_time_ms: 17690.111\n",
      "    update_time_ms: 5.964\n",
      "  timestamp: 1631890002\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 534000\n",
      "  training_iteration: 534\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 51.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   534</td><td style=\"text-align: right;\">         11656.4</td><td style=\"text-align: right;\">534000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 535000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-47-10\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 537\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18515391108827126\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7617089119222429\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004793290986213705\n",
      "          policy_loss: -0.10736397732463147\n",
      "          total_loss: -0.11350528146657679\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0005882877057754538\n",
      "    num_agent_steps_sampled: 535000\n",
      "    num_agent_steps_trained: 535000\n",
      "    num_steps_sampled: 535000\n",
      "    num_steps_trained: 535000\n",
      "  iterations_since_restore: 535\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.34\n",
      "    ram_util_percent: 45.46\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06771009284798936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.9703143398689\n",
      "    mean_inference_ms: 2.4610184932492793\n",
      "    mean_raw_obs_processing_ms: 0.8782719064583715\n",
      "  time_since_restore: 11684.413296222687\n",
      "  time_this_iter_s: 28.054415225982666\n",
      "  time_total_s: 11684.413296222687\n",
      "  timers:\n",
      "    learn_throughput: 319.093\n",
      "    learn_time_ms: 3133.883\n",
      "    load_throughput: 25822.368\n",
      "    load_time_ms: 38.726\n",
      "    sample_throughput: 54.7\n",
      "    sample_time_ms: 18281.375\n",
      "    update_time_ms: 6.446\n",
      "  timestamp: 1631890030\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 535000\n",
      "  training_iteration: 535\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 61.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   535</td><td style=\"text-align: right;\">         11684.4</td><td style=\"text-align: right;\">535000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 536000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-47-35\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 538\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09257695554413563\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.351236253314548\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014455971279027875\n",
      "          policy_loss: -0.1067020133137703\n",
      "          total_loss: -0.12879225611686707\n",
      "          vf_explained_var: -0.8103535175323486\n",
      "          vf_loss: 8.383099086333661e-05\n",
      "    num_agent_steps_sampled: 536000\n",
      "    num_agent_steps_trained: 536000\n",
      "    num_steps_sampled: 536000\n",
      "    num_steps_trained: 536000\n",
      "  iterations_since_restore: 536\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.16571428571429\n",
      "    ram_util_percent: 49.29999999999999\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06771398688283789\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.96913582791007\n",
      "    mean_inference_ms: 2.4611001497782397\n",
      "    mean_raw_obs_processing_ms: 0.8782268461098619\n",
      "  time_since_restore: 11709.317108631134\n",
      "  time_this_iter_s: 24.903812408447266\n",
      "  time_total_s: 11709.317108631134\n",
      "  timers:\n",
      "    learn_throughput: 320.892\n",
      "    learn_time_ms: 3116.31\n",
      "    load_throughput: 25545.165\n",
      "    load_time_ms: 39.146\n",
      "    sample_throughput: 53.759\n",
      "    sample_time_ms: 18601.531\n",
      "    update_time_ms: 6.491\n",
      "  timestamp: 1631890055\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 536000\n",
      "  training_iteration: 536\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 63.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   536</td><td style=\"text-align: right;\">         11709.3</td><td style=\"text-align: right;\">536000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 537000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-47-57\n",
      "  done: false\n",
      "  episode_len_mean: 996.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 539\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09257695554413563\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7377292288674249\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006250222009695961\n",
      "          policy_loss: -0.1487932365387678\n",
      "          total_loss: -0.15487270669804679\n",
      "          vf_explained_var: -0.7610600590705872\n",
      "          vf_loss: 0.0007191960043403217\n",
      "    num_agent_steps_sampled: 537000\n",
      "    num_agent_steps_trained: 537000\n",
      "    num_steps_sampled: 537000\n",
      "    num_steps_trained: 537000\n",
      "  iterations_since_restore: 537\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.7548387096774\n",
      "    ram_util_percent: 50.177419354838705\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06771790078130176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.968006764309255\n",
      "    mean_inference_ms: 2.461183231656647\n",
      "    mean_raw_obs_processing_ms: 0.8781845178566008\n",
      "  time_since_restore: 11731.05027294159\n",
      "  time_this_iter_s: 21.733164310455322\n",
      "  time_total_s: 11731.05027294159\n",
      "  timers:\n",
      "    learn_throughput: 319.295\n",
      "    learn_time_ms: 3131.899\n",
      "    load_throughput: 24944.922\n",
      "    load_time_ms: 40.088\n",
      "    sample_throughput: 54.223\n",
      "    sample_time_ms: 18442.366\n",
      "    update_time_ms: 6.571\n",
      "  timestamp: 1631890077\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 537000\n",
      "  training_iteration: 537\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 63.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   537</td><td style=\"text-align: right;\">         11731.1</td><td style=\"text-align: right;\">537000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 538000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-48-36\n",
      "  done: false\n",
      "  episode_len_mean: 995.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 540\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09257695554413563\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7678143733077579\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02178269642830632\n",
      "          policy_loss: -0.15460666401518716\n",
      "          total_loss: -0.1592309546139505\n",
      "          vf_explained_var: -0.13869254291057587\n",
      "          vf_loss: 0.001037276129419398\n",
      "    num_agent_steps_sampled: 538000\n",
      "    num_agent_steps_trained: 538000\n",
      "    num_steps_sampled: 538000\n",
      "    num_steps_trained: 538000\n",
      "  iterations_since_restore: 538\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.14642857142859\n",
      "    ram_util_percent: 50.30357142857142\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06772183117078362\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.966956134013598\n",
      "    mean_inference_ms: 2.461269274695424\n",
      "    mean_raw_obs_processing_ms: 0.87847496440649\n",
      "  time_since_restore: 11770.338388204575\n",
      "  time_this_iter_s: 39.28811526298523\n",
      "  time_total_s: 11770.338388204575\n",
      "  timers:\n",
      "    learn_throughput: 318.238\n",
      "    learn_time_ms: 3142.307\n",
      "    load_throughput: 26380.316\n",
      "    load_time_ms: 37.907\n",
      "    sample_throughput: 49.374\n",
      "    sample_time_ms: 20253.608\n",
      "    update_time_ms: 7.937\n",
      "  timestamp: 1631890116\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 538000\n",
      "  training_iteration: 538\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 63.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   538</td><td style=\"text-align: right;\">         11770.3</td><td style=\"text-align: right;\">538000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               995</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 539000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-48-58\n",
      "  done: false\n",
      "  episode_len_mean: 995.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 541\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.13886543331620343\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7911307440863715\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014709507420804225\n",
      "          policy_loss: -0.07979834406740136\n",
      "          total_loss: -0.08480490555779802\n",
      "          vf_explained_var: -0.6022223234176636\n",
      "          vf_loss: 0.0008621025772299618\n",
      "    num_agent_steps_sampled: 539000\n",
      "    num_agent_steps_trained: 539000\n",
      "    num_steps_sampled: 539000\n",
      "    num_steps_trained: 539000\n",
      "  iterations_since_restore: 539\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.12187500000002\n",
      "    ram_util_percent: 50.446875\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06772593137386639\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.965991250497664\n",
      "    mean_inference_ms: 2.461358008437389\n",
      "    mean_raw_obs_processing_ms: 0.8787678302168652\n",
      "  time_since_restore: 11792.643415927887\n",
      "  time_this_iter_s: 22.305027723312378\n",
      "  time_total_s: 11792.643415927887\n",
      "  timers:\n",
      "    learn_throughput: 319.059\n",
      "    learn_time_ms: 3134.216\n",
      "    load_throughput: 25742.002\n",
      "    load_time_ms: 38.847\n",
      "    sample_throughput: 49.089\n",
      "    sample_time_ms: 20371.35\n",
      "    update_time_ms: 7.378\n",
      "  timestamp: 1631890138\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 539000\n",
      "  training_iteration: 539\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 63.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   539</td><td style=\"text-align: right;\">         11792.6</td><td style=\"text-align: right;\">539000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               995</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 540000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-49-19\n",
      "  done: false\n",
      "  episode_len_mean: 995.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 542\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.13886543331620343\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7235541462898254\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01860723252093616\n",
      "          policy_loss: -0.014249851223495272\n",
      "          total_loss: -0.017568881809711456\n",
      "          vf_explained_var: -0.743344783782959\n",
      "          vf_loss: 0.001332609424005366\n",
      "    num_agent_steps_sampled: 540000\n",
      "    num_agent_steps_trained: 540000\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "  iterations_since_restore: 540\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.98333333333333\n",
      "    ram_util_percent: 50.529999999999994\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06772982483138039\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.965053593670488\n",
      "    mean_inference_ms: 2.461446988828721\n",
      "    mean_raw_obs_processing_ms: 0.8790625919311859\n",
      "  time_since_restore: 11813.44307231903\n",
      "  time_this_iter_s: 20.7996563911438\n",
      "  time_total_s: 11813.44307231903\n",
      "  timers:\n",
      "    learn_throughput: 317.394\n",
      "    learn_time_ms: 3150.661\n",
      "    load_throughput: 24658.756\n",
      "    load_time_ms: 40.554\n",
      "    sample_throughput: 49.205\n",
      "    sample_time_ms: 20322.978\n",
      "    update_time_ms: 7.611\n",
      "  timestamp: 1631890159\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 540\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 63.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   540</td><td style=\"text-align: right;\">         11813.4</td><td style=\"text-align: right;\">540000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               995</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 541000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-49-39\n",
      "  done: false\n",
      "  episode_len_mean: 995.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 543\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.13886543331620343\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6689455648263295\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015742677608566208\n",
      "          policy_loss: -0.030821738060977724\n",
      "          total_loss: -0.0342298588818974\n",
      "          vf_explained_var: -0.6894143223762512\n",
      "          vf_loss: 0.0010952198748580284\n",
      "    num_agent_steps_sampled: 541000\n",
      "    num_agent_steps_trained: 541000\n",
      "    num_steps_sampled: 541000\n",
      "    num_steps_trained: 541000\n",
      "  iterations_since_restore: 541\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.63214285714285\n",
      "    ram_util_percent: 50.61785714285715\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06773374846014359\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.964112451577751\n",
      "    mean_inference_ms: 2.4615371485924276\n",
      "    mean_raw_obs_processing_ms: 0.8793597081679073\n",
      "  time_since_restore: 11833.337211370468\n",
      "  time_this_iter_s: 19.894139051437378\n",
      "  time_total_s: 11833.337211370468\n",
      "  timers:\n",
      "    learn_throughput: 316.522\n",
      "    learn_time_ms: 3159.341\n",
      "    load_throughput: 24983.673\n",
      "    load_time_ms: 40.026\n",
      "    sample_throughput: 50.029\n",
      "    sample_time_ms: 19988.532\n",
      "    update_time_ms: 7.625\n",
      "  timestamp: 1631890179\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 541000\n",
      "  training_iteration: 541\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 64.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   541</td><td style=\"text-align: right;\">         11833.3</td><td style=\"text-align: right;\">541000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               995</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 542000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-49-59\n",
      "  done: false\n",
      "  episode_len_mean: 995.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 544\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.13886543331620343\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8515688869688245\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015258789845663203\n",
      "          policy_loss: -0.028889907482597562\n",
      "          total_loss: -0.04468412467588981\n",
      "          vf_explained_var: -0.9466232061386108\n",
      "          vf_loss: 0.0006025551268572195\n",
      "    num_agent_steps_sampled: 542000\n",
      "    num_agent_steps_trained: 542000\n",
      "    num_steps_sampled: 542000\n",
      "    num_steps_trained: 542000\n",
      "  iterations_since_restore: 542\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.90689655172413\n",
      "    ram_util_percent: 50.71724137931034\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06773777679833577\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.963206231501452\n",
      "    mean_inference_ms: 2.461629630257654\n",
      "    mean_raw_obs_processing_ms: 0.879659299576609\n",
      "  time_since_restore: 11853.26292848587\n",
      "  time_this_iter_s: 19.92571711540222\n",
      "  time_total_s: 11853.26292848587\n",
      "  timers:\n",
      "    learn_throughput: 315.149\n",
      "    learn_time_ms: 3173.101\n",
      "    load_throughput: 24200.495\n",
      "    load_time_ms: 41.321\n",
      "    sample_throughput: 50.048\n",
      "    sample_time_ms: 19980.869\n",
      "    update_time_ms: 8.146\n",
      "  timestamp: 1631890199\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 542000\n",
      "  training_iteration: 542\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 63.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   542</td><td style=\"text-align: right;\">         11853.3</td><td style=\"text-align: right;\">542000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               995</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 543000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-50-20\n",
      "  done: false\n",
      "  episode_len_mean: 995.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 545\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.13886543331620343\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.954607120487425\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01497211437031691\n",
      "          policy_loss: -0.08294960514952739\n",
      "          total_loss: -0.08971437104046345\n",
      "          vf_explained_var: -0.9520131349563599\n",
      "          vf_loss: 0.0007021948643442657\n",
      "    num_agent_steps_sampled: 543000\n",
      "    num_agent_steps_trained: 543000\n",
      "    num_steps_sampled: 543000\n",
      "    num_steps_trained: 543000\n",
      "  iterations_since_restore: 543\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.03793103448277\n",
      "    ram_util_percent: 50.837931034482764\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06774178496216304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.962222732505222\n",
      "    mean_inference_ms: 2.4617224534548168\n",
      "    mean_raw_obs_processing_ms: 0.8799607105835568\n",
      "  time_since_restore: 11873.527325630188\n",
      "  time_this_iter_s: 20.264397144317627\n",
      "  time_total_s: 11873.527325630188\n",
      "  timers:\n",
      "    learn_throughput: 312.609\n",
      "    learn_time_ms: 3198.885\n",
      "    load_throughput: 24245.975\n",
      "    load_time_ms: 41.244\n",
      "    sample_throughput: 49.611\n",
      "    sample_time_ms: 20156.96\n",
      "    update_time_ms: 8.232\n",
      "  timestamp: 1631890220\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 543000\n",
      "  training_iteration: 543\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 64.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   543</td><td style=\"text-align: right;\">         11873.5</td><td style=\"text-align: right;\">543000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               995</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 544000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-50-39\n",
      "  done: false\n",
      "  episode_len_mean: 995.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 546\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.13886543331620343\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8373499797450171\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011584133319510246\n",
      "          policy_loss: -0.04968231775694423\n",
      "          total_loss: -0.05561392286585437\n",
      "          vf_explained_var: -0.7966041564941406\n",
      "          vf_loss: 0.0008332585712196305\n",
      "    num_agent_steps_sampled: 544000\n",
      "    num_agent_steps_trained: 544000\n",
      "    num_steps_sampled: 544000\n",
      "    num_steps_trained: 544000\n",
      "  iterations_since_restore: 544\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.06785714285715\n",
      "    ram_util_percent: 50.94642857142857\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06774569877446555\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.961190265663316\n",
      "    mean_inference_ms: 2.461812513253028\n",
      "    mean_raw_obs_processing_ms: 0.8802635294095451\n",
      "  time_since_restore: 11892.985483407974\n",
      "  time_this_iter_s: 19.458157777786255\n",
      "  time_total_s: 11892.985483407974\n",
      "  timers:\n",
      "    learn_throughput: 298.34\n",
      "    learn_time_ms: 3351.883\n",
      "    load_throughput: 23932.728\n",
      "    load_time_ms: 41.784\n",
      "    sample_throughput: 49.381\n",
      "    sample_time_ms: 20250.585\n",
      "    update_time_ms: 8.303\n",
      "  timestamp: 1631890239\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 544000\n",
      "  training_iteration: 544\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 64.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   544</td><td style=\"text-align: right;\">           11893</td><td style=\"text-align: right;\">544000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               995</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 545000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-51-00\n",
      "  done: false\n",
      "  episode_len_mean: 995.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 547\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.13886543331620343\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8040902296702067\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01367623891965053\n",
      "          policy_loss: -0.020471386363108954\n",
      "          total_loss: -0.02548679821193218\n",
      "          vf_explained_var: -0.7668613195419312\n",
      "          vf_loss: 0.0011263327514623395\n",
      "    num_agent_steps_sampled: 545000\n",
      "    num_agent_steps_trained: 545000\n",
      "    num_steps_sampled: 545000\n",
      "    num_steps_trained: 545000\n",
      "  iterations_since_restore: 545\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.85862068965517\n",
      "    ram_util_percent: 50.975862068965505\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06774958785097451\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.96016972417279\n",
      "    mean_inference_ms: 2.4619030787921985\n",
      "    mean_raw_obs_processing_ms: 0.8805681208965662\n",
      "  time_since_restore: 11913.755491733551\n",
      "  time_this_iter_s: 20.770008325576782\n",
      "  time_total_s: 11913.755491733551\n",
      "  timers:\n",
      "    learn_throughput: 297.206\n",
      "    learn_time_ms: 3364.673\n",
      "    load_throughput: 23112.882\n",
      "    load_time_ms: 43.266\n",
      "    sample_throughput: 51.259\n",
      "    sample_time_ms: 19508.778\n",
      "    update_time_ms: 7.681\n",
      "  timestamp: 1631890260\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 545000\n",
      "  training_iteration: 545\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 64.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   545</td><td style=\"text-align: right;\">         11913.8</td><td style=\"text-align: right;\">545000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               995</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 546000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-51-20\n",
      "  done: false\n",
      "  episode_len_mean: 995.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 548\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.13886543331620343\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6901408056418101\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012269038215945353\n",
      "          policy_loss: -0.03012615351213349\n",
      "          total_loss: -0.03465963610344463\n",
      "          vf_explained_var: -0.08161798119544983\n",
      "          vf_loss: 0.0006641816212019573\n",
      "    num_agent_steps_sampled: 546000\n",
      "    num_agent_steps_trained: 546000\n",
      "    num_steps_sampled: 546000\n",
      "    num_steps_trained: 546000\n",
      "  iterations_since_restore: 546\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.27931034482758\n",
      "    ram_util_percent: 51.03103448275862\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06775349495744779\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.959195447256832\n",
      "    mean_inference_ms: 2.46199568855388\n",
      "    mean_raw_obs_processing_ms: 0.8808752097191068\n",
      "  time_since_restore: 11933.787879705429\n",
      "  time_this_iter_s: 20.03238797187805\n",
      "  time_total_s: 11933.787879705429\n",
      "  timers:\n",
      "    learn_throughput: 297.121\n",
      "    learn_time_ms: 3365.637\n",
      "    load_throughput: 23141.907\n",
      "    load_time_ms: 43.212\n",
      "    sample_throughput: 52.574\n",
      "    sample_time_ms: 19020.843\n",
      "    update_time_ms: 7.644\n",
      "  timestamp: 1631890280\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 546000\n",
      "  training_iteration: 546\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 64.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   546</td><td style=\"text-align: right;\">         11933.8</td><td style=\"text-align: right;\">546000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               995</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 547000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-51-41\n",
      "  done: false\n",
      "  episode_len_mean: 995.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 549\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.13886543331620343\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6317944818072849\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01215432860326781\n",
      "          policy_loss: -0.03766509820189741\n",
      "          total_loss: -0.0416075684544113\n",
      "          vf_explained_var: -0.9124952554702759\n",
      "          vf_loss: 0.0006876569703712852\n",
      "    num_agent_steps_sampled: 547000\n",
      "    num_agent_steps_trained: 547000\n",
      "    num_steps_sampled: 547000\n",
      "    num_steps_trained: 547000\n",
      "  iterations_since_restore: 547\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.60000000000001\n",
      "    ram_util_percent: 51.010000000000005\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0677573404724171\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.958238201892582\n",
      "    mean_inference_ms: 2.4620872778682097\n",
      "    mean_raw_obs_processing_ms: 0.8811846816997455\n",
      "  time_since_restore: 11955.050581932068\n",
      "  time_this_iter_s: 21.262702226638794\n",
      "  time_total_s: 11955.050581932068\n",
      "  timers:\n",
      "    learn_throughput: 296.55\n",
      "    learn_time_ms: 3372.108\n",
      "    load_throughput: 23248.476\n",
      "    load_time_ms: 43.014\n",
      "    sample_throughput: 52.722\n",
      "    sample_time_ms: 18967.457\n",
      "    update_time_ms: 7.717\n",
      "  timestamp: 1631890301\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 547000\n",
      "  training_iteration: 547\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 64.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   547</td><td style=\"text-align: right;\">         11955.1</td><td style=\"text-align: right;\">547000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               995</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 548000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-52-01\n",
      "  done: false\n",
      "  episode_len_mean: 996.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 550\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.13886543331620343\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.2654536149568028\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003950761348992958\n",
      "          policy_loss: -0.10433581781884034\n",
      "          total_loss: -0.1063335683196783\n",
      "          vf_explained_var: 0.646876335144043\n",
      "          vf_loss: 0.00010816338976332594\n",
      "    num_agent_steps_sampled: 548000\n",
      "    num_agent_steps_trained: 548000\n",
      "    num_steps_sampled: 548000\n",
      "    num_steps_trained: 548000\n",
      "  iterations_since_restore: 548\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.64999999999999\n",
      "    ram_util_percent: 51.15\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06776115219170542\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.957281511714195\n",
      "    mean_inference_ms: 2.462181292208909\n",
      "    mean_raw_obs_processing_ms: 0.8810274553414318\n",
      "  time_since_restore: 11974.79595708847\n",
      "  time_this_iter_s: 19.745375156402588\n",
      "  time_total_s: 11974.79595708847\n",
      "  timers:\n",
      "    learn_throughput: 295.419\n",
      "    learn_time_ms: 3385.021\n",
      "    load_throughput: 23178.444\n",
      "    load_time_ms: 43.144\n",
      "    sample_throughput: 58.812\n",
      "    sample_time_ms: 17003.212\n",
      "    update_time_ms: 6.526\n",
      "  timestamp: 1631890321\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 548000\n",
      "  training_iteration: 548\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 64.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   548</td><td style=\"text-align: right;\">         11974.8</td><td style=\"text-align: right;\">548000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             996.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 549000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-52-22\n",
      "  done: false\n",
      "  episode_len_mean: 996.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 551\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06943271665810172\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8609856426715851\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019660310190528482\n",
      "          policy_loss: -0.03563034472366174\n",
      "          total_loss: -0.04182898998260498\n",
      "          vf_explained_var: -0.9549055099487305\n",
      "          vf_loss: 0.001046142238839012\n",
      "    num_agent_steps_sampled: 549000\n",
      "    num_agent_steps_trained: 549000\n",
      "    num_steps_sampled: 549000\n",
      "    num_steps_trained: 549000\n",
      "  iterations_since_restore: 549\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.86\n",
      "    ram_util_percent: 51.166666666666664\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06776504683559965\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.956340734917912\n",
      "    mean_inference_ms: 2.462277868405303\n",
      "    mean_raw_obs_processing_ms: 0.8808736258197223\n",
      "  time_since_restore: 11995.583030700684\n",
      "  time_this_iter_s: 20.787073612213135\n",
      "  time_total_s: 11995.583030700684\n",
      "  timers:\n",
      "    learn_throughput: 293.578\n",
      "    learn_time_ms: 3406.248\n",
      "    load_throughput: 22926.033\n",
      "    load_time_ms: 43.619\n",
      "    sample_throughput: 59.417\n",
      "    sample_time_ms: 16830.143\n",
      "    update_time_ms: 6.495\n",
      "  timestamp: 1631890342\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 549000\n",
      "  training_iteration: 549\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 64.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   549</td><td style=\"text-align: right;\">         11995.6</td><td style=\"text-align: right;\">549000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             996.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 550000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-52-42\n",
      "  done: false\n",
      "  episode_len_mean: 996.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 552\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06943271665810172\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0520539187722735\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01713677066602687\n",
      "          policy_loss: -0.03643836176229848\n",
      "          total_loss: -0.0450913673473729\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0006776819008842318\n",
      "    num_agent_steps_sampled: 550000\n",
      "    num_agent_steps_trained: 550000\n",
      "    num_steps_sampled: 550000\n",
      "    num_steps_trained: 550000\n",
      "  iterations_since_restore: 550\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.22999999999999\n",
      "    ram_util_percent: 51.28000000000001\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06776905491333297\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.955450721728932\n",
      "    mean_inference_ms: 2.4623757979682734\n",
      "    mean_raw_obs_processing_ms: 0.8807224426541975\n",
      "  time_since_restore: 12016.17519903183\n",
      "  time_this_iter_s: 20.59216833114624\n",
      "  time_total_s: 12016.17519903183\n",
      "  timers:\n",
      "    learn_throughput: 293.84\n",
      "    learn_time_ms: 3403.216\n",
      "    load_throughput: 22908.302\n",
      "    load_time_ms: 43.652\n",
      "    sample_throughput: 59.481\n",
      "    sample_time_ms: 16812.15\n",
      "    update_time_ms: 6.582\n",
      "  timestamp: 1631890362\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 550000\n",
      "  training_iteration: 550\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 64.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   550</td><td style=\"text-align: right;\">         12016.2</td><td style=\"text-align: right;\">550000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             996.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 551000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-53-02\n",
      "  done: false\n",
      "  episode_len_mean: 996.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 553\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06943271665810172\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.641075274017122\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.037220112427298425\n",
      "          policy_loss: -0.056643428405125935\n",
      "          total_loss: -0.059711064563857184\n",
      "          vf_explained_var: 0.3616931438446045\n",
      "          vf_loss: 0.0007588251616754051\n",
      "    num_agent_steps_sampled: 551000\n",
      "    num_agent_steps_trained: 551000\n",
      "    num_steps_sampled: 551000\n",
      "    num_steps_trained: 551000\n",
      "  iterations_since_restore: 551\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.14642857142856\n",
      "    ram_util_percent: 51.30357142857143\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0677729992457682\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.954562367052402\n",
      "    mean_inference_ms: 2.462473350777284\n",
      "    mean_raw_obs_processing_ms: 0.8805728943229054\n",
      "  time_since_restore: 12035.834413528442\n",
      "  time_this_iter_s: 19.65921449661255\n",
      "  time_total_s: 12035.834413528442\n",
      "  timers:\n",
      "    learn_throughput: 296.581\n",
      "    learn_time_ms: 3371.757\n",
      "    load_throughput: 22709.931\n",
      "    load_time_ms: 44.034\n",
      "    sample_throughput: 59.453\n",
      "    sample_time_ms: 16819.9\n",
      "    update_time_ms: 6.479\n",
      "  timestamp: 1631890382\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 551000\n",
      "  training_iteration: 551\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 64.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   551</td><td style=\"text-align: right;\">         12035.8</td><td style=\"text-align: right;\">551000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             996.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 552000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-53-22\n",
      "  done: false\n",
      "  episode_len_mean: 996.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 554\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10414907498715255\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5263188011116452\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01387713567277468\n",
      "          policy_loss: 0.019991359776920743\n",
      "          total_loss: 0.01660028232468499\n",
      "          vf_explained_var: 0.17517971992492676\n",
      "          vf_loss: 0.0004268202887841552\n",
      "    num_agent_steps_sampled: 552000\n",
      "    num_agent_steps_trained: 552000\n",
      "    num_steps_sampled: 552000\n",
      "    num_steps_trained: 552000\n",
      "  iterations_since_restore: 552\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.04285714285716\n",
      "    ram_util_percent: 51.342857142857156\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0677769310824751\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.953722097989132\n",
      "    mean_inference_ms: 2.4625724295213134\n",
      "    mean_raw_obs_processing_ms: 0.8804260247020116\n",
      "  time_since_restore: 12055.769590139389\n",
      "  time_this_iter_s: 19.935176610946655\n",
      "  time_total_s: 12055.769590139389\n",
      "  timers:\n",
      "    learn_throughput: 295.48\n",
      "    learn_time_ms: 3384.328\n",
      "    load_throughput: 22181.111\n",
      "    load_time_ms: 45.083\n",
      "    sample_throughput: 59.496\n",
      "    sample_time_ms: 16807.763\n",
      "    update_time_ms: 6.082\n",
      "  timestamp: 1631890402\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 552000\n",
      "  training_iteration: 552\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 64.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   552</td><td style=\"text-align: right;\">         12055.8</td><td style=\"text-align: right;\">552000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             996.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 553000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-53-43\n",
      "  done: false\n",
      "  episode_len_mean: 996.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 555\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10414907498715255\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7334414137734306\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03657967502369408\n",
      "          policy_loss: -0.031096112148629295\n",
      "          total_loss: -0.043453501330481634\n",
      "          vf_explained_var: -0.36016806960105896\n",
      "          vf_loss: 0.001167285750206146\n",
      "    num_agent_steps_sampled: 553000\n",
      "    num_agent_steps_trained: 553000\n",
      "    num_steps_sampled: 553000\n",
      "    num_steps_trained: 553000\n",
      "  iterations_since_restore: 553\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.02666666666669\n",
      "    ram_util_percent: 51.38333333333333\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06778086960658243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.952937401029436\n",
      "    mean_inference_ms: 2.4626721181333515\n",
      "    mean_raw_obs_processing_ms: 0.8802820388419077\n",
      "  time_since_restore: 12076.957637786865\n",
      "  time_this_iter_s: 21.188047647476196\n",
      "  time_total_s: 12076.957637786865\n",
      "  timers:\n",
      "    learn_throughput: 296.407\n",
      "    learn_time_ms: 3373.735\n",
      "    load_throughput: 22305.002\n",
      "    load_time_ms: 44.833\n",
      "    sample_throughput: 59.137\n",
      "    sample_time_ms: 16909.937\n",
      "    update_time_ms: 7.204\n",
      "  timestamp: 1631890423\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 553000\n",
      "  training_iteration: 553\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 64.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   553</td><td style=\"text-align: right;\">           12077</td><td style=\"text-align: right;\">553000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             996.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 554000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-54-04\n",
      "  done: false\n",
      "  episode_len_mean: 996.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 556\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15622361248072886\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1225010368559096\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.024614808451999256\n",
      "          policy_loss: -0.026257587348421416\n",
      "          total_loss: -0.043419818000661005\n",
      "          vf_explained_var: -0.6154891848564148\n",
      "          vf_loss: 0.0002173658935741211\n",
      "    num_agent_steps_sampled: 554000\n",
      "    num_agent_steps_trained: 554000\n",
      "    num_steps_sampled: 554000\n",
      "    num_steps_trained: 554000\n",
      "  iterations_since_restore: 554\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.95862068965519\n",
      "    ram_util_percent: 51.37931034482759\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0677847825290482\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.952203376995932\n",
      "    mean_inference_ms: 2.4627722799371825\n",
      "    mean_raw_obs_processing_ms: 0.8801412618937013\n",
      "  time_since_restore: 12097.224107027054\n",
      "  time_this_iter_s: 20.2664692401886\n",
      "  time_total_s: 12097.224107027054\n",
      "  timers:\n",
      "    learn_throughput: 298.092\n",
      "    learn_time_ms: 3354.672\n",
      "    load_throughput: 21346.112\n",
      "    load_time_ms: 46.847\n",
      "    sample_throughput: 58.797\n",
      "    sample_time_ms: 17007.673\n",
      "    update_time_ms: 7.287\n",
      "  timestamp: 1631890444\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 554000\n",
      "  training_iteration: 554\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 64.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   554</td><td style=\"text-align: right;\">         12097.2</td><td style=\"text-align: right;\">554000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             996.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 555000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-54-24\n",
      "  done: false\n",
      "  episode_len_mean: 996.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 557\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.003426965077718\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015472207837929833\n",
      "          policy_loss: -0.01391542282783323\n",
      "          total_loss: -0.030122434265083736\n",
      "          vf_explained_var: -0.25269582867622375\n",
      "          vf_loss: 0.00020157211882987111\n",
      "    num_agent_steps_sampled: 555000\n",
      "    num_agent_steps_trained: 555000\n",
      "    num_steps_sampled: 555000\n",
      "    num_steps_trained: 555000\n",
      "  iterations_since_restore: 555\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.02000000000001\n",
      "    ram_util_percent: 51.406666666666666\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06778870581204276\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.95152031609572\n",
      "    mean_inference_ms: 2.462872029543699\n",
      "    mean_raw_obs_processing_ms: 0.8800034834181959\n",
      "  time_since_restore: 12117.763202905655\n",
      "  time_this_iter_s: 20.539095878601074\n",
      "  time_total_s: 12117.763202905655\n",
      "  timers:\n",
      "    learn_throughput: 299.863\n",
      "    learn_time_ms: 3334.853\n",
      "    load_throughput: 21739.208\n",
      "    load_time_ms: 46.0\n",
      "    sample_throughput: 58.84\n",
      "    sample_time_ms: 16995.268\n",
      "    update_time_ms: 7.256\n",
      "  timestamp: 1631890464\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 555000\n",
      "  training_iteration: 555\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 64.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   555</td><td style=\"text-align: right;\">         12117.8</td><td style=\"text-align: right;\">555000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             996.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 556000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-54-44\n",
      "  done: false\n",
      "  episode_len_mean: 996.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 558\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2122255947854783\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010823173225033027\n",
      "          policy_loss: 0.05427639186382294\n",
      "          total_loss: 0.03507832280463642\n",
      "          vf_explained_var: -0.273141086101532\n",
      "          vf_loss: 0.0003879358381204333\n",
      "    num_agent_steps_sampled: 556000\n",
      "    num_agent_steps_trained: 556000\n",
      "    num_steps_sampled: 556000\n",
      "    num_steps_trained: 556000\n",
      "  iterations_since_restore: 556\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.51111111111112\n",
      "    ram_util_percent: 51.39999999999999\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06779257712034402\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.950817220195443\n",
      "    mean_inference_ms: 2.4629711758565977\n",
      "    mean_raw_obs_processing_ms: 0.8798685582809331\n",
      "  time_since_restore: 12137.07840871811\n",
      "  time_this_iter_s: 19.315205812454224\n",
      "  time_total_s: 12137.07840871811\n",
      "  timers:\n",
      "    learn_throughput: 299.617\n",
      "    learn_time_ms: 3337.591\n",
      "    load_throughput: 22080.921\n",
      "    load_time_ms: 45.288\n",
      "    sample_throughput: 59.1\n",
      "    sample_time_ms: 16920.487\n",
      "    update_time_ms: 7.348\n",
      "  timestamp: 1631890484\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 556000\n",
      "  training_iteration: 556\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 64.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   556</td><td style=\"text-align: right;\">         12137.1</td><td style=\"text-align: right;\">556000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             996.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 557000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-55-05\n",
      "  done: false\n",
      "  episode_len_mean: 996.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 559\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.243946894009908\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011401337344944472\n",
      "          policy_loss: -0.021929761477642588\n",
      "          total_loss: -0.030740551981661056\n",
      "          vf_explained_var: -0.39025434851646423\n",
      "          vf_loss: 0.000956939901677995\n",
      "    num_agent_steps_sampled: 557000\n",
      "    num_agent_steps_trained: 557000\n",
      "    num_steps_sampled: 557000\n",
      "    num_steps_trained: 557000\n",
      "  iterations_since_restore: 557\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.08387096774193\n",
      "    ram_util_percent: 51.40322580645162\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06779643876665575\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.950173150608626\n",
      "    mean_inference_ms: 2.4630706461246765\n",
      "    mean_raw_obs_processing_ms: 0.8797349948077858\n",
      "  time_since_restore: 12158.61889076233\n",
      "  time_this_iter_s: 21.54048204421997\n",
      "  time_total_s: 12158.61889076233\n",
      "  timers:\n",
      "    learn_throughput: 299.359\n",
      "    learn_time_ms: 3340.475\n",
      "    load_throughput: 22492.774\n",
      "    load_time_ms: 44.459\n",
      "    sample_throughput: 59.011\n",
      "    sample_time_ms: 16946.131\n",
      "    update_time_ms: 7.296\n",
      "  timestamp: 1631890505\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 557000\n",
      "  training_iteration: 557\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 64.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   557</td><td style=\"text-align: right;\">         12158.6</td><td style=\"text-align: right;\">557000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             996.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 558000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-55-25\n",
      "  done: false\n",
      "  episode_len_mean: 996.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 560\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.470904509226481\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01065743543801342\n",
      "          policy_loss: 0.04823159151193168\n",
      "          total_loss: 0.026112957919637362\n",
      "          vf_explained_var: -0.6041109561920166\n",
      "          vf_loss: 9.299604847304485e-05\n",
      "    num_agent_steps_sampled: 558000\n",
      "    num_agent_steps_trained: 558000\n",
      "    num_steps_sampled: 558000\n",
      "    num_steps_trained: 558000\n",
      "  iterations_since_restore: 558\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.85714285714285\n",
      "    ram_util_percent: 51.5\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06780027561594587\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.949537483414296\n",
      "    mean_inference_ms: 2.463168448310547\n",
      "    mean_raw_obs_processing_ms: 0.8796040133416436\n",
      "  time_since_restore: 12178.372661352158\n",
      "  time_this_iter_s: 19.75377058982849\n",
      "  time_total_s: 12178.372661352158\n",
      "  timers:\n",
      "    learn_throughput: 301.113\n",
      "    learn_time_ms: 3321.017\n",
      "    load_throughput: 21498.16\n",
      "    load_time_ms: 46.516\n",
      "    sample_throughput: 58.946\n",
      "    sample_time_ms: 16964.62\n",
      "    update_time_ms: 7.152\n",
      "  timestamp: 1631890525\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 558000\n",
      "  training_iteration: 558\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 64.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   558</td><td style=\"text-align: right;\">         12178.4</td><td style=\"text-align: right;\">558000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             996.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 559000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-55-44\n",
      "  done: false\n",
      "  episode_len_mean: 996.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 561\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.435788165198432\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0144648268887157\n",
      "          policy_loss: -0.0027323000443478427\n",
      "          total_loss: -0.023533248321877587\n",
      "          vf_explained_var: -0.810911238193512\n",
      "          vf_loss: 0.00016731032554970524\n",
      "    num_agent_steps_sampled: 559000\n",
      "    num_agent_steps_trained: 559000\n",
      "    num_steps_sampled: 559000\n",
      "    num_steps_trained: 559000\n",
      "  iterations_since_restore: 559\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.58148148148148\n",
      "    ram_util_percent: 51.43333333333333\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06780416932924223\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.9488804844438\n",
      "    mean_inference_ms: 2.4632670780520702\n",
      "    mean_raw_obs_processing_ms: 0.8794756811181301\n",
      "  time_since_restore: 12197.411180496216\n",
      "  time_this_iter_s: 19.038519144058228\n",
      "  time_total_s: 12197.411180496216\n",
      "  timers:\n",
      "    learn_throughput: 302.654\n",
      "    learn_time_ms: 3304.098\n",
      "    load_throughput: 22808.419\n",
      "    load_time_ms: 43.843\n",
      "    sample_throughput: 59.491\n",
      "    sample_time_ms: 16809.15\n",
      "    update_time_ms: 7.202\n",
      "  timestamp: 1631890544\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 559000\n",
      "  training_iteration: 559\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 64.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   559</td><td style=\"text-align: right;\">         12197.4</td><td style=\"text-align: right;\">559000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             996.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 560000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-56-03\n",
      "  done: false\n",
      "  episode_len_mean: 996.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 562\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.638985766304864\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011118142319694138\n",
      "          policy_loss: -0.03158053654349512\n",
      "          total_loss: -0.05522711219059096\n",
      "          vf_explained_var: -0.7559643387794495\n",
      "          vf_loss: 0.0001379084200531603\n",
      "    num_agent_steps_sampled: 560000\n",
      "    num_agent_steps_trained: 560000\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "  iterations_since_restore: 560\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.67407407407407\n",
      "    ram_util_percent: 51.45555555555556\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06780809770811336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.948257980759374\n",
      "    mean_inference_ms: 2.463367338581085\n",
      "    mean_raw_obs_processing_ms: 0.8793503088866234\n",
      "  time_since_restore: 12216.348965644836\n",
      "  time_this_iter_s: 18.937785148620605\n",
      "  time_total_s: 12216.348965644836\n",
      "  timers:\n",
      "    learn_throughput: 303.006\n",
      "    learn_time_ms: 3300.264\n",
      "    load_throughput: 22739.801\n",
      "    load_time_ms: 43.976\n",
      "    sample_throughput: 60.068\n",
      "    sample_time_ms: 16647.919\n",
      "    update_time_ms: 6.928\n",
      "  timestamp: 1631890563\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 560\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   560</td><td style=\"text-align: right;\">         12216.3</td><td style=\"text-align: right;\">560000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             996.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 561000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-56-23\n",
      "  done: false\n",
      "  episode_len_mean: 996.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 563\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6260332187016804\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011774787196701435\n",
      "          policy_loss: -0.12032005590283208\n",
      "          total_loss: -0.14371976773771974\n",
      "          vf_explained_var: -0.17836827039718628\n",
      "          vf_loss: 0.00010137118519600739\n",
      "    num_agent_steps_sampled: 561000\n",
      "    num_agent_steps_trained: 561000\n",
      "    num_steps_sampled: 561000\n",
      "    num_steps_trained: 561000\n",
      "  iterations_since_restore: 561\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.4551724137931\n",
      "    ram_util_percent: 51.46896551724139\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06781206366376084\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.947648168370629\n",
      "    mean_inference_ms: 2.463468090200001\n",
      "    mean_raw_obs_processing_ms: 0.879227583140438\n",
      "  time_since_restore: 12236.324700593948\n",
      "  time_this_iter_s: 19.97573494911194\n",
      "  time_total_s: 12236.324700593948\n",
      "  timers:\n",
      "    learn_throughput: 301.248\n",
      "    learn_time_ms: 3319.52\n",
      "    load_throughput: 22985.536\n",
      "    load_time_ms: 43.506\n",
      "    sample_throughput: 60.021\n",
      "    sample_time_ms: 16660.779\n",
      "    update_time_ms: 6.954\n",
      "  timestamp: 1631890583\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 561000\n",
      "  training_iteration: 561\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 64.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   561</td><td style=\"text-align: right;\">         12236.3</td><td style=\"text-align: right;\">561000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             996.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 562000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-56-41\n",
      "  done: false\n",
      "  episode_len_mean: 996.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 564\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6208938439687093\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01687875997952533\n",
      "          policy_loss: -0.06003773294182287\n",
      "          total_loss: -0.08212081626471546\n",
      "          vf_explained_var: -0.4520702660083771\n",
      "          vf_loss: 0.00017056520960573432\n",
      "    num_agent_steps_sampled: 562000\n",
      "    num_agent_steps_trained: 562000\n",
      "    num_steps_sampled: 562000\n",
      "    num_steps_trained: 562000\n",
      "  iterations_since_restore: 562\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.07307692307693\n",
      "    ram_util_percent: 51.353846153846156\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06781598370056496\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.947021073379936\n",
      "    mean_inference_ms: 2.4635669669248332\n",
      "    mean_raw_obs_processing_ms: 0.8791080139252778\n",
      "  time_since_restore: 12254.664723873138\n",
      "  time_this_iter_s: 18.340023279190063\n",
      "  time_total_s: 12254.664723873138\n",
      "  timers:\n",
      "    learn_throughput: 302.906\n",
      "    learn_time_ms: 3301.355\n",
      "    load_throughput: 23565.677\n",
      "    load_time_ms: 42.435\n",
      "    sample_throughput: 60.531\n",
      "    sample_time_ms: 16520.456\n",
      "    update_time_ms: 6.959\n",
      "  timestamp: 1631890601\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 562000\n",
      "  training_iteration: 562\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 64.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   562</td><td style=\"text-align: right;\">         12254.7</td><td style=\"text-align: right;\">562000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             996.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 563000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-56-59\n",
      "  done: false\n",
      "  episode_len_mean: 996.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 565\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.679516617457072\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008578888717271797\n",
      "          policy_loss: -0.12596239373087884\n",
      "          total_loss: -0.150668649127086\n",
      "          vf_explained_var: -0.4396516680717468\n",
      "          vf_loss: 7.857323933219757e-05\n",
      "    num_agent_steps_sampled: 563000\n",
      "    num_agent_steps_trained: 563000\n",
      "    num_steps_sampled: 563000\n",
      "    num_steps_trained: 563000\n",
      "  iterations_since_restore: 563\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.77307692307691\n",
      "    ram_util_percent: 51.580769230769235\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06781983289554958\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.946340054485901\n",
      "    mean_inference_ms: 2.463662371409472\n",
      "    mean_raw_obs_processing_ms: 0.8789906503770883\n",
      "  time_since_restore: 12272.729162454605\n",
      "  time_this_iter_s: 18.064438581466675\n",
      "  time_total_s: 12272.729162454605\n",
      "  timers:\n",
      "    learn_throughput: 304.436\n",
      "    learn_time_ms: 3284.761\n",
      "    load_throughput: 23580.197\n",
      "    load_time_ms: 42.408\n",
      "    sample_throughput: 61.629\n",
      "    sample_time_ms: 16226.024\n",
      "    update_time_ms: 5.781\n",
      "  timestamp: 1631890619\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 563000\n",
      "  training_iteration: 563\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   563</td><td style=\"text-align: right;\">         12272.7</td><td style=\"text-align: right;\">563000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             996.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 564000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-57-19\n",
      "  done: false\n",
      "  episode_len_mean: 996.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 566\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6522661871380278\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007334050864540111\n",
      "          policy_loss: 0.020488780964579848\n",
      "          total_loss: -0.004192402751909362\n",
      "          vf_explained_var: -0.24534155428409576\n",
      "          vf_loss: 0.0001228506003422404\n",
      "    num_agent_steps_sampled: 564000\n",
      "    num_agent_steps_trained: 564000\n",
      "    num_steps_sampled: 564000\n",
      "    num_steps_trained: 564000\n",
      "  iterations_since_restore: 564\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.84285714285716\n",
      "    ram_util_percent: 51.550000000000004\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06782377179282383\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.945619825427194\n",
      "    mean_inference_ms: 2.4637580620997923\n",
      "    mean_raw_obs_processing_ms: 0.8788759971969212\n",
      "  time_since_restore: 12292.213168144226\n",
      "  time_this_iter_s: 19.48400568962097\n",
      "  time_total_s: 12292.213168144226\n",
      "  timers:\n",
      "    learn_throughput: 303.827\n",
      "    learn_time_ms: 3291.35\n",
      "    load_throughput: 24696.812\n",
      "    load_time_ms: 40.491\n",
      "    sample_throughput: 61.945\n",
      "    sample_time_ms: 16143.24\n",
      "    update_time_ms: 5.781\n",
      "  timestamp: 1631890639\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 564000\n",
      "  training_iteration: 564\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 64.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   564</td><td style=\"text-align: right;\">         12292.2</td><td style=\"text-align: right;\">564000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             996.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 565000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-57-41\n",
      "  done: false\n",
      "  episode_len_mean: 996.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 567\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.61730444961124\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010009049059739105\n",
      "          policy_loss: -0.0854279124074512\n",
      "          total_loss: -0.1091393085817496\n",
      "          vf_explained_var: -0.673344612121582\n",
      "          vf_loss: 0.00011617147324614053\n",
      "    num_agent_steps_sampled: 565000\n",
      "    num_agent_steps_trained: 565000\n",
      "    num_steps_sampled: 565000\n",
      "    num_steps_trained: 565000\n",
      "  iterations_since_restore: 565\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.66129032258065\n",
      "    ram_util_percent: 51.5258064516129\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06782780176262011\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.944893015871044\n",
      "    mean_inference_ms: 2.4638542976727793\n",
      "    mean_raw_obs_processing_ms: 0.8787630701642088\n",
      "  time_since_restore: 12313.896896362305\n",
      "  time_this_iter_s: 21.683728218078613\n",
      "  time_total_s: 12313.896896362305\n",
      "  timers:\n",
      "    learn_throughput: 304.118\n",
      "    learn_time_ms: 3288.194\n",
      "    load_throughput: 24670.33\n",
      "    load_time_ms: 40.535\n",
      "    sample_throughput: 61.461\n",
      "    sample_time_ms: 16270.456\n",
      "    update_time_ms: 6.019\n",
      "  timestamp: 1631890661\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 565000\n",
      "  training_iteration: 565\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 64.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   565</td><td style=\"text-align: right;\">         12313.9</td><td style=\"text-align: right;\">565000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             996.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 566000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-58-01\n",
      "  done: false\n",
      "  episode_len_mean: 996.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 568\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5829479161236021\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015653202063300498\n",
      "          policy_loss: 0.048568521646989715\n",
      "          total_loss: 0.04717894546273682\n",
      "          vf_explained_var: -0.5312014818191528\n",
      "          vf_loss: 0.0007718025868396378\n",
      "    num_agent_steps_sampled: 566000\n",
      "    num_agent_steps_trained: 566000\n",
      "    num_steps_sampled: 566000\n",
      "    num_steps_trained: 566000\n",
      "  iterations_since_restore: 566\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.36551724137931\n",
      "    ram_util_percent: 51.55862068965516\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06783181420311746\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.944164452075432\n",
      "    mean_inference_ms: 2.463948413598713\n",
      "    mean_raw_obs_processing_ms: 0.8786520402711663\n",
      "  time_since_restore: 12334.38681602478\n",
      "  time_this_iter_s: 20.489919662475586\n",
      "  time_total_s: 12334.38681602478\n",
      "  timers:\n",
      "    learn_throughput: 304.279\n",
      "    learn_time_ms: 3286.455\n",
      "    load_throughput: 24068.995\n",
      "    load_time_ms: 41.547\n",
      "    sample_throughput: 61.015\n",
      "    sample_time_ms: 16389.32\n",
      "    update_time_ms: 5.978\n",
      "  timestamp: 1631890681\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 566000\n",
      "  training_iteration: 566\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 64.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   566</td><td style=\"text-align: right;\">         12334.4</td><td style=\"text-align: right;\">566000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             996.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 567000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-58-20\n",
      "  done: false\n",
      "  episode_len_mean: 996.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 569\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6968694819344416\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006839514190083622\n",
      "          policy_loss: -0.002756153502398067\n",
      "          total_loss: -0.027967258335815537\n",
      "          vf_explained_var: -0.30175158381462097\n",
      "          vf_loss: 0.00015484710609143805\n",
      "    num_agent_steps_sampled: 567000\n",
      "    num_agent_steps_trained: 567000\n",
      "    num_steps_sampled: 567000\n",
      "    num_steps_trained: 567000\n",
      "  iterations_since_restore: 567\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.21851851851852\n",
      "    ram_util_percent: 51.474074074074075\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06783574306437574\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.943424631157576\n",
      "    mean_inference_ms: 2.4640417902836766\n",
      "    mean_raw_obs_processing_ms: 0.8785436614025809\n",
      "  time_since_restore: 12353.059081554413\n",
      "  time_this_iter_s: 18.67226552963257\n",
      "  time_total_s: 12353.059081554413\n",
      "  timers:\n",
      "    learn_throughput: 306.575\n",
      "    learn_time_ms: 3261.844\n",
      "    load_throughput: 23513.77\n",
      "    load_time_ms: 42.528\n",
      "    sample_throughput: 62.011\n",
      "    sample_time_ms: 16126.073\n",
      "    update_time_ms: 6.075\n",
      "  timestamp: 1631890700\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 567000\n",
      "  training_iteration: 567\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 64.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   567</td><td style=\"text-align: right;\">         12353.1</td><td style=\"text-align: right;\">567000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             996.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 568000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-58-57\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 570\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.662915439075894\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00923913383267681\n",
      "          policy_loss: -0.0557419040447308\n",
      "          total_loss: -0.08005404944221178\n",
      "          vf_explained_var: -0.4709133505821228\n",
      "          vf_loss: 0.00015195438543034673\n",
      "    num_agent_steps_sampled: 568000\n",
      "    num_agent_steps_trained: 568000\n",
      "    num_steps_sampled: 568000\n",
      "    num_steps_trained: 568000\n",
      "  iterations_since_restore: 568\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.23653846153844\n",
      "    ram_util_percent: 51.52307692307693\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06783969799765774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.942681451733497\n",
      "    mean_inference_ms: 2.464133971984678\n",
      "    mean_raw_obs_processing_ms: 0.878773614473382\n",
      "  time_since_restore: 12389.772943258286\n",
      "  time_this_iter_s: 36.71386170387268\n",
      "  time_total_s: 12389.772943258286\n",
      "  timers:\n",
      "    learn_throughput: 305.576\n",
      "    learn_time_ms: 3272.513\n",
      "    load_throughput: 23419.672\n",
      "    load_time_ms: 42.699\n",
      "    sample_throughput: 56.145\n",
      "    sample_time_ms: 17811.105\n",
      "    update_time_ms: 6.155\n",
      "  timestamp: 1631890737\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 568000\n",
      "  training_iteration: 568\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   568</td><td style=\"text-align: right;\">         12389.8</td><td style=\"text-align: right;\">568000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 569000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-59-16\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 571\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3286849008666146\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00920355159561418\n",
      "          policy_loss: -0.04772463823772139\n",
      "          total_loss: -0.06861335424085459\n",
      "          vf_explained_var: -0.648851215839386\n",
      "          vf_loss: 0.0002414150358409491\n",
      "    num_agent_steps_sampled: 569000\n",
      "    num_agent_steps_trained: 569000\n",
      "    num_steps_sampled: 569000\n",
      "    num_steps_trained: 569000\n",
      "  iterations_since_restore: 569\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.07407407407408\n",
      "    ram_util_percent: 51.766666666666666\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06784366445968956\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.94189227848137\n",
      "    mean_inference_ms: 2.4642246350528936\n",
      "    mean_raw_obs_processing_ms: 0.8790053016640766\n",
      "  time_since_restore: 12408.65892624855\n",
      "  time_this_iter_s: 18.885982990264893\n",
      "  time_total_s: 12408.65892624855\n",
      "  timers:\n",
      "    learn_throughput: 306.059\n",
      "    learn_time_ms: 3267.339\n",
      "    load_throughput: 21891.819\n",
      "    load_time_ms: 45.679\n",
      "    sample_throughput: 56.187\n",
      "    sample_time_ms: 17797.631\n",
      "    update_time_ms: 6.128\n",
      "  timestamp: 1631890756\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 569000\n",
      "  training_iteration: 569\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   569</td><td style=\"text-align: right;\">         12408.7</td><td style=\"text-align: right;\">569000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 570000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-59-34\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 572\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4096210598945618\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016860103145273798\n",
      "          policy_loss: -0.05947778556081984\n",
      "          total_loss: -0.07921777756677734\n",
      "          vf_explained_var: 0.13797637820243835\n",
      "          vf_loss: 0.0004052987065102166\n",
      "    num_agent_steps_sampled: 570000\n",
      "    num_agent_steps_trained: 570000\n",
      "    num_steps_sampled: 570000\n",
      "    num_steps_trained: 570000\n",
      "  iterations_since_restore: 570\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.8423076923077\n",
      "    ram_util_percent: 51.896153846153844\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06784761349048929\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.941087555321474\n",
      "    mean_inference_ms: 2.4643152840972578\n",
      "    mean_raw_obs_processing_ms: 0.8792389890839394\n",
      "  time_since_restore: 12426.852941036224\n",
      "  time_this_iter_s: 18.19401478767395\n",
      "  time_total_s: 12426.852941036224\n",
      "  timers:\n",
      "    learn_throughput: 306.358\n",
      "    learn_time_ms: 3264.159\n",
      "    load_throughput: 21769.017\n",
      "    load_time_ms: 45.937\n",
      "    sample_throughput: 56.414\n",
      "    sample_time_ms: 17726.11\n",
      "    update_time_ms: 6.198\n",
      "  timestamp: 1631890774\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 570000\n",
      "  training_iteration: 570\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   570</td><td style=\"text-align: right;\">         12426.9</td><td style=\"text-align: right;\">570000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 571000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_14-59-52\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 573\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2923766096433003\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012703305511585149\n",
      "          policy_loss: -0.05121866514285405\n",
      "          total_loss: -0.07060125006569756\n",
      "          vf_explained_var: -0.5271025896072388\n",
      "          vf_loss: 0.0005643452199497763\n",
      "    num_agent_steps_sampled: 571000\n",
      "    num_agent_steps_trained: 571000\n",
      "    num_steps_sampled: 571000\n",
      "    num_steps_trained: 571000\n",
      "  iterations_since_restore: 571\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.83076923076923\n",
      "    ram_util_percent: 51.91923076923077\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0678514708499939\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.940229269953328\n",
      "    mean_inference_ms: 2.4644035188538274\n",
      "    mean_raw_obs_processing_ms: 0.8794745952797238\n",
      "  time_since_restore: 12445.250884056091\n",
      "  time_this_iter_s: 18.397943019866943\n",
      "  time_total_s: 12445.250884056091\n",
      "  timers:\n",
      "    learn_throughput: 306.135\n",
      "    learn_time_ms: 3266.533\n",
      "    load_throughput: 22473.756\n",
      "    load_time_ms: 44.496\n",
      "    sample_throughput: 56.924\n",
      "    sample_time_ms: 17567.259\n",
      "    update_time_ms: 6.249\n",
      "  timestamp: 1631890792\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 571000\n",
      "  training_iteration: 571\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   571</td><td style=\"text-align: right;\">         12445.3</td><td style=\"text-align: right;\">571000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 572000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-00-14\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 574\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1066295597288343\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011156854152626\n",
      "          policy_loss: -0.05363951875931687\n",
      "          total_loss: -0.07150725482238664\n",
      "          vf_explained_var: -0.44370681047439575\n",
      "          vf_loss: 0.0005841124371297863\n",
      "    num_agent_steps_sampled: 572000\n",
      "    num_agent_steps_trained: 572000\n",
      "    num_steps_sampled: 572000\n",
      "    num_steps_trained: 572000\n",
      "  iterations_since_restore: 572\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.009375\n",
      "    ram_util_percent: 51.784375\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06785536012576165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.939360047568968\n",
      "    mean_inference_ms: 2.4644910928491193\n",
      "    mean_raw_obs_processing_ms: 0.879712308619201\n",
      "  time_since_restore: 12467.436664581299\n",
      "  time_this_iter_s: 22.18578052520752\n",
      "  time_total_s: 12467.436664581299\n",
      "  timers:\n",
      "    learn_throughput: 305.954\n",
      "    learn_time_ms: 3268.463\n",
      "    load_throughput: 21840.055\n",
      "    load_time_ms: 45.787\n",
      "    sample_throughput: 55.715\n",
      "    sample_time_ms: 17948.504\n",
      "    update_time_ms: 6.228\n",
      "  timestamp: 1631890814\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 572000\n",
      "  training_iteration: 572\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   572</td><td style=\"text-align: right;\">         12467.4</td><td style=\"text-align: right;\">572000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 573000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-00-35\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 575\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.437464944521586\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009436071905337507\n",
      "          policy_loss: 0.009918878310256535\n",
      "          total_loss: -0.012006838454140557\n",
      "          vf_explained_var: -0.5372171401977539\n",
      "          vf_loss: 0.00023772601744869663\n",
      "    num_agent_steps_sampled: 573000\n",
      "    num_agent_steps_trained: 573000\n",
      "    num_steps_sampled: 573000\n",
      "    num_steps_trained: 573000\n",
      "  iterations_since_restore: 573\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.35862068965517\n",
      "    ram_util_percent: 52.01379310344828\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0678592533988494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.938545292322837\n",
      "    mean_inference_ms: 2.464579418887299\n",
      "    mean_raw_obs_processing_ms: 0.8799520184570635\n",
      "  time_since_restore: 12487.940576076508\n",
      "  time_this_iter_s: 20.50391149520874\n",
      "  time_total_s: 12487.940576076508\n",
      "  timers:\n",
      "    learn_throughput: 304.992\n",
      "    learn_time_ms: 3278.777\n",
      "    load_throughput: 21505.799\n",
      "    load_time_ms: 46.499\n",
      "    sample_throughput: 55.002\n",
      "    sample_time_ms: 18181.095\n",
      "    update_time_ms: 6.441\n",
      "  timestamp: 1631890835\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 573000\n",
      "  training_iteration: 573\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   573</td><td style=\"text-align: right;\">         12487.9</td><td style=\"text-align: right;\">573000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 574000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-00-54\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 576\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6806440035502117\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009903574385635001\n",
      "          policy_loss: -0.05665203873068094\n",
      "          total_loss: -0.08098321214525236\n",
      "          vf_explained_var: 0.2119816690683365\n",
      "          vf_loss: 0.00015450532212134426\n",
      "    num_agent_steps_sampled: 574000\n",
      "    num_agent_steps_trained: 574000\n",
      "    num_steps_sampled: 574000\n",
      "    num_steps_trained: 574000\n",
      "  iterations_since_restore: 574\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.59629629629629\n",
      "    ram_util_percent: 51.937037037037044\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06786309258286549\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.93771839532337\n",
      "    mean_inference_ms: 2.4646662119369593\n",
      "    mean_raw_obs_processing_ms: 0.880194148052924\n",
      "  time_since_restore: 12506.86190199852\n",
      "  time_this_iter_s: 18.92132592201233\n",
      "  time_total_s: 12506.86190199852\n",
      "  timers:\n",
      "    learn_throughput: 302.705\n",
      "    learn_time_ms: 3303.544\n",
      "    load_throughput: 20753.35\n",
      "    load_time_ms: 48.185\n",
      "    sample_throughput: 55.259\n",
      "    sample_time_ms: 18096.564\n",
      "    update_time_ms: 7.593\n",
      "  timestamp: 1631890854\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 574000\n",
      "  training_iteration: 574\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   574</td><td style=\"text-align: right;\">         12506.9</td><td style=\"text-align: right;\">574000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 575000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-01-17\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 577\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3822740965419347\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009018431425815425\n",
      "          policy_loss: -0.029204171412210496\n",
      "          total_loss: -0.05067782394277553\n",
      "          vf_explained_var: -0.5335296392440796\n",
      "          vf_loss: 0.00023575125695616509\n",
      "    num_agent_steps_sampled: 575000\n",
      "    num_agent_steps_trained: 575000\n",
      "    num_steps_sampled: 575000\n",
      "    num_steps_trained: 575000\n",
      "  iterations_since_restore: 575\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.5235294117647\n",
      "    ram_util_percent: 51.97647058823529\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06786690647831152\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.936979772540012\n",
      "    mean_inference_ms: 2.4647533634688052\n",
      "    mean_raw_obs_processing_ms: 0.8804381529017732\n",
      "  time_since_restore: 12529.987453222275\n",
      "  time_this_iter_s: 23.125551223754883\n",
      "  time_total_s: 12529.987453222275\n",
      "  timers:\n",
      "    learn_throughput: 299.571\n",
      "    learn_time_ms: 3338.111\n",
      "    load_throughput: 20480.03\n",
      "    load_time_ms: 48.828\n",
      "    sample_throughput: 54.929\n",
      "    sample_time_ms: 18205.465\n",
      "    update_time_ms: 7.442\n",
      "  timestamp: 1631890877\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 575000\n",
      "  training_iteration: 575\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   575</td><td style=\"text-align: right;\">           12530</td><td style=\"text-align: right;\">575000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 576000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-01-37\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 578\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6236688799328274\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013658974994022414\n",
      "          policy_loss: -0.025104049717386563\n",
      "          total_loss: -0.04792577007578479\n",
      "          vf_explained_var: -0.0030788329895585775\n",
      "          vf_loss: 0.00021418574868208552\n",
      "    num_agent_steps_sampled: 576000\n",
      "    num_agent_steps_trained: 576000\n",
      "    num_steps_sampled: 576000\n",
      "    num_steps_trained: 576000\n",
      "  iterations_since_restore: 576\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.39642857142857\n",
      "    ram_util_percent: 52.09642857142857\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06787063776335887\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.936260378590616\n",
      "    mean_inference_ms: 2.464838930839772\n",
      "    mean_raw_obs_processing_ms: 0.8806837529669622\n",
      "  time_since_restore: 12549.685855150223\n",
      "  time_this_iter_s: 19.698401927947998\n",
      "  time_total_s: 12549.685855150223\n",
      "  timers:\n",
      "    learn_throughput: 298.523\n",
      "    learn_time_ms: 3349.829\n",
      "    load_throughput: 21990.251\n",
      "    load_time_ms: 45.475\n",
      "    sample_throughput: 55.196\n",
      "    sample_time_ms: 18117.381\n",
      "    update_time_ms: 7.995\n",
      "  timestamp: 1631890897\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 576000\n",
      "  training_iteration: 576\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   576</td><td style=\"text-align: right;\">         12549.7</td><td style=\"text-align: right;\">576000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 577000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-01-56\n",
      "  done: false\n",
      "  episode_len_mean: 994.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 579\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5879409896002876\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012180231074549401\n",
      "          policy_loss: -0.0639421786285109\n",
      "          total_loss: -0.08679967768904236\n",
      "          vf_explained_var: -0.7602620720863342\n",
      "          vf_loss: 0.00016765052544845578\n",
      "    num_agent_steps_sampled: 577000\n",
      "    num_agent_steps_trained: 577000\n",
      "    num_steps_sampled: 577000\n",
      "    num_steps_trained: 577000\n",
      "  iterations_since_restore: 577\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.12962962962965\n",
      "    ram_util_percent: 52.15185185185186\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06787422763718581\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.935558568072686\n",
      "    mean_inference_ms: 2.464920945074988\n",
      "    mean_raw_obs_processing_ms: 0.8805030587966185\n",
      "  time_since_restore: 12568.969962835312\n",
      "  time_this_iter_s: 19.28410768508911\n",
      "  time_total_s: 12568.969962835312\n",
      "  timers:\n",
      "    learn_throughput: 298.682\n",
      "    learn_time_ms: 3348.037\n",
      "    load_throughput: 22274.761\n",
      "    load_time_ms: 44.894\n",
      "    sample_throughput: 55.002\n",
      "    sample_time_ms: 18181.05\n",
      "    update_time_ms: 7.957\n",
      "  timestamp: 1631890916\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 577000\n",
      "  training_iteration: 577\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   577</td><td style=\"text-align: right;\">           12569</td><td style=\"text-align: right;\">577000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 578000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-02-16\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 580\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.561847350332472\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008449757957618962\n",
      "          policy_loss: -0.06048896002272765\n",
      "          total_loss: -0.08388709702218572\n",
      "          vf_explained_var: -0.9057321548461914\n",
      "          vf_loss: 0.00024025826066160033\n",
      "    num_agent_steps_sampled: 578000\n",
      "    num_agent_steps_trained: 578000\n",
      "    num_steps_sampled: 578000\n",
      "    num_steps_trained: 578000\n",
      "  iterations_since_restore: 578\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.73103448275862\n",
      "    ram_util_percent: 52.15172413793104\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06787777305829366\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.934831742603887\n",
      "    mean_inference_ms: 2.465003566316195\n",
      "    mean_raw_obs_processing_ms: 0.8803118030711898\n",
      "  time_since_restore: 12588.81540608406\n",
      "  time_this_iter_s: 19.84544324874878\n",
      "  time_total_s: 12588.81540608406\n",
      "  timers:\n",
      "    learn_throughput: 297.437\n",
      "    learn_time_ms: 3362.053\n",
      "    load_throughput: 23634.475\n",
      "    load_time_ms: 42.311\n",
      "    sample_throughput: 60.67\n",
      "    sample_time_ms: 16482.515\n",
      "    update_time_ms: 8.019\n",
      "  timestamp: 1631890936\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 578000\n",
      "  training_iteration: 578\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   578</td><td style=\"text-align: right;\">         12588.8</td><td style=\"text-align: right;\">578000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 579000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-02-35\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 581\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5544453620910645\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014705247967498133\n",
      "          policy_loss: -0.0020080693893962435\n",
      "          total_loss: -0.023952976862589518\n",
      "          vf_explained_var: -0.653937578201294\n",
      "          vf_loss: 0.00015358297769125785\n",
      "    num_agent_steps_sampled: 579000\n",
      "    num_agent_steps_trained: 579000\n",
      "    num_steps_sampled: 579000\n",
      "    num_steps_trained: 579000\n",
      "  iterations_since_restore: 579\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.45769230769231\n",
      "    ram_util_percent: 51.99615384615384\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06788123566912545\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.934032700016516\n",
      "    mean_inference_ms: 2.4650860551509997\n",
      "    mean_raw_obs_processing_ms: 0.8801233311397154\n",
      "  time_since_restore: 12607.19680070877\n",
      "  time_this_iter_s: 18.381394624710083\n",
      "  time_total_s: 12607.19680070877\n",
      "  timers:\n",
      "    learn_throughput: 295.807\n",
      "    learn_time_ms: 3380.581\n",
      "    load_throughput: 24723.962\n",
      "    load_time_ms: 40.447\n",
      "    sample_throughput: 60.917\n",
      "    sample_time_ms: 16415.695\n",
      "    update_time_ms: 8.109\n",
      "  timestamp: 1631890955\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 579000\n",
      "  training_iteration: 579\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   579</td><td style=\"text-align: right;\">         12607.2</td><td style=\"text-align: right;\">579000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 580000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-02-54\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 582\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4211009793811376\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012393349810374982\n",
      "          policy_loss: -0.06340384669601917\n",
      "          total_loss: -0.08447545766830444\n",
      "          vf_explained_var: -0.9444121718406677\n",
      "          vf_loss: 0.00023519565274909837\n",
      "    num_agent_steps_sampled: 580000\n",
      "    num_agent_steps_trained: 580000\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 580000\n",
      "  iterations_since_restore: 580\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.10370370370372\n",
      "    ram_util_percent: 52.13703703703703\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06788463396957949\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.933245020563756\n",
      "    mean_inference_ms: 2.46516927458947\n",
      "    mean_raw_obs_processing_ms: 0.879937748156512\n",
      "  time_since_restore: 12626.534553289413\n",
      "  time_this_iter_s: 19.3377525806427\n",
      "  time_total_s: 12626.534553289413\n",
      "  timers:\n",
      "    learn_throughput: 296.446\n",
      "    learn_time_ms: 3373.292\n",
      "    load_throughput: 24789.968\n",
      "    load_time_ms: 40.339\n",
      "    sample_throughput: 60.471\n",
      "    sample_time_ms: 16536.917\n",
      "    update_time_ms: 8.468\n",
      "  timestamp: 1631890974\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 580\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   580</td><td style=\"text-align: right;\">         12626.5</td><td style=\"text-align: right;\">580000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 581000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-03-15\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 583\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4844477732976276\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011927712522387926\n",
      "          policy_loss: -0.038445991981360644\n",
      "          total_loss: -0.060356528953545624\n",
      "          vf_explained_var: -0.8684854507446289\n",
      "          vf_loss: 0.0001388530551796268\n",
      "    num_agent_steps_sampled: 581000\n",
      "    num_agent_steps_trained: 581000\n",
      "    num_steps_sampled: 581000\n",
      "    num_steps_trained: 581000\n",
      "  iterations_since_restore: 581\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.03548387096774\n",
      "    ram_util_percent: 52.01290322580646\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06788810150513791\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.932516465696654\n",
      "    mean_inference_ms: 2.4652552033294635\n",
      "    mean_raw_obs_processing_ms: 0.8797551541806119\n",
      "  time_since_restore: 12647.819792985916\n",
      "  time_this_iter_s: 21.285239696502686\n",
      "  time_total_s: 12647.819792985916\n",
      "  timers:\n",
      "    learn_throughput: 297.627\n",
      "    learn_time_ms: 3359.906\n",
      "    load_throughput: 24210.204\n",
      "    load_time_ms: 41.305\n",
      "    sample_throughput: 59.389\n",
      "    sample_time_ms: 16838.087\n",
      "    update_time_ms: 8.463\n",
      "  timestamp: 1631890995\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 581000\n",
      "  training_iteration: 581\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   581</td><td style=\"text-align: right;\">         12647.8</td><td style=\"text-align: right;\">581000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 582000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-03-35\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 584\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1515368766254848\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011797362284721362\n",
      "          policy_loss: -0.04243981407748328\n",
      "          total_loss: -0.06104813673430019\n",
      "          vf_explained_var: -0.6377089619636536\n",
      "          vf_loss: 0.00014250494568841533\n",
      "    num_agent_steps_sampled: 582000\n",
      "    num_agent_steps_trained: 582000\n",
      "    num_steps_sampled: 582000\n",
      "    num_steps_trained: 582000\n",
      "  iterations_since_restore: 582\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.37857142857142\n",
      "    ram_util_percent: 52.13928571428572\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0678915721170823\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.931810160725258\n",
      "    mean_inference_ms: 2.4653439434054025\n",
      "    mean_raw_obs_processing_ms: 0.8795751229534657\n",
      "  time_since_restore: 12667.349286317825\n",
      "  time_this_iter_s: 19.52949333190918\n",
      "  time_total_s: 12667.349286317825\n",
      "  timers:\n",
      "    learn_throughput: 297.129\n",
      "    learn_time_ms: 3365.541\n",
      "    load_throughput: 24714.304\n",
      "    load_time_ms: 40.462\n",
      "    sample_throughput: 60.361\n",
      "    sample_time_ms: 16567.064\n",
      "    update_time_ms: 8.585\n",
      "  timestamp: 1631891015\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 582000\n",
      "  training_iteration: 582\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   582</td><td style=\"text-align: right;\">         12667.3</td><td style=\"text-align: right;\">582000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 583000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-03-54\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 585\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.015105558766259\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013210085631212584\n",
      "          policy_loss: -0.014351967308256362\n",
      "          total_loss: -0.0312975169883834\n",
      "          vf_explained_var: -0.48095715045928955\n",
      "          vf_loss: 0.00010991447124979459\n",
      "    num_agent_steps_sampled: 583000\n",
      "    num_agent_steps_trained: 583000\n",
      "    num_steps_sampled: 583000\n",
      "    num_steps_trained: 583000\n",
      "  iterations_since_restore: 583\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.75\n",
      "    ram_util_percent: 52.27500000000001\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06789504764989171\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.9311063167717\n",
      "    mean_inference_ms: 2.4654344137801654\n",
      "    mean_raw_obs_processing_ms: 0.879397985846798\n",
      "  time_since_restore: 12686.839102268219\n",
      "  time_this_iter_s: 19.489815950393677\n",
      "  time_total_s: 12686.839102268219\n",
      "  timers:\n",
      "    learn_throughput: 296.142\n",
      "    learn_time_ms: 3376.761\n",
      "    load_throughput: 26055.17\n",
      "    load_time_ms: 38.38\n",
      "    sample_throughput: 60.766\n",
      "    sample_time_ms: 16456.558\n",
      "    update_time_ms: 8.621\n",
      "  timestamp: 1631891034\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 583000\n",
      "  training_iteration: 583\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   583</td><td style=\"text-align: right;\">         12686.8</td><td style=\"text-align: right;\">583000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 584000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-04-16\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 586\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6115366134378646\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011135177780480671\n",
      "          policy_loss: -0.03881690005461375\n",
      "          total_loss: -0.0453215077933338\n",
      "          vf_explained_var: -0.8548425436019897\n",
      "          vf_loss: 0.007001394313637219\n",
      "    num_agent_steps_sampled: 584000\n",
      "    num_agent_steps_trained: 584000\n",
      "    num_steps_sampled: 584000\n",
      "    num_steps_trained: 584000\n",
      "  iterations_since_restore: 584\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.03666666666666\n",
      "    ram_util_percent: 52.20000000000001\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06789859962602006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.930444982577166\n",
      "    mean_inference_ms: 2.4655293654736474\n",
      "    mean_raw_obs_processing_ms: 0.8792241324784317\n",
      "  time_since_restore: 12708.133101701736\n",
      "  time_this_iter_s: 21.293999433517456\n",
      "  time_total_s: 12708.133101701736\n",
      "  timers:\n",
      "    learn_throughput: 297.959\n",
      "    learn_time_ms: 3356.172\n",
      "    load_throughput: 24902.86\n",
      "    load_time_ms: 40.156\n",
      "    sample_throughput: 59.834\n",
      "    sample_time_ms: 16712.937\n",
      "    update_time_ms: 7.988\n",
      "  timestamp: 1631891056\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 584000\n",
      "  training_iteration: 584\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   584</td><td style=\"text-align: right;\">         12708.1</td><td style=\"text-align: right;\">584000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 585000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-04-38\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 587\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.46764312761887494\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00804734628477964\n",
      "          policy_loss: -0.11979177089201079\n",
      "          total_loss: -0.1224026215573152\n",
      "          vf_explained_var: 0.08727512508630753\n",
      "          vf_loss: 0.00017980172784493536\n",
      "    num_agent_steps_sampled: 585000\n",
      "    num_agent_steps_trained: 585000\n",
      "    num_steps_sampled: 585000\n",
      "    num_steps_trained: 585000\n",
      "  iterations_since_restore: 585\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.590625\n",
      "    ram_util_percent: 52.140625\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06790210592687285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.92984422863497\n",
      "    mean_inference_ms: 2.4656275450241463\n",
      "    mean_raw_obs_processing_ms: 0.8790532261833539\n",
      "  time_since_restore: 12730.347565412521\n",
      "  time_this_iter_s: 22.214463710784912\n",
      "  time_total_s: 12730.347565412521\n",
      "  timers:\n",
      "    learn_throughput: 299.249\n",
      "    learn_time_ms: 3341.693\n",
      "    load_throughput: 24885.735\n",
      "    load_time_ms: 40.184\n",
      "    sample_throughput: 60.108\n",
      "    sample_time_ms: 16636.631\n",
      "    update_time_ms: 7.956\n",
      "  timestamp: 1631891078\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 585000\n",
      "  training_iteration: 585\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   585</td><td style=\"text-align: right;\">         12730.3</td><td style=\"text-align: right;\">585000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 586000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-05-00\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 588\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.937202090687222\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013394588243448267\n",
      "          policy_loss: -0.056467995021699205\n",
      "          total_loss: -0.07255176478582952\n",
      "          vf_explained_var: -0.989919900894165\n",
      "          vf_loss: 0.00014942453041536888\n",
      "    num_agent_steps_sampled: 586000\n",
      "    num_agent_steps_trained: 586000\n",
      "    num_steps_sampled: 586000\n",
      "    num_steps_trained: 586000\n",
      "  iterations_since_restore: 586\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.32258064516128\n",
      "    ram_util_percent: 52.23225806451614\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06790567995950239\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.92927231593595\n",
      "    mean_inference_ms: 2.4657281066457903\n",
      "    mean_raw_obs_processing_ms: 0.8788852479921325\n",
      "  time_since_restore: 12752.506422758102\n",
      "  time_this_iter_s: 22.158857345581055\n",
      "  time_total_s: 12752.506422758102\n",
      "  timers:\n",
      "    learn_throughput: 299.824\n",
      "    learn_time_ms: 3335.288\n",
      "    load_throughput: 23524.927\n",
      "    load_time_ms: 42.508\n",
      "    sample_throughput: 59.222\n",
      "    sample_time_ms: 16885.524\n",
      "    update_time_ms: 7.907\n",
      "  timestamp: 1631891100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 586000\n",
      "  training_iteration: 586\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   586</td><td style=\"text-align: right;\">         12752.5</td><td style=\"text-align: right;\">586000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 587000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-05-20\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 589\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.942072652445899\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009602703024522501\n",
      "          policy_loss: -0.01627119295299053\n",
      "          total_loss: -0.033303797927995524\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00013786977922589157\n",
      "    num_agent_steps_sampled: 587000\n",
      "    num_agent_steps_trained: 587000\n",
      "    num_steps_sampled: 587000\n",
      "    num_steps_trained: 587000\n",
      "  iterations_since_restore: 587\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.4896551724138\n",
      "    ram_util_percent: 52.14827586206897\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06790920440326303\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.928701531716378\n",
      "    mean_inference_ms: 2.465827750934261\n",
      "    mean_raw_obs_processing_ms: 0.878720488461046\n",
      "  time_since_restore: 12772.581920862198\n",
      "  time_this_iter_s: 20.07549810409546\n",
      "  time_total_s: 12772.581920862198\n",
      "  timers:\n",
      "    learn_throughput: 299.964\n",
      "    learn_time_ms: 3333.738\n",
      "    load_throughput: 23390.273\n",
      "    load_time_ms: 42.753\n",
      "    sample_throughput: 58.951\n",
      "    sample_time_ms: 16963.252\n",
      "    update_time_ms: 8.166\n",
      "  timestamp: 1631891120\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 587000\n",
      "  training_iteration: 587\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   587</td><td style=\"text-align: right;\">         12772.6</td><td style=\"text-align: right;\">587000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 588000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-05-42\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 590\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.794098260667589\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019870763640083014\n",
      "          policy_loss: 0.04544288623664114\n",
      "          total_loss: 0.03240635204646322\n",
      "          vf_explained_var: -0.520483672618866\n",
      "          vf_loss: 0.00024802348802445016\n",
      "    num_agent_steps_sampled: 588000\n",
      "    num_agent_steps_trained: 588000\n",
      "    num_steps_sampled: 588000\n",
      "    num_steps_trained: 588000\n",
      "  iterations_since_restore: 588\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.17096774193547\n",
      "    ram_util_percent: 52.26129032258067\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06791270173308254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.928206040200946\n",
      "    mean_inference_ms: 2.4659302413804727\n",
      "    mean_raw_obs_processing_ms: 0.8785583969638684\n",
      "  time_since_restore: 12793.850572109222\n",
      "  time_this_iter_s: 21.268651247024536\n",
      "  time_total_s: 12793.850572109222\n",
      "  timers:\n",
      "    learn_throughput: 300.318\n",
      "    learn_time_ms: 3329.809\n",
      "    load_throughput: 22373.201\n",
      "    load_time_ms: 44.696\n",
      "    sample_throughput: 58.456\n",
      "    sample_time_ms: 17107.008\n",
      "    update_time_ms: 8.499\n",
      "  timestamp: 1631891142\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 588000\n",
      "  training_iteration: 588\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   588</td><td style=\"text-align: right;\">         12793.9</td><td style=\"text-align: right;\">588000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 589000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-06-02\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 591\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9245053781403436\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01201015139334416\n",
      "          policy_loss: -0.008407964929938316\n",
      "          total_loss: -0.024594831301106346\n",
      "          vf_explained_var: -0.6329765915870667\n",
      "          vf_loss: 0.0002437839453907347\n",
      "    num_agent_steps_sampled: 589000\n",
      "    num_agent_steps_trained: 589000\n",
      "    num_steps_sampled: 589000\n",
      "    num_steps_trained: 589000\n",
      "  iterations_since_restore: 589\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.68620689655172\n",
      "    ram_util_percent: 52.317241379310346\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06791620382960802\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.927736208298379\n",
      "    mean_inference_ms: 2.4660327328123004\n",
      "    mean_raw_obs_processing_ms: 0.8783988825884722\n",
      "  time_since_restore: 12814.47473692894\n",
      "  time_this_iter_s: 20.624164819717407\n",
      "  time_total_s: 12814.47473692894\n",
      "  timers:\n",
      "    learn_throughput: 300.916\n",
      "    learn_time_ms: 3323.192\n",
      "    load_throughput: 21282.327\n",
      "    load_time_ms: 46.987\n",
      "    sample_throughput: 57.685\n",
      "    sample_time_ms: 17335.574\n",
      "    update_time_ms: 8.577\n",
      "  timestamp: 1631891162\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 589000\n",
      "  training_iteration: 589\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   589</td><td style=\"text-align: right;\">         12814.5</td><td style=\"text-align: right;\">589000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 590000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-06-23\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 592\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0191497166951495\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013381965056140999\n",
      "          policy_loss: -0.054703261372115876\n",
      "          total_loss: -0.07157926445619928\n",
      "          vf_explained_var: -0.42569002509117126\n",
      "          vf_loss: 0.00017962583903378496\n",
      "    num_agent_steps_sampled: 590000\n",
      "    num_agent_steps_trained: 590000\n",
      "    num_steps_sampled: 590000\n",
      "    num_steps_trained: 590000\n",
      "  iterations_since_restore: 590\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.72666666666667\n",
      "    ram_util_percent: 52.29333333333334\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06791965098390058\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.927272889890055\n",
      "    mean_inference_ms: 2.466133394150847\n",
      "    mean_raw_obs_processing_ms: 0.8782426247254109\n",
      "  time_since_restore: 12835.39831995964\n",
      "  time_this_iter_s: 20.923583030700684\n",
      "  time_total_s: 12835.39831995964\n",
      "  timers:\n",
      "    learn_throughput: 299.811\n",
      "    learn_time_ms: 3335.435\n",
      "    load_throughput: 21327.497\n",
      "    load_time_ms: 46.888\n",
      "    sample_throughput: 57.201\n",
      "    sample_time_ms: 17482.118\n",
      "    update_time_ms: 8.654\n",
      "  timestamp: 1631891183\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 590000\n",
      "  training_iteration: 590\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   590</td><td style=\"text-align: right;\">         12835.4</td><td style=\"text-align: right;\">590000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 591000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-06-44\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 593\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8990597208340962\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007496890559809937\n",
      "          policy_loss: -0.192388563685947\n",
      "          total_loss: -0.20940285589959887\n",
      "          vf_explained_var: -0.4315352141857147\n",
      "          vf_loss: 0.0002195154593007626\n",
      "    num_agent_steps_sampled: 591000\n",
      "    num_agent_steps_trained: 591000\n",
      "    num_steps_sampled: 591000\n",
      "    num_steps_trained: 591000\n",
      "  iterations_since_restore: 591\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.39666666666668\n",
      "    ram_util_percent: 52.32333333333333\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0679230570500431\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.926839715762483\n",
      "    mean_inference_ms: 2.4662320886807554\n",
      "    mean_raw_obs_processing_ms: 0.8780802414321738\n",
      "  time_since_restore: 12856.22043633461\n",
      "  time_this_iter_s: 20.822116374969482\n",
      "  time_total_s: 12856.22043633461\n",
      "  timers:\n",
      "    learn_throughput: 299.803\n",
      "    learn_time_ms: 3335.526\n",
      "    load_throughput: 21333.8\n",
      "    load_time_ms: 46.874\n",
      "    sample_throughput: 57.36\n",
      "    sample_time_ms: 17433.713\n",
      "    update_time_ms: 8.961\n",
      "  timestamp: 1631891204\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 591000\n",
      "  training_iteration: 591\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   591</td><td style=\"text-align: right;\">         12856.2</td><td style=\"text-align: right;\">591000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 592000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-07-07\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 594\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.069639119837019\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012086422710730155\n",
      "          policy_loss: -0.07060315617256695\n",
      "          total_loss: -0.08818884185618825\n",
      "          vf_explained_var: -0.4247291684150696\n",
      "          vf_loss: 0.0002784233813549185\n",
      "    num_agent_steps_sampled: 592000\n",
      "    num_agent_steps_trained: 592000\n",
      "    num_steps_sampled: 592000\n",
      "    num_steps_trained: 592000\n",
      "  iterations_since_restore: 592\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.05\n",
      "    ram_util_percent: 52.221875\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0679264330174152\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.926456566848328\n",
      "    mean_inference_ms: 2.466329425576078\n",
      "    mean_raw_obs_processing_ms: 0.8779195775418245\n",
      "  time_since_restore: 12879.165432929993\n",
      "  time_this_iter_s: 22.94499659538269\n",
      "  time_total_s: 12879.165432929993\n",
      "  timers:\n",
      "    learn_throughput: 299.653\n",
      "    learn_time_ms: 3337.193\n",
      "    load_throughput: 20886.127\n",
      "    load_time_ms: 47.879\n",
      "    sample_throughput: 56.265\n",
      "    sample_time_ms: 17773.044\n",
      "    update_time_ms: 8.869\n",
      "  timestamp: 1631891227\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 592000\n",
      "  training_iteration: 592\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   592</td><td style=\"text-align: right;\">         12879.2</td><td style=\"text-align: right;\">592000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 593000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-07-27\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 595\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3776824686262343\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010981608097891933\n",
      "          policy_loss: -0.019446026699410545\n",
      "          total_loss: -0.04057107518116633\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 7.83981899076025e-05\n",
      "    num_agent_steps_sampled: 593000\n",
      "    num_agent_steps_trained: 593000\n",
      "    num_steps_sampled: 593000\n",
      "    num_steps_trained: 593000\n",
      "  iterations_since_restore: 593\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.77931034482758\n",
      "    ram_util_percent: 52.2448275862069\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0679296983637584\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.926001228218514\n",
      "    mean_inference_ms: 2.466425085294785\n",
      "    mean_raw_obs_processing_ms: 0.8777607489034636\n",
      "  time_since_restore: 12899.201408863068\n",
      "  time_this_iter_s: 20.03597593307495\n",
      "  time_total_s: 12899.201408863068\n",
      "  timers:\n",
      "    learn_throughput: 299.04\n",
      "    learn_time_ms: 3344.036\n",
      "    load_throughput: 20103.212\n",
      "    load_time_ms: 49.743\n",
      "    sample_throughput: 56.121\n",
      "    sample_time_ms: 17818.539\n",
      "    update_time_ms: 8.691\n",
      "  timestamp: 1631891247\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 593000\n",
      "  training_iteration: 593\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   593</td><td style=\"text-align: right;\">         12899.2</td><td style=\"text-align: right;\">593000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 594000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-07-47\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 596\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5093060546451147\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010385804305844026\n",
      "          policy_loss: -0.0016360011365678575\n",
      "          total_loss: -0.024176591663207445\n",
      "          vf_explained_var: -0.5735158920288086\n",
      "          vf_loss: 0.00011870783599887444\n",
      "    num_agent_steps_sampled: 594000\n",
      "    num_agent_steps_trained: 594000\n",
      "    num_steps_sampled: 594000\n",
      "    num_steps_trained: 594000\n",
      "  iterations_since_restore: 594\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.51071428571429\n",
      "    ram_util_percent: 52.24285714285714\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0679328333246275\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.925482620799672\n",
      "    mean_inference_ms: 2.466520054486959\n",
      "    mean_raw_obs_processing_ms: 0.8776040181931981\n",
      "  time_since_restore: 12918.604525327682\n",
      "  time_this_iter_s: 19.403116464614868\n",
      "  time_total_s: 12918.604525327682\n",
      "  timers:\n",
      "    learn_throughput: 298.639\n",
      "    learn_time_ms: 3348.521\n",
      "    load_throughput: 21342.669\n",
      "    load_time_ms: 46.854\n",
      "    sample_throughput: 56.724\n",
      "    sample_time_ms: 17629.314\n",
      "    update_time_ms: 8.217\n",
      "  timestamp: 1631891267\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 594000\n",
      "  training_iteration: 594\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   594</td><td style=\"text-align: right;\">         12918.6</td><td style=\"text-align: right;\">594000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 595000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-08-05\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 597\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.553683869043986\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00979498646881988\n",
      "          policy_loss: -0.010744216665625573\n",
      "          total_loss: -0.03385964184999466\n",
      "          vf_explained_var: -0.09917036443948746\n",
      "          vf_loss: 0.0001261012529413266\n",
      "    num_agent_steps_sampled: 595000\n",
      "    num_agent_steps_trained: 595000\n",
      "    num_steps_sampled: 595000\n",
      "    num_steps_trained: 595000\n",
      "  iterations_since_restore: 595\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.53846153846153\n",
      "    ram_util_percent: 52.388461538461534\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06793596507196206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.924870633747927\n",
      "    mean_inference_ms: 2.4666145102235433\n",
      "    mean_raw_obs_processing_ms: 0.8774498969808946\n",
      "  time_since_restore: 12937.300203084946\n",
      "  time_this_iter_s: 18.695677757263184\n",
      "  time_total_s: 12937.300203084946\n",
      "  timers:\n",
      "    learn_throughput: 300.718\n",
      "    learn_time_ms: 3325.378\n",
      "    load_throughput: 21197.485\n",
      "    load_time_ms: 47.175\n",
      "    sample_throughput: 57.805\n",
      "    sample_time_ms: 17299.424\n",
      "    update_time_ms: 8.31\n",
      "  timestamp: 1631891285\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 595000\n",
      "  training_iteration: 595\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   595</td><td style=\"text-align: right;\">         12937.3</td><td style=\"text-align: right;\">595000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 596000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-08-24\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 598\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4570093340343897\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008676989995587907\n",
      "          policy_loss: -0.049535786815815504\n",
      "          total_loss: -0.07181915177239312\n",
      "          vf_explained_var: -0.5687093734741211\n",
      "          vf_loss: 0.0002534004253094382\n",
      "    num_agent_steps_sampled: 596000\n",
      "    num_agent_steps_trained: 596000\n",
      "    num_steps_sampled: 596000\n",
      "    num_steps_trained: 596000\n",
      "  iterations_since_restore: 596\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.87037037037037\n",
      "    ram_util_percent: 52.37037037037036\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06793904874159726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.92421651156063\n",
      "    mean_inference_ms: 2.4667076522524995\n",
      "    mean_raw_obs_processing_ms: 0.8772982435501692\n",
      "  time_since_restore: 12956.189324855804\n",
      "  time_this_iter_s: 18.889121770858765\n",
      "  time_total_s: 12956.189324855804\n",
      "  timers:\n",
      "    learn_throughput: 301.34\n",
      "    learn_time_ms: 3318.515\n",
      "    load_throughput: 20849.032\n",
      "    load_time_ms: 47.964\n",
      "    sample_throughput: 58.892\n",
      "    sample_time_ms: 16980.273\n",
      "    update_time_ms: 7.792\n",
      "  timestamp: 1631891304\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 596000\n",
      "  training_iteration: 596\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   596</td><td style=\"text-align: right;\">         12956.2</td><td style=\"text-align: right;\">596000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 597000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-08-44\n",
      "  done: false\n",
      "  episode_len_mean: 996.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 599\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.595793525377909\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007124383418420719\n",
      "          policy_loss: -0.05374619174334738\n",
      "          total_loss: -0.07776749456922213\n",
      "          vf_explained_var: -0.36658981442451477\n",
      "          vf_loss: 0.000267137118081943\n",
      "    num_agent_steps_sampled: 597000\n",
      "    num_agent_steps_trained: 597000\n",
      "    num_steps_sampled: 597000\n",
      "    num_steps_trained: 597000\n",
      "  iterations_since_restore: 597\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.07241379310344\n",
      "    ram_util_percent: 52.255172413793105\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06794177654041615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.923541217030595\n",
      "    mean_inference_ms: 2.4668008333643723\n",
      "    mean_raw_obs_processing_ms: 0.8771493111360817\n",
      "  time_since_restore: 12976.184247016907\n",
      "  time_this_iter_s: 19.994922161102295\n",
      "  time_total_s: 12976.184247016907\n",
      "  timers:\n",
      "    learn_throughput: 300.087\n",
      "    learn_time_ms: 3332.372\n",
      "    load_throughput: 20706.365\n",
      "    load_time_ms: 48.294\n",
      "    sample_throughput: 58.961\n",
      "    sample_time_ms: 16960.463\n",
      "    update_time_ms: 7.559\n",
      "  timestamp: 1631891324\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 597000\n",
      "  training_iteration: 597\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   597</td><td style=\"text-align: right;\">         12976.2</td><td style=\"text-align: right;\">597000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            996.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 598000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-09-22\n",
      "  done: false\n",
      "  episode_len_mean: 994.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 600\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2846919430626764\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008957065748146873\n",
      "          policy_loss: -0.0056526063217057125\n",
      "          total_loss: -0.026175793260335922\n",
      "          vf_explained_var: -0.5172091722488403\n",
      "          vf_loss: 0.00022477210923170786\n",
      "    num_agent_steps_sampled: 598000\n",
      "    num_agent_steps_trained: 598000\n",
      "    num_steps_sampled: 598000\n",
      "    num_steps_trained: 598000\n",
      "  iterations_since_restore: 598\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.53454545454545\n",
      "    ram_util_percent: 52.39272727272728\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06794448271952992\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.922836961871186\n",
      "    mean_inference_ms: 2.4668918399017428\n",
      "    mean_raw_obs_processing_ms: 0.8773393304972585\n",
      "  time_since_restore: 13014.401025056839\n",
      "  time_this_iter_s: 38.21677803993225\n",
      "  time_total_s: 13014.401025056839\n",
      "  timers:\n",
      "    learn_throughput: 301.276\n",
      "    learn_time_ms: 3319.212\n",
      "    load_throughput: 21290.44\n",
      "    load_time_ms: 46.969\n",
      "    sample_throughput: 53.568\n",
      "    sample_time_ms: 18667.762\n",
      "    update_time_ms: 8.374\n",
      "  timestamp: 1631891362\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 598000\n",
      "  training_iteration: 598\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   598</td><td style=\"text-align: right;\">         13014.4</td><td style=\"text-align: right;\">598000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 599000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-09-42\n",
      "  done: false\n",
      "  episode_len_mean: 994.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 601\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7717611650625864\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007485471240383133\n",
      "          policy_loss: -0.08361969300442272\n",
      "          total_loss: -0.09821708343095249\n",
      "          vf_explained_var: -0.45560699701309204\n",
      "          vf_loss: 0.001366108347140956\n",
      "    num_agent_steps_sampled: 599000\n",
      "    num_agent_steps_trained: 599000\n",
      "    num_steps_sampled: 599000\n",
      "    num_steps_trained: 599000\n",
      "  iterations_since_restore: 599\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.43333333333334\n",
      "    ram_util_percent: 52.55185185185184\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06794714788250117\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.92203695905166\n",
      "    mean_inference_ms: 2.466981565014173\n",
      "    mean_raw_obs_processing_ms: 0.8775313308219195\n",
      "  time_since_restore: 13033.447794437408\n",
      "  time_this_iter_s: 19.046769380569458\n",
      "  time_total_s: 13033.447794437408\n",
      "  timers:\n",
      "    learn_throughput: 300.909\n",
      "    learn_time_ms: 3323.267\n",
      "    load_throughput: 21191.156\n",
      "    load_time_ms: 47.189\n",
      "    sample_throughput: 54.036\n",
      "    sample_time_ms: 18506.129\n",
      "    update_time_ms: 8.208\n",
      "  timestamp: 1631891382\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 599000\n",
      "  training_iteration: 599\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         13033.4</td><td style=\"text-align: right;\">599000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 600000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-10-02\n",
      "  done: false\n",
      "  episode_len_mean: 994.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 602\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6003358678685294\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007843107760293656\n",
      "          policy_loss: -0.049132282535235085\n",
      "          total_loss: -0.052390221175220276\n",
      "          vf_explained_var: 0.275616317987442\n",
      "          vf_loss: 0.0009075035971666997\n",
      "    num_agent_steps_sampled: 600000\n",
      "    num_agent_steps_trained: 600000\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "  iterations_since_restore: 600\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.97500000000001\n",
      "    ram_util_percent: 52.489285714285714\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0679497669223264\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.921166047519442\n",
      "    mean_inference_ms: 2.467069878895828\n",
      "    mean_raw_obs_processing_ms: 0.8777246692109923\n",
      "  time_since_restore: 13053.391468048096\n",
      "  time_this_iter_s: 19.943673610687256\n",
      "  time_total_s: 13053.391468048096\n",
      "  timers:\n",
      "    learn_throughput: 300.694\n",
      "    learn_time_ms: 3325.642\n",
      "    load_throughput: 21749.037\n",
      "    load_time_ms: 45.979\n",
      "    sample_throughput: 54.327\n",
      "    sample_time_ms: 18406.978\n",
      "    update_time_ms: 7.897\n",
      "  timestamp: 1631891402\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 600\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   600</td><td style=\"text-align: right;\">         13053.4</td><td style=\"text-align: right;\">600000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 601000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-10-22\n",
      "  done: false\n",
      "  episode_len_mean: 994.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 603\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6099478383858998\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.027440828041624756\n",
      "          policy_loss: 0.005270146330197652\n",
      "          total_loss: -0.003962841692070166\n",
      "          vf_explained_var: -0.2864294946193695\n",
      "          vf_loss: 0.00043613168462697\n",
      "    num_agent_steps_sampled: 601000\n",
      "    num_agent_steps_trained: 601000\n",
      "    num_steps_sampled: 601000\n",
      "    num_steps_trained: 601000\n",
      "  iterations_since_restore: 601\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.00999999999999\n",
      "    ram_util_percent: 52.56666666666667\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06795229492847485\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.920247347105894\n",
      "    mean_inference_ms: 2.4671569568293674\n",
      "    mean_raw_obs_processing_ms: 0.8779190049775871\n",
      "  time_since_restore: 13073.862128257751\n",
      "  time_this_iter_s: 20.47066020965576\n",
      "  time_total_s: 13073.862128257751\n",
      "  timers:\n",
      "    learn_throughput: 299.985\n",
      "    learn_time_ms: 3333.499\n",
      "    load_throughput: 22399.691\n",
      "    load_time_ms: 44.643\n",
      "    sample_throughput: 54.526\n",
      "    sample_time_ms: 18339.862\n",
      "    update_time_ms: 8.559\n",
      "  timestamp: 1631891422\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 601000\n",
      "  training_iteration: 601\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   601</td><td style=\"text-align: right;\">         13073.9</td><td style=\"text-align: right;\">601000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 602000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-10-41\n",
      "  done: false\n",
      "  episode_len_mean: 994.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 604\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8094873289267221\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013238318592022028\n",
      "          policy_loss: -0.014341400087707573\n",
      "          total_loss: -0.02726730431119601\n",
      "          vf_explained_var: -0.6744744181632996\n",
      "          vf_loss: 0.000515656068071419\n",
      "    num_agent_steps_sampled: 602000\n",
      "    num_agent_steps_trained: 602000\n",
      "    num_steps_sampled: 602000\n",
      "    num_steps_trained: 602000\n",
      "  iterations_since_restore: 602\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.29629629629629\n",
      "    ram_util_percent: 52.44814814814814\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06795481417888138\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.919318753776345\n",
      "    mean_inference_ms: 2.467243980749334\n",
      "    mean_raw_obs_processing_ms: 0.8781148285045884\n",
      "  time_since_restore: 13093.17005610466\n",
      "  time_this_iter_s: 19.30792784690857\n",
      "  time_total_s: 13093.17005610466\n",
      "  timers:\n",
      "    learn_throughput: 301.492\n",
      "    learn_time_ms: 3316.834\n",
      "    load_throughput: 22481.489\n",
      "    load_time_ms: 44.481\n",
      "    sample_throughput: 55.577\n",
      "    sample_time_ms: 17993.211\n",
      "    update_time_ms: 8.582\n",
      "  timestamp: 1631891441\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 602000\n",
      "  training_iteration: 602\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   602</td><td style=\"text-align: right;\">         13093.2</td><td style=\"text-align: right;\">602000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 603000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-11-01\n",
      "  done: false\n",
      "  episode_len_mean: 994.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 605\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.343361417452494\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010670575880834024\n",
      "          policy_loss: -0.023000370628303953\n",
      "          total_loss: -0.042472365436454614\n",
      "          vf_explained_var: -0.9692794680595398\n",
      "          vf_loss: 0.0002108774666238686\n",
      "    num_agent_steps_sampled: 603000\n",
      "    num_agent_steps_trained: 603000\n",
      "    num_steps_sampled: 603000\n",
      "    num_steps_trained: 603000\n",
      "  iterations_since_restore: 603\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.17037037037035\n",
      "    ram_util_percent: 52.56296296296296\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06795737175819686\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.918384303455145\n",
      "    mean_inference_ms: 2.467331746455387\n",
      "    mean_raw_obs_processing_ms: 0.8783122522257915\n",
      "  time_since_restore: 13112.231340885162\n",
      "  time_this_iter_s: 19.06128478050232\n",
      "  time_total_s: 13112.231340885162\n",
      "  timers:\n",
      "    learn_throughput: 302.967\n",
      "    learn_time_ms: 3300.693\n",
      "    load_throughput: 23270.468\n",
      "    load_time_ms: 42.973\n",
      "    sample_throughput: 55.826\n",
      "    sample_time_ms: 17912.856\n",
      "    update_time_ms: 8.865\n",
      "  timestamp: 1631891461\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 603000\n",
      "  training_iteration: 603\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 65.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   603</td><td style=\"text-align: right;\">         13112.2</td><td style=\"text-align: right;\">603000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 604000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-11-20\n",
      "  done: false\n",
      "  episode_len_mean: 994.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 606\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8239725510279337\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010174691003151083\n",
      "          policy_loss: -0.03002727230389913\n",
      "          total_loss: -0.044353344498409164\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0003372146250007467\n",
      "    num_agent_steps_sampled: 604000\n",
      "    num_agent_steps_trained: 604000\n",
      "    num_steps_sampled: 604000\n",
      "    num_steps_trained: 604000\n",
      "  iterations_since_restore: 604\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.6892857142857\n",
      "    ram_util_percent: 52.525\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0679599448919845\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.917415273032265\n",
      "    mean_inference_ms: 2.467420030552915\n",
      "    mean_raw_obs_processing_ms: 0.8785118588065276\n",
      "  time_since_restore: 13131.73401093483\n",
      "  time_this_iter_s: 19.50267004966736\n",
      "  time_total_s: 13131.73401093483\n",
      "  timers:\n",
      "    learn_throughput: 303.776\n",
      "    learn_time_ms: 3291.904\n",
      "    load_throughput: 22641.696\n",
      "    load_time_ms: 44.166\n",
      "    sample_throughput: 55.773\n",
      "    sample_time_ms: 17929.733\n",
      "    update_time_ms: 9.141\n",
      "  timestamp: 1631891480\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 604000\n",
      "  training_iteration: 604\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   604</td><td style=\"text-align: right;\">         13131.7</td><td style=\"text-align: right;\">604000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 605000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-11-40\n",
      "  done: false\n",
      "  episode_len_mean: 994.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 607\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9665326277414958\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008023482379280366\n",
      "          policy_loss: -0.10874253283772203\n",
      "          total_loss: -0.1253662694659498\n",
      "          vf_explained_var: -0.5337090492248535\n",
      "          vf_loss: 0.00022130912798780223\n",
      "    num_agent_steps_sampled: 605000\n",
      "    num_agent_steps_trained: 605000\n",
      "    num_steps_sampled: 605000\n",
      "    num_steps_trained: 605000\n",
      "  iterations_since_restore: 605\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.57500000000002\n",
      "    ram_util_percent: 52.660714285714285\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06796250752944447\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.916436628310537\n",
      "    mean_inference_ms: 2.4675065980256075\n",
      "    mean_raw_obs_processing_ms: 0.8787132582947905\n",
      "  time_since_restore: 13151.280054330826\n",
      "  time_this_iter_s: 19.546043395996094\n",
      "  time_total_s: 13151.280054330826\n",
      "  timers:\n",
      "    learn_throughput: 303.104\n",
      "    learn_time_ms: 3299.194\n",
      "    load_throughput: 23730.808\n",
      "    load_time_ms: 42.139\n",
      "    sample_throughput: 55.524\n",
      "    sample_time_ms: 18010.151\n",
      "    update_time_ms: 9.111\n",
      "  timestamp: 1631891500\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 605000\n",
      "  training_iteration: 605\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   605</td><td style=\"text-align: right;\">         13151.3</td><td style=\"text-align: right;\">605000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 606000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-12-01\n",
      "  done: false\n",
      "  episode_len_mean: 994.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 608\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8831373148494297\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012034564292482959\n",
      "          policy_loss: -0.05707103562437826\n",
      "          total_loss: -0.07129117554674545\n",
      "          vf_explained_var: -0.9624158143997192\n",
      "          vf_loss: 0.00038104851733401627\n",
      "    num_agent_steps_sampled: 606000\n",
      "    num_agent_steps_trained: 606000\n",
      "    num_steps_sampled: 606000\n",
      "    num_steps_trained: 606000\n",
      "  iterations_since_restore: 606\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.31666666666666\n",
      "    ram_util_percent: 52.69666666666668\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06796505063578304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.91544719875078\n",
      "    mean_inference_ms: 2.4675913486714673\n",
      "    mean_raw_obs_processing_ms: 0.8789161440549781\n",
      "  time_since_restore: 13172.089458227158\n",
      "  time_this_iter_s: 20.809403896331787\n",
      "  time_total_s: 13172.089458227158\n",
      "  timers:\n",
      "    learn_throughput: 302.203\n",
      "    learn_time_ms: 3309.032\n",
      "    load_throughput: 23829.662\n",
      "    load_time_ms: 41.965\n",
      "    sample_throughput: 55.148\n",
      "    sample_time_ms: 18133.158\n",
      "    update_time_ms: 9.178\n",
      "  timestamp: 1631891521\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 606000\n",
      "  training_iteration: 606\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   606</td><td style=\"text-align: right;\">         13172.1</td><td style=\"text-align: right;\">606000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 607000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-12-21\n",
      "  done: false\n",
      "  episode_len_mean: 994.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 609\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.908911289109124\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008700483171523013\n",
      "          policy_loss: -0.02344395470701986\n",
      "          total_loss: -0.039295391117533045\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.000179429244841837\n",
      "    num_agent_steps_sampled: 607000\n",
      "    num_agent_steps_trained: 607000\n",
      "    num_steps_sampled: 607000\n",
      "    num_steps_trained: 607000\n",
      "  iterations_since_restore: 607\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.51724137931035\n",
      "    ram_util_percent: 52.65862068965516\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06796767357168032\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.914419798265008\n",
      "    mean_inference_ms: 2.467677316181756\n",
      "    mean_raw_obs_processing_ms: 0.8791209462007104\n",
      "  time_since_restore: 13192.772199869156\n",
      "  time_this_iter_s: 20.68274164199829\n",
      "  time_total_s: 13192.772199869156\n",
      "  timers:\n",
      "    learn_throughput: 302.428\n",
      "    learn_time_ms: 3306.577\n",
      "    load_throughput: 23868.853\n",
      "    load_time_ms: 41.896\n",
      "    sample_throughput: 54.931\n",
      "    sample_time_ms: 18204.568\n",
      "    update_time_ms: 9.21\n",
      "  timestamp: 1631891541\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 607000\n",
      "  training_iteration: 607\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   607</td><td style=\"text-align: right;\">         13192.8</td><td style=\"text-align: right;\">607000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 608000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-12-41\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 610\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0050632264879016\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010143041920149799\n",
      "          policy_loss: -0.07034894915090667\n",
      "          total_loss: -0.08659224038322767\n",
      "          vf_explained_var: -0.47916820645332336\n",
      "          vf_loss: 0.00024203125536183102\n",
      "    num_agent_steps_sampled: 608000\n",
      "    num_agent_steps_trained: 608000\n",
      "    num_steps_sampled: 608000\n",
      "    num_steps_trained: 608000\n",
      "  iterations_since_restore: 608\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.03448275862068\n",
      "    ram_util_percent: 52.682758620689654\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06797025289926716\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.913393746112554\n",
      "    mean_inference_ms: 2.4677617748293716\n",
      "    mean_raw_obs_processing_ms: 0.8789896499727001\n",
      "  time_since_restore: 13212.976277828217\n",
      "  time_this_iter_s: 20.20407795906067\n",
      "  time_total_s: 13212.976277828217\n",
      "  timers:\n",
      "    learn_throughput: 301.965\n",
      "    learn_time_ms: 3311.637\n",
      "    load_throughput: 22977.93\n",
      "    load_time_ms: 43.52\n",
      "    sample_throughput: 60.985\n",
      "    sample_time_ms: 16397.564\n",
      "    update_time_ms: 9.55\n",
      "  timestamp: 1631891561\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 608000\n",
      "  training_iteration: 608\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   608</td><td style=\"text-align: right;\">           13213</td><td style=\"text-align: right;\">608000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 609000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-13-00\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 611\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.95047347413169\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0065267620502766814\n",
      "          policy_loss: -0.02191620207288199\n",
      "          total_loss: -0.03896803831060727\n",
      "          vf_explained_var: -0.99717777967453\n",
      "          vf_loss: 0.00015871861374358156\n",
      "    num_agent_steps_sampled: 609000\n",
      "    num_agent_steps_trained: 609000\n",
      "    num_steps_sampled: 609000\n",
      "    num_steps_trained: 609000\n",
      "  iterations_since_restore: 609\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.78518518518518\n",
      "    ram_util_percent: 52.766666666666666\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06797272607556754\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.912316867582687\n",
      "    mean_inference_ms: 2.46784277857177\n",
      "    mean_raw_obs_processing_ms: 0.8788599725641099\n",
      "  time_since_restore: 13231.604387760162\n",
      "  time_this_iter_s: 18.6281099319458\n",
      "  time_total_s: 13231.604387760162\n",
      "  timers:\n",
      "    learn_throughput: 304.06\n",
      "    learn_time_ms: 3288.827\n",
      "    load_throughput: 23040.613\n",
      "    load_time_ms: 43.402\n",
      "    sample_throughput: 61.056\n",
      "    sample_time_ms: 16378.307\n",
      "    update_time_ms: 9.829\n",
      "  timestamp: 1631891580\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 609000\n",
      "  training_iteration: 609\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   609</td><td style=\"text-align: right;\">         13231.6</td><td style=\"text-align: right;\">609000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 610000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-13-21\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 612\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8743288199106851\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008473264410027999\n",
      "          policy_loss: -0.036567417076892324\n",
      "          total_loss: -0.05216535493317578\n",
      "          vf_explained_var: -0.8357949256896973\n",
      "          vf_loss: 0.00016697000650714875\n",
      "    num_agent_steps_sampled: 610000\n",
      "    num_agent_steps_trained: 610000\n",
      "    num_steps_sampled: 610000\n",
      "    num_steps_trained: 610000\n",
      "  iterations_since_restore: 610\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.48\n",
      "    ram_util_percent: 52.67000000000001\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06797508232913435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.911217128308682\n",
      "    mean_inference_ms: 2.467922416271871\n",
      "    mean_raw_obs_processing_ms: 0.8787316507306133\n",
      "  time_since_restore: 13252.659754991531\n",
      "  time_this_iter_s: 21.05536723136902\n",
      "  time_total_s: 13252.659754991531\n",
      "  timers:\n",
      "    learn_throughput: 303.728\n",
      "    learn_time_ms: 3292.415\n",
      "    load_throughput: 22379.062\n",
      "    load_time_ms: 44.685\n",
      "    sample_throughput: 60.661\n",
      "    sample_time_ms: 16485.133\n",
      "    update_time_ms: 9.729\n",
      "  timestamp: 1631891601\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 610000\n",
      "  training_iteration: 610\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   610</td><td style=\"text-align: right;\">         13252.7</td><td style=\"text-align: right;\">610000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 611000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-13-41\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 613\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.558781698015001\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011093133741892795\n",
      "          policy_loss: -0.015492795366379949\n",
      "          total_loss: -0.026833877795272403\n",
      "          vf_explained_var: -0.3471444547176361\n",
      "          vf_loss: 0.0003474629256137026\n",
      "    num_agent_steps_sampled: 611000\n",
      "    num_agent_steps_trained: 611000\n",
      "    num_steps_sampled: 611000\n",
      "    num_steps_trained: 611000\n",
      "  iterations_since_restore: 611\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.75\n",
      "    ram_util_percent: 52.682142857142864\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06797738353883628\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.910100779331428\n",
      "    mean_inference_ms: 2.468002999101694\n",
      "    mean_raw_obs_processing_ms: 0.8786045388192417\n",
      "  time_since_restore: 13272.32526397705\n",
      "  time_this_iter_s: 19.66550898551941\n",
      "  time_total_s: 13272.32526397705\n",
      "  timers:\n",
      "    learn_throughput: 305.544\n",
      "    learn_time_ms: 3272.854\n",
      "    load_throughput: 21249.388\n",
      "    load_time_ms: 47.06\n",
      "    sample_throughput: 60.793\n",
      "    sample_time_ms: 16449.193\n",
      "    update_time_ms: 8.789\n",
      "  timestamp: 1631891621\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 611000\n",
      "  training_iteration: 611\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   611</td><td style=\"text-align: right;\">         13272.3</td><td style=\"text-align: right;\">611000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 612000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-14-01\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 614\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8025052044126721\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007276749543781255\n",
      "          policy_loss: -0.030085295252501965\n",
      "          total_loss: -0.04530604863539338\n",
      "          vf_explained_var: -0.9242734909057617\n",
      "          vf_loss: 0.0002464956810096434\n",
      "    num_agent_steps_sampled: 612000\n",
      "    num_agent_steps_trained: 612000\n",
      "    num_steps_sampled: 612000\n",
      "    num_steps_trained: 612000\n",
      "  iterations_since_restore: 612\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.64642857142857\n",
      "    ram_util_percent: 52.689285714285724\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06797960108689233\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.908892154169669\n",
      "    mean_inference_ms: 2.4680813563823265\n",
      "    mean_raw_obs_processing_ms: 0.8784792360813677\n",
      "  time_since_restore: 13291.99529504776\n",
      "  time_this_iter_s: 19.67003107070923\n",
      "  time_total_s: 13291.99529504776\n",
      "  timers:\n",
      "    learn_throughput: 304.027\n",
      "    learn_time_ms: 3289.178\n",
      "    load_throughput: 22372.783\n",
      "    load_time_ms: 44.697\n",
      "    sample_throughput: 60.716\n",
      "    sample_time_ms: 16470.2\n",
      "    update_time_ms: 9.084\n",
      "  timestamp: 1631891641\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 612000\n",
      "  training_iteration: 612\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   612</td><td style=\"text-align: right;\">           13292</td><td style=\"text-align: right;\">612000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 613000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-14-20\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 615\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.143802983231015\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010350396475879523\n",
      "          policy_loss: -0.0073479376319381924\n",
      "          total_loss: -0.024828801345494058\n",
      "          vf_explained_var: -0.5854500532150269\n",
      "          vf_loss: 0.00031896983077280716\n",
      "    num_agent_steps_sampled: 613000\n",
      "    num_agent_steps_trained: 613000\n",
      "    num_steps_sampled: 613000\n",
      "    num_steps_trained: 613000\n",
      "  iterations_since_restore: 613\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.43333333333332\n",
      "    ram_util_percent: 52.63333333333333\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06798174906516455\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.907655406761386\n",
      "    mean_inference_ms: 2.4681582364442707\n",
      "    mean_raw_obs_processing_ms: 0.8783560806810772\n",
      "  time_since_restore: 13311.159638404846\n",
      "  time_this_iter_s: 19.16434335708618\n",
      "  time_total_s: 13311.159638404846\n",
      "  timers:\n",
      "    learn_throughput: 302.621\n",
      "    learn_time_ms: 3304.458\n",
      "    load_throughput: 21528.494\n",
      "    load_time_ms: 46.45\n",
      "    sample_throughput: 60.736\n",
      "    sample_time_ms: 16464.632\n",
      "    update_time_ms: 8.739\n",
      "  timestamp: 1631891660\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 613000\n",
      "  training_iteration: 613\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   613</td><td style=\"text-align: right;\">         13311.2</td><td style=\"text-align: right;\">613000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 614000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-14-41\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 616\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9065478218926324\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00905052266634772\n",
      "          policy_loss: -0.046612685815327695\n",
      "          total_loss: -0.062254552356898786\n",
      "          vf_explained_var: -0.376857727766037\n",
      "          vf_loss: 0.00024232451849254884\n",
      "    num_agent_steps_sampled: 614000\n",
      "    num_agent_steps_trained: 614000\n",
      "    num_steps_sampled: 614000\n",
      "    num_steps_trained: 614000\n",
      "  iterations_since_restore: 614\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.61290322580645\n",
      "    ram_util_percent: 52.75806451612902\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06798377980295309\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.906417435774483\n",
      "    mean_inference_ms: 2.4682321886564993\n",
      "    mean_raw_obs_processing_ms: 0.8782344935018825\n",
      "  time_since_restore: 13332.255316972733\n",
      "  time_this_iter_s: 21.095678567886353\n",
      "  time_total_s: 13332.255316972733\n",
      "  timers:\n",
      "    learn_throughput: 301.507\n",
      "    learn_time_ms: 3316.677\n",
      "    load_throughput: 21495.637\n",
      "    load_time_ms: 46.521\n",
      "    sample_throughput: 60.198\n",
      "    sample_time_ms: 16611.9\n",
      "    update_time_ms: 8.781\n",
      "  timestamp: 1631891681\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 614000\n",
      "  training_iteration: 614\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   614</td><td style=\"text-align: right;\">         13332.3</td><td style=\"text-align: right;\">614000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 615000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-15-00\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 617\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1047958029641047\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0081212785927125\n",
      "          policy_loss: -0.03937645718041394\n",
      "          total_loss: -0.05742339655343029\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0001463623731372839\n",
      "    num_agent_steps_sampled: 615000\n",
      "    num_agent_steps_trained: 615000\n",
      "    num_steps_sampled: 615000\n",
      "    num_steps_trained: 615000\n",
      "  iterations_since_restore: 615\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.20370370370371\n",
      "    ram_util_percent: 52.74814814814815\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0679856677142652\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.905120098065732\n",
      "    mean_inference_ms: 2.4683036202803854\n",
      "    mean_raw_obs_processing_ms: 0.8781151879350308\n",
      "  time_since_restore: 13351.145284891129\n",
      "  time_this_iter_s: 18.889967918395996\n",
      "  time_total_s: 13351.145284891129\n",
      "  timers:\n",
      "    learn_throughput: 300.425\n",
      "    learn_time_ms: 3328.617\n",
      "    load_throughput: 20547.27\n",
      "    load_time_ms: 48.668\n",
      "    sample_throughput: 60.487\n",
      "    sample_time_ms: 16532.398\n",
      "    update_time_ms: 8.646\n",
      "  timestamp: 1631891700\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 615000\n",
      "  training_iteration: 615\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   615</td><td style=\"text-align: right;\">         13351.1</td><td style=\"text-align: right;\">615000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 616000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-15-21\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 618\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1661844624413384\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008399159175235878\n",
      "          policy_loss: -0.027757311736543975\n",
      "          total_loss: -0.0460852497153812\n",
      "          vf_explained_var: -0.7034638524055481\n",
      "          vf_loss: 0.00038157408304060583\n",
      "    num_agent_steps_sampled: 616000\n",
      "    num_agent_steps_trained: 616000\n",
      "    num_steps_sampled: 616000\n",
      "    num_steps_trained: 616000\n",
      "  iterations_since_restore: 616\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.67333333333333\n",
      "    ram_util_percent: 52.75333333333334\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06798746427498795\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.903812187667425\n",
      "    mean_inference_ms: 2.46837247181526\n",
      "    mean_raw_obs_processing_ms: 0.8779973416943045\n",
      "  time_since_restore: 13372.606605768204\n",
      "  time_this_iter_s: 21.461320877075195\n",
      "  time_total_s: 13372.606605768204\n",
      "  timers:\n",
      "    learn_throughput: 301.216\n",
      "    learn_time_ms: 3319.88\n",
      "    load_throughput: 20657.149\n",
      "    load_time_ms: 48.409\n",
      "    sample_throughput: 60.006\n",
      "    sample_time_ms: 16664.994\n",
      "    update_time_ms: 9.002\n",
      "  timestamp: 1631891721\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 616000\n",
      "  training_iteration: 616\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   616</td><td style=\"text-align: right;\">         13372.6</td><td style=\"text-align: right;\">616000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 617000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-15-39\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 619\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.30189934041765\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015487548235357895\n",
      "          policy_loss: -0.053915222651428646\n",
      "          total_loss: -0.07122652588619126\n",
      "          vf_explained_var: -0.520687460899353\n",
      "          vf_loss: 0.0002637674685198969\n",
      "    num_agent_steps_sampled: 617000\n",
      "    num_agent_steps_trained: 617000\n",
      "    num_steps_sampled: 617000\n",
      "    num_steps_trained: 617000\n",
      "  iterations_since_restore: 617\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.96153846153847\n",
      "    ram_util_percent: 52.91153846153846\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06798922941970975\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.902447027644817\n",
      "    mean_inference_ms: 2.468438303858129\n",
      "    mean_raw_obs_processing_ms: 0.8778809810186984\n",
      "  time_since_restore: 13390.595914840698\n",
      "  time_this_iter_s: 17.989309072494507\n",
      "  time_total_s: 13390.595914840698\n",
      "  timers:\n",
      "    learn_throughput: 301.957\n",
      "    learn_time_ms: 3311.725\n",
      "    load_throughput: 20798.941\n",
      "    load_time_ms: 48.079\n",
      "    sample_throughput: 60.96\n",
      "    sample_time_ms: 16404.251\n",
      "    update_time_ms: 9.001\n",
      "  timestamp: 1631891739\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 617000\n",
      "  training_iteration: 617\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   617</td><td style=\"text-align: right;\">         13390.6</td><td style=\"text-align: right;\">617000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 618000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-15-58\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 620\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2906272093455\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009003857999061655\n",
      "          policy_loss: -0.029310744255781174\n",
      "          total_loss: -0.0486683185522755\n",
      "          vf_explained_var: 0.179457426071167\n",
      "          vf_loss: 0.00038381497921970246\n",
      "    num_agent_steps_sampled: 618000\n",
      "    num_agent_steps_trained: 618000\n",
      "    num_steps_sampled: 618000\n",
      "    num_steps_trained: 618000\n",
      "  iterations_since_restore: 618\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.40370370370368\n",
      "    ram_util_percent: 52.65555555555555\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06799099736412448\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.901036172958454\n",
      "    mean_inference_ms: 2.4685014907036793\n",
      "    mean_raw_obs_processing_ms: 0.8777658544929089\n",
      "  time_since_restore: 13409.287051916122\n",
      "  time_this_iter_s: 18.691137075424194\n",
      "  time_total_s: 13409.287051916122\n",
      "  timers:\n",
      "    learn_throughput: 302.01\n",
      "    learn_time_ms: 3311.145\n",
      "    load_throughput: 21659.323\n",
      "    load_time_ms: 46.169\n",
      "    sample_throughput: 61.515\n",
      "    sample_time_ms: 16256.155\n",
      "    update_time_ms: 8.208\n",
      "  timestamp: 1631891758\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 618000\n",
      "  training_iteration: 618\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   618</td><td style=\"text-align: right;\">         13409.3</td><td style=\"text-align: right;\">618000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 619000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-16-18\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 621\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0467131866349115\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014447031487912751\n",
      "          policy_loss: -0.019892229346765412\n",
      "          total_loss: -0.03506556269195345\n",
      "          vf_explained_var: -0.5950038433074951\n",
      "          vf_loss: 0.00021562114133202056\n",
      "    num_agent_steps_sampled: 619000\n",
      "    num_agent_steps_trained: 619000\n",
      "    num_steps_sampled: 619000\n",
      "    num_steps_trained: 619000\n",
      "  iterations_since_restore: 619\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.38148148148149\n",
      "    ram_util_percent: 52.74074074074074\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06799268416647705\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.899563679055895\n",
      "    mean_inference_ms: 2.4685623127909375\n",
      "    mean_raw_obs_processing_ms: 0.8776533371587004\n",
      "  time_since_restore: 13428.701488018036\n",
      "  time_this_iter_s: 19.414436101913452\n",
      "  time_total_s: 13428.701488018036\n",
      "  timers:\n",
      "    learn_throughput: 300.136\n",
      "    learn_time_ms: 3331.823\n",
      "    load_throughput: 21733.823\n",
      "    load_time_ms: 46.011\n",
      "    sample_throughput: 61.299\n",
      "    sample_time_ms: 16313.458\n",
      "    update_time_ms: 8.144\n",
      "  timestamp: 1631891778\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 619000\n",
      "  training_iteration: 619\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   619</td><td style=\"text-align: right;\">         13428.7</td><td style=\"text-align: right;\">619000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 620000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-16-38\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 622\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6362840745184157\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010366497405645663\n",
      "          policy_loss: -0.05202982794079516\n",
      "          total_loss: -0.06420402884897258\n",
      "          vf_explained_var: -0.5723239779472351\n",
      "          vf_loss: 0.0005447824217198003\n",
      "    num_agent_steps_sampled: 620000\n",
      "    num_agent_steps_trained: 620000\n",
      "    num_steps_sampled: 620000\n",
      "    num_steps_trained: 620000\n",
      "  iterations_since_restore: 620\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.38333333333334\n",
      "    ram_util_percent: 52.63666666666667\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06799435201541922\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.89807802364556\n",
      "    mean_inference_ms: 2.468621324971323\n",
      "    mean_raw_obs_processing_ms: 0.877542647450787\n",
      "  time_since_restore: 13449.193438768387\n",
      "  time_this_iter_s: 20.491950750350952\n",
      "  time_total_s: 13449.193438768387\n",
      "  timers:\n",
      "    learn_throughput: 300.424\n",
      "    learn_time_ms: 3328.63\n",
      "    load_throughput: 21890.517\n",
      "    load_time_ms: 45.682\n",
      "    sample_throughput: 61.652\n",
      "    sample_time_ms: 16219.945\n",
      "    update_time_ms: 8.242\n",
      "  timestamp: 1631891798\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 620000\n",
      "  training_iteration: 620\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   620</td><td style=\"text-align: right;\">         13449.2</td><td style=\"text-align: right;\">620000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 621000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-16-58\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 623\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8079773730701871\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008665497892807444\n",
      "          policy_loss: 0.042749636206361984\n",
      "          total_loss: 0.028195935984452566\n",
      "          vf_explained_var: -0.7251244187355042\n",
      "          vf_loss: 0.00048012418806643435\n",
      "    num_agent_steps_sampled: 621000\n",
      "    num_agent_steps_trained: 621000\n",
      "    num_steps_sampled: 621000\n",
      "    num_steps_trained: 621000\n",
      "  iterations_since_restore: 621\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.86551724137931\n",
      "    ram_util_percent: 52.8344827586207\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06799615594460531\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.896573564768794\n",
      "    mean_inference_ms: 2.4686805002342362\n",
      "    mean_raw_obs_processing_ms: 0.8774342522678051\n",
      "  time_since_restore: 13469.464943408966\n",
      "  time_this_iter_s: 20.271504640579224\n",
      "  time_total_s: 13469.464943408966\n",
      "  timers:\n",
      "    learn_throughput: 299.071\n",
      "    learn_time_ms: 3343.684\n",
      "    load_throughput: 22067.398\n",
      "    load_time_ms: 45.316\n",
      "    sample_throughput: 61.479\n",
      "    sample_time_ms: 16265.845\n",
      "    update_time_ms: 8.21\n",
      "  timestamp: 1631891818\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 621000\n",
      "  training_iteration: 621\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   621</td><td style=\"text-align: right;\">         13469.5</td><td style=\"text-align: right;\">621000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 622000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-17-20\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 624\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6903169631958008\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00804577425236997\n",
      "          policy_loss: -0.07216337685369784\n",
      "          total_loss: -0.08588722712463803\n",
      "          vf_explained_var: -0.5314889550209045\n",
      "          vf_loss: 0.00035120366087034604\n",
      "    num_agent_steps_sampled: 622000\n",
      "    num_agent_steps_trained: 622000\n",
      "    num_steps_sampled: 622000\n",
      "    num_steps_trained: 622000\n",
      "  iterations_since_restore: 622\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.25666666666665\n",
      "    ram_util_percent: 52.92\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06799795651469964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.895068624379144\n",
      "    mean_inference_ms: 2.468739355039611\n",
      "    mean_raw_obs_processing_ms: 0.8773285753207664\n",
      "  time_since_restore: 13490.529087305069\n",
      "  time_this_iter_s: 21.064143896102905\n",
      "  time_total_s: 13490.529087305069\n",
      "  timers:\n",
      "    learn_throughput: 299.457\n",
      "    learn_time_ms: 3339.378\n",
      "    load_throughput: 21160.579\n",
      "    load_time_ms: 47.258\n",
      "    sample_throughput: 60.945\n",
      "    sample_time_ms: 16408.169\n",
      "    update_time_ms: 7.954\n",
      "  timestamp: 1631891840\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 622000\n",
      "  training_iteration: 622\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   622</td><td style=\"text-align: right;\">         13490.5</td><td style=\"text-align: right;\">622000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 623000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-17-39\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 625\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7924632840686374\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008722540144668204\n",
      "          policy_loss: -0.011481832298967574\n",
      "          total_loss: -0.025997241752015218\n",
      "          vf_explained_var: -0.815427303314209\n",
      "          vf_loss: 0.00034322322870947473\n",
      "    num_agent_steps_sampled: 623000\n",
      "    num_agent_steps_trained: 623000\n",
      "    num_steps_sampled: 623000\n",
      "    num_steps_trained: 623000\n",
      "  iterations_since_restore: 623\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.91785714285713\n",
      "    ram_util_percent: 52.88928571428573\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06799970840115722\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.893543062582117\n",
      "    mean_inference_ms: 2.4687981878780265\n",
      "    mean_raw_obs_processing_ms: 0.8772247321920683\n",
      "  time_since_restore: 13510.419965982437\n",
      "  time_this_iter_s: 19.890878677368164\n",
      "  time_total_s: 13510.419965982437\n",
      "  timers:\n",
      "    learn_throughput: 299.959\n",
      "    learn_time_ms: 3333.79\n",
      "    load_throughput: 21166.538\n",
      "    load_time_ms: 47.244\n",
      "    sample_throughput: 60.661\n",
      "    sample_time_ms: 16484.996\n",
      "    update_time_ms: 9.174\n",
      "  timestamp: 1631891859\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 623000\n",
      "  training_iteration: 623\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   623</td><td style=\"text-align: right;\">         13510.4</td><td style=\"text-align: right;\">623000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 624000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-17-58\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 626\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2699043247434827\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016940446363769564\n",
      "          policy_loss: -0.022351605279578104\n",
      "          total_loss: -0.038779724554883106\n",
      "          vf_explained_var: -0.7463254928588867\n",
      "          vf_loss: 0.0003163064607482132\n",
      "    num_agent_steps_sampled: 624000\n",
      "    num_agent_steps_trained: 624000\n",
      "    num_steps_sampled: 624000\n",
      "    num_steps_trained: 624000\n",
      "  iterations_since_restore: 624\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.54814814814814\n",
      "    ram_util_percent: 52.78148148148148\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0680014105161359\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.891961324571565\n",
      "    mean_inference_ms: 2.468855814359439\n",
      "    mean_raw_obs_processing_ms: 0.8771225370379114\n",
      "  time_since_restore: 13529.038988351822\n",
      "  time_this_iter_s: 18.619022369384766\n",
      "  time_total_s: 13529.038988351822\n",
      "  timers:\n",
      "    learn_throughput: 299.673\n",
      "    learn_time_ms: 3336.972\n",
      "    load_throughput: 21445.213\n",
      "    load_time_ms: 46.63\n",
      "    sample_throughput: 61.595\n",
      "    sample_time_ms: 16235.211\n",
      "    update_time_ms: 8.851\n",
      "  timestamp: 1631891878\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 624000\n",
      "  training_iteration: 624\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   624</td><td style=\"text-align: right;\">           13529</td><td style=\"text-align: right;\">624000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 625000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-18-18\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 627\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5248950242996218\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00893357142409123\n",
      "          policy_loss: -0.04565817545064622\n",
      "          total_loss: -0.0676669912205802\n",
      "          vf_explained_var: -0.2424076497554779\n",
      "          vf_loss: 9.99552391640969e-05\n",
      "    num_agent_steps_sampled: 625000\n",
      "    num_agent_steps_trained: 625000\n",
      "    num_steps_sampled: 625000\n",
      "    num_steps_trained: 625000\n",
      "  iterations_since_restore: 625\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.31481481481481\n",
      "    ram_util_percent: 52.66666666666666\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06800320508802178\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.890340020960867\n",
      "    mean_inference_ms: 2.4689116932134905\n",
      "    mean_raw_obs_processing_ms: 0.8770219686758834\n",
      "  time_since_restore: 13548.480371236801\n",
      "  time_this_iter_s: 19.441382884979248\n",
      "  time_total_s: 13548.480371236801\n",
      "  timers:\n",
      "    learn_throughput: 301.371\n",
      "    learn_time_ms: 3318.164\n",
      "    load_throughput: 21547.473\n",
      "    load_time_ms: 46.409\n",
      "    sample_throughput: 61.314\n",
      "    sample_time_ms: 16309.378\n",
      "    update_time_ms: 8.869\n",
      "  timestamp: 1631891898\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 625000\n",
      "  training_iteration: 625\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   625</td><td style=\"text-align: right;\">         13548.5</td><td style=\"text-align: right;\">625000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 626000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-18-36\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 628\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.35150312808164\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3442056205537583\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02453553810416851\n",
      "          policy_loss: -0.03145414027902815\n",
      "          total_loss: -0.046011487560139765\n",
      "          vf_explained_var: 0.16238680481910706\n",
      "          vf_loss: 0.00026039058268704845\n",
      "    num_agent_steps_sampled: 626000\n",
      "    num_agent_steps_trained: 626000\n",
      "    num_steps_sampled: 626000\n",
      "    num_steps_trained: 626000\n",
      "  iterations_since_restore: 626\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.12222222222223\n",
      "    ram_util_percent: 52.7037037037037\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06800497079829326\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.88866977467631\n",
      "    mean_inference_ms: 2.468966991939613\n",
      "    mean_raw_obs_processing_ms: 0.8769232388810313\n",
      "  time_since_restore: 13567.074760913849\n",
      "  time_this_iter_s: 18.59438967704773\n",
      "  time_total_s: 13567.074760913849\n",
      "  timers:\n",
      "    learn_throughput: 301.082\n",
      "    learn_time_ms: 3321.352\n",
      "    load_throughput: 21493.908\n",
      "    load_time_ms: 46.525\n",
      "    sample_throughput: 62.42\n",
      "    sample_time_ms: 16020.559\n",
      "    update_time_ms: 8.501\n",
      "  timestamp: 1631891916\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 626000\n",
      "  training_iteration: 626\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   626</td><td style=\"text-align: right;\">         13567.1</td><td style=\"text-align: right;\">626000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 627000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-18-55\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 629\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5272546921224599\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2957657231224906\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008441367446275667\n",
      "          policy_loss: 0.005048244363731808\n",
      "          total_loss: -0.013256866816017362\n",
      "          vf_explained_var: 0.10139671713113785\n",
      "          vf_loss: 0.00020179340879167285\n",
      "    num_agent_steps_sampled: 627000\n",
      "    num_agent_steps_trained: 627000\n",
      "    num_steps_sampled: 627000\n",
      "    num_steps_trained: 627000\n",
      "  iterations_since_restore: 627\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.15185185185184\n",
      "    ram_util_percent: 52.851851851851855\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06800671948238177\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.886928731226883\n",
      "    mean_inference_ms: 2.4690217834435146\n",
      "    mean_raw_obs_processing_ms: 0.876826440747953\n",
      "  time_since_restore: 13586.1131670475\n",
      "  time_this_iter_s: 19.038406133651733\n",
      "  time_total_s: 13586.1131670475\n",
      "  timers:\n",
      "    learn_throughput: 300.098\n",
      "    learn_time_ms: 3332.248\n",
      "    load_throughput: 21389.405\n",
      "    load_time_ms: 46.752\n",
      "    sample_throughput: 62.057\n",
      "    sample_time_ms: 16114.27\n",
      "    update_time_ms: 8.524\n",
      "  timestamp: 1631891935\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 627000\n",
      "  training_iteration: 627\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   627</td><td style=\"text-align: right;\">         13586.1</td><td style=\"text-align: right;\">627000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=334)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 628000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-19-34\n",
      "  done: false\n",
      "  episode_len_mean: 994.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 630\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5272546921224599\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.39002615875668\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007359042539799729\n",
      "          policy_loss: -0.015869485752450094\n",
      "          total_loss: -0.03552881677945455\n",
      "          vf_explained_var: -0.9452159404754639\n",
      "          vf_loss: 0.00036083914310691196\n",
      "    num_agent_steps_sampled: 628000\n",
      "    num_agent_steps_trained: 628000\n",
      "    num_steps_sampled: 628000\n",
      "    num_steps_trained: 628000\n",
      "  iterations_since_restore: 628\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.78000000000002\n",
      "    ram_util_percent: 52.91818181818183\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06800851280951821\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.885141271250333\n",
      "    mean_inference_ms: 2.4690752180075735\n",
      "    mean_raw_obs_processing_ms: 0.8770557298348486\n",
      "  time_since_restore: 13624.497762680054\n",
      "  time_this_iter_s: 38.3845956325531\n",
      "  time_total_s: 13624.497762680054\n",
      "  timers:\n",
      "    learn_throughput: 300.654\n",
      "    learn_time_ms: 3326.08\n",
      "    load_throughput: 20683.217\n",
      "    load_time_ms: 48.348\n",
      "    sample_throughput: 55.283\n",
      "    sample_time_ms: 18088.595\n",
      "    update_time_ms: 7.889\n",
      "  timestamp: 1631891974\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 628000\n",
      "  training_iteration: 628\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   628</td><td style=\"text-align: right;\">         13624.5</td><td style=\"text-align: right;\">628000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 629000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-19-55\n",
      "  done: false\n",
      "  episode_len_mean: 994.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 631\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5272546921224599\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.265276512834761\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010661316963738166\n",
      "          policy_loss: -0.05249709515935845\n",
      "          total_loss: -0.06898265373375681\n",
      "          vf_explained_var: -0.36950933933258057\n",
      "          vf_loss: 0.000545975045234728\n",
      "    num_agent_steps_sampled: 629000\n",
      "    num_agent_steps_trained: 629000\n",
      "    num_steps_sampled: 629000\n",
      "    num_steps_trained: 629000\n",
      "  iterations_since_restore: 629\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.70666666666666\n",
      "    ram_util_percent: 52.900000000000006\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0680102057682481\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.883358004924908\n",
      "    mean_inference_ms: 2.4691282498825085\n",
      "    mean_raw_obs_processing_ms: 0.8772859874810065\n",
      "  time_since_restore: 13645.70677113533\n",
      "  time_this_iter_s: 21.20900845527649\n",
      "  time_total_s: 13645.70677113533\n",
      "  timers:\n",
      "    learn_throughput: 301.562\n",
      "    learn_time_ms: 3316.07\n",
      "    load_throughput: 20745.949\n",
      "    load_time_ms: 48.202\n",
      "    sample_throughput: 54.706\n",
      "    sample_time_ms: 18279.386\n",
      "    update_time_ms: 7.655\n",
      "  timestamp: 1631891995\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 629000\n",
      "  training_iteration: 629\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   629</td><td style=\"text-align: right;\">         13645.7</td><td style=\"text-align: right;\">629000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 630000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-20-17\n",
      "  done: false\n",
      "  episode_len_mean: 994.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 632\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5272546921224599\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3928192536036175\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010182009176607526\n",
      "          policy_loss: -0.02345834941499763\n",
      "          total_loss: -0.04180133019884427\n",
      "          vf_explained_var: -0.25337058305740356\n",
      "          vf_loss: 0.000216701432923906\n",
      "    num_agent_steps_sampled: 630000\n",
      "    num_agent_steps_trained: 630000\n",
      "    num_steps_sampled: 630000\n",
      "    num_steps_trained: 630000\n",
      "  iterations_since_restore: 630\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.64193548387097\n",
      "    ram_util_percent: 52.964516129032255\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0680119233991864\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.881583083717969\n",
      "    mean_inference_ms: 2.4691818580240166\n",
      "    mean_raw_obs_processing_ms: 0.8775173531218011\n",
      "  time_since_restore: 13667.332209825516\n",
      "  time_this_iter_s: 21.625438690185547\n",
      "  time_total_s: 13667.332209825516\n",
      "  timers:\n",
      "    learn_throughput: 301.608\n",
      "    learn_time_ms: 3315.559\n",
      "    load_throughput: 20735.303\n",
      "    load_time_ms: 48.227\n",
      "    sample_throughput: 54.248\n",
      "    sample_time_ms: 18433.84\n",
      "    update_time_ms: 7.564\n",
      "  timestamp: 1631892017\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 630000\n",
      "  training_iteration: 630\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 67.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   630</td><td style=\"text-align: right;\">         13667.3</td><td style=\"text-align: right;\">630000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 631000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-20-35\n",
      "  done: false\n",
      "  episode_len_mean: 994.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 633\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5272546921224599\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5993638118108113\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004293481761843094\n",
      "          policy_loss: 0.0009138883091509342\n",
      "          total_loss: -0.022762518272631697\n",
      "          vf_explained_var: -0.990725040435791\n",
      "          vf_loss: 5.347474131111893e-05\n",
      "    num_agent_steps_sampled: 631000\n",
      "    num_agent_steps_trained: 631000\n",
      "    num_steps_sampled: 631000\n",
      "    num_steps_trained: 631000\n",
      "  iterations_since_restore: 631\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.83461538461538\n",
      "    ram_util_percent: 53.08461538461539\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06801358921946363\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.879731237459259\n",
      "    mean_inference_ms: 2.4692343590244326\n",
      "    mean_raw_obs_processing_ms: 0.8777497102999443\n",
      "  time_since_restore: 13685.657847166061\n",
      "  time_this_iter_s: 18.325637340545654\n",
      "  time_total_s: 13685.657847166061\n",
      "  timers:\n",
      "    learn_throughput: 301.12\n",
      "    learn_time_ms: 3320.931\n",
      "    load_throughput: 20692.41\n",
      "    load_time_ms: 48.327\n",
      "    sample_throughput: 54.843\n",
      "    sample_time_ms: 18233.861\n",
      "    update_time_ms: 7.611\n",
      "  timestamp: 1631892035\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 631000\n",
      "  training_iteration: 631\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 67.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   631</td><td style=\"text-align: right;\">         13685.7</td><td style=\"text-align: right;\">631000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 632000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-20-54\n",
      "  done: false\n",
      "  episode_len_mean: 994.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 634\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.26362734606122995\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3587077842818367\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01273201702745218\n",
      "          policy_loss: -0.04231714105440511\n",
      "          total_loss: -0.0623312340842353\n",
      "          vf_explained_var: -0.4190864861011505\n",
      "          vf_loss: 0.00021647656445919792\n",
      "    num_agent_steps_sampled: 632000\n",
      "    num_agent_steps_trained: 632000\n",
      "    num_steps_sampled: 632000\n",
      "    num_steps_trained: 632000\n",
      "  iterations_since_restore: 632\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.93571428571428\n",
      "    ram_util_percent: 53.02499999999999\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0680153201584674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.877872719787433\n",
      "    mean_inference_ms: 2.4692889072780657\n",
      "    mean_raw_obs_processing_ms: 0.877983647844195\n",
      "  time_since_restore: 13704.989221096039\n",
      "  time_this_iter_s: 19.331373929977417\n",
      "  time_total_s: 13704.989221096039\n",
      "  timers:\n",
      "    learn_throughput: 300.894\n",
      "    learn_time_ms: 3323.432\n",
      "    load_throughput: 21369.756\n",
      "    load_time_ms: 46.795\n",
      "    sample_throughput: 55.37\n",
      "    sample_time_ms: 18060.283\n",
      "    update_time_ms: 7.556\n",
      "  timestamp: 1631892054\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 632000\n",
      "  training_iteration: 632\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   632</td><td style=\"text-align: right;\">           13705</td><td style=\"text-align: right;\">632000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 633000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-21-13\n",
      "  done: false\n",
      "  episode_len_mean: 994.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 635\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.26362734606122995\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4608979596032037\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008634583544225628\n",
      "          policy_loss: -0.03762772394758132\n",
      "          total_loss: -0.05980458172659079\n",
      "          vf_explained_var: -0.8246957659721375\n",
      "          vf_loss: 0.00015581204417003391\n",
      "    num_agent_steps_sampled: 633000\n",
      "    num_agent_steps_trained: 633000\n",
      "    num_steps_sampled: 633000\n",
      "    num_steps_trained: 633000\n",
      "  iterations_since_restore: 633\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.88518518518516\n",
      "    ram_util_percent: 53.00370370370371\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06801714616536157\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.876029569528225\n",
      "    mean_inference_ms: 2.469346927709255\n",
      "    mean_raw_obs_processing_ms: 0.8782195499230118\n",
      "  time_since_restore: 13723.825939655304\n",
      "  time_this_iter_s: 18.836718559265137\n",
      "  time_total_s: 13723.825939655304\n",
      "  timers:\n",
      "    learn_throughput: 302.29\n",
      "    learn_time_ms: 3308.084\n",
      "    load_throughput: 21487.345\n",
      "    load_time_ms: 46.539\n",
      "    sample_throughput: 55.643\n",
      "    sample_time_ms: 17971.632\n",
      "    update_time_ms: 6.494\n",
      "  timestamp: 1631892073\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 633000\n",
      "  training_iteration: 633\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   633</td><td style=\"text-align: right;\">         13723.8</td><td style=\"text-align: right;\">633000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 634000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-21-32\n",
      "  done: false\n",
      "  episode_len_mean: 994.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 636\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.26362734606122995\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.38253909084532\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006870255911525814\n",
      "          policy_loss: -0.04268805090751913\n",
      "          total_loss: -0.06451757564726802\n",
      "          vf_explained_var: -0.9140740036964417\n",
      "          vf_loss: 0.00018467738618836745\n",
      "    num_agent_steps_sampled: 634000\n",
      "    num_agent_steps_trained: 634000\n",
      "    num_steps_sampled: 634000\n",
      "    num_steps_trained: 634000\n",
      "  iterations_since_restore: 634\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.01851851851852\n",
      "    ram_util_percent: 53.0925925925926\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06801900882930201\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.874201829246227\n",
      "    mean_inference_ms: 2.469405893741647\n",
      "    mean_raw_obs_processing_ms: 0.8784571485527386\n",
      "  time_since_restore: 13742.633518695831\n",
      "  time_this_iter_s: 18.807579040527344\n",
      "  time_total_s: 13742.633518695831\n",
      "  timers:\n",
      "    learn_throughput: 303.515\n",
      "    learn_time_ms: 3294.73\n",
      "    load_throughput: 23033.578\n",
      "    load_time_ms: 43.415\n",
      "    sample_throughput: 55.536\n",
      "    sample_time_ms: 18006.394\n",
      "    update_time_ms: 6.635\n",
      "  timestamp: 1631892092\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 634000\n",
      "  training_iteration: 634\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 67.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   634</td><td style=\"text-align: right;\">         13742.6</td><td style=\"text-align: right;\">634000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 635000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-21-51\n",
      "  done: false\n",
      "  episode_len_mean: 994.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 637\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.26362734606122995\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.196476656860775\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00816672773045929\n",
      "          policy_loss: -0.04276911638961691\n",
      "          total_loss: -0.06213736095362239\n",
      "          vf_explained_var: -0.8443229794502258\n",
      "          vf_loss: 0.0004435501694994552\n",
      "    num_agent_steps_sampled: 635000\n",
      "    num_agent_steps_trained: 635000\n",
      "    num_steps_sampled: 635000\n",
      "    num_steps_trained: 635000\n",
      "  iterations_since_restore: 635\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.0\n",
      "    ram_util_percent: 53.23076923076924\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06802076271998658\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.872214089205059\n",
      "    mean_inference_ms: 2.469460124570873\n",
      "    mean_raw_obs_processing_ms: 0.8786957489880716\n",
      "  time_since_restore: 13761.320974588394\n",
      "  time_this_iter_s: 18.687455892562866\n",
      "  time_total_s: 13761.320974588394\n",
      "  timers:\n",
      "    learn_throughput: 301.365\n",
      "    learn_time_ms: 3318.235\n",
      "    load_throughput: 23526.115\n",
      "    load_time_ms: 42.506\n",
      "    sample_throughput: 55.84\n",
      "    sample_time_ms: 17908.42\n",
      "    update_time_ms: 6.656\n",
      "  timestamp: 1631892111\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 635000\n",
      "  training_iteration: 635\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 67.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   635</td><td style=\"text-align: right;\">         13761.3</td><td style=\"text-align: right;\">635000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 636000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-22-10\n",
      "  done: false\n",
      "  episode_len_mean: 994.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 638\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.26362734606122995\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8070839332209694\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006263105699599652\n",
      "          policy_loss: -0.05201959659655889\n",
      "          total_loss: -0.06790085658431053\n",
      "          vf_explained_var: -0.6799647212028503\n",
      "          vf_loss: 0.0005384505777328741\n",
      "    num_agent_steps_sampled: 636000\n",
      "    num_agent_steps_trained: 636000\n",
      "    num_steps_sampled: 636000\n",
      "    num_steps_trained: 636000\n",
      "  iterations_since_restore: 636\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.6222222222222\n",
      "    ram_util_percent: 53.26296296296296\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0680224802859753\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.870126380568918\n",
      "    mean_inference_ms: 2.4695105715224157\n",
      "    mean_raw_obs_processing_ms: 0.8789362516521737\n",
      "  time_since_restore: 13780.065580129623\n",
      "  time_this_iter_s: 18.744605541229248\n",
      "  time_total_s: 13780.065580129623\n",
      "  timers:\n",
      "    learn_throughput: 302.47\n",
      "    learn_time_ms: 3306.11\n",
      "    load_throughput: 24572.079\n",
      "    load_time_ms: 40.697\n",
      "    sample_throughput: 55.75\n",
      "    sample_time_ms: 17937.259\n",
      "    update_time_ms: 6.667\n",
      "  timestamp: 1631892130\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 636000\n",
      "  training_iteration: 636\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 67.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   636</td><td style=\"text-align: right;\">         13780.1</td><td style=\"text-align: right;\">636000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 637000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-22-30\n",
      "  done: false\n",
      "  episode_len_mean: 994.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 639\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.26362734606122995\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2358662459585403\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013344613066009951\n",
      "          policy_loss: -0.03770637199696567\n",
      "          total_loss: -0.0562845289396743\n",
      "          vf_explained_var: -0.409164160490036\n",
      "          vf_loss: 0.00026249919807115574\n",
      "    num_agent_steps_sampled: 637000\n",
      "    num_agent_steps_trained: 637000\n",
      "    num_steps_sampled: 637000\n",
      "    num_steps_trained: 637000\n",
      "  iterations_since_restore: 637\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.10689655172415\n",
      "    ram_util_percent: 53.20344827586207\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06802417772474832\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.868015881812745\n",
      "    mean_inference_ms: 2.4695608260195248\n",
      "    mean_raw_obs_processing_ms: 0.8791781321662616\n",
      "  time_since_restore: 13800.26109623909\n",
      "  time_this_iter_s: 20.195516109466553\n",
      "  time_total_s: 13800.26109623909\n",
      "  timers:\n",
      "    learn_throughput: 301.281\n",
      "    learn_time_ms: 3319.165\n",
      "    load_throughput: 25048.981\n",
      "    load_time_ms: 39.922\n",
      "    sample_throughput: 55.431\n",
      "    sample_time_ms: 18040.493\n",
      "    update_time_ms: 6.833\n",
      "  timestamp: 1631892150\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 637000\n",
      "  training_iteration: 637\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 67.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   637</td><td style=\"text-align: right;\">         13800.3</td><td style=\"text-align: right;\">637000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 638000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-22-50\n",
      "  done: false\n",
      "  episode_len_mean: 995.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 640\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.26362734606122995\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.364290421538883\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009694638870082489\n",
      "          policy_loss: -0.0496038358244631\n",
      "          total_loss: -0.07032453273940417\n",
      "          vf_explained_var: -0.9922937154769897\n",
      "          vf_loss: 0.0003664333167080258\n",
      "    num_agent_steps_sampled: 638000\n",
      "    num_agent_steps_trained: 638000\n",
      "    num_steps_sampled: 638000\n",
      "    num_steps_trained: 638000\n",
      "  iterations_since_restore: 638\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.65714285714284\n",
      "    ram_util_percent: 53.22142857142858\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06802583512494037\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.865883198334652\n",
      "    mean_inference_ms: 2.469608913183582\n",
      "    mean_raw_obs_processing_ms: 0.8790916242960328\n",
      "  time_since_restore: 13820.078698158264\n",
      "  time_this_iter_s: 19.817601919174194\n",
      "  time_total_s: 13820.078698158264\n",
      "  timers:\n",
      "    learn_throughput: 301.334\n",
      "    learn_time_ms: 3318.577\n",
      "    load_throughput: 26120.22\n",
      "    load_time_ms: 38.285\n",
      "    sample_throughput: 61.779\n",
      "    sample_time_ms: 16186.743\n",
      "    update_time_ms: 6.76\n",
      "  timestamp: 1631892170\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 638000\n",
      "  training_iteration: 638\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 67.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   638</td><td style=\"text-align: right;\">         13820.1</td><td style=\"text-align: right;\">638000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 639000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-23-10\n",
      "  done: false\n",
      "  episode_len_mean: 995.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 641\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.26362734606122995\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0288484162754483\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011651414426192927\n",
      "          policy_loss: -0.03863444566312763\n",
      "          total_loss: -0.05554631436243653\n",
      "          vf_explained_var: -0.5497144460678101\n",
      "          vf_loss: 0.00030498205339123764\n",
      "    num_agent_steps_sampled: 639000\n",
      "    num_agent_steps_trained: 639000\n",
      "    num_steps_sampled: 639000\n",
      "    num_steps_trained: 639000\n",
      "  iterations_since_restore: 639\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.72068965517244\n",
      "    ram_util_percent: 53.07931034482759\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06802732304830045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.8637115027098\n",
      "    mean_inference_ms: 2.4696551723464095\n",
      "    mean_raw_obs_processing_ms: 0.8790068875520131\n",
      "  time_since_restore: 13839.96779203415\n",
      "  time_this_iter_s: 19.88909387588501\n",
      "  time_total_s: 13839.96779203415\n",
      "  timers:\n",
      "    learn_throughput: 300.001\n",
      "    learn_time_ms: 3333.324\n",
      "    load_throughput: 26128.21\n",
      "    load_time_ms: 38.273\n",
      "    sample_throughput: 62.346\n",
      "    sample_time_ms: 16039.402\n",
      "    update_time_ms: 6.765\n",
      "  timestamp: 1631892190\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 639000\n",
      "  training_iteration: 639\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   639</td><td style=\"text-align: right;\">           13840</td><td style=\"text-align: right;\">639000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 640000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-23-28\n",
      "  done: false\n",
      "  episode_len_mean: 995.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 642\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.26362734606122995\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.374757209089067\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006361447502823648\n",
      "          policy_loss: -0.04299831498000357\n",
      "          total_loss: -0.06485267180121607\n",
      "          vf_explained_var: -0.9601516127586365\n",
      "          vf_loss: 0.00021616196436298197\n",
      "    num_agent_steps_sampled: 640000\n",
      "    num_agent_steps_trained: 640000\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "  iterations_since_restore: 640\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.54230769230769\n",
      "    ram_util_percent: 53.15384615384615\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0680288260915461\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.861512982988126\n",
      "    mean_inference_ms: 2.4697012901378788\n",
      "    mean_raw_obs_processing_ms: 0.8789244667786094\n",
      "  time_since_restore: 13858.614666938782\n",
      "  time_this_iter_s: 18.64687490463257\n",
      "  time_total_s: 13858.614666938782\n",
      "  timers:\n",
      "    learn_throughput: 300.846\n",
      "    learn_time_ms: 3323.965\n",
      "    load_throughput: 25986.659\n",
      "    load_time_ms: 38.481\n",
      "    sample_throughput: 63.493\n",
      "    sample_time_ms: 15749.879\n",
      "    update_time_ms: 7.303\n",
      "  timestamp: 1631892208\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 640\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   640</td><td style=\"text-align: right;\">         13858.6</td><td style=\"text-align: right;\">640000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 641000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-23-47\n",
      "  done: false\n",
      "  episode_len_mean: 995.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 643\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.26362734606122995\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4976070006688436\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006320048535288514\n",
      "          policy_loss: -0.020636477041989565\n",
      "          total_loss: -0.04383877117393745\n",
      "          vf_explained_var: -0.7770454287528992\n",
      "          vf_loss: 0.00010763930925653161\n",
      "    num_agent_steps_sampled: 641000\n",
      "    num_agent_steps_trained: 641000\n",
      "    num_steps_sampled: 641000\n",
      "    num_steps_trained: 641000\n",
      "  iterations_since_restore: 641\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.17307692307692\n",
      "    ram_util_percent: 53.138461538461556\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06803024572762019\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.859298200091601\n",
      "    mean_inference_ms: 2.469744872310779\n",
      "    mean_raw_obs_processing_ms: 0.8788440785462807\n",
      "  time_since_restore: 13876.77634048462\n",
      "  time_this_iter_s: 18.161673545837402\n",
      "  time_total_s: 13876.77634048462\n",
      "  timers:\n",
      "    learn_throughput: 299.725\n",
      "    learn_time_ms: 3336.391\n",
      "    load_throughput: 26422.311\n",
      "    load_time_ms: 37.847\n",
      "    sample_throughput: 63.607\n",
      "    sample_time_ms: 15721.647\n",
      "    update_time_ms: 7.392\n",
      "  timestamp: 1631892227\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 641000\n",
      "  training_iteration: 641\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 66.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   641</td><td style=\"text-align: right;\">         13876.8</td><td style=\"text-align: right;\">641000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 642000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-24-05\n",
      "  done: false\n",
      "  episode_len_mean: 995.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 644\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.26362734606122995\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.652866564856635\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005862735403117369\n",
      "          policy_loss: -0.05096826437446806\n",
      "          total_loss: -0.07583956658426258\n",
      "          vf_explained_var: -0.8788576126098633\n",
      "          vf_loss: 0.0001117869686267012\n",
      "    num_agent_steps_sampled: 642000\n",
      "    num_agent_steps_trained: 642000\n",
      "    num_steps_sampled: 642000\n",
      "    num_steps_trained: 642000\n",
      "  iterations_since_restore: 642\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.99629629629632\n",
      "    ram_util_percent: 53.1\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06803156473736754\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.85707210694366\n",
      "    mean_inference_ms: 2.4697880146596787\n",
      "    mean_raw_obs_processing_ms: 0.8787659423711901\n",
      "  time_since_restore: 13895.434430360794\n",
      "  time_this_iter_s: 18.658089876174927\n",
      "  time_total_s: 13895.434430360794\n",
      "  timers:\n",
      "    learn_throughput: 300.765\n",
      "    learn_time_ms: 3324.86\n",
      "    load_throughput: 25212.987\n",
      "    load_time_ms: 39.662\n",
      "    sample_throughput: 63.842\n",
      "    sample_time_ms: 15663.776\n",
      "    update_time_ms: 7.716\n",
      "  timestamp: 1631892245\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 642000\n",
      "  training_iteration: 642\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 67.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   642</td><td style=\"text-align: right;\">         13895.4</td><td style=\"text-align: right;\">642000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_d74d3_00000:\n",
      "  agent_timesteps_total: 643000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_15-24-25\n",
      "  done: false\n",
      "  episode_len_mean: 995.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 645\n",
      "  experiment_id: 07f70cbd5fb847dfafeefd9a680181c0\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.26362734606122995\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5354156348440382\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011451674264380587\n",
      "          policy_loss: -0.04258376900106668\n",
      "          total_loss: -0.054573836751903096\n",
      "          vf_explained_var: -0.7680838704109192\n",
      "          vf_loss: 0.000345116782101387\n",
      "    num_agent_steps_sampled: 643000\n",
      "    num_agent_steps_trained: 643000\n",
      "    num_steps_sampled: 643000\n",
      "    num_steps_trained: 643000\n",
      "  iterations_since_restore: 643\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.73793103448276\n",
      "    ram_util_percent: 53.224137931034484\n",
      "  pid: 333\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06803289926575995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.854848792793527\n",
      "    mean_inference_ms: 2.469831063149197\n",
      "    mean_raw_obs_processing_ms: 0.8786899633291486\n",
      "  time_since_restore: 13915.424787521362\n",
      "  time_this_iter_s: 19.990357160568237\n",
      "  time_total_s: 13915.424787521362\n",
      "  timers:\n",
      "    learn_throughput: 299.228\n",
      "    learn_time_ms: 3341.93\n",
      "    load_throughput: 25084.05\n",
      "    load_time_ms: 39.866\n",
      "    sample_throughput: 63.444\n",
      "    sample_time_ms: 15762.027\n",
      "    update_time_ms: 7.529\n",
      "  timestamp: 1631892265\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 643000\n",
      "  training_iteration: 643\n",
      "  trial_id: d74d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 67.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/76.3 GiB heap, 0.0/36.69 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /root/ray_results/PPO_2021-09-17_11-31-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_d74d3_00000</td><td>RUNNING </td><td>10.55.229.87:333</td><td style=\"text-align: right;\">   643</td><td style=\"text-align: right;\">         13915.4</td><td style=\"text-align: right;\">643000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 1,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 1000,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO C17 pretrained\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger])#callbacks=[\n",
    "        #    CustomLoggerCallback(),\n",
    "        #])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
