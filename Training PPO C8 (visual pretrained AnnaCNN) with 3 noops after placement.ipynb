{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "#from models import VisualEncoder\n",
    "from train import *\n",
    "from wrappers_2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9fd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4, padding=0),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(64, 512, kernel_size=2, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        features_dim = 512\n",
    "        self.encoder = VisualEncoder()\n",
    "        self.encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AnnaCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.action_head = nn.Linear(features_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(features_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.encoder.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        if self.use_cuda:\n",
    "            obs.cuda()\n",
    "            \n",
    "        features = self.encoder(obs)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "    env.update_taskset(TaskSet(preset=['C8']))\n",
    "    env = PovOnlyWrapper(env)\n",
    "    env = SelectAndPlace(env)\n",
    "    env = Discretization(env, flat_action_space('human-level'))\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/6 CPUs, 0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-06 12:42:43,681\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-10-06 12:42:43,696\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to auto resume run with id e618f_00000 but id e6792_00000 is set.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=1133330)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1133330)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO C8 pretrained (AnnaCNN) (3 noops after placement)</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/e6792_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/e6792_00000</a><br/>\n",
       "                Run data is saved locally in <code>/IGLU-Minecraft/wandb/run-20211006_124244-e6792_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1133330)\u001b[0m 2021-10-06 12:42:47,899\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=1133330)\u001b[0m 2021-10-06 12:42:47,900\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=1133331)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1133331)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=1133330)\u001b[0m 2021-10-06 12:42:56,190\tINFO trainable.py:109 -- Trainable.setup took 11.280 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=1133330)\u001b[0m 2021-10-06 12:42:56,191\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=1133331)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1133331)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=1133331)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1133331)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=1133331)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=1133331)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-44-23\n",
      "  done: false\n",
      "  episode_len_mean: 375.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.314123249053955\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03125776325824559\n",
      "          policy_loss: -0.07530977502465248\n",
      "          total_loss: -0.02365747218330701\n",
      "          vf_explained_var: 0.7709622979164124\n",
      "          vf_loss: 0.0685419831217991\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.77680000000001\n",
      "    ram_util_percent: 78.01200000000001\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045128754683426926\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 84.15256084857526\n",
      "    mean_inference_ms: 1.6821995600834714\n",
      "    mean_raw_obs_processing_ms: 0.2037607111059107\n",
      "  time_since_restore: 86.96482110023499\n",
      "  time_this_iter_s: 86.96482110023499\n",
      "  time_total_s: 86.96482110023499\n",
      "  timers:\n",
      "    learn_throughput: 1514.473\n",
      "    learn_time_ms: 660.295\n",
      "    load_throughput: 54784.535\n",
      "    load_time_ms: 18.253\n",
      "    sample_throughput: 11.591\n",
      "    sample_time_ms: 86275.593\n",
      "    update_time_ms: 6.553\n",
      "  timestamp: 1633524263\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         86.9648</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               375</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-44-51\n",
      "  done: false\n",
      "  episode_len_mean: 361.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 5\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.322902117835151\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012161669822374637\n",
      "          policy_loss: -0.06756407370169958\n",
      "          total_loss: -0.04765793109933535\n",
      "          vf_explained_var: 0.6702104806900024\n",
      "          vf_loss: 0.039486660559972124\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.95128205128205\n",
      "    ram_util_percent: 77.46666666666667\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04817988937029614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 66.53496870346024\n",
      "    mean_inference_ms: 1.6443386651542098\n",
      "    mean_raw_obs_processing_ms: 0.20584089600566222\n",
      "  time_since_restore: 114.91197967529297\n",
      "  time_this_iter_s: 27.947158575057983\n",
      "  time_total_s: 114.91197967529297\n",
      "  timers:\n",
      "    learn_throughput: 1539.674\n",
      "    learn_time_ms: 649.488\n",
      "    load_throughput: 57178.94\n",
      "    load_time_ms: 17.489\n",
      "    sample_throughput: 17.612\n",
      "    sample_time_ms: 56780.962\n",
      "    update_time_ms: 4.334\n",
      "  timestamp: 1633524291\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         114.912</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               361</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-45-18\n",
      "  done: false\n",
      "  episode_len_mean: 350.375\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 8\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3784685161378647\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012322832692780613\n",
      "          policy_loss: 0.06370737404666013\n",
      "          total_loss: 0.05670995715384682\n",
      "          vf_explained_var: 0.8983887434005737\n",
      "          vf_loss: 0.01309041828951902\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.1625\n",
      "    ram_util_percent: 77.2175\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04842944863888854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 58.40776363696264\n",
      "    mean_inference_ms: 1.6377845005364575\n",
      "    mean_raw_obs_processing_ms: 0.2064906013392222\n",
      "  time_since_restore: 142.49171352386475\n",
      "  time_this_iter_s: 27.579733848571777\n",
      "  time_total_s: 142.49171352386475\n",
      "  timers:\n",
      "    learn_throughput: 1570.305\n",
      "    learn_time_ms: 636.819\n",
      "    load_throughput: 57030.444\n",
      "    load_time_ms: 17.534\n",
      "    sample_throughput: 21.351\n",
      "    sample_time_ms: 46835.53\n",
      "    update_time_ms: 3.641\n",
      "  timestamp: 1633524318\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         142.492</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           350.375</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-45-47\n",
      "  done: false\n",
      "  episode_len_mean: 346.6363636363636\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 11\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.362176521619161\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013392603421455243\n",
      "          policy_loss: 0.07080499331156413\n",
      "          total_loss: 0.05847318255239063\n",
      "          vf_explained_var: 0.8645395636558533\n",
      "          vf_loss: 0.0072721756063401696\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.2875\n",
      "    ram_util_percent: 77.2175\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04828980632174829\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 53.41379902269361\n",
      "    mean_inference_ms: 1.6323415346652903\n",
      "    mean_raw_obs_processing_ms: 0.20773497332237217\n",
      "  time_since_restore: 170.91097354888916\n",
      "  time_this_iter_s: 28.419260025024414\n",
      "  time_total_s: 170.91097354888916\n",
      "  timers:\n",
      "    learn_throughput: 1541.566\n",
      "    learn_time_ms: 648.691\n",
      "    load_throughput: 57284.757\n",
      "    load_time_ms: 17.457\n",
      "    sample_throughput: 23.779\n",
      "    sample_time_ms: 42054.543\n",
      "    update_time_ms: 3.233\n",
      "  timestamp: 1633524347\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         170.911</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           346.636</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-46-15\n",
      "  done: false\n",
      "  episode_len_mean: 342.85714285714283\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 14\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.376375593079461\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013742724192950738\n",
      "          policy_loss: -0.014393066697650485\n",
      "          total_loss: -0.03003637029065026\n",
      "          vf_explained_var: 0.7836697697639465\n",
      "          vf_loss: 0.0039976349721352255\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 5000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.66585365853658\n",
      "    ram_util_percent: 77.64390243902439\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.048016794867918815\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 49.943401955726834\n",
      "    mean_inference_ms: 1.630828144919705\n",
      "    mean_raw_obs_processing_ms: 0.20881245968635348\n",
      "  time_since_restore: 199.30997252464294\n",
      "  time_this_iter_s: 28.398998975753784\n",
      "  time_total_s: 199.30997252464294\n",
      "  timers:\n",
      "    learn_throughput: 1526.144\n",
      "    learn_time_ms: 655.246\n",
      "    load_throughput: 61017.643\n",
      "    load_time_ms: 16.389\n",
      "    sample_throughput: 25.521\n",
      "    sample_time_ms: 39182.924\n",
      "    update_time_ms: 3.032\n",
      "  timestamp: 1633524375\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          199.31</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           342.857</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-46-43\n",
      "  done: false\n",
      "  episode_len_mean: 339.11764705882354\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 17\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3854885869556\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010513445181325033\n",
      "          policy_loss: -0.02513536661863327\n",
      "          total_loss: -0.043245673179626465\n",
      "          vf_explained_var: 0.6319712400436401\n",
      "          vf_loss: 0.0025905473519944482\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.09743589743591\n",
      "    ram_util_percent: 77.94871794871794\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04777205266543957\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 47.33307598195059\n",
      "    mean_inference_ms: 1.6297438068012382\n",
      "    mean_raw_obs_processing_ms: 0.20970185515252993\n",
      "  time_since_restore: 226.72062134742737\n",
      "  time_this_iter_s: 27.410648822784424\n",
      "  time_total_s: 226.72062134742737\n",
      "  timers:\n",
      "    learn_throughput: 1539.425\n",
      "    learn_time_ms: 649.593\n",
      "    load_throughput: 60363.884\n",
      "    load_time_ms: 16.566\n",
      "    sample_throughput: 26.944\n",
      "    sample_time_ms: 37113.478\n",
      "    update_time_ms: 2.857\n",
      "  timestamp: 1633524403\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         226.721</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           339.118</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 7000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-47-09\n",
      "  done: false\n",
      "  episode_len_mean: 337.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 20\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.365061291058858\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011536900511820313\n",
      "          policy_loss: -0.10392913694183031\n",
      "          total_loss: -0.12249978979428609\n",
      "          vf_explained_var: 0.2073507159948349\n",
      "          vf_loss: 0.0016188948615712838\n",
      "    num_agent_steps_sampled: 7000\n",
      "    num_agent_steps_trained: 7000\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 7000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.26052631578948\n",
      "    ram_util_percent: 78.01052631578946\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0475295903104417\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 45.265487582071614\n",
      "    mean_inference_ms: 1.628148235757784\n",
      "    mean_raw_obs_processing_ms: 0.21004895053336797\n",
      "  time_since_restore: 253.276034116745\n",
      "  time_this_iter_s: 26.555412769317627\n",
      "  time_total_s: 253.276034116745\n",
      "  timers:\n",
      "    learn_throughput: 1535.303\n",
      "    learn_time_ms: 651.337\n",
      "    load_throughput: 59291.35\n",
      "    load_time_ms: 16.866\n",
      "    sample_throughput: 28.163\n",
      "    sample_time_ms: 35507.097\n",
      "    update_time_ms: 2.701\n",
      "  timestamp: 1633524429\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         253.276</td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            337.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-47-34\n",
      "  done: false\n",
      "  episode_len_mean: 336.8695652173913\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 23\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.313660012351142\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010862536389515122\n",
      "          policy_loss: -0.103098423861795\n",
      "          total_loss: -0.12220511502689785\n",
      "          vf_explained_var: 0.16595174372196198\n",
      "          vf_loss: 0.0007711474683472059\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.97500000000001\n",
      "    ram_util_percent: 78.12222222222222\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047311251738073025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 43.560389684358235\n",
      "    mean_inference_ms: 1.6260599880869815\n",
      "    mean_raw_obs_processing_ms: 0.2102649298492955\n",
      "  time_since_restore: 278.52705478668213\n",
      "  time_this_iter_s: 25.251020669937134\n",
      "  time_total_s: 278.52705478668213\n",
      "  timers:\n",
      "    learn_throughput: 1539.472\n",
      "    learn_time_ms: 649.573\n",
      "    load_throughput: 55605.803\n",
      "    load_time_ms: 17.984\n",
      "    sample_throughput: 29.291\n",
      "    sample_time_ms: 34140.368\n",
      "    update_time_ms: 2.63\n",
      "  timestamp: 1633524454\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         278.527</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            336.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-48-00\n",
      "  done: false\n",
      "  episode_len_mean: 335.3076923076923\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 26\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.290780848926968\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01384619869331654\n",
      "          policy_loss: -0.16786854084995056\n",
      "          total_loss: -0.18587764700253803\n",
      "          vf_explained_var: 0.03023543953895569\n",
      "          vf_loss: 0.0007448479109573075\n",
      "    num_agent_steps_sampled: 9000\n",
      "    num_agent_steps_trained: 9000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.90540540540542\n",
      "    ram_util_percent: 78.35945945945947\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04712318363835514\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 42.13103428373275\n",
      "    mean_inference_ms: 1.6242592066266168\n",
      "    mean_raw_obs_processing_ms: 0.21031588694736772\n",
      "  time_since_restore: 304.1048743724823\n",
      "  time_this_iter_s: 25.57781958580017\n",
      "  time_total_s: 304.1048743724823\n",
      "  timers:\n",
      "    learn_throughput: 1538.743\n",
      "    learn_time_ms: 649.881\n",
      "    load_throughput: 55808.798\n",
      "    load_time_ms: 17.918\n",
      "    sample_throughput: 30.199\n",
      "    sample_time_ms: 33113.674\n",
      "    update_time_ms: 2.58\n",
      "  timestamp: 1633524480\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         304.105</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           335.308</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1133331)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1133331)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-48-42\n",
      "  done: false\n",
      "  episode_len_mean: 331.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 30\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.236787184079488\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008242130012501372\n",
      "          policy_loss: 0.027933080535795955\n",
      "          total_loss: 0.008397500072088506\n",
      "          vf_explained_var: -0.060134727507829666\n",
      "          vf_loss: 0.00035965178071314263\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.19999999999999\n",
      "    ram_util_percent: 77.82033898305085\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04688264898735342\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.5549045394735\n",
      "    mean_inference_ms: 1.6213214856981508\n",
      "    mean_raw_obs_processing_ms: 0.43294984561636213\n",
      "  time_since_restore: 345.82190918922424\n",
      "  time_this_iter_s: 41.71703481674194\n",
      "  time_total_s: 345.82190918922424\n",
      "  timers:\n",
      "    learn_throughput: 1549.02\n",
      "    learn_time_ms: 645.569\n",
      "    load_throughput: 55894.243\n",
      "    load_time_ms: 17.891\n",
      "    sample_throughput: 29.489\n",
      "    sample_time_ms: 33910.966\n",
      "    update_time_ms: 2.483\n",
      "  timestamp: 1633524522\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         345.822</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             331.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 11000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-49-13\n",
      "  done: false\n",
      "  episode_len_mean: 329.09090909090907\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 33\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2810988638136123\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008344771828471152\n",
      "          policy_loss: 0.006251493013567395\n",
      "          total_loss: -0.013795589448677169\n",
      "          vf_explained_var: -0.5249354839324951\n",
      "          vf_loss: 0.00026047463882908535\n",
      "    num_agent_steps_sampled: 11000\n",
      "    num_agent_steps_trained: 11000\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 11000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.81818181818181\n",
      "    ram_util_percent: 79.56818181818181\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046715232210102216\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 39.60698422947325\n",
      "    mean_inference_ms: 1.618994149815682\n",
      "    mean_raw_obs_processing_ms: 0.5508039908349628\n",
      "  time_since_restore: 376.62242794036865\n",
      "  time_this_iter_s: 30.80051875114441\n",
      "  time_total_s: 376.62242794036865\n",
      "  timers:\n",
      "    learn_throughput: 1558.41\n",
      "    learn_time_ms: 641.68\n",
      "    load_throughput: 56917.762\n",
      "    load_time_ms: 17.569\n",
      "    sample_throughput: 35.337\n",
      "    sample_time_ms: 28299.166\n",
      "    update_time_ms: 2.023\n",
      "  timestamp: 1633524553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 11\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         376.622</td><td style=\"text-align: right;\">11000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           329.091</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-49-39\n",
      "  done: false\n",
      "  episode_len_mean: 328.3888888888889\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 36\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.285874507162306\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006900220482851359\n",
      "          policy_loss: 0.00032045001991921\n",
      "          total_loss: -0.020019838224268623\n",
      "          vf_explained_var: -0.698535144329071\n",
      "          vf_loss: 0.00044839028970373974\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.15789473684211\n",
      "    ram_util_percent: 79.45526315789476\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04655980359730009\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.77274043445131\n",
      "    mean_inference_ms: 1.6165162762837668\n",
      "    mean_raw_obs_processing_ms: 0.6384394273913226\n",
      "  time_since_restore: 402.82547664642334\n",
      "  time_this_iter_s: 26.203048706054688\n",
      "  time_total_s: 402.82547664642334\n",
      "  timers:\n",
      "    learn_throughput: 1562.683\n",
      "    learn_time_ms: 639.925\n",
      "    load_throughput: 56665.235\n",
      "    load_time_ms: 17.648\n",
      "    sample_throughput: 35.554\n",
      "    sample_time_ms: 28126.386\n",
      "    update_time_ms: 1.992\n",
      "  timestamp: 1633524579\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         402.825</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           328.389</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 13000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-50-04\n",
      "  done: false\n",
      "  episode_len_mean: 327.7692307692308\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 39\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3127404001024034\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005966940454645926\n",
      "          policy_loss: -0.04952314417395327\n",
      "          total_loss: -0.07060728278011083\n",
      "          vf_explained_var: -0.5271923542022705\n",
      "          vf_loss: 0.0002531814921591528\n",
      "    num_agent_steps_sampled: 13000\n",
      "    num_agent_steps_trained: 13000\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 13000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.05555555555554\n",
      "    ram_util_percent: 79.2361111111111\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046411548409510686\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.02641413838159\n",
      "    mean_inference_ms: 1.6140012961526047\n",
      "    mean_raw_obs_processing_ms: 0.7043000790499877\n",
      "  time_since_restore: 428.0373442173004\n",
      "  time_this_iter_s: 25.211867570877075\n",
      "  time_total_s: 428.0373442173004\n",
      "  timers:\n",
      "    learn_throughput: 1562.255\n",
      "    learn_time_ms: 640.1\n",
      "    load_throughput: 56119.499\n",
      "    load_time_ms: 17.819\n",
      "    sample_throughput: 35.856\n",
      "    sample_time_ms: 27889.229\n",
      "    update_time_ms: 1.944\n",
      "  timestamp: 1633524604\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 13\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         428.037</td><td style=\"text-align: right;\">13000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           327.769</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 14000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-50-30\n",
      "  done: false\n",
      "  episode_len_mean: 327.7142857142857\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 42\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3214550760057238\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0076867318772030225\n",
      "          policy_loss: 0.0028622201540403898\n",
      "          total_loss: -0.017711424910359912\n",
      "          vf_explained_var: -0.9944319725036621\n",
      "          vf_loss: 0.00033488523768028245\n",
      "    num_agent_steps_sampled: 14000\n",
      "    num_agent_steps_trained: 14000\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.56756756756756\n",
      "    ram_util_percent: 79.08648648648646\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04627013777110399\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.35949923755443\n",
      "    mean_inference_ms: 1.6114603456435619\n",
      "    mean_raw_obs_processing_ms: 0.7541212168222285\n",
      "  time_since_restore: 454.2041277885437\n",
      "  time_this_iter_s: 26.166783571243286\n",
      "  time_total_s: 454.2041277885437\n",
      "  timers:\n",
      "    learn_throughput: 1580.452\n",
      "    learn_time_ms: 632.73\n",
      "    load_throughput: 55965.094\n",
      "    load_time_ms: 17.868\n",
      "    sample_throughput: 36.139\n",
      "    sample_time_ms: 27671.289\n",
      "    update_time_ms: 1.976\n",
      "  timestamp: 1633524630\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 14\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         454.204</td><td style=\"text-align: right;\">14000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           327.714</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-50-56\n",
      "  done: false\n",
      "  episode_len_mean: 327.4888888888889\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 45\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2007294495900473\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007217968058247701\n",
      "          policy_loss: -0.005507272626790735\n",
      "          total_loss: -0.025074481964111328\n",
      "          vf_explained_var: -0.8617287278175354\n",
      "          vf_loss: 0.0002746953347620244\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_agent_steps_trained: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.76756756756754\n",
      "    ram_util_percent: 79.17297297297296\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04613841506361398\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 36.75893827316151\n",
      "    mean_inference_ms: 1.6089688102402755\n",
      "    mean_raw_obs_processing_ms: 0.7919679530229566\n",
      "  time_since_restore: 480.258332490921\n",
      "  time_this_iter_s: 26.05420470237732\n",
      "  time_total_s: 480.258332490921\n",
      "  timers:\n",
      "    learn_throughput: 1598.43\n",
      "    learn_time_ms: 625.614\n",
      "    load_throughput: 54066.264\n",
      "    load_time_ms: 18.496\n",
      "    sample_throughput: 36.438\n",
      "    sample_time_ms: 27443.624\n",
      "    update_time_ms: 1.932\n",
      "  timestamp: 1633524656\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         480.258</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           327.489</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-51-22\n",
      "  done: false\n",
      "  episode_len_mean: 327.3125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 48\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2730541202757095\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0073819474559066916\n",
      "          policy_loss: -0.02271429600401057\n",
      "          total_loss: -0.04283455676502652\n",
      "          vf_explained_var: -0.9602513909339905\n",
      "          vf_loss: 0.000395694850804931\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.67027027027028\n",
      "    ram_util_percent: 79.2972972972973\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046014242004387466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 36.21464291983366\n",
      "    mean_inference_ms: 1.6065307782116254\n",
      "    mean_raw_obs_processing_ms: 0.8206997465940744\n",
      "  time_since_restore: 506.2120752334595\n",
      "  time_this_iter_s: 25.953742742538452\n",
      "  time_total_s: 506.2120752334595\n",
      "  timers:\n",
      "    learn_throughput: 1603.985\n",
      "    learn_time_ms: 623.447\n",
      "    load_throughput: 53883.243\n",
      "    load_time_ms: 18.559\n",
      "    sample_throughput: 36.63\n",
      "    sample_time_ms: 27300.042\n",
      "    update_time_ms: 1.902\n",
      "  timestamp: 1633524682\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 16\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         506.212</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           327.312</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 17000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-51-48\n",
      "  done: false\n",
      "  episode_len_mean: 327.1372549019608\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 51\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.321487906244066\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010456893157812905\n",
      "          policy_loss: 0.010167422476742002\n",
      "          total_loss: -0.009641550357143085\n",
      "          vf_explained_var: -0.8112577795982361\n",
      "          vf_loss: 0.00026883827449637466\n",
      "    num_agent_steps_sampled: 17000\n",
      "    num_agent_steps_trained: 17000\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 17000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.47297297297298\n",
      "    ram_util_percent: 79.41351351351351\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04589901652445546\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.717798185306\n",
      "    mean_inference_ms: 1.604201412884477\n",
      "    mean_raw_obs_processing_ms: 0.8424168291242571\n",
      "  time_since_restore: 531.9152314662933\n",
      "  time_this_iter_s: 25.703156232833862\n",
      "  time_total_s: 531.9152314662933\n",
      "  timers:\n",
      "    learn_throughput: 1617.146\n",
      "    learn_time_ms: 618.373\n",
      "    load_throughput: 54469.853\n",
      "    load_time_ms: 18.359\n",
      "    sample_throughput: 36.737\n",
      "    sample_time_ms: 27220.201\n",
      "    update_time_ms: 1.904\n",
      "  timestamp: 1633524708\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 17\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         531.915</td><td style=\"text-align: right;\">17000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           327.137</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-52-14\n",
      "  done: false\n",
      "  episode_len_mean: 326.94545454545454\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 55\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.310197530852424\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007837101945060059\n",
      "          policy_loss: 0.011503797935114966\n",
      "          total_loss: -0.008990224243866073\n",
      "          vf_explained_var: -0.9371430277824402\n",
      "          vf_loss: 0.0002568220794071547\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.4\n",
      "    ram_util_percent: 79.5108108108108\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045756739853039546\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.121666810636\n",
      "    mean_inference_ms: 1.6012565379532526\n",
      "    mean_raw_obs_processing_ms: 0.8638425685861717\n",
      "  time_since_restore: 557.7071759700775\n",
      "  time_this_iter_s: 25.79194450378418\n",
      "  time_total_s: 557.7071759700775\n",
      "  timers:\n",
      "    learn_throughput: 1621.072\n",
      "    learn_time_ms: 616.876\n",
      "    load_throughput: 57343.574\n",
      "    load_time_ms: 17.439\n",
      "    sample_throughput: 36.66\n",
      "    sample_time_ms: 27277.632\n",
      "    update_time_ms: 1.871\n",
      "  timestamp: 1633524734\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         557.707</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           326.945</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 19000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-52-40\n",
      "  done: false\n",
      "  episode_len_mean: 326.8448275862069\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 58\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3142656167348226\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01044895249009969\n",
      "          policy_loss: -0.03088379053192006\n",
      "          total_loss: -0.05052395709272888\n",
      "          vf_explained_var: -0.7606180906295776\n",
      "          vf_loss: 0.0003678045248913501\n",
      "    num_agent_steps_sampled: 19000\n",
      "    num_agent_steps_trained: 19000\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 19000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.48918918918919\n",
      "    ram_util_percent: 79.63783783783781\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04565838387813398\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 34.717124294057186\n",
      "    mean_inference_ms: 1.599162464071599\n",
      "    mean_raw_obs_processing_ms: 0.8754147404364908\n",
      "  time_since_restore: 583.4591746330261\n",
      "  time_this_iter_s: 25.75199866294861\n",
      "  time_total_s: 583.4591746330261\n",
      "  timers:\n",
      "    learn_throughput: 1634.085\n",
      "    learn_time_ms: 611.963\n",
      "    load_throughput: 57305.811\n",
      "    load_time_ms: 17.45\n",
      "    sample_throughput: 36.63\n",
      "    sample_time_ms: 27300.216\n",
      "    update_time_ms: 1.836\n",
      "  timestamp: 1633524760\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 19\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         583.459</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           326.845</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1133331)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1133331)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-53-24\n",
      "  done: false\n",
      "  episode_len_mean: 325.655737704918\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 61\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.322466278076172\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00837210314297252\n",
      "          policy_loss: -0.011901268218126562\n",
      "          total_loss: -0.032379786525335574\n",
      "          vf_explained_var: -0.5687780380249023\n",
      "          vf_loss: 0.00023451319749357127\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.4421875\n",
      "    ram_util_percent: 80.00625\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04556550239412615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 34.3474839722852\n",
      "    mean_inference_ms: 1.597156026537264\n",
      "    mean_raw_obs_processing_ms: 0.9255478450740379\n",
      "  time_since_restore: 628.2350702285767\n",
      "  time_this_iter_s: 44.77589559555054\n",
      "  time_total_s: 628.2350702285767\n",
      "  timers:\n",
      "    learn_throughput: 1634.652\n",
      "    learn_time_ms: 611.751\n",
      "    load_throughput: 57546.875\n",
      "    load_time_ms: 17.377\n",
      "    sample_throughput: 36.223\n",
      "    sample_time_ms: 27606.397\n",
      "    update_time_ms: 1.855\n",
      "  timestamp: 1633524804\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 20\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         628.235</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           325.656</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-53-50\n",
      "  done: false\n",
      "  episode_len_mean: 325.125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 64\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3498921553293863\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0067338021432290455\n",
      "          policy_loss: 0.004905929954515563\n",
      "          total_loss: -0.016370230354368687\n",
      "          vf_explained_var: -0.625443160533905\n",
      "          vf_loss: 0.0002026210407267273\n",
      "    num_agent_steps_sampled: 21000\n",
      "    num_agent_steps_trained: 21000\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 21000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.87297297297297\n",
      "    ram_util_percent: 79.7027027027027\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045477208137562355\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 34.004409687932394\n",
      "    mean_inference_ms: 1.5952577994199788\n",
      "    mean_raw_obs_processing_ms: 0.9671898155350247\n",
      "  time_since_restore: 654.2614750862122\n",
      "  time_this_iter_s: 26.026404857635498\n",
      "  time_total_s: 654.2614750862122\n",
      "  timers:\n",
      "    learn_throughput: 1638.056\n",
      "    learn_time_ms: 610.48\n",
      "    load_throughput: 56551.016\n",
      "    load_time_ms: 17.683\n",
      "    sample_throughput: 36.859\n",
      "    sample_time_ms: 27130.062\n",
      "    update_time_ms: 1.838\n",
      "  timestamp: 1633524830\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 21\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         654.261</td><td style=\"text-align: right;\">21000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           325.125</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 22000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-54-16\n",
      "  done: false\n",
      "  episode_len_mean: 326.0149253731343\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 67\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.348033645417955\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005441397772202588\n",
      "          policy_loss: -0.0060876026956571475\n",
      "          total_loss: -0.026999319758680133\n",
      "          vf_explained_var: -0.9221914410591125\n",
      "          vf_loss: 0.0009362005099925833\n",
      "    num_agent_steps_sampled: 22000\n",
      "    num_agent_steps_trained: 22000\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.72222222222223\n",
      "    ram_util_percent: 79.55000000000001\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04539424615547307\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.68401860629284\n",
      "    mean_inference_ms: 1.593437717539132\n",
      "    mean_raw_obs_processing_ms: 1.0018211463531292\n",
      "  time_since_restore: 679.7817552089691\n",
      "  time_this_iter_s: 25.520280122756958\n",
      "  time_total_s: 679.7817552089691\n",
      "  timers:\n",
      "    learn_throughput: 1638.559\n",
      "    learn_time_ms: 610.292\n",
      "    load_throughput: 57993.012\n",
      "    load_time_ms: 17.243\n",
      "    sample_throughput: 36.952\n",
      "    sample_time_ms: 27062.354\n",
      "    update_time_ms: 1.847\n",
      "  timestamp: 1633524856\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 22\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         679.782</td><td style=\"text-align: right;\">22000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           326.015</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 23000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-54-41\n",
      "  done: false\n",
      "  episode_len_mean: 326.4714285714286\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 70\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2689546505610148\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008626306763759119\n",
      "          policy_loss: -0.04618263269464175\n",
      "          total_loss: -0.06573387278864781\n",
      "          vf_explained_var: -0.8899770379066467\n",
      "          vf_loss: 0.0005504127910222047\n",
      "    num_agent_steps_sampled: 23000\n",
      "    num_agent_steps_trained: 23000\n",
      "    num_steps_sampled: 23000\n",
      "    num_steps_trained: 23000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.65277777777779\n",
      "    ram_util_percent: 79.51666666666668\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04531495192431712\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.383737311842665\n",
      "    mean_inference_ms: 1.5916598673073907\n",
      "    mean_raw_obs_processing_ms: 1.0306176043067377\n",
      "  time_since_restore: 705.0877256393433\n",
      "  time_this_iter_s: 25.305970430374146\n",
      "  time_total_s: 705.0877256393433\n",
      "  timers:\n",
      "    learn_throughput: 1642.937\n",
      "    learn_time_ms: 608.666\n",
      "    load_throughput: 58347.01\n",
      "    load_time_ms: 17.139\n",
      "    sample_throughput: 36.936\n",
      "    sample_time_ms: 27073.498\n",
      "    update_time_ms: 1.849\n",
      "  timestamp: 1633524881\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 23\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         705.088</td><td style=\"text-align: right;\">23000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           326.471</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-55-06\n",
      "  done: false\n",
      "  episode_len_mean: 327.6027397260274\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 73\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4056336773766414\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005864289224910212\n",
      "          policy_loss: -0.029971211900313696\n",
      "          total_loss: -0.051674305357866816\n",
      "          vf_explained_var: -0.5788211822509766\n",
      "          vf_loss: 0.000593957603086892\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.4888888888889\n",
      "    ram_util_percent: 79.57222222222221\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045239784420336826\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.10065319886898\n",
      "    mean_inference_ms: 1.5899470680519232\n",
      "    mean_raw_obs_processing_ms: 1.054521881183734\n",
      "  time_since_restore: 729.8213272094727\n",
      "  time_this_iter_s: 24.733601570129395\n",
      "  time_total_s: 729.8213272094727\n",
      "  timers:\n",
      "    learn_throughput: 1643.15\n",
      "    learn_time_ms: 608.587\n",
      "    load_throughput: 57726.267\n",
      "    load_time_ms: 17.323\n",
      "    sample_throughput: 37.133\n",
      "    sample_time_ms: 26930.114\n",
      "    update_time_ms: 1.791\n",
      "  timestamp: 1633524906\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 24\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         729.821</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           327.603</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 25000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-55-31\n",
      "  done: false\n",
      "  episode_len_mean: 327.9736842105263\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 76\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.389535975456238\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014476585233006862\n",
      "          policy_loss: -0.04547526128590107\n",
      "          total_loss: -0.06468831093774902\n",
      "          vf_explained_var: -0.8937914371490479\n",
      "          vf_loss: 0.00033933703064879713\n",
      "    num_agent_steps_sampled: 25000\n",
      "    num_agent_steps_trained: 25000\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 25000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.59428571428572\n",
      "    ram_util_percent: 79.70285714285714\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04516812017978513\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.833257793683806\n",
      "    mean_inference_ms: 1.5883087261026354\n",
      "    mean_raw_obs_processing_ms: 1.0743131630857854\n",
      "  time_since_restore: 754.5562839508057\n",
      "  time_this_iter_s: 24.734956741333008\n",
      "  time_total_s: 754.5562839508057\n",
      "  timers:\n",
      "    learn_throughput: 1638.027\n",
      "    learn_time_ms: 610.491\n",
      "    load_throughput: 57238.009\n",
      "    load_time_ms: 17.471\n",
      "    sample_throughput: 37.319\n",
      "    sample_time_ms: 26796.137\n",
      "    update_time_ms: 1.797\n",
      "  timestamp: 1633524931\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 25\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         754.556</td><td style=\"text-align: right;\">25000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           327.974</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 26000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-55-56\n",
      "  done: false\n",
      "  episode_len_mean: 328.27848101265823\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 79\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.408798329035441\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006921102380545029\n",
      "          policy_loss: -0.021830982776979604\n",
      "          total_loss: -0.042934997379779814\n",
      "          vf_explained_var: -0.5182417631149292\n",
      "          vf_loss: 0.0009076392904616013\n",
      "    num_agent_steps_sampled: 26000\n",
      "    num_agent_steps_trained: 26000\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 26000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.46666666666667\n",
      "    ram_util_percent: 79.84166666666667\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0451003455308228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.58046191833875\n",
      "    mean_inference_ms: 1.5867389145040804\n",
      "    mean_raw_obs_processing_ms: 1.0906182844130554\n",
      "  time_since_restore: 779.4101357460022\n",
      "  time_this_iter_s: 24.853851795196533\n",
      "  time_total_s: 779.4101357460022\n",
      "  timers:\n",
      "    learn_throughput: 1634.983\n",
      "    learn_time_ms: 611.627\n",
      "    load_throughput: 57125.05\n",
      "    load_time_ms: 17.505\n",
      "    sample_throughput: 37.474\n",
      "    sample_time_ms: 26684.925\n",
      "    update_time_ms: 1.807\n",
      "  timestamp: 1633524956\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 26\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">          779.41</td><td style=\"text-align: right;\">26000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           328.278</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 27000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-56-20\n",
      "  done: false\n",
      "  episode_len_mean: 328.890243902439\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 82\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3735316620932685\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009872801160536572\n",
      "          policy_loss: -0.023671327489945625\n",
      "          total_loss: -0.04370854198932648\n",
      "          vf_explained_var: -0.9058646559715271\n",
      "          vf_loss: 0.000736261936455978\n",
      "    num_agent_steps_sampled: 27000\n",
      "    num_agent_steps_trained: 27000\n",
      "    num_steps_sampled: 27000\n",
      "    num_steps_trained: 27000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.64285714285714\n",
      "    ram_util_percent: 79.99142857142857\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04503592053745245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.34057964126483\n",
      "    mean_inference_ms: 1.5852567351726325\n",
      "    mean_raw_obs_processing_ms: 1.1039623702778358\n",
      "  time_since_restore: 803.9240338802338\n",
      "  time_this_iter_s: 24.513898134231567\n",
      "  time_total_s: 803.9240338802338\n",
      "  timers:\n",
      "    learn_throughput: 1634.24\n",
      "    learn_time_ms: 611.905\n",
      "    load_throughput: 56340.447\n",
      "    load_time_ms: 17.749\n",
      "    sample_throughput: 37.643\n",
      "    sample_time_ms: 26565.373\n",
      "    update_time_ms: 1.793\n",
      "  timestamp: 1633524980\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 27\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         803.924</td><td style=\"text-align: right;\">27000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            328.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-56-44\n",
      "  done: false\n",
      "  episode_len_mean: 329.95238095238096\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 84\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.208626813358731\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010744365376582958\n",
      "          policy_loss: -0.047010169240335624\n",
      "          total_loss: -0.06448256656941441\n",
      "          vf_explained_var: -0.9744651913642883\n",
      "          vf_loss: 0.0013905608179306404\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.73529411764706\n",
      "    ram_util_percent: 80.22647058823529\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04499463536244376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.18647402351913\n",
      "    mean_inference_ms: 1.584291584193319\n",
      "    mean_raw_obs_processing_ms: 1.1112296897923652\n",
      "  time_since_restore: 827.9988944530487\n",
      "  time_this_iter_s: 24.07486057281494\n",
      "  time_total_s: 827.9988944530487\n",
      "  timers:\n",
      "    learn_throughput: 1637.911\n",
      "    learn_time_ms: 610.534\n",
      "    load_throughput: 55367.134\n",
      "    load_time_ms: 18.061\n",
      "    sample_throughput: 37.886\n",
      "    sample_time_ms: 26394.745\n",
      "    update_time_ms: 1.781\n",
      "  timestamp: 1633525004\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 28\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         827.999</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           329.952</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 29000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-57-10\n",
      "  done: false\n",
      "  episode_len_mean: 329.8965517241379\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 87\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.253613519668579\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010521932501262734\n",
      "          policy_loss: -0.07505845816598998\n",
      "          total_loss: -0.09386778647700945\n",
      "          vf_explained_var: -0.8164140582084656\n",
      "          vf_loss: 0.0005702261560751747\n",
      "    num_agent_steps_sampled: 29000\n",
      "    num_agent_steps_trained: 29000\n",
      "    num_steps_sampled: 29000\n",
      "    num_steps_trained: 29000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.35833333333333\n",
      "    ram_util_percent: 80.49166666666666\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0449349709975811\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.964963047653796\n",
      "    mean_inference_ms: 1.5828835554861325\n",
      "    mean_raw_obs_processing_ms: 1.120087230283565\n",
      "  time_since_restore: 853.259731054306\n",
      "  time_this_iter_s: 25.260836601257324\n",
      "  time_total_s: 853.259731054306\n",
      "  timers:\n",
      "    learn_throughput: 1634.831\n",
      "    learn_time_ms: 611.684\n",
      "    load_throughput: 55123.73\n",
      "    load_time_ms: 18.141\n",
      "    sample_throughput: 37.959\n",
      "    sample_time_ms: 26344.432\n",
      "    update_time_ms: 1.775\n",
      "  timestamp: 1633525030\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29000\n",
      "  training_iteration: 29\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">          853.26</td><td style=\"text-align: right;\">29000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           329.897</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1133331)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1133331)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-57-49\n",
      "  done: false\n",
      "  episode_len_mean: 330.24444444444447\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 90\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3061464495129056\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010082798817364635\n",
      "          policy_loss: -0.06554363473421998\n",
      "          total_loss: -0.08511059929927191\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00046965819120588193\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.1375\n",
      "    ram_util_percent: 80.11607142857143\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04487758282233832\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.7531274270382\n",
      "    mean_inference_ms: 1.5815432252320167\n",
      "    mean_raw_obs_processing_ms: 1.1446025475018786\n",
      "  time_since_restore: 892.6809756755829\n",
      "  time_this_iter_s: 39.421244621276855\n",
      "  time_total_s: 892.6809756755829\n",
      "  timers:\n",
      "    learn_throughput: 1633.478\n",
      "    learn_time_ms: 612.191\n",
      "    load_throughput: 54795.342\n",
      "    load_time_ms: 18.25\n",
      "    sample_throughput: 38.747\n",
      "    sample_time_ms: 25808.38\n",
      "    update_time_ms: 1.776\n",
      "  timestamp: 1633525069\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 30\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         892.681</td><td style=\"text-align: right;\">30000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           330.244</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 31000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-58-17\n",
      "  done: false\n",
      "  episode_len_mean: 330.68817204301075\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 93\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.35583078066508\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007577602962546671\n",
      "          policy_loss: 0.016388566460874346\n",
      "          total_loss: -0.004228497710492876\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0006679625733037634\n",
      "    num_agent_steps_sampled: 31000\n",
      "    num_agent_steps_trained: 31000\n",
      "    num_steps_sampled: 31000\n",
      "    num_steps_trained: 31000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.71794871794872\n",
      "    ram_util_percent: 80.44358974358977\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04482389020701301\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.554295948376588\n",
      "    mean_inference_ms: 1.5802699982527906\n",
      "    mean_raw_obs_processing_ms: 1.1658195764137027\n",
      "  time_since_restore: 920.1066036224365\n",
      "  time_this_iter_s: 27.425627946853638\n",
      "  time_total_s: 920.1066036224365\n",
      "  timers:\n",
      "    learn_throughput: 1634.038\n",
      "    learn_time_ms: 611.981\n",
      "    load_throughput: 54230.472\n",
      "    load_time_ms: 18.44\n",
      "    sample_throughput: 38.538\n",
      "    sample_time_ms: 25948.354\n",
      "    update_time_ms: 1.777\n",
      "  timestamp: 1633525097\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31000\n",
      "  training_iteration: 31\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         920.107</td><td style=\"text-align: right;\">31000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           330.688</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-58-40\n",
      "  done: false\n",
      "  episode_len_mean: 331.625\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 96\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.407492240269979\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009487561775224702\n",
      "          policy_loss: -0.030074406953321562\n",
      "          total_loss: -0.05063056796789169\n",
      "          vf_explained_var: -0.9856142401695251\n",
      "          vf_loss: 0.0006724917564295336\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.45454545454545\n",
      "    ram_util_percent: 80.11212121212122\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04477674703517376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.363099846138795\n",
      "    mean_inference_ms: 1.5790479794231882\n",
      "    mean_raw_obs_processing_ms: 1.184147321204487\n",
      "  time_since_restore: 943.2327406406403\n",
      "  time_this_iter_s: 23.126137018203735\n",
      "  time_total_s: 943.2327406406403\n",
      "  timers:\n",
      "    learn_throughput: 1637.443\n",
      "    learn_time_ms: 610.708\n",
      "    load_throughput: 52742.685\n",
      "    load_time_ms: 18.96\n",
      "    sample_throughput: 38.896\n",
      "    sample_time_ms: 25709.749\n",
      "    update_time_ms: 1.763\n",
      "  timestamp: 1633525120\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 32\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         943.233</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           331.625</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 33000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-59-03\n",
      "  done: false\n",
      "  episode_len_mean: 332.67676767676767\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 99\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.403721496793959\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005912229759308948\n",
      "          policy_loss: -0.06254274518125587\n",
      "          total_loss: -0.08431535512208939\n",
      "          vf_explained_var: -0.9985917806625366\n",
      "          vf_loss: 0.0004909348277174609\n",
      "    num_agent_steps_sampled: 33000\n",
      "    num_agent_steps_trained: 33000\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 33000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.47058823529412\n",
      "    ram_util_percent: 79.85294117647061\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04473119243766393\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.17938071359024\n",
      "    mean_inference_ms: 1.5778703443560171\n",
      "    mean_raw_obs_processing_ms: 1.199936533148925\n",
      "  time_since_restore: 966.616456747055\n",
      "  time_this_iter_s: 23.383716106414795\n",
      "  time_total_s: 966.616456747055\n",
      "  timers:\n",
      "    learn_throughput: 1633.672\n",
      "    learn_time_ms: 612.118\n",
      "    load_throughput: 52640.679\n",
      "    load_time_ms: 18.997\n",
      "    sample_throughput: 39.191\n",
      "    sample_time_ms: 25516.136\n",
      "    update_time_ms: 1.775\n",
      "  timestamp: 1633525143\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 33\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         966.616</td><td style=\"text-align: right;\">33000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           332.677</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 34000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-59-28\n",
      "  done: false\n",
      "  episode_len_mean: 332.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 102\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2780280960930717\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008803403149943417\n",
      "          policy_loss: -0.05441572086678611\n",
      "          total_loss: -0.07405948518878884\n",
      "          vf_explained_var: -0.9877132177352905\n",
      "          vf_loss: 0.000495494605274871\n",
      "    num_agent_steps_sampled: 34000\n",
      "    num_agent_steps_trained: 34000\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 34000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.39722222222223\n",
      "    ram_util_percent: 79.80277777777779\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04467986892404746\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.9412902503834\n",
      "    mean_inference_ms: 1.5746384750022862\n",
      "    mean_raw_obs_processing_ms: 1.2336954892034566\n",
      "  time_since_restore: 991.8191301822662\n",
      "  time_this_iter_s: 25.20267343521118\n",
      "  time_total_s: 991.8191301822662\n",
      "  timers:\n",
      "    learn_throughput: 1633.618\n",
      "    learn_time_ms: 612.138\n",
      "    load_throughput: 52801.047\n",
      "    load_time_ms: 18.939\n",
      "    sample_throughput: 39.119\n",
      "    sample_time_ms: 25563.053\n",
      "    update_time_ms: 1.789\n",
      "  timestamp: 1633525168\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 34\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         991.819</td><td style=\"text-align: right;\">34000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            332.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 35000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_12-59-53\n",
      "  done: false\n",
      "  episode_len_mean: 331.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 105\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2034784065352544\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007870286726643859\n",
      "          policy_loss: -0.04719396945503023\n",
      "          total_loss: -0.06634017324282063\n",
      "          vf_explained_var: -0.936732292175293\n",
      "          vf_loss: 0.0005274936066901622\n",
      "    num_agent_steps_sampled: 35000\n",
      "    num_agent_steps_trained: 35000\n",
      "    num_steps_sampled: 35000\n",
      "    num_steps_trained: 35000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.3257142857143\n",
      "    ram_util_percent: 79.85428571428572\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04447103045587308\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.05184599445633\n",
      "    mean_inference_ms: 1.5722315278341625\n",
      "    mean_raw_obs_processing_ms: 1.27606099812258\n",
      "  time_since_restore: 1016.5704674720764\n",
      "  time_this_iter_s: 24.75133728981018\n",
      "  time_total_s: 1016.5704674720764\n",
      "  timers:\n",
      "    learn_throughput: 1635.903\n",
      "    learn_time_ms: 611.283\n",
      "    load_throughput: 53538.178\n",
      "    load_time_ms: 18.678\n",
      "    sample_throughput: 39.115\n",
      "    sample_time_ms: 25565.771\n",
      "    update_time_ms: 1.786\n",
      "  timestamp: 1633525193\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 35\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         1016.57</td><td style=\"text-align: right;\">35000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            331.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_13-00-20\n",
      "  done: false\n",
      "  episode_len_mean: 331.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 107\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.097370070881314\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009876248191967956\n",
      "          policy_loss: -0.06713449996378687\n",
      "          total_loss: -0.08469131348861589\n",
      "          vf_explained_var: -0.8037344813346863\n",
      "          vf_loss: 0.00045401279397386436\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.48684210526318\n",
      "    ram_util_percent: 80.01315789473682\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04435834975935776\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.656787496572036\n",
      "    mean_inference_ms: 1.5704438718103018\n",
      "    mean_raw_obs_processing_ms: 1.3034917749089245\n",
      "  time_since_restore: 1042.9311945438385\n",
      "  time_this_iter_s: 26.360727071762085\n",
      "  time_total_s: 1042.9311945438385\n",
      "  timers:\n",
      "    learn_throughput: 1636.097\n",
      "    learn_time_ms: 611.211\n",
      "    load_throughput: 53329.332\n",
      "    load_time_ms: 18.751\n",
      "    sample_throughput: 38.886\n",
      "    sample_time_ms: 25716.496\n",
      "    update_time_ms: 1.794\n",
      "  timestamp: 1633525220\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 36\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         1042.93</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            331.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 37000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_13-00-45\n",
      "  done: false\n",
      "  episode_len_mean: 331.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 110\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.147047514385647\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007426045684688552\n",
      "          policy_loss: -0.03157592833869987\n",
      "          total_loss: -0.050217043567034936\n",
      "          vf_explained_var: -0.8911110758781433\n",
      "          vf_loss: 0.0006015459078803865\n",
      "    num_agent_steps_sampled: 37000\n",
      "    num_agent_steps_trained: 37000\n",
      "    num_steps_sampled: 37000\n",
      "    num_steps_trained: 37000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.65405405405404\n",
      "    ram_util_percent: 80.10810810810807\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044207088561031144\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.157989720237847\n",
      "    mean_inference_ms: 1.5679211779921869\n",
      "    mean_raw_obs_processing_ms: 1.3434468294985424\n",
      "  time_since_restore: 1068.5870826244354\n",
      "  time_this_iter_s: 25.655888080596924\n",
      "  time_total_s: 1068.5870826244354\n",
      "  timers:\n",
      "    learn_throughput: 1637.966\n",
      "    learn_time_ms: 610.513\n",
      "    load_throughput: 53627.577\n",
      "    load_time_ms: 18.647\n",
      "    sample_throughput: 38.712\n",
      "    sample_time_ms: 25831.538\n",
      "    update_time_ms: 1.809\n",
      "  timestamp: 1633525245\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37000\n",
      "  training_iteration: 37\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         1068.59</td><td style=\"text-align: right;\">37000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            331.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 38000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_13-01-12\n",
      "  done: false\n",
      "  episode_len_mean: 331.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 113\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.10894718170166\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010579781437267123\n",
      "          policy_loss: -0.024451651258601083\n",
      "          total_loss: -0.041768253677421145\n",
      "          vf_explained_var: -0.9145022630691528\n",
      "          vf_loss: 0.000598932343806761\n",
      "    num_agent_steps_sampled: 38000\n",
      "    num_agent_steps_trained: 38000\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 38000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.58108108108107\n",
      "    ram_util_percent: 80.22702702702705\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044082323922871085\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.76340947860493\n",
      "    mean_inference_ms: 1.5653237857587206\n",
      "    mean_raw_obs_processing_ms: 1.3822717244563547\n",
      "  time_since_restore: 1094.8240280151367\n",
      "  time_this_iter_s: 26.236945390701294\n",
      "  time_total_s: 1094.8240280151367\n",
      "  timers:\n",
      "    learn_throughput: 1637.665\n",
      "    learn_time_ms: 610.625\n",
      "    load_throughput: 53367.331\n",
      "    load_time_ms: 18.738\n",
      "    sample_throughput: 38.391\n",
      "    sample_time_ms: 26047.46\n",
      "    update_time_ms: 1.815\n",
      "  timestamp: 1633525272\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 38\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         1094.82</td><td style=\"text-align: right;\">38000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            331.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 39000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_13-01-39\n",
      "  done: false\n",
      "  episode_len_mean: 332.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 117\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0892696804470487\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009036276780107579\n",
      "          policy_loss: -0.07493243370619085\n",
      "          total_loss: -0.09257437605410815\n",
      "          vf_explained_var: -0.9231986403465271\n",
      "          vf_loss: 0.000539873615748042\n",
      "    num_agent_steps_sampled: 39000\n",
      "    num_agent_steps_trained: 39000\n",
      "    num_steps_sampled: 39000\n",
      "    num_steps_trained: 39000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.3925\n",
      "    ram_util_percent: 80.3475\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04393888005628335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.33787027389979\n",
      "    mean_inference_ms: 1.5617541303046636\n",
      "    mean_raw_obs_processing_ms: 1.4326819640159638\n",
      "  time_since_restore: 1122.5072524547577\n",
      "  time_this_iter_s: 27.68322443962097\n",
      "  time_total_s: 1122.5072524547577\n",
      "  timers:\n",
      "    learn_throughput: 1638.385\n",
      "    learn_time_ms: 610.357\n",
      "    load_throughput: 53430.148\n",
      "    load_time_ms: 18.716\n",
      "    sample_throughput: 38.037\n",
      "    sample_time_ms: 26289.904\n",
      "    update_time_ms: 1.844\n",
      "  timestamp: 1633525299\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 39\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         1122.51</td><td style=\"text-align: right;\">39000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            332.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1133331)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1133331)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_13-02-21\n",
      "  done: false\n",
      "  episode_len_mean: 332.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 120\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1570902877383764\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008236065249003843\n",
      "          policy_loss: -0.06079000437425242\n",
      "          total_loss: -0.07912651745395528\n",
      "          vf_explained_var: -0.7133936285972595\n",
      "          vf_loss: 0.000763570097883025\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.49152542372883\n",
      "    ram_util_percent: 80.4830508474576\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04384758754320652\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.081025425353904\n",
      "    mean_inference_ms: 1.5592342653102662\n",
      "    mean_raw_obs_processing_ms: 1.4816532725899112\n",
      "  time_since_restore: 1164.3786051273346\n",
      "  time_this_iter_s: 41.871352672576904\n",
      "  time_total_s: 1164.3786051273346\n",
      "  timers:\n",
      "    learn_throughput: 1641.002\n",
      "    learn_time_ms: 609.384\n",
      "    load_throughput: 53369.572\n",
      "    load_time_ms: 18.737\n",
      "    sample_throughput: 37.685\n",
      "    sample_time_ms: 26535.794\n",
      "    update_time_ms: 1.865\n",
      "  timestamp: 1633525341\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 40\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         1164.38</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            332.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 41000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_13-02-49\n",
      "  done: false\n",
      "  episode_len_mean: 332.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 123\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3250815682941015\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011092340554225031\n",
      "          policy_loss: -0.05347403647998969\n",
      "          total_loss: -0.07279929456611474\n",
      "          vf_explained_var: -0.215880885720253\n",
      "          vf_loss: 0.0005978553599561565\n",
      "    num_agent_steps_sampled: 41000\n",
      "    num_agent_steps_trained: 41000\n",
      "    num_steps_sampled: 41000\n",
      "    num_steps_trained: 41000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.64\n",
      "    ram_util_percent: 80.4025\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04376451867707734\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.865256795185413\n",
      "    mean_inference_ms: 1.5569038120433196\n",
      "    mean_raw_obs_processing_ms: 1.529430581799921\n",
      "  time_since_restore: 1192.3384938240051\n",
      "  time_this_iter_s: 27.959888696670532\n",
      "  time_total_s: 1192.3384938240051\n",
      "  timers:\n",
      "    learn_throughput: 1639.959\n",
      "    learn_time_ms: 609.771\n",
      "    load_throughput: 53450.643\n",
      "    load_time_ms: 18.709\n",
      "    sample_throughput: 37.61\n",
      "    sample_time_ms: 26588.821\n",
      "    update_time_ms: 1.862\n",
      "  timestamp: 1633525369\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 41000\n",
      "  training_iteration: 41\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         1192.34</td><td style=\"text-align: right;\">41000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            332.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 42000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_13-03-16\n",
      "  done: false\n",
      "  episode_len_mean: 332.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 126\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.120810332563188\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012061861135919847\n",
      "          policy_loss: -0.06035431012925174\n",
      "          total_loss: -0.07678661919716331\n",
      "          vf_explained_var: -0.7408331036567688\n",
      "          vf_loss: 0.0011572327495539664\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.58947368421055\n",
      "    ram_util_percent: 80.38947368421056\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04368584370296674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.679507842882096\n",
      "    mean_inference_ms: 1.554600979264575\n",
      "    mean_raw_obs_processing_ms: 1.5760882668293967\n",
      "  time_since_restore: 1218.9405279159546\n",
      "  time_this_iter_s: 26.602034091949463\n",
      "  time_total_s: 1218.9405279159546\n",
      "  timers:\n",
      "    learn_throughput: 1637.108\n",
      "    learn_time_ms: 610.833\n",
      "    load_throughput: 53366.992\n",
      "    load_time_ms: 18.738\n",
      "    sample_throughput: 37.126\n",
      "    sample_time_ms: 26935.152\n",
      "    update_time_ms: 2.035\n",
      "  timestamp: 1633525396\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 42\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         1218.94</td><td style=\"text-align: right;\">42000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            332.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 43000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_13-03-44\n",
      "  done: false\n",
      "  episode_len_mean: 332.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 129\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9153467734654746\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01085485517591908\n",
      "          policy_loss: -0.020947319641709327\n",
      "          total_loss: -0.03575647593372398\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0010878545487584131\n",
      "    num_agent_steps_sampled: 43000\n",
      "    num_agent_steps_trained: 43000\n",
      "    num_steps_sampled: 43000\n",
      "    num_steps_trained: 43000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.53\n",
      "    ram_util_percent: 80.1325\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043617502452218716\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.520114419696057\n",
      "    mean_inference_ms: 1.5525370963078826\n",
      "    mean_raw_obs_processing_ms: 1.5715686787730812\n",
      "  time_since_restore: 1247.0353834629059\n",
      "  time_this_iter_s: 28.094855546951294\n",
      "  time_total_s: 1247.0353834629059\n",
      "  timers:\n",
      "    learn_throughput: 1638.669\n",
      "    learn_time_ms: 610.251\n",
      "    load_throughput: 53776.575\n",
      "    load_time_ms: 18.595\n",
      "    sample_throughput: 36.487\n",
      "    sample_time_ms: 27407.044\n",
      "    update_time_ms: 2.028\n",
      "  timestamp: 1633525424\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 43000\n",
      "  training_iteration: 43\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         1247.04</td><td style=\"text-align: right;\">43000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            332.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_13-04-09\n",
      "  done: false\n",
      "  episode_len_mean: 334.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 132\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1626873705122205\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011524925953223509\n",
      "          policy_loss: -0.0027407256265481314\n",
      "          total_loss: -0.019972376939323213\n",
      "          vf_explained_var: -0.0691569447517395\n",
      "          vf_loss: 0.0009377434303233814\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.61351351351351\n",
      "    ram_util_percent: 80.0027027027027\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04355462770953799\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.36309330909322\n",
      "    mean_inference_ms: 1.5505880423638176\n",
      "    mean_raw_obs_processing_ms: 1.5690172790942916\n",
      "  time_since_restore: 1272.6147017478943\n",
      "  time_this_iter_s: 25.579318284988403\n",
      "  time_total_s: 1272.6147017478943\n",
      "  timers:\n",
      "    learn_throughput: 1637.056\n",
      "    learn_time_ms: 610.853\n",
      "    load_throughput: 52620.866\n",
      "    load_time_ms: 19.004\n",
      "    sample_throughput: 36.438\n",
      "    sample_time_ms: 27443.775\n",
      "    update_time_ms: 2.014\n",
      "  timestamp: 1633525449\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 44\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         1272.61</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             334.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 45000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_13-04-38\n",
      "  done: false\n",
      "  episode_len_mean: 334.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 135\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8971095893118117\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016809751043756982\n",
      "          policy_loss: -0.05067929575840632\n",
      "          total_loss: -0.06324128276771969\n",
      "          vf_explained_var: -0.9059434533119202\n",
      "          vf_loss: 0.001366183002311219\n",
      "    num_agent_steps_sampled: 45000\n",
      "    num_agent_steps_trained: 45000\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 45000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.84146341463415\n",
      "    ram_util_percent: 80.0219512195122\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043497935837327945\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.219578393102662\n",
      "    mean_inference_ms: 1.5488208193794395\n",
      "    mean_raw_obs_processing_ms: 1.569514606978984\n",
      "  time_since_restore: 1301.5930831432343\n",
      "  time_this_iter_s: 28.978381395339966\n",
      "  time_total_s: 1301.5930831432343\n",
      "  timers:\n",
      "    learn_throughput: 1637.502\n",
      "    learn_time_ms: 610.686\n",
      "    load_throughput: 52323.63\n",
      "    load_time_ms: 19.112\n",
      "    sample_throughput: 35.885\n",
      "    sample_time_ms: 27866.624\n",
      "    update_time_ms: 2.014\n",
      "  timestamp: 1633525478\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 45\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         1301.59</td><td style=\"text-align: right;\">45000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            334.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 46000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_13-05-06\n",
      "  done: false\n",
      "  episode_len_mean: 335.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 138\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1644340382681952\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009941847155828616\n",
      "          policy_loss: -0.013617851605845822\n",
      "          total_loss: -0.03154214207703868\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0007374961330141458\n",
      "    num_agent_steps_sampled: 46000\n",
      "    num_agent_steps_trained: 46000\n",
      "    num_steps_sampled: 46000\n",
      "    num_steps_trained: 46000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.35\n",
      "    ram_util_percent: 80.11499999999998\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043447108223902235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.09192044514557\n",
      "    mean_inference_ms: 1.5472313384143381\n",
      "    mean_raw_obs_processing_ms: 1.5724875835581897\n",
      "  time_since_restore: 1329.2873311042786\n",
      "  time_this_iter_s: 27.69424796104431\n",
      "  time_total_s: 1329.2873311042786\n",
      "  timers:\n",
      "    learn_throughput: 1587.679\n",
      "    learn_time_ms: 629.85\n",
      "    load_throughput: 53298.295\n",
      "    load_time_ms: 18.762\n",
      "    sample_throughput: 35.739\n",
      "    sample_time_ms: 27980.943\n",
      "    update_time_ms: 2.032\n",
      "  timestamp: 1633525506\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 46000\n",
      "  training_iteration: 46\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         1329.29</td><td style=\"text-align: right;\">46000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            335.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 47000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_13-05-44\n",
      "  done: false\n",
      "  episode_len_mean: 334.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 141\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.552020165655348\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012966486947421963\n",
      "          policy_loss: 0.0002511448330349392\n",
      "          total_loss: -0.005128479583395852\n",
      "          vf_explained_var: -0.5727739334106445\n",
      "          vf_loss: 0.0062506320524132915\n",
      "    num_agent_steps_sampled: 47000\n",
      "    num_agent_steps_trained: 47000\n",
      "    num_steps_sampled: 47000\n",
      "    num_steps_trained: 47000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.31886792452829\n",
      "    ram_util_percent: 80.13207547169812\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04340400627706957\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.983389745832174\n",
      "    mean_inference_ms: 1.5458755370291604\n",
      "    mean_raw_obs_processing_ms: 1.5774929747317765\n",
      "  time_since_restore: 1366.7013692855835\n",
      "  time_this_iter_s: 37.41403818130493\n",
      "  time_total_s: 1366.7013692855835\n",
      "  timers:\n",
      "    learn_throughput: 1581.647\n",
      "    learn_time_ms: 632.252\n",
      "    load_throughput: 53878.121\n",
      "    load_time_ms: 18.56\n",
      "    sample_throughput: 34.3\n",
      "    sample_time_ms: 29154.59\n",
      "    update_time_ms: 2.045\n",
      "  timestamp: 1633525544\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47000\n",
      "  training_iteration: 47\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">          1366.7</td><td style=\"text-align: right;\">47000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            334.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_13-06-15\n",
      "  done: false\n",
      "  episode_len_mean: 333.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 144\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0591684367921617\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010981980479081404\n",
      "          policy_loss: -0.04011547983520561\n",
      "          total_loss: -0.05345658043192492\n",
      "          vf_explained_var: -0.7357677221298218\n",
      "          vf_loss: 0.003955989455183347\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.83333333333333\n",
      "    ram_util_percent: 80.09555555555553\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04336571369945513\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.887761434280506\n",
      "    mean_inference_ms: 1.544655700553948\n",
      "    mean_raw_obs_processing_ms: 1.5841526411498246\n",
      "  time_since_restore: 1398.0822772979736\n",
      "  time_this_iter_s: 31.380908012390137\n",
      "  time_total_s: 1398.0822772979736\n",
      "  timers:\n",
      "    learn_throughput: 1579.218\n",
      "    learn_time_ms: 633.225\n",
      "    load_throughput: 54884.397\n",
      "    load_time_ms: 18.22\n",
      "    sample_throughput: 33.706\n",
      "    sample_time_ms: 29668.124\n",
      "    update_time_ms: 2.053\n",
      "  timestamp: 1633525575\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 48\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         1398.08</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            333.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 49000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_13-06-46\n",
      "  done: false\n",
      "  episode_len_mean: 333.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 147\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0325666533576117\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019823614067207944\n",
      "          policy_loss: -0.10895727475484213\n",
      "          total_loss: -0.12172380089759827\n",
      "          vf_explained_var: 0.08389554172754288\n",
      "          vf_loss: 0.0016120561896564645\n",
      "    num_agent_steps_sampled: 49000\n",
      "    num_agent_steps_trained: 49000\n",
      "    num_steps_sampled: 49000\n",
      "    num_steps_trained: 49000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.77555555555556\n",
      "    ram_util_percent: 80.1822222222222\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0433313062442594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.803710801393496\n",
      "    mean_inference_ms: 1.5435484382048434\n",
      "    mean_raw_obs_processing_ms: 1.5921811430386428\n",
      "  time_since_restore: 1429.3569271564484\n",
      "  time_this_iter_s: 31.27464985847473\n",
      "  time_total_s: 1429.3569271564484\n",
      "  timers:\n",
      "    learn_throughput: 1580.069\n",
      "    learn_time_ms: 632.884\n",
      "    load_throughput: 55280.733\n",
      "    load_time_ms: 18.089\n",
      "    sample_throughput: 33.303\n",
      "    sample_time_ms: 30027.741\n",
      "    update_time_ms: 2.034\n",
      "  timestamp: 1633525606\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49000\n",
      "  training_iteration: 49\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         1429.36</td><td style=\"text-align: right;\">49000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            333.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1133331)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1133331)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 50000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_13-07-29\n",
      "  done: false\n",
      "  episode_len_mean: 333.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 150\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.252225504981147\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016479285317874945\n",
      "          policy_loss: -0.1177473639862405\n",
      "          total_loss: -0.13404225996798938\n",
      "          vf_explained_var: 0.31422150135040283\n",
      "          vf_loss: 0.0012835713530269762\n",
      "    num_agent_steps_sampled: 50000\n",
      "    num_agent_steps_trained: 50000\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.54754098360657\n",
      "    ram_util_percent: 79.93114754098359\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04330061934470437\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.728002123644284\n",
      "    mean_inference_ms: 1.5425655402536387\n",
      "    mean_raw_obs_processing_ms: 1.6107455452212864\n",
      "  time_since_restore: 1472.3954787254333\n",
      "  time_this_iter_s: 43.038551568984985\n",
      "  time_total_s: 1472.3954787254333\n",
      "  timers:\n",
      "    learn_throughput: 1572.869\n",
      "    learn_time_ms: 635.781\n",
      "    load_throughput: 56073.883\n",
      "    load_time_ms: 17.834\n",
      "    sample_throughput: 33.176\n",
      "    sample_time_ms: 30141.826\n",
      "    update_time_ms: 2.022\n",
      "  timestamp: 1633525649\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 50\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">          1472.4</td><td style=\"text-align: right;\">50000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             333.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 51000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_13-07-58\n",
      "  done: false\n",
      "  episode_len_mean: 333.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 153\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.935144805908203\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014093096071284113\n",
      "          policy_loss: -0.09362847014433807\n",
      "          total_loss: -0.10631914840390284\n",
      "          vf_explained_var: 0.016667569056153297\n",
      "          vf_loss: 0.0024328428850923147\n",
      "    num_agent_steps_sampled: 51000\n",
      "    num_agent_steps_trained: 51000\n",
      "    num_steps_sampled: 51000\n",
      "    num_steps_trained: 51000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.16341463414635\n",
      "    ram_util_percent: 80.09024390243901\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043274042382081525\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.660663119689442\n",
      "    mean_inference_ms: 1.5416890274368003\n",
      "    mean_raw_obs_processing_ms: 1.6300292607959153\n",
      "  time_since_restore: 1501.2902114391327\n",
      "  time_this_iter_s: 28.89473271369934\n",
      "  time_total_s: 1501.2902114391327\n",
      "  timers:\n",
      "    learn_throughput: 1566.087\n",
      "    learn_time_ms: 638.534\n",
      "    load_throughput: 56281.931\n",
      "    load_time_ms: 17.768\n",
      "    sample_throughput: 33.077\n",
      "    sample_time_ms: 30232.508\n",
      "    update_time_ms: 2.053\n",
      "  timestamp: 1633525678\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 51000\n",
      "  training_iteration: 51\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         1501.29</td><td style=\"text-align: right;\">51000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            333.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_13-08-29\n",
      "  done: false\n",
      "  episode_len_mean: 335.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 156\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0375153753492565\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015136936425567448\n",
      "          policy_loss: -0.08249399620625708\n",
      "          total_loss: -0.0969814233481884\n",
      "          vf_explained_var: -0.2819005250930786\n",
      "          vf_loss: 0.0013466459711910122\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.56444444444445\n",
      "    ram_util_percent: 80.18444444444444\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04325001724393604\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.5997047288175\n",
      "    mean_inference_ms: 1.5408979601298691\n",
      "    mean_raw_obs_processing_ms: 1.649417632709445\n",
      "  time_since_restore: 1532.1715955734253\n",
      "  time_this_iter_s: 30.881384134292603\n",
      "  time_total_s: 1532.1715955734253\n",
      "  timers:\n",
      "    learn_throughput: 1554.929\n",
      "    learn_time_ms: 643.116\n",
      "    load_throughput: 55442.736\n",
      "    load_time_ms: 18.037\n",
      "    sample_throughput: 32.62\n",
      "    sample_time_ms: 30655.69\n",
      "    update_time_ms: 1.908\n",
      "  timestamp: 1633525709\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 52\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         1532.17</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             335.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 53000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_13-08-59\n",
      "  done: false\n",
      "  episode_len_mean: 335.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 159\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9296559797392951\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013098978658695692\n",
      "          policy_loss: -0.10042593197690117\n",
      "          total_loss: -0.11470966852373547\n",
      "          vf_explained_var: -0.8627484440803528\n",
      "          vf_loss: 0.001083129344947843\n",
      "    num_agent_steps_sampled: 53000\n",
      "    num_agent_steps_trained: 53000\n",
      "    num_steps_sampled: 53000\n",
      "    num_steps_trained: 53000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.29285714285714\n",
      "    ram_util_percent: 80.1047619047619\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043228188982127554\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.545185285526273\n",
      "    mean_inference_ms: 1.5401742854034555\n",
      "    mean_raw_obs_processing_ms: 1.6608363725013342\n",
      "  time_since_restore: 1561.8446474075317\n",
      "  time_this_iter_s: 29.673051834106445\n",
      "  time_total_s: 1561.8446474075317\n",
      "  timers:\n",
      "    learn_throughput: 1556.015\n",
      "    learn_time_ms: 642.667\n",
      "    load_throughput: 55314.415\n",
      "    load_time_ms: 18.078\n",
      "    sample_throughput: 32.453\n",
      "    sample_time_ms: 30813.793\n",
      "    update_time_ms: 1.921\n",
      "  timestamp: 1633525739\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 53000\n",
      "  training_iteration: 53\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         1561.84</td><td style=\"text-align: right;\">53000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            335.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 54000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_13-09-30\n",
      "  done: false\n",
      "  episode_len_mean: 336.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 162\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9911303149329291\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012616515569734648\n",
      "          policy_loss: -0.08666903773943584\n",
      "          total_loss: -0.1017064266734653\n",
      "          vf_explained_var: -0.6622142195701599\n",
      "          vf_loss: 0.0010889576592793067\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_steps_sampled: 54000\n",
      "    num_steps_trained: 54000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.27272727272727\n",
      "    ram_util_percent: 80.1\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043208385719260736\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.496060057028107\n",
      "    mean_inference_ms: 1.539505135993986\n",
      "    mean_raw_obs_processing_ms: 1.6560669089692674\n",
      "  time_since_restore: 1592.7682526111603\n",
      "  time_this_iter_s: 30.92360520362854\n",
      "  time_total_s: 1592.7682526111603\n",
      "  timers:\n",
      "    learn_throughput: 1556.383\n",
      "    learn_time_ms: 642.515\n",
      "    load_throughput: 56828.074\n",
      "    load_time_ms: 17.597\n",
      "    sample_throughput: 31.899\n",
      "    sample_time_ms: 31348.85\n",
      "    update_time_ms: 1.932\n",
      "  timestamp: 1633525770\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 54000\n",
      "  training_iteration: 54\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         1592.77</td><td style=\"text-align: right;\">54000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            336.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_e6792_00000:\n",
      "  agent_timesteps_total: 55000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-06_13-10-04\n",
      "  done: false\n",
      "  episode_len_mean: 336.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 165\n",
      "  experiment_id: 9621e3adcf6148feac28dfc5a7380f79\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.009744980600145\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01235479926881623\n",
      "          policy_loss: -0.021569707265330685\n",
      "          total_loss: -0.037243089597258305\n",
      "          vf_explained_var: -0.0017711579566821456\n",
      "          vf_loss: 0.0007176288096363552\n",
      "    num_agent_steps_sampled: 55000\n",
      "    num_agent_steps_trained: 55000\n",
      "    num_steps_sampled: 55000\n",
      "    num_steps_trained: 55000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.02448979591838\n",
      "    ram_util_percent: 80.26530612244899\n",
      "  pid: 1133330\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04319079024706987\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.455694387389304\n",
      "    mean_inference_ms: 1.5388912559640031\n",
      "    mean_raw_obs_processing_ms: 1.6528280035375362\n",
      "  time_since_restore: 1627.1834659576416\n",
      "  time_this_iter_s: 34.41521334648132\n",
      "  time_total_s: 1627.1834659576416\n",
      "  timers:\n",
      "    learn_throughput: 1557.819\n",
      "    learn_time_ms: 641.923\n",
      "    load_throughput: 57315.913\n",
      "    load_time_ms: 17.447\n",
      "    sample_throughput: 31.355\n",
      "    sample_time_ms: 31893.259\n",
      "    update_time_ms: 1.935\n",
      "  timestamp: 1633525804\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55000\n",
      "  training_iteration: 55\n",
      "  trial_id: e6792_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.23 GiB heap, 0.0/3.12 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-06_12-42-43<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_e6792_00000</td><td>RUNNING </td><td>192.168.1.100:1133330</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         1627.18</td><td style=\"text-align: right;\">55000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            336.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "analysis = tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 1,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 1000,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO C8 pretrained (AnnaCNN) (3 noops after placement)\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
