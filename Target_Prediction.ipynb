{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_oFrItdFzONE"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/iglu-contest/iglu\n",
    "!apt-get -qq install openjdk-8-jdk xvfb > /dev/null\n",
    "!update-alternatives --config java\n",
    "!sudo add-apt-repository ppa:openjdk-r/ppa\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install openjdk-8-jdk\n",
    "!sudo apt-get install xvfb\n",
    "!pip uninstall -y iglu && pip install git+https://github.com/iglu-contest/iglu.git\n",
    "!pip install gym==0.18.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pbf9D1rKzSja"
   },
   "outputs": [],
   "source": [
    "# exec this cell ONLY in colab\n",
    "!wget -q https://raw.githubusercontent.com/iglu-contest/tutorials/main/env/colab_setup.sh -O - | sh > /dev/null 2>&1\n",
    "!pip install -q pyvirtualdisplay\n",
    "from pyvirtualdisplay import Display\n",
    "disp = Display(backend=\"xvnc\", size=(800, 600))\n",
    "disp.start();\n",
    "# for local notebooks instead launch jupyter as: xvfb-run -s \"-screen 0 640x480x24\" jupyter ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_PRDOvpZzTMZ"
   },
   "outputs": [],
   "source": [
    "import iglu\n",
    "import gym\n",
    "from iglu.tasks import RandomTasks\n",
    "from iglu.tasks.task_set import TaskSet\n",
    "\n",
    "#env = gym.make('IGLUSilentBuilder-v0')\n",
    "#obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AkgcJ4hazc-C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.1.0.tar.gz (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers>=0.10.3\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 103.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /root/miniconda/envs/py37/lib/python3.7/site-packages/tqdm-4.61.2-py3.7.egg (from sentence-transformers) (4.61.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in /root/miniconda/envs/py37/lib/python3.7/site-packages (from sentence-transformers) (1.9.0)\n",
      "Requirement already satisfied: torchvision in /root/miniconda/envs/py37/lib/python3.7/site-packages (from sentence-transformers) (0.10.0)\n",
      "Requirement already satisfied: numpy in /root/miniconda/envs/py37/lib/python3.7/site-packages (from sentence-transformers) (1.20.3)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.2 MB 8.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /root/miniconda/envs/py37/lib/python3.7/site-packages (from sentence-transformers) (1.6.3)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.6.5-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 11.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 8.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /root/miniconda/envs/py37/lib/python3.7/site-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda/envs/py37/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.0)\n",
      "Requirement already satisfied: importlib-metadata in /root/miniconda/envs/py37/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda/envs/py37/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in /root/miniconda/envs/py37/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /root/miniconda/envs/py37/lib/python3.7/site-packages/requests-2.25.1-py3.7.egg (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.25.1)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 15.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2021.10.23-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (748 kB)\n",
      "\u001b[K     |████████████████████████████████| 748 kB 14.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /root/miniconda/envs/py37/lib/python3.7/site-packages/pyparsing-3.0.0b2-py3.7.egg (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.0b2)\n",
      "Requirement already satisfied: zipp>=0.5 in /root/miniconda/envs/py37/lib/python3.7/site-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.5.0)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 16.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /root/miniconda/envs/py37/lib/python3.7/site-packages (from nltk->sentence-transformers) (8.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda/envs/py37/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /root/miniconda/envs/py37/lib/python3.7/site-packages/chardet-4.0.0-py3.7.egg (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /root/miniconda/envs/py37/lib/python3.7/site-packages/idna-2.10-py3.7.egg (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/miniconda/envs/py37/lib/python3.7/site-packages/urllib3-1.26.6-py3.7.egg (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.26.6)\n",
      "Requirement already satisfied: six in /root/miniconda/envs/py37/lib/python3.7/site-packages/six-1.16.0-py3.7.egg (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.16.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /root/miniconda/envs/py37/lib/python3.7/site-packages (from torchvision->sentence-transformers) (8.2.0)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.1.0-py3-none-any.whl size=120999 sha256=22b21909baa76973e2b13497cfabc99a968569c404256faca747da30b8651aa4\n",
      "  Stored in directory: /root/.cache/pip/wheels/90/f0/bb/ed1add84da70092ea526466eadc2bfb197c4bcb8d4fa5f7bad\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: regex, joblib, tokenizers, threadpoolctl, sacremoses, huggingface-hub, transformers, sentencepiece, scikit-learn, nltk, sentence-transformers\n",
      "Successfully installed huggingface-hub-0.0.19 joblib-1.1.0 nltk-3.6.5 regex-2021.10.23 sacremoses-0.0.46 scikit-learn-1.0.1 sentence-transformers-2.1.0 sentencepiece-0.1.96 threadpoolctl-3.0.0 tokenizers-0.10.3 transformers-4.11.3\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81fdc1c129448a9b72c0687d8786b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/737 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1c8dbb882d4253b18b6342c49ca208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/9.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6946ea70fb5441eb9201b070eafd2140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/653 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be057fe25dc94a939e81b23086e912ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e4929fc9684814a034d382a126113a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/15.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8401498356da47d58baf9f6afdfacbe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd93432eb4c47ea93d63675d65f88a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298f50b0548b44308a07f77ad8596bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd8b50354be44c8a34086d41b8be853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507c70dedcdc4c5fb21311207e5f109e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78fb0e5c4947427d9e75bba002d42dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04895273075c4c97954c62ce928ec2d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/333 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f0e33c380144a6b53a1dfa9fb3be26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e96e85a047b3414c9befba80c1a6f8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ef054fc9934e4b97040abf82a36de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install sentence-transformers\n",
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "nltk.download('punkt')\n",
    "bert_sentence = SentenceTransformer('all-distilroberta-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_N_Tc76_04kF"
   },
   "outputs": [],
   "source": [
    "targets = []\n",
    "chats = []\n",
    "for i in range(1,156):\n",
    "  if ('C'+str(i)) == 'C38': continue\n",
    "  targets.append(TaskSet(preset=['C'+str(i)]).sample().target_grid)\n",
    "  chats.append(bert_sentence.encode(TaskSet(preset=['C'+str(i)]).sample().chat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "f9YOXVDf08Uo"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "class TargetDataset(Dataset):\n",
    "    def __init__(self, target_list, chat_list):\n",
    "        self.target_list = target_list\n",
    "        self.chat_list = chat_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target = self.target_list[idx]\n",
    "        target_tensor_target = torch.tensor(target, dtype=torch.long)\n",
    "        # target_tensor_input = one_hot(torch.tensor(target, dtype=torch.long), num_classes=7).permute(3, 0, 1, 2)\n",
    "        chat_tensor = self.chat_list[idx]\n",
    "        return chat_tensor, target_tensor_target\n",
    "    \n",
    "training_dataset = TargetDataset(targets, chats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nUe8QZwQ0_VE"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "DdB40qKZ1Bau"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "class TargetDecoder(nn.Module):\n",
    "    def __init__(self, features_dim=768):\n",
    "        super(TargetDecoder, self).__init__()\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(features_dim, 15680))\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.ConvTranspose3d(64, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(), \n",
    "            nn.ConvTranspose3d(32, 7, kernel_size=3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.reshape(x.shape[0], 64, 5, 7, 7)\n",
    "        x = self.cnn(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "target_decoder = TargetDecoder().to(device)\n",
    "optimizer = optim.Adam(target_decoder.parameters(), lr=1e-3)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "TJaPP7f21jKW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | loss: 0.043094551749527456\n",
      "epoch: 1 | loss: 0.04088730327785015\n",
      "epoch: 2 | loss: 0.04016119558364153\n",
      "epoch: 3 | loss: 0.039527260884642604\n",
      "epoch: 4 | loss: 0.03710999926552176\n",
      "epoch: 5 | loss: 0.033643096871674064\n",
      "epoch: 6 | loss: 0.029962214455008507\n",
      "epoch: 7 | loss: 0.02785949329845607\n",
      "epoch: 8 | loss: 0.025902612414211035\n",
      "epoch: 9 | loss: 0.024425185285508633\n",
      "epoch: 10 | loss: 0.025513942074030637\n",
      "epoch: 11 | loss: 0.02567363395355642\n",
      "epoch: 12 | loss: 0.023767703352496027\n",
      "epoch: 13 | loss: 0.02086513042449951\n",
      "epoch: 14 | loss: 0.01964312852360308\n",
      "epoch: 15 | loss: 0.01705207610502839\n",
      "epoch: 16 | loss: 0.015394671354442835\n",
      "epoch: 17 | loss: 0.01437988739926368\n",
      "epoch: 18 | loss: 0.01568834511563182\n",
      "epoch: 19 | loss: 0.013960746768862008\n",
      "epoch: 20 | loss: 0.012519727065227925\n",
      "epoch: 21 | loss: 0.012677266029641032\n",
      "epoch: 22 | loss: 0.012790787499397993\n",
      "epoch: 23 | loss: 0.011986225680448114\n",
      "epoch: 24 | loss: 0.010217207274399698\n",
      "epoch: 25 | loss: 0.008862713351845741\n",
      "epoch: 26 | loss: 0.0072957263677380976\n",
      "epoch: 27 | loss: 0.007420053787063808\n",
      "epoch: 28 | loss: 0.0067903554416261615\n",
      "epoch: 29 | loss: 0.0052040325594134625\n",
      "epoch: 30 | loss: 0.004733279650099576\n",
      "epoch: 31 | loss: 0.004471632052445784\n",
      "epoch: 32 | loss: 0.004119728435762226\n",
      "epoch: 33 | loss: 0.003975754871498793\n",
      "epoch: 34 | loss: 0.003273611731128767\n",
      "epoch: 35 | loss: 0.0029225410922663285\n",
      "epoch: 36 | loss: 0.0024598328920546917\n",
      "epoch: 37 | loss: 0.0022643493284704164\n",
      "epoch: 38 | loss: 0.0021443899255245923\n",
      "epoch: 39 | loss: 0.003344721984467469\n",
      "epoch: 40 | loss: 0.003518660977715626\n",
      "epoch: 41 | loss: 0.0036776501568965615\n",
      "epoch: 42 | loss: 0.0032401436124928295\n",
      "epoch: 43 | loss: 0.0032204170187469573\n",
      "epoch: 44 | loss: 0.006436914613004774\n",
      "epoch: 45 | loss: 0.010843773535452784\n",
      "epoch: 46 | loss: 0.015677489433437587\n",
      "epoch: 47 | loss: 0.015412009693682194\n",
      "epoch: 48 | loss: 0.01403320711106062\n",
      "epoch: 49 | loss: 0.010037586931139231\n",
      "epoch: 50 | loss: 0.00959493984701112\n",
      "epoch: 51 | loss: 0.0074542033253237605\n",
      "epoch: 52 | loss: 0.0067704114830121395\n",
      "epoch: 53 | loss: 0.005407348414883018\n",
      "epoch: 54 | loss: 0.0035411845194175838\n",
      "epoch: 55 | loss: 0.0020992304642277306\n",
      "epoch: 56 | loss: 0.0014621674548834563\n",
      "epoch: 57 | loss: 0.0010757275958894752\n",
      "epoch: 58 | loss: 0.0009423455456271767\n",
      "epoch: 59 | loss: 0.0008590476665631286\n",
      "epoch: 60 | loss: 0.0008026085321034771\n",
      "epoch: 61 | loss: 0.00077121540234657\n",
      "epoch: 62 | loss: 0.0007608415871800389\n",
      "epoch: 63 | loss: 0.000690299755660817\n",
      "epoch: 64 | loss: 0.0006595284590730444\n",
      "epoch: 65 | loss: 0.0006294786908256356\n",
      "epoch: 66 | loss: 0.0006035238548065536\n",
      "epoch: 67 | loss: 0.0005807476059999317\n",
      "epoch: 68 | loss: 0.0005562145772273653\n",
      "epoch: 69 | loss: 0.0005810880076751346\n",
      "epoch: 70 | loss: 0.0005127256317791762\n",
      "epoch: 71 | loss: 0.0004968771881976863\n",
      "epoch: 72 | loss: 0.0004717942629213212\n",
      "epoch: 73 | loss: 0.0004598643994540907\n",
      "epoch: 74 | loss: 0.0004401347421662649\n",
      "epoch: 75 | loss: 0.00042793525935849177\n",
      "epoch: 76 | loss: 0.0004361943436379079\n",
      "epoch: 77 | loss: 0.0003980918101660791\n",
      "epoch: 78 | loss: 0.0003808416258834768\n",
      "epoch: 79 | loss: 0.000369123172822583\n",
      "epoch: 80 | loss: 0.0003584740466976655\n",
      "epoch: 81 | loss: 0.0003470944067885284\n",
      "epoch: 82 | loss: 0.00033646028641669544\n",
      "epoch: 83 | loss: 0.0003316656959214015\n",
      "epoch: 84 | loss: 0.0003195636467353324\n",
      "epoch: 85 | loss: 0.00030312144590425303\n",
      "epoch: 86 | loss: 0.0002940217202194617\n",
      "epoch: 87 | loss: 0.00034422086537233554\n",
      "epoch: 88 | loss: 0.00027771802724601\n",
      "epoch: 89 | loss: 0.00026624639986039256\n",
      "epoch: 90 | loss: 0.0002795418569803587\n",
      "epoch: 91 | loss: 0.0002518710451113293\n",
      "epoch: 92 | loss: 0.00024835540043568474\n",
      "epoch: 93 | loss: 0.0003392858323422843\n",
      "epoch: 94 | loss: 0.00023111513282856322\n",
      "epoch: 95 | loss: 0.00022258903863985323\n",
      "epoch: 96 | loss: 0.0002139740485290531\n",
      "epoch: 97 | loss: 0.0002080092609503481\n",
      "epoch: 98 | loss: 0.00021644930566253607\n",
      "epoch: 99 | loss: 0.00019656019376270705\n",
      "epoch: 100 | loss: 0.00019523754326655762\n",
      "epoch: 101 | loss: 0.0001860526386735728\n",
      "epoch: 102 | loss: 0.000310000571062119\n",
      "epoch: 103 | loss: 0.0001738206353820715\n",
      "epoch: 104 | loss: 0.00017817979933170135\n",
      "epoch: 105 | loss: 0.00016431932854175103\n",
      "epoch: 106 | loss: 0.0001596388499820023\n",
      "epoch: 107 | loss: 0.0001548790953165735\n",
      "epoch: 108 | loss: 0.0001495436735694966\n",
      "epoch: 109 | loss: 0.00014581412096958957\n",
      "epoch: 110 | loss: 0.0001612283486792876\n",
      "epoch: 111 | loss: 0.00013837017031619324\n",
      "epoch: 112 | loss: 0.00013473778444677008\n",
      "epoch: 113 | loss: 0.0001301778016568278\n",
      "epoch: 114 | loss: 0.00012699745557256392\n",
      "epoch: 115 | loss: 0.00012340097655396675\n",
      "epoch: 116 | loss: 0.00011987357729594805\n",
      "epoch: 117 | loss: 0.00011663370601127099\n",
      "epoch: 118 | loss: 0.00011418342701290385\n",
      "epoch: 119 | loss: 0.00011165943114974653\n",
      "epoch: 120 | loss: 0.000135664444314898\n",
      "epoch: 121 | loss: 0.00010539299546508119\n",
      "epoch: 122 | loss: 0.00010389910489720933\n",
      "epoch: 123 | loss: 0.00011812754846687312\n",
      "epoch: 124 | loss: 9.594955758984725e-05\n",
      "epoch: 125 | loss: 9.253692469428643e-05\n",
      "epoch: 126 | loss: 8.987187447928591e-05\n",
      "epoch: 127 | loss: 8.78299696069007e-05\n",
      "epoch: 128 | loss: 8.498572974531271e-05\n",
      "epoch: 129 | loss: 8.594017735958914e-05\n",
      "epoch: 130 | loss: 8.171890399353288e-05\n",
      "epoch: 131 | loss: 8.010204542188148e-05\n",
      "epoch: 132 | loss: 7.68795830936142e-05\n",
      "epoch: 133 | loss: 7.427636696775153e-05\n",
      "epoch: 134 | loss: 7.20232621915784e-05\n",
      "epoch: 135 | loss: 7.147498054109747e-05\n",
      "epoch: 136 | loss: 6.851616153653594e-05\n",
      "epoch: 137 | loss: 6.687803802378767e-05\n",
      "epoch: 138 | loss: 8.5114826060817e-05\n",
      "epoch: 139 | loss: 6.448920275943237e-05\n",
      "epoch: 140 | loss: 6.200943660132907e-05\n",
      "epoch: 141 | loss: 6.311577762971866e-05\n",
      "epoch: 142 | loss: 5.885138639314391e-05\n",
      "epoch: 143 | loss: 5.747046611759288e-05\n",
      "epoch: 144 | loss: 5.565468418353703e-05\n",
      "epoch: 145 | loss: 5.456451572172227e-05\n",
      "epoch: 146 | loss: 5.3476464927371126e-05\n",
      "epoch: 147 | loss: 5.18326924975554e-05\n",
      "epoch: 148 | loss: 5.018903825657617e-05\n",
      "epoch: 149 | loss: 5.2222652220734746e-05\n",
      "epoch: 150 | loss: 4.7891076951600554e-05\n",
      "epoch: 151 | loss: 4.7537471664327315e-05\n",
      "epoch: 152 | loss: 8.016451038201921e-05\n",
      "epoch: 153 | loss: 4.4234602330561754e-05\n",
      "epoch: 154 | loss: 4.3394751628511585e-05\n",
      "epoch: 155 | loss: 4.2468808806006565e-05\n",
      "epoch: 156 | loss: 4.128981977373769e-05\n",
      "epoch: 157 | loss: 4.243247824433638e-05\n",
      "epoch: 158 | loss: 3.894705082529981e-05\n",
      "epoch: 159 | loss: 3.777414840442361e-05\n",
      "epoch: 160 | loss: 3.761827963444375e-05\n",
      "epoch: 161 | loss: 4.0035184565567763e-05\n",
      "epoch: 162 | loss: 3.513121469040925e-05\n",
      "epoch: 163 | loss: 3.5867832434632876e-05\n",
      "epoch: 164 | loss: 3.380744790320023e-05\n",
      "epoch: 165 | loss: 3.2443196846543285e-05\n",
      "epoch: 166 | loss: 3.196007583028404e-05\n",
      "epoch: 167 | loss: 3.4604254983605644e-05\n",
      "epoch: 168 | loss: 3.123799640434299e-05\n",
      "epoch: 169 | loss: 2.9709951490985986e-05\n",
      "epoch: 170 | loss: 2.935504285233037e-05\n",
      "epoch: 171 | loss: 3.225954026220279e-05\n",
      "epoch: 172 | loss: 2.8081599384677248e-05\n",
      "epoch: 173 | loss: 2.7550467189030313e-05\n",
      "epoch: 174 | loss: 2.646599352829071e-05\n",
      "epoch: 175 | loss: 2.617753093545616e-05\n",
      "epoch: 176 | loss: 2.539943103556652e-05\n",
      "epoch: 177 | loss: 2.50081804324509e-05\n",
      "epoch: 178 | loss: 2.564442829680047e-05\n",
      "epoch: 179 | loss: 2.3782741698141764e-05\n",
      "epoch: 180 | loss: 2.3382661379400815e-05\n",
      "epoch: 181 | loss: 2.3515111297456315e-05\n",
      "epoch: 182 | loss: 2.235161390444773e-05\n",
      "epoch: 183 | loss: 2.1978948961987043e-05\n",
      "epoch: 184 | loss: 2.146205657709288e-05\n",
      "epoch: 185 | loss: 2.123283774153606e-05\n",
      "epoch: 186 | loss: 2.13557346341986e-05\n",
      "epoch: 187 | loss: 2.029056857963951e-05\n",
      "epoch: 188 | loss: 1.9833275791825146e-05\n",
      "epoch: 189 | loss: 2.0063488864252577e-05\n",
      "epoch: 190 | loss: 1.9111688936845893e-05\n",
      "epoch: 191 | loss: 1.8913121380137454e-05\n",
      "epoch: 192 | loss: 1.828994841162057e-05\n",
      "epoch: 193 | loss: 1.8320836920793228e-05\n",
      "epoch: 194 | loss: 1.7624531244564424e-05\n",
      "epoch: 195 | loss: 1.8133429409772362e-05\n",
      "epoch: 196 | loss: 1.735361586270301e-05\n",
      "epoch: 197 | loss: 1.6692689291630815e-05\n",
      "epoch: 198 | loss: 1.8629500584665947e-05\n",
      "epoch: 199 | loss: 1.5976892512981067e-05\n",
      "epoch: 200 | loss: 1.5836632621812895e-05\n",
      "epoch: 201 | loss: 1.866676980171178e-05\n",
      "epoch: 202 | loss: 1.5060348971474014e-05\n",
      "epoch: 203 | loss: 1.4735122397269151e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 204 | loss: 1.4505299202483001e-05\n",
      "epoch: 205 | loss: 1.4353677249800966e-05\n",
      "epoch: 206 | loss: 1.4191325772117124e-05\n",
      "epoch: 207 | loss: 1.4032018430043536e-05\n",
      "epoch: 208 | loss: 1.3759782655142772e-05\n",
      "epoch: 209 | loss: 1.3266514406495844e-05\n",
      "epoch: 210 | loss: 1.3077803487249184e-05\n",
      "epoch: 211 | loss: 1.2755791067320387e-05\n",
      "epoch: 212 | loss: 1.2759171011111902e-05\n",
      "epoch: 213 | loss: 1.311027438077872e-05\n",
      "epoch: 214 | loss: 1.2023996930565772e-05\n",
      "epoch: 215 | loss: 1.1779747654827589e-05\n",
      "epoch: 216 | loss: 1.1685820845741546e-05\n",
      "epoch: 217 | loss: 1.1456596098469163e-05\n",
      "epoch: 218 | loss: 1.2457481693672889e-05\n",
      "epoch: 219 | loss: 1.1070737537011155e-05\n",
      "epoch: 220 | loss: 1.0835468930281423e-05\n",
      "epoch: 221 | loss: 1.0635637158884492e-05\n",
      "epoch: 222 | loss: 1.0442969804103086e-05\n",
      "epoch: 223 | loss: 1.2521979476787237e-05\n",
      "epoch: 224 | loss: 1.0212398990461224e-05\n",
      "epoch: 225 | loss: 1.103255696079941e-05\n",
      "epoch: 226 | loss: 9.768943107246741e-06\n",
      "epoch: 227 | loss: 9.735521894072008e-06\n",
      "epoch: 228 | loss: 9.5406214313698e-06\n",
      "epoch: 229 | loss: 9.251936876353285e-06\n",
      "epoch: 230 | loss: 9.36761414322973e-06\n",
      "epoch: 231 | loss: 9.277493734316523e-06\n",
      "epoch: 232 | loss: 8.928169040700596e-06\n",
      "epoch: 233 | loss: 8.7071685356932e-06\n",
      "epoch: 234 | loss: 8.558871593322693e-06\n",
      "epoch: 235 | loss: 8.621914247441964e-06\n",
      "epoch: 236 | loss: 8.409782617491146e-06\n",
      "epoch: 237 | loss: 8.275779987343412e-06\n",
      "epoch: 238 | loss: 8.87973110366147e-06\n",
      "epoch: 239 | loss: 7.879776796926308e-06\n",
      "epoch: 240 | loss: 7.704988630052866e-06\n",
      "epoch: 241 | loss: 7.747073232167167e-06\n",
      "epoch: 242 | loss: 7.70442833868401e-06\n",
      "epoch: 243 | loss: 7.573567313556851e-06\n",
      "epoch: 244 | loss: 7.259621406774386e-06\n",
      "epoch: 245 | loss: 7.264353195068907e-06\n",
      "epoch: 246 | loss: 7.0728765763306e-06\n",
      "epoch: 247 | loss: 6.994202908572333e-06\n",
      "epoch: 248 | loss: 1.0651530510585872e-05\n",
      "epoch: 249 | loss: 7.452536715391034e-06\n",
      "epoch: 250 | loss: 6.576563811222513e-06\n",
      "epoch: 251 | loss: 6.509703348456242e-06\n",
      "epoch: 252 | loss: 6.303867570522925e-06\n",
      "epoch: 253 | loss: 6.3350741640988416e-06\n",
      "epoch: 254 | loss: 6.108480391731064e-06\n",
      "epoch: 255 | loss: 6.144413629272094e-06\n",
      "epoch: 256 | loss: 6.108134095939022e-06\n",
      "epoch: 257 | loss: 5.892058896961316e-06\n",
      "epoch: 258 | loss: 5.84751218184465e-06\n",
      "epoch: 259 | loss: 5.7304348672460035e-06\n",
      "epoch: 260 | loss: 5.630018347346777e-06\n",
      "epoch: 261 | loss: 5.603092972705781e-06\n",
      "epoch: 262 | loss: 5.422912690278281e-06\n",
      "epoch: 263 | loss: 5.592018220568207e-06\n",
      "epoch: 264 | loss: 5.309471015380041e-06\n",
      "epoch: 265 | loss: 5.383986746210212e-06\n",
      "epoch: 266 | loss: 5.105575544916973e-06\n",
      "epoch: 267 | loss: 5.276886065530562e-06\n",
      "epoch: 268 | loss: 5.009732996086314e-06\n",
      "epoch: 269 | loss: 4.877832441252394e-06\n",
      "epoch: 270 | loss: 5.7873583671153025e-06\n",
      "epoch: 271 | loss: 4.821686371769829e-06\n",
      "epoch: 272 | loss: 4.881298588088612e-06\n",
      "epoch: 273 | loss: 4.603396189395426e-06\n",
      "epoch: 274 | loss: 4.521110611221957e-06\n",
      "epoch: 275 | loss: 4.506556138039741e-06\n",
      "epoch: 276 | loss: 4.942201741187091e-06\n",
      "epoch: 277 | loss: 4.386906095987797e-06\n",
      "epoch: 278 | loss: 4.27609705013765e-06\n",
      "epoch: 279 | loss: 4.238558096858469e-06\n",
      "epoch: 280 | loss: 4.169543899479322e-06\n",
      "epoch: 281 | loss: 4.178569139412503e-06\n",
      "epoch: 282 | loss: 4.420173775088188e-06\n",
      "epoch: 283 | loss: 3.97050738456528e-06\n",
      "epoch: 284 | loss: 4.020053432896021e-06\n",
      "epoch: 285 | loss: 3.968947663679501e-06\n",
      "epoch: 286 | loss: 3.965543311323927e-06\n",
      "epoch: 287 | loss: 3.908089280457716e-06\n",
      "epoch: 288 | loss: 3.7764374269499966e-06\n",
      "epoch: 289 | loss: 3.660744823719142e-06\n",
      "epoch: 290 | loss: 5.415598272406896e-06\n",
      "epoch: 291 | loss: 3.6234488277386844e-06\n",
      "epoch: 292 | loss: 3.5857458499322093e-06\n",
      "epoch: 293 | loss: 3.4867313559061587e-06\n",
      "epoch: 294 | loss: 3.3736190488298236e-06\n",
      "epoch: 295 | loss: 3.905255624658821e-06\n",
      "epoch: 296 | loss: 3.5879767779078975e-06\n",
      "epoch: 297 | loss: 3.248638128638959e-06\n",
      "epoch: 298 | loss: 3.2281189589866697e-06\n",
      "epoch: 299 | loss: 3.1478921044936213e-06\n",
      "epoch: 300 | loss: 3.148637651406716e-06\n",
      "epoch: 301 | loss: 3.136801694836322e-06\n",
      "epoch: 302 | loss: 3.0111546067246307e-06\n",
      "epoch: 303 | loss: 3.016689237256287e-06\n",
      "epoch: 304 | loss: 2.930823595193033e-06\n",
      "epoch: 305 | loss: 2.9783792626858486e-06\n",
      "epoch: 306 | loss: 2.871244450375343e-06\n",
      "epoch: 307 | loss: 2.938953602438232e-06\n",
      "epoch: 308 | loss: 2.7878326022801046e-06\n",
      "epoch: 309 | loss: 2.898019499752991e-06\n",
      "epoch: 310 | loss: 2.8118452888747926e-06\n",
      "epoch: 311 | loss: 2.6797442529868933e-06\n",
      "epoch: 312 | loss: 2.6212749574483497e-06\n",
      "epoch: 313 | loss: 2.670434639640007e-06\n",
      "epoch: 314 | loss: 2.6045643011229913e-06\n",
      "epoch: 315 | loss: 2.6003044354183657e-06\n",
      "epoch: 316 | loss: 2.8177920938787793e-06\n",
      "epoch: 317 | loss: 2.4900024385487995e-06\n",
      "epoch: 318 | loss: 2.421188548851205e-06\n",
      "epoch: 319 | loss: 2.4749076942498505e-06\n",
      "epoch: 320 | loss: 2.4387161516870037e-06\n",
      "epoch: 321 | loss: 2.3685438691245508e-06\n",
      "epoch: 322 | loss: 2.2887741565114083e-06\n",
      "epoch: 323 | loss: 2.3701349164184648e-06\n",
      "epoch: 324 | loss: 2.2201890821804683e-06\n",
      "epoch: 325 | loss: 2.3624945413303067e-06\n",
      "epoch: 326 | loss: 2.164792155667783e-06\n",
      "epoch: 327 | loss: 2.1757562649327156e-06\n",
      "epoch: 328 | loss: 2.106190144957054e-06\n",
      "epoch: 329 | loss: 2.128405017742807e-06\n",
      "epoch: 330 | loss: 2.0744439154896098e-06\n",
      "epoch: 331 | loss: 2.055906486475578e-06\n",
      "epoch: 332 | loss: 2.011361533504896e-06\n",
      "epoch: 333 | loss: 1.983953117701276e-06\n",
      "epoch: 334 | loss: 1.983050762532912e-06\n",
      "epoch: 335 | loss: 1.943741069965199e-06\n",
      "epoch: 336 | loss: 1.917782410032487e-06\n",
      "epoch: 337 | loss: 1.9178728194901852e-06\n",
      "epoch: 338 | loss: 1.8513888925042465e-06\n",
      "epoch: 339 | loss: 1.8487355248453241e-06\n",
      "epoch: 340 | loss: 1.9662454860736033e-06\n",
      "epoch: 341 | loss: 1.7850842809252753e-06\n",
      "epoch: 342 | loss: 1.7988182918315943e-06\n",
      "epoch: 343 | loss: 1.790118832900589e-06\n",
      "epoch: 344 | loss: 1.7164367662303448e-06\n",
      "epoch: 345 | loss: 1.694130789076098e-06\n",
      "epoch: 346 | loss: 1.684807634205754e-06\n",
      "epoch: 347 | loss: 2.4147613572722547e-06\n",
      "epoch: 348 | loss: 1.678291843631996e-06\n",
      "epoch: 349 | loss: 1.6318508329504766e-06\n",
      "epoch: 350 | loss: 1.709102022573461e-06\n",
      "epoch: 351 | loss: 1.6223560564299077e-06\n",
      "epoch: 352 | loss: 1.5602310838858102e-06\n",
      "epoch: 353 | loss: 1.5380444608581457e-06\n",
      "epoch: 354 | loss: 1.5209994558063044e-06\n",
      "epoch: 355 | loss: 1.608800447172598e-06\n",
      "epoch: 356 | loss: 1.4655233599114582e-06\n",
      "epoch: 357 | loss: 1.4590429202598898e-06\n",
      "epoch: 358 | loss: 1.53340207873498e-06\n",
      "epoch: 359 | loss: 1.519879639033661e-06\n",
      "epoch: 360 | loss: 1.4750093157545053e-06\n",
      "epoch: 361 | loss: 1.3803870558604102e-06\n",
      "epoch: 362 | loss: 1.3584842264435792e-06\n",
      "epoch: 363 | loss: 1.3513191078118324e-06\n",
      "epoch: 364 | loss: 1.3530212086720894e-06\n",
      "epoch: 365 | loss: 1.3092713558648938e-06\n",
      "epoch: 366 | loss: 1.3685819368447483e-06\n",
      "epoch: 367 | loss: 1.3269299920182221e-06\n",
      "epoch: 368 | loss: 1.2552700553669639e-06\n",
      "epoch: 369 | loss: 1.24721899794622e-06\n",
      "epoch: 370 | loss: 1.2572371360874968e-06\n",
      "epoch: 371 | loss: 1.2223701006064403e-06\n",
      "epoch: 372 | loss: 1.2048955710497467e-06\n",
      "epoch: 373 | loss: 1.1729546208982811e-06\n",
      "epoch: 374 | loss: 1.1643216751622275e-06\n",
      "epoch: 375 | loss: 1.188034441668151e-06\n",
      "epoch: 376 | loss: 1.1679507991857463e-06\n",
      "epoch: 377 | loss: 1.2044732869753715e-06\n",
      "epoch: 378 | loss: 1.12564980128127e-06\n",
      "epoch: 379 | loss: 1.0959662859022501e-06\n",
      "epoch: 380 | loss: 1.1043427079471257e-06\n",
      "epoch: 381 | loss: 1.089786444197216e-06\n",
      "epoch: 382 | loss: 1.0606128569179418e-06\n",
      "epoch: 383 | loss: 1.0477424154942128e-06\n",
      "epoch: 384 | loss: 1.0343883403152176e-06\n",
      "epoch: 385 | loss: 1.0242431500273597e-06\n",
      "epoch: 386 | loss: 1.0508003398967957e-06\n",
      "epoch: 387 | loss: 1.0105569387519609e-06\n",
      "epoch: 388 | loss: 1.0709836466560319e-06\n",
      "epoch: 389 | loss: 9.76805694108407e-07\n",
      "epoch: 390 | loss: 1.0186874504825027e-06\n",
      "epoch: 391 | loss: 9.536278199107074e-07\n",
      "epoch: 392 | loss: 9.363590407929223e-07\n",
      "epoch: 393 | loss: 1.0114740931044253e-06\n",
      "epoch: 394 | loss: 9.309171090876589e-07\n",
      "epoch: 395 | loss: 9.336626874301146e-07\n",
      "epoch: 396 | loss: 8.855759276116259e-07\n",
      "epoch: 397 | loss: 8.697886674013944e-07\n",
      "epoch: 398 | loss: 8.69483007903682e-07\n",
      "epoch: 399 | loss: 8.58135439329999e-07\n",
      "epoch: 400 | loss: 8.539626264791878e-07\n",
      "epoch: 401 | loss: 8.312903442941888e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 402 | loss: 8.343290630818956e-07\n",
      "epoch: 403 | loss: 8.346944071035978e-07\n",
      "epoch: 404 | loss: 8.752803978495649e-07\n",
      "epoch: 405 | loss: 8.059441498176057e-07\n",
      "epoch: 406 | loss: 8.075945778784899e-07\n",
      "epoch: 407 | loss: 7.875361120568414e-07\n",
      "epoch: 408 | loss: 7.607723450320236e-07\n",
      "epoch: 409 | loss: 7.684085673531626e-07\n",
      "epoch: 410 | loss: 7.534090876504252e-07\n",
      "epoch: 411 | loss: 7.522830316020191e-07\n",
      "epoch: 412 | loss: 7.359233890724682e-07\n",
      "epoch: 413 | loss: 7.29234344021279e-07\n",
      "epoch: 414 | loss: 7.229541743924983e-07\n",
      "epoch: 415 | loss: 7.057143534439092e-07\n",
      "epoch: 416 | loss: 7.147055740119868e-07\n",
      "epoch: 417 | loss: 7.152756325012888e-07\n",
      "epoch: 418 | loss: 7.159894096275821e-07\n",
      "epoch: 419 | loss: 7.193107393277387e-07\n",
      "epoch: 420 | loss: 6.718433830599224e-07\n",
      "epoch: 421 | loss: 6.551442687907639e-07\n",
      "epoch: 422 | loss: 6.63968984326857e-07\n",
      "epoch: 423 | loss: 6.871180261214249e-07\n",
      "epoch: 424 | loss: 6.275549733203434e-07\n",
      "epoch: 425 | loss: 6.383589749248131e-07\n",
      "epoch: 426 | loss: 6.169405210698642e-07\n",
      "epoch: 427 | loss: 6.070546895386997e-07\n",
      "epoch: 428 | loss: 6.068067307296588e-07\n",
      "epoch: 429 | loss: 5.936975004061651e-07\n",
      "epoch: 430 | loss: 5.971106105562285e-07\n",
      "epoch: 431 | loss: 6.07463613988557e-07\n",
      "epoch: 432 | loss: 5.789278624490634e-07\n",
      "epoch: 433 | loss: 5.7349749624791e-07\n",
      "epoch: 434 | loss: 5.920497578415507e-07\n",
      "epoch: 435 | loss: 5.61937060439277e-07\n",
      "epoch: 436 | loss: 6.293253264288978e-07\n",
      "epoch: 437 | loss: 5.426646694672854e-07\n",
      "epoch: 438 | loss: 5.429141978652296e-07\n",
      "epoch: 439 | loss: 5.255646962964988e-07\n",
      "epoch: 440 | loss: 5.202897042977384e-07\n",
      "epoch: 441 | loss: 5.157581195902594e-07\n",
      "epoch: 442 | loss: 5.07377771619133e-07\n",
      "epoch: 443 | loss: 5.018783257426663e-07\n",
      "epoch: 444 | loss: 4.958688954559421e-07\n",
      "epoch: 445 | loss: 5.276836510859084e-07\n",
      "epoch: 446 | loss: 4.904768125868486e-07\n",
      "epoch: 447 | loss: 4.886437928064424e-07\n",
      "epoch: 448 | loss: 5.239140193680214e-07\n",
      "epoch: 449 | loss: 4.79871007996735e-07\n",
      "epoch: 450 | loss: 4.764133393564407e-07\n",
      "epoch: 451 | loss: 4.59477864467317e-07\n",
      "epoch: 452 | loss: 4.5011431524244474e-07\n",
      "epoch: 453 | loss: 4.855377483181656e-07\n",
      "epoch: 454 | loss: 4.5081224868681603e-07\n",
      "epoch: 455 | loss: 4.695408122756817e-07\n",
      "epoch: 456 | loss: 4.357898482965084e-07\n",
      "epoch: 457 | loss: 4.319296280641538e-07\n",
      "epoch: 458 | loss: 4.245937056168714e-07\n",
      "epoch: 459 | loss: 4.230272104166488e-07\n",
      "epoch: 460 | loss: 4.351457228324307e-07\n",
      "epoch: 461 | loss: 4.091348245083282e-07\n",
      "epoch: 462 | loss: 4.0506860941036395e-07\n",
      "epoch: 463 | loss: 4.6668922237813604e-07\n",
      "epoch: 464 | loss: 4.2948981544554953e-07\n",
      "epoch: 465 | loss: 3.912381615123195e-07\n",
      "epoch: 466 | loss: 3.8399499295849183e-07\n",
      "epoch: 467 | loss: 3.78872728745705e-07\n",
      "epoch: 468 | loss: 3.875624685178991e-07\n",
      "epoch: 469 | loss: 3.7040600879834075e-07\n",
      "epoch: 470 | loss: 3.8378246429715547e-07\n",
      "epoch: 471 | loss: 3.6697051513101543e-07\n",
      "epoch: 472 | loss: 3.6464740276187514e-07\n",
      "epoch: 473 | loss: 3.806430445507658e-07\n",
      "epoch: 474 | loss: 3.733624808432978e-07\n",
      "epoch: 475 | loss: 3.457871393663936e-07\n",
      "epoch: 476 | loss: 3.41686967075816e-07\n",
      "epoch: 477 | loss: 3.476880213071354e-07\n",
      "epoch: 478 | loss: 3.606720845539257e-07\n",
      "epoch: 479 | loss: 3.298396144657545e-07\n",
      "epoch: 480 | loss: 3.3776702892396313e-07\n",
      "epoch: 481 | loss: 3.243235408234568e-07\n",
      "epoch: 482 | loss: 3.703346393990614e-07\n",
      "epoch: 483 | loss: 3.2575292649994483e-07\n",
      "epoch: 484 | loss: 3.1721614526247775e-07\n",
      "epoch: 485 | loss: 3.308143700309074e-07\n",
      "epoch: 486 | loss: 3.0625151623553395e-07\n",
      "epoch: 487 | loss: 3.495720243051892e-07\n",
      "epoch: 488 | loss: 2.973451257659576e-07\n",
      "epoch: 489 | loss: 3.007346911942932e-07\n",
      "epoch: 490 | loss: 2.928972186566625e-07\n",
      "epoch: 491 | loss: 2.9018320866214255e-07\n",
      "epoch: 492 | loss: 2.9459704578016497e-07\n",
      "epoch: 493 | loss: 3.056576666438104e-07\n",
      "epoch: 494 | loss: 2.782003960533075e-07\n",
      "epoch: 495 | loss: 2.7697458548914256e-07\n",
      "epoch: 496 | loss: 2.7076765292122217e-07\n",
      "epoch: 497 | loss: 2.772867567557569e-07\n",
      "epoch: 498 | loss: 3.7225122824224853e-07\n",
      "epoch: 499 | loss: 2.84431298069876e-07\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 500\n",
    "for epoch in range(EPOCHS):\n",
    "  target_decoder.train()\n",
    "  train_loss = []\n",
    "  for target_tensor_input, target_tensor_target in train_dataloader:\n",
    "    target_tensor_input = target_tensor_input.float().to(device)\n",
    "    target_tensor_target = target_tensor_target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    predict = target_decoder(target_tensor_input)\n",
    "    loss = loss_function(predict, target_tensor_target)\n",
    "    train_loss.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  train_loss = np.array(train_loss).mean()\n",
    "  print(f\"epoch: {epoch} | loss: {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Target Prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
