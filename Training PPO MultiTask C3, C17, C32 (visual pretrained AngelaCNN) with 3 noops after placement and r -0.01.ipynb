{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "#from models import VisualEncoder\n",
    "from train import *\n",
    "from wrappers_2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9fd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=2, stride=2, padding=0),  \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(), \n",
    "            nn.Conv2d(128, 256, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import one_hot\n",
    "\n",
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        visual_features_dim = 512\n",
    "        target_features_dim = 9 * 11 * 11 \n",
    "        self.visual_encoder = VisualEncoder()\n",
    "        self.visual_encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AngelaCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.target_encoder = nn.Sequential(\n",
    "            nn.Conv3d(7, 1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        policy_hidden_dim = 256 \n",
    "        self.policy_network = nn.Sequential(\n",
    "            nn.Linear(visual_features_dim + target_features_dim, 1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, policy_hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "            nn.ELU(),\n",
    "            #nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "            #nn.ELU(),\n",
    "        )\n",
    "        self.action_head = nn.Linear(policy_hidden_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(policy_hidden_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.visual_encoder.cuda()\n",
    "            self.target_encoder.cuda()\n",
    "            self.policy_network.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs']\n",
    "        pov = obs['pov'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        target = one_hot(obs['target_grid'].long(), num_classes=7).permute(0, 4, 1, 2, 3).float()\n",
    "        if self.use_cuda:\n",
    "            pov.cuda()\n",
    "            target.cuda()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            visual_features = self.visual_encoder(pov)\n",
    "            \n",
    "        target_features = self.target_encoder(target)\n",
    "        target_features = target_features.reshape(target_features.shape[0], -1)\n",
    "        features = torch.cat([visual_features, target_features], dim=1)\n",
    "        features = self.policy_network(features)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc09c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualObservationWrapper(ObsWrapper):\n",
    "    def __init__(self, env, include_target=False):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = {   \n",
    "            'pov': gym.spaces.Box(low=0, high=255, shape=(64, 64, 3)),\n",
    "            'inventory': gym.spaces.Box(low=0.0, high=20.0, shape=(6,)),\n",
    "            'compass': gym.spaces.Box(low=-180.0, high=180.0, shape=(1,))\n",
    "        }\n",
    "        if include_target:\n",
    "            self.observation_space['target_grid'] = \\\n",
    "                gym.spaces.Box(low=0, high=6, shape=(9, 11, 11))\n",
    "        self.observation_space = gym.spaces.Dict(self.observation_space)\n",
    "\n",
    "    def observation(self, obs, reward=None, done=None, info=None):\n",
    "        if info is not None:\n",
    "            if 'target_grid' in info:\n",
    "                target_grid = info['target_grid']\n",
    "                del info['target_grid']\n",
    "            else:\n",
    "                logger.error(f'info: {info}')\n",
    "                if hasattr(self.unwrapped, 'should_reset'):\n",
    "                    self.unwrapped.should_reset(True)\n",
    "                target_grid = self.env.unwrapped.tasks.current.target_grid\n",
    "        else:\n",
    "            target_grid = self.env.unwrapped.tasks.current.target_grid\n",
    "        return {\n",
    "            'pov': obs['pov'].astype(np.float32),\n",
    "            'inventory': obs['inventory'],\n",
    "            'compass': np.array([obs['compass']['angle'].item()]),\n",
    "            'target_grid': target_grid\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "class RewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def reward(self, rew):\n",
    "        if rew == 0:\n",
    "            rew = -0.01\n",
    "        return rew\n",
    "    \n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "    env.update_taskset(TaskSet(preset=['C3', 'C17', 'C32']))\n",
    "    #env = PovOnlyWrapper(env)\n",
    "    env = VisualObservationWrapper(env, include_target=True)\n",
    "    env = SelectAndPlace(env)\n",
    "    env = Discretization(env, flat_action_space('human-level'))\n",
    "    env = RewardWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 21:09:08,341\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-10-26 21:09:08,426\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=206)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=206)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO MultiTask (C3, C17, C32) pretrained (AngelaCNN) (3 noops after placement) r: -0.01</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/f560f_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/f560f_00000</a><br/>\n",
       "                Run data is saved locally in <code>/IGLU-Minecraft/wandb/run-20211026_210908-f560f_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=206)\u001b[0m 2021-10-26 21:09:11,737\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=206)\u001b[0m 2021-10-26 21:09:11,737\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=206)\u001b[0m 2021-10-26 21:09:31,767\tINFO trainable.py:109 -- Trainable.setup took 22.430 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=206)\u001b[0m 2021-10-26 21:09:31,768\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-11-12\n",
      "  done: false\n",
      "  episode_len_mean: 410.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4.079999999999957\n",
      "  episode_reward_mean: -4.104999999999957\n",
      "  episode_reward_min: -4.129999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8817786587609184\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006693930526574989\n",
      "          policy_loss: -0.20149057110150656\n",
      "          total_loss: -0.22687244100703133\n",
      "          vf_explained_var: 0.011597350239753723\n",
      "          vf_loss: 0.0020971302007738914\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.071724137931035\n",
      "    ram_util_percent: 17.554482758620686\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03913423040887336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 96.59368579799717\n",
      "    mean_inference_ms: 3.2386241497455184\n",
      "    mean_raw_obs_processing_ms: 0.1976127986545925\n",
      "  time_since_restore: 101.04580044746399\n",
      "  time_this_iter_s: 101.04580044746399\n",
      "  time_total_s: 101.04580044746399\n",
      "  timers:\n",
      "    learn_throughput: 1273.771\n",
      "    learn_time_ms: 785.07\n",
      "    load_throughput: 32670.753\n",
      "    load_time_ms: 30.608\n",
      "    sample_throughput: 9.978\n",
      "    sample_time_ms: 100221.124\n",
      "    update_time_ms: 3.699\n",
      "  timestamp: 1635282672\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         101.046</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">  -4.105</td><td style=\"text-align: right;\">               -4.08</td><td style=\"text-align: right;\">               -4.13</td><td style=\"text-align: right;\">             410.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-11-34\n",
      "  done: false\n",
      "  episode_len_mean: 409.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4.079999999999957\n",
      "  episode_reward_mean: -7.7274999999999565\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 4\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8707015249464245\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009151069443873519\n",
      "          policy_loss: 0.03347935097085105\n",
      "          total_loss: 0.2642831400036812\n",
      "          vf_explained_var: 0.33199697732925415\n",
      "          vf_loss: 0.2576805866219931\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.36129032258065\n",
      "    ram_util_percent: 20.258064516129032\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03902503808702005\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 77.06727942946688\n",
      "    mean_inference_ms: 2.991628505406424\n",
      "    mean_raw_obs_processing_ms: 0.20535192102674316\n",
      "  time_since_restore: 123.04720664024353\n",
      "  time_this_iter_s: 22.00140619277954\n",
      "  time_total_s: 123.04720664024353\n",
      "  timers:\n",
      "    learn_throughput: 1194.574\n",
      "    learn_time_ms: 837.118\n",
      "    load_throughput: 29744.305\n",
      "    load_time_ms: 33.62\n",
      "    sample_throughput: 16.495\n",
      "    sample_time_ms: 60622.925\n",
      "    update_time_ms: 21.54\n",
      "  timestamp: 1635282694\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         123.047</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\"> -7.7275</td><td style=\"text-align: right;\">               -4.08</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">             409.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-12-01\n",
      "  done: false\n",
      "  episode_len_mean: 409.57142857142856\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4.019999999999959\n",
      "  episode_reward_mean: -6.171428571428528\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 7\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8689043336444433\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010045820838391043\n",
      "          policy_loss: -0.018307800342639288\n",
      "          total_loss: -0.03426090627908707\n",
      "          vf_explained_var: 0.12687383592128754\n",
      "          vf_loss: 0.010726768181969722\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.2\n",
      "    ram_util_percent: 22.95\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04160075878044752\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 63.70020909153318\n",
      "    mean_inference_ms: 2.8232298202697734\n",
      "    mean_raw_obs_processing_ms: 0.21248124170669835\n",
      "  time_since_restore: 149.55124282836914\n",
      "  time_this_iter_s: 26.50403618812561\n",
      "  time_total_s: 149.55124282836914\n",
      "  timers:\n",
      "    learn_throughput: 1210.413\n",
      "    learn_time_ms: 826.164\n",
      "    load_throughput: 33862.536\n",
      "    load_time_ms: 29.531\n",
      "    sample_throughput: 20.484\n",
      "    sample_time_ms: 48819.345\n",
      "    update_time_ms: 15.56\n",
      "  timestamp: 1635282721\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         149.551</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">-6.17143</td><td style=\"text-align: right;\">               -4.02</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           409.571</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-12-29\n",
      "  done: false\n",
      "  episode_len_mean: 407.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4.009999999999959\n",
      "  episode_reward_mean: -5.952222222222179\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 9\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8358463340335422\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009348850852260782\n",
      "          policy_loss: -0.043690372361905046\n",
      "          total_loss: 0.2846730943562256\n",
      "          vf_explained_var: 0.050481654703617096\n",
      "          vf_loss: 0.3548521632121669\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.040000000000006\n",
      "    ram_util_percent: 23.919999999999995\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041892173805220284\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 58.56233269951508\n",
      "    mean_inference_ms: 2.746196167607516\n",
      "    mean_raw_obs_processing_ms: 0.21352800978829076\n",
      "  time_since_restore: 177.41390705108643\n",
      "  time_this_iter_s: 27.862664222717285\n",
      "  time_total_s: 177.41390705108643\n",
      "  timers:\n",
      "    learn_throughput: 1234.698\n",
      "    learn_time_ms: 809.915\n",
      "    load_throughput: 35300.612\n",
      "    load_time_ms: 28.328\n",
      "    sample_throughput: 23.051\n",
      "    sample_time_ms: 43381.827\n",
      "    update_time_ms: 12.565\n",
      "  timestamp: 1635282749\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         177.414</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-5.95222</td><td style=\"text-align: right;\">               -4.01</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">               407</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-12-52\n",
      "  done: false\n",
      "  episode_len_mean: 407.9166666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.9099999999999606\n",
      "  episode_reward_mean: -5.490833333333291\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 12\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.843253983391656\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007908187637346378\n",
      "          policy_loss: 0.04381274075971709\n",
      "          total_loss: 0.024656585769520865\n",
      "          vf_explained_var: 0.1311345249414444\n",
      "          vf_loss: 0.0076947473920881745\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 5000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.0090909090909\n",
      "    ram_util_percent: 23.927272727272726\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04184428188061671\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 53.037898668769884\n",
      "    mean_inference_ms: 2.66191091834277\n",
      "    mean_raw_obs_processing_ms: 0.214244510665303\n",
      "  time_since_restore: 200.63255906105042\n",
      "  time_this_iter_s: 23.21865200996399\n",
      "  time_total_s: 200.63255906105042\n",
      "  timers:\n",
      "    learn_throughput: 1250.501\n",
      "    learn_time_ms: 799.68\n",
      "    load_throughput: 36413.314\n",
      "    load_time_ms: 27.462\n",
      "    sample_throughput: 25.516\n",
      "    sample_time_ms: 39191.168\n",
      "    update_time_ms: 10.744\n",
      "  timestamp: 1635282772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         200.633</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">-5.49083</td><td style=\"text-align: right;\">               -3.91</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           407.917</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-13-12\n",
      "  done: false\n",
      "  episode_len_mean: 405.92857142857144\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.9099999999999606\n",
      "  episode_reward_mean: -5.2692857142856715\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 14\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8125938918855455\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010553975512047604\n",
      "          policy_loss: -0.07968706952200996\n",
      "          total_loss: -0.09060552856988377\n",
      "          vf_explained_var: 0.39417898654937744\n",
      "          vf_loss: 0.015096682307517362\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.51034482758621\n",
      "    ram_util_percent: 24.058620689655175\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041697228574463056\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 50.20637115017815\n",
      "    mean_inference_ms: 2.6185804626616287\n",
      "    mean_raw_obs_processing_ms: 0.21372546197654357\n",
      "  time_since_restore: 220.76013040542603\n",
      "  time_this_iter_s: 20.12757134437561\n",
      "  time_total_s: 220.76013040542603\n",
      "  timers:\n",
      "    learn_throughput: 1262.165\n",
      "    learn_time_ms: 792.29\n",
      "    load_throughput: 36908.532\n",
      "    load_time_ms: 27.094\n",
      "    sample_throughput: 27.869\n",
      "    sample_time_ms: 35882.565\n",
      "    update_time_ms: 9.542\n",
      "  timestamp: 1635282792\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">          220.76</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">-5.26929</td><td style=\"text-align: right;\">               -3.91</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           405.929</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 7000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-13-37\n",
      "  done: false\n",
      "  episode_len_mean: 409.4117647058824\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.9099999999999606\n",
      "  episode_reward_mean: -5.090588235294075\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 17\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7999437544080945\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009619589861244246\n",
      "          policy_loss: 0.04422048297193315\n",
      "          total_loss: 0.02630374473002222\n",
      "          vf_explained_var: 0.528251588344574\n",
      "          vf_loss: 0.008158782916143536\n",
      "    num_agent_steps_sampled: 7000\n",
      "    num_agent_steps_trained: 7000\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 7000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.94411764705882\n",
      "    ram_util_percent: 24.041176470588237\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041441775363526064\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 46.90423886220915\n",
      "    mean_inference_ms: 2.566957650807975\n",
      "    mean_raw_obs_processing_ms: 0.2132907546752518\n",
      "  time_since_restore: 245.13494086265564\n",
      "  time_this_iter_s: 24.374810457229614\n",
      "  time_total_s: 245.13494086265564\n",
      "  timers:\n",
      "    learn_throughput: 1268.98\n",
      "    learn_time_ms: 788.034\n",
      "    load_throughput: 37353.757\n",
      "    load_time_ms: 26.771\n",
      "    sample_throughput: 29.304\n",
      "    sample_time_ms: 34124.997\n",
      "    update_time_ms: 8.71\n",
      "  timestamp: 1635282817\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         245.135</td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\">-5.09059</td><td style=\"text-align: right;\">               -3.91</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           409.412</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-14-00\n",
      "  done: false\n",
      "  episode_len_mean: 410.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.9099999999999606\n",
      "  episode_reward_mean: -4.991578947368378\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 19\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.808088692029317\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010383728035554768\n",
      "          policy_loss: 0.04414480510685179\n",
      "          total_loss: 0.023917792903052435\n",
      "          vf_explained_var: 0.5834179520606995\n",
      "          vf_loss: 0.0057771309563476175\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.77878787878787\n",
      "    ram_util_percent: 24.05757575757576\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04126968359194405\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 45.08539645223615\n",
      "    mean_inference_ms: 2.538866602150457\n",
      "    mean_raw_obs_processing_ms: 0.21271020669689078\n",
      "  time_since_restore: 268.14372634887695\n",
      "  time_this_iter_s: 23.008785486221313\n",
      "  time_total_s: 268.14372634887695\n",
      "  timers:\n",
      "    learn_throughput: 1273.67\n",
      "    learn_time_ms: 785.133\n",
      "    load_throughput: 37927.855\n",
      "    load_time_ms: 26.366\n",
      "    sample_throughput: 31.033\n",
      "    sample_time_ms: 32223.933\n",
      "    update_time_ms: 8.049\n",
      "  timestamp: 1635282840\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         268.144</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-4.99158</td><td style=\"text-align: right;\">               -3.91</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">               410</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-14-25\n",
      "  done: false\n",
      "  episode_len_mean: 407.3636363636364\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.789999999999963\n",
      "  episode_reward_mean: -4.843636363636321\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 22\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7849916166729396\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00995967397750062\n",
      "          policy_loss: -0.01054296592871348\n",
      "          total_loss: -0.02931419387459755\n",
      "          vf_explained_var: 0.29957467317581177\n",
      "          vf_loss: 0.007086752699170675\n",
      "    num_agent_steps_sampled: 9000\n",
      "    num_agent_steps_trained: 9000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.63055555555556\n",
      "    ram_util_percent: 24.43333333333333\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04101840211613443\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 42.86072283438135\n",
      "    mean_inference_ms: 2.5035542322830424\n",
      "    mean_raw_obs_processing_ms: 0.21208883551400184\n",
      "  time_since_restore: 293.32792019844055\n",
      "  time_this_iter_s: 25.1841938495636\n",
      "  time_total_s: 293.32792019844055\n",
      "  timers:\n",
      "    learn_throughput: 1268.505\n",
      "    learn_time_ms: 788.33\n",
      "    load_throughput: 40097.231\n",
      "    load_time_ms: 24.939\n",
      "    sample_throughput: 31.899\n",
      "    sample_time_ms: 31348.865\n",
      "    update_time_ms: 7.599\n",
      "  timestamp: 1635282865\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         293.328</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">-4.84364</td><td style=\"text-align: right;\">               -3.79</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           407.364</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-14-46\n",
      "  done: false\n",
      "  episode_len_mean: 405.4166666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.7399999999999642\n",
      "  episode_reward_mean: -4.759999999999958\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 24\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7531207111146716\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012296335182541421\n",
      "          policy_loss: 0.03669446926150057\n",
      "          total_loss: 0.01581298170818223\n",
      "          vf_explained_var: 0.8574175238609314\n",
      "          vf_loss: 0.004190452605123735\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.06129032258065\n",
      "    ram_util_percent: 29.022580645161295\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04086238363711699\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 41.59755488317913\n",
      "    mean_inference_ms: 2.483671429637432\n",
      "    mean_raw_obs_processing_ms: 0.2115541219207732\n",
      "  time_since_restore: 314.613783121109\n",
      "  time_this_iter_s: 21.285862922668457\n",
      "  time_total_s: 314.613783121109\n",
      "  timers:\n",
      "    learn_throughput: 1269.066\n",
      "    learn_time_ms: 787.981\n",
      "    load_throughput: 41276.141\n",
      "    load_time_ms: 24.227\n",
      "    sample_throughput: 33.045\n",
      "    sample_time_ms: 30261.472\n",
      "    update_time_ms: 7.249\n",
      "  timestamp: 1635282886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         314.614</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">   -4.76</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           405.417</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 11000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-15-07\n",
      "  done: false\n",
      "  episode_len_mean: 405.037037037037\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.7399999999999642\n",
      "  episode_reward_mean: -4.677777777777736\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 27\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.732754847738478\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01045083620668485\n",
      "          policy_loss: -0.08798544775280688\n",
      "          total_loss: -0.10930989169412189\n",
      "          vf_explained_var: 0.6850016713142395\n",
      "          vf_loss: 0.003912937245331705\n",
      "    num_agent_steps_sampled: 11000\n",
      "    num_agent_steps_trained: 11000\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 11000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.02413793103447\n",
      "    ram_util_percent: 29.365517241379305\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040658813773865626\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 39.94906756168613\n",
      "    mean_inference_ms: 2.458197710212865\n",
      "    mean_raw_obs_processing_ms: 0.21104741623091638\n",
      "  time_since_restore: 335.1979515552521\n",
      "  time_this_iter_s: 20.584168434143066\n",
      "  time_total_s: 335.1979515552521\n",
      "  timers:\n",
      "    learn_throughput: 1274.268\n",
      "    learn_time_ms: 784.764\n",
      "    load_throughput: 42536.121\n",
      "    load_time_ms: 23.509\n",
      "    sample_throughput: 45.006\n",
      "    sample_time_ms: 22219.403\n",
      "    update_time_ms: 7.23\n",
      "  timestamp: 1635282907\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 11\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         335.198</td><td style=\"text-align: right;\">11000</td><td style=\"text-align: right;\">-4.67778</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           405.037</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-15-28\n",
      "  done: false\n",
      "  episode_len_mean: 403.0689655172414\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.6899999999999653\n",
      "  episode_reward_mean: -4.6148275862068555\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 29\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6927959256702\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01113482702274195\n",
      "          policy_loss: -0.022642872027224963\n",
      "          total_loss: -0.04421991888019774\n",
      "          vf_explained_var: 0.8580195903778076\n",
      "          vf_loss: 0.003123946528648958\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.10645161290323\n",
      "    ram_util_percent: 29.345161290322572\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04053031051271363\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.99219204110062\n",
      "    mean_inference_ms: 2.4432481540210063\n",
      "    mean_raw_obs_processing_ms: 0.21071120764798965\n",
      "  time_since_restore: 356.8931918144226\n",
      "  time_this_iter_s: 21.695240259170532\n",
      "  time_total_s: 356.8931918144226\n",
      "  timers:\n",
      "    learn_throughput: 1293.47\n",
      "    learn_time_ms: 773.114\n",
      "    load_throughput: 45030.802\n",
      "    load_time_ms: 22.207\n",
      "    sample_throughput: 45.033\n",
      "    sample_time_ms: 22205.937\n",
      "    update_time_ms: 3.784\n",
      "  timestamp: 1635282928\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         356.893</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-4.61483</td><td style=\"text-align: right;\">               -3.69</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           403.069</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 13000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-16-08\n",
      "  done: false\n",
      "  episode_len_mean: 400.53125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.534687499999959\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 32\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6194247828589545\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01353420948382252\n",
      "          policy_loss: 0.05509546262522538\n",
      "          total_loss: 0.03491605791366763\n",
      "          vf_explained_var: 0.8602737188339233\n",
      "          vf_loss: 0.0033080038582233504\n",
      "    num_agent_steps_sampled: 13000\n",
      "    num_agent_steps_trained: 13000\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 13000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.00701754385966\n",
      "    ram_util_percent: 29.37368421052631\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04035503172958571\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.72657138627014\n",
      "    mean_inference_ms: 2.4235311097464143\n",
      "    mean_raw_obs_processing_ms: 0.3406965199195098\n",
      "  time_since_restore: 396.69198298454285\n",
      "  time_this_iter_s: 39.79879117012024\n",
      "  time_total_s: 396.69198298454285\n",
      "  timers:\n",
      "    learn_throughput: 1300.613\n",
      "    learn_time_ms: 768.868\n",
      "    load_throughput: 44489.626\n",
      "    load_time_ms: 22.477\n",
      "    sample_throughput: 42.399\n",
      "    sample_time_ms: 23585.257\n",
      "    update_time_ms: 3.782\n",
      "  timestamp: 1635282968\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 13\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         396.692</td><td style=\"text-align: right;\">13000</td><td style=\"text-align: right;\">-4.53469</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           400.531</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 14000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-16-29\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.498235294117606\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 34\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.607422473695543\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010976618506545242\n",
      "          policy_loss: -0.12882483957542312\n",
      "          total_loss: -0.1504777034951581\n",
      "          vf_explained_var: 0.9385809898376465\n",
      "          vf_loss: 0.0022260394568244615\n",
      "    num_agent_steps_sampled: 14000\n",
      "    num_agent_steps_trained: 14000\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.72068965517241\n",
      "    ram_util_percent: 29.879310344827587\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04024684745464091\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 36.97352432182555\n",
      "    mean_inference_ms: 2.4118278261666983\n",
      "    mean_raw_obs_processing_ms: 0.4086701801500722\n",
      "  time_since_restore: 417.41986083984375\n",
      "  time_this_iter_s: 20.727877855300903\n",
      "  time_total_s: 417.41986083984375\n",
      "  timers:\n",
      "    learn_throughput: 1297.79\n",
      "    learn_time_ms: 770.541\n",
      "    load_throughput: 45003.938\n",
      "    load_time_ms: 22.22\n",
      "    sample_throughput: 43.725\n",
      "    sample_time_ms: 22870.35\n",
      "    update_time_ms: 3.776\n",
      "  timestamp: 1635282989\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 14\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">          417.42</td><td style=\"text-align: right;\">14000</td><td style=\"text-align: right;\">-4.49824</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">               400</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-16-49\n",
      "  done: false\n",
      "  episode_len_mean: 401.1621621621622\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.469459459459418\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 37\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6100649409823946\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0071737179211148255\n",
      "          policy_loss: 0.06902458874715699\n",
      "          total_loss: 0.047182806871003576\n",
      "          vf_explained_var: 0.92244553565979\n",
      "          vf_loss: 0.002824123889634696\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_agent_steps_trained: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.96896551724137\n",
      "    ram_util_percent: 29.77241379310345\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04009483480627376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.95428971633632\n",
      "    mean_inference_ms: 2.395931031459726\n",
      "    mean_raw_obs_processing_ms: 0.48998700287855124\n",
      "  time_since_restore: 437.68640089035034\n",
      "  time_this_iter_s: 20.266540050506592\n",
      "  time_total_s: 437.68640089035034\n",
      "  timers:\n",
      "    learn_throughput: 1300.165\n",
      "    learn_time_ms: 769.133\n",
      "    load_throughput: 44936.965\n",
      "    load_time_ms: 22.253\n",
      "    sample_throughput: 44.294\n",
      "    sample_time_ms: 22576.506\n",
      "    update_time_ms: 3.777\n",
      "  timestamp: 1635283009\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         437.686</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">-4.46946</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           401.162</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-17-09\n",
      "  done: false\n",
      "  episode_len_mean: 402.35897435897436\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.456410256410215\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 39\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.588376930024889\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011768352402437178\n",
      "          policy_loss: -0.004901071306731966\n",
      "          total_loss: -0.02441776411400901\n",
      "          vf_explained_var: 0.6484857201576233\n",
      "          vf_loss: 0.004013406161943243\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.91379310344827\n",
      "    ram_util_percent: 29.817241379310342\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0400026925591692\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.33824775983812\n",
      "    mean_inference_ms: 2.3864825320368577\n",
      "    mean_raw_obs_processing_ms: 0.5333621198498322\n",
      "  time_since_restore: 457.9193742275238\n",
      "  time_this_iter_s: 20.232973337173462\n",
      "  time_total_s: 457.9193742275238\n",
      "  timers:\n",
      "    learn_throughput: 1299.558\n",
      "    learn_time_ms: 769.492\n",
      "    load_throughput: 47280.22\n",
      "    load_time_ms: 21.15\n",
      "    sample_throughput: 44.272\n",
      "    sample_time_ms: 22587.387\n",
      "    update_time_ms: 4.008\n",
      "  timestamp: 1635283029\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 16\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         457.919</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-4.45641</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           402.359</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 17000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-17-30\n",
      "  done: false\n",
      "  episode_len_mean: 402.8809523809524\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.430714285714244\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 42\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5697849538591173\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010722033289922944\n",
      "          policy_loss: -0.022641359104050532\n",
      "          total_loss: -0.04232432544231415\n",
      "          vf_explained_var: 0.681905210018158\n",
      "          vf_loss: 0.0038704751576814386\n",
      "    num_agent_steps_sampled: 17000\n",
      "    num_agent_steps_trained: 17000\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 17000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.67000000000002\n",
      "    ram_util_percent: 29.903333333333325\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03987762383512928\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 34.49579966409228\n",
      "    mean_inference_ms: 2.373790860643432\n",
      "    mean_raw_obs_processing_ms: 0.5860495276545338\n",
      "  time_since_restore: 478.35104537010193\n",
      "  time_this_iter_s: 20.431671142578125\n",
      "  time_total_s: 478.35104537010193\n",
      "  timers:\n",
      "    learn_throughput: 1301.597\n",
      "    learn_time_ms: 768.287\n",
      "    load_throughput: 49582.926\n",
      "    load_time_ms: 20.168\n",
      "    sample_throughput: 45.055\n",
      "    sample_time_ms: 22195.308\n",
      "    update_time_ms: 3.985\n",
      "  timestamp: 1635283050\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 17\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         478.351</td><td style=\"text-align: right;\">17000</td><td style=\"text-align: right;\">-4.43071</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           402.881</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-17-51\n",
      "  done: false\n",
      "  episode_len_mean: 402.45454545454544\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.408181818181777\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 44\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5368358506096733\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011425428353076623\n",
      "          policy_loss: -0.01748035413523515\n",
      "          total_loss: -0.036167510723074274\n",
      "          vf_explained_var: 0.8530434370040894\n",
      "          vf_loss: 0.004396113653719011\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.80344827586208\n",
      "    ram_util_percent: 29.9103448275862\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03980159417790538\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.98188757271063\n",
      "    mean_inference_ms: 2.3661306519603587\n",
      "    mean_raw_obs_processing_ms: 0.6144765228556553\n",
      "  time_since_restore: 498.9030568599701\n",
      "  time_this_iter_s: 20.552011489868164\n",
      "  time_total_s: 498.9030568599701\n",
      "  timers:\n",
      "    learn_throughput: 1272.707\n",
      "    learn_time_ms: 785.727\n",
      "    load_throughput: 51956.387\n",
      "    load_time_ms: 19.247\n",
      "    sample_throughput: 44.918\n",
      "    sample_time_ms: 22262.744\n",
      "    update_time_ms: 4.003\n",
      "  timestamp: 1635283071\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         498.903</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">-4.40818</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           402.455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 19000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-18-13\n",
      "  done: false\n",
      "  episode_len_mean: 403.8936170212766\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.398085106382937\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 47\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5071538819207086\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011984452638829666\n",
      "          policy_loss: -0.08643786178694832\n",
      "          total_loss: -0.10261799212959077\n",
      "          vf_explained_var: 0.7404918074607849\n",
      "          vf_loss: 0.006494516828873505\n",
      "    num_agent_steps_sampled: 19000\n",
      "    num_agent_steps_trained: 19000\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 19000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.95625\n",
      "    ram_util_percent: 29.915625\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039706596181382295\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.27432142556308\n",
      "    mean_inference_ms: 2.355848593202813\n",
      "    mean_raw_obs_processing_ms: 0.6493629571403485\n",
      "  time_since_restore: 521.1392951011658\n",
      "  time_this_iter_s: 22.23623824119568\n",
      "  time_total_s: 521.1392951011658\n",
      "  timers:\n",
      "    learn_throughput: 1102.082\n",
      "    learn_time_ms: 907.374\n",
      "    load_throughput: 46472.326\n",
      "    load_time_ms: 21.518\n",
      "    sample_throughput: 45.779\n",
      "    sample_time_ms: 21843.926\n",
      "    update_time_ms: 4.078\n",
      "  timestamp: 1635283093\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 19\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         521.139</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\">-4.39809</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           403.894</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-18-35\n",
      "  done: false\n",
      "  episode_len_mean: 403.9795918367347\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.384285714285673\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 49\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5349192168977526\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009693121081491856\n",
      "          policy_loss: 0.1305569537811809\n",
      "          total_loss: 0.112049091524548\n",
      "          vf_explained_var: 0.7686625719070435\n",
      "          vf_loss: 0.004902706458880048\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.8483870967742\n",
      "    ram_util_percent: 29.864516129032253\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039649228608297754\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.83951898572919\n",
      "    mean_inference_ms: 2.349594102181045\n",
      "    mean_raw_obs_processing_ms: 0.668307058911029\n",
      "  time_since_restore: 543.0562746524811\n",
      "  time_this_iter_s: 21.916979551315308\n",
      "  time_total_s: 543.0562746524811\n",
      "  timers:\n",
      "    learn_throughput: 968.159\n",
      "    learn_time_ms: 1032.888\n",
      "    load_throughput: 45156.27\n",
      "    load_time_ms: 22.145\n",
      "    sample_throughput: 45.913\n",
      "    sample_time_ms: 21780.53\n",
      "    update_time_ms: 4.449\n",
      "  timestamp: 1635283115\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 20\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         543.056</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-4.38429</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">            403.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-18-56\n",
      "  done: false\n",
      "  episode_len_mean: 403.88235294117646\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.369803921568585\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 51\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.455001409848531\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010219689819946767\n",
      "          policy_loss: -0.1382840986053149\n",
      "          total_loss: -0.15338130046923956\n",
      "          vf_explained_var: 0.7235661745071411\n",
      "          vf_loss: 0.007408875611791801\n",
      "    num_agent_steps_sampled: 21000\n",
      "    num_agent_steps_trained: 21000\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 21000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.69677419354842\n",
      "    ram_util_percent: 29.874193548387094\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03959569112378574\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.42825938601518\n",
      "    mean_inference_ms: 2.3437293139398716\n",
      "    mean_raw_obs_processing_ms: 0.6840505739570606\n",
      "  time_since_restore: 564.5485641956329\n",
      "  time_this_iter_s: 21.492289543151855\n",
      "  time_total_s: 564.5485641956329\n",
      "  timers:\n",
      "    learn_throughput: 859.177\n",
      "    learn_time_ms: 1163.905\n",
      "    load_throughput: 44230.696\n",
      "    load_time_ms: 22.609\n",
      "    sample_throughput: 45.999\n",
      "    sample_time_ms: 21739.8\n",
      "    update_time_ms: 4.489\n",
      "  timestamp: 1635283136\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 21\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         564.549</td><td style=\"text-align: right;\">21000</td><td style=\"text-align: right;\"> -4.3698</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           403.882</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 22000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-19-16\n",
      "  done: false\n",
      "  episode_len_mean: 404.75925925925924\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.360185185185143\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 54\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.507863407664829\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010158073835006975\n",
      "          policy_loss: 0.0876531040502919\n",
      "          total_loss: 0.07150597034229172\n",
      "          vf_explained_var: 0.7023659944534302\n",
      "          vf_loss: 0.006899882913825826\n",
      "    num_agent_steps_sampled: 22000\n",
      "    num_agent_steps_trained: 22000\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.86666666666667\n",
      "    ram_util_percent: 29.777777777777782\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03952041621446524\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.853119832984024\n",
      "    mean_inference_ms: 2.3355454796190895\n",
      "    mean_raw_obs_processing_ms: 0.7033889834211103\n",
      "  time_since_restore: 584.0211901664734\n",
      "  time_this_iter_s: 19.472625970840454\n",
      "  time_total_s: 584.0211901664734\n",
      "  timers:\n",
      "    learn_throughput: 861.186\n",
      "    learn_time_ms: 1161.189\n",
      "    load_throughput: 46081.176\n",
      "    load_time_ms: 21.701\n",
      "    sample_throughput: 46.466\n",
      "    sample_time_ms: 21521.268\n",
      "    update_time_ms: 4.361\n",
      "  timestamp: 1635283156\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 22\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         584.021</td><td style=\"text-align: right;\">22000</td><td style=\"text-align: right;\">-4.36019</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           404.759</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 23000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-19-35\n",
      "  done: false\n",
      "  episode_len_mean: 405.2857142857143\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.354285714285672\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 56\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.490634650654263\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011465433029377812\n",
      "          policy_loss: -0.08033123678631253\n",
      "          total_loss: -0.09488666405280431\n",
      "          vf_explained_var: 0.4009546935558319\n",
      "          vf_loss: 0.008057833380169339\n",
      "    num_agent_steps_sampled: 23000\n",
      "    num_agent_steps_trained: 23000\n",
      "    num_steps_sampled: 23000\n",
      "    num_steps_trained: 23000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.85357142857143\n",
      "    ram_util_percent: 29.767857142857146\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03947255781047959\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.49450404178281\n",
      "    mean_inference_ms: 2.330441652437028\n",
      "    mean_raw_obs_processing_ms: 0.7138395140073571\n",
      "  time_since_restore: 603.2008316516876\n",
      "  time_this_iter_s: 19.179641485214233\n",
      "  time_total_s: 603.2008316516876\n",
      "  timers:\n",
      "    learn_throughput: 861.254\n",
      "    learn_time_ms: 1161.098\n",
      "    load_throughput: 47224.432\n",
      "    load_time_ms: 21.175\n",
      "    sample_throughput: 51.388\n",
      "    sample_time_ms: 19459.957\n",
      "    update_time_ms: 4.355\n",
      "  timestamp: 1635283175\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 23\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         603.201</td><td style=\"text-align: right;\">23000</td><td style=\"text-align: right;\">-4.35429</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           405.286</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-19-55\n",
      "  done: false\n",
      "  episode_len_mean: 405.47457627118644\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.340847457627077\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 59\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4814838409423827\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011547361620291597\n",
      "          policy_loss: 0.028688969877031116\n",
      "          total_loss: 0.013346583147843679\n",
      "          vf_explained_var: 0.6899649500846863\n",
      "          vf_loss: 0.007162977817157904\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.51724137931033\n",
      "    ram_util_percent: 29.796551724137924\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039405246094069986\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.991892881566724\n",
      "    mean_inference_ms: 2.323325890584193\n",
      "    mean_raw_obs_processing_ms: 0.7265964886984069\n",
      "  time_since_restore: 623.4434962272644\n",
      "  time_this_iter_s: 20.242664575576782\n",
      "  time_total_s: 623.4434962272644\n",
      "  timers:\n",
      "    learn_throughput: 860.94\n",
      "    learn_time_ms: 1161.522\n",
      "    load_throughput: 46487.16\n",
      "    load_time_ms: 21.511\n",
      "    sample_throughput: 51.518\n",
      "    sample_time_ms: 19410.665\n",
      "    update_time_ms: 4.363\n",
      "  timestamp: 1635283195\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 24\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         623.443</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">-4.34085</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           405.475</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 25000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-20-32\n",
      "  done: false\n",
      "  episode_len_mean: 405.0655737704918\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.327377049180286\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 61\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4540601518419054\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010463422418651468\n",
      "          policy_loss: -0.09640533692306942\n",
      "          total_loss: -0.11134051101075279\n",
      "          vf_explained_var: 0.7281672358512878\n",
      "          vf_loss: 0.007512736780336126\n",
      "    num_agent_steps_sampled: 25000\n",
      "    num_agent_steps_trained: 25000\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 25000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.85769230769232\n",
      "    ram_util_percent: 29.957692307692316\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03936238317780466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.679173652968128\n",
      "    mean_inference_ms: 2.318866755185007\n",
      "    mean_raw_obs_processing_ms: 0.7542444612925077\n",
      "  time_since_restore: 660.2200162410736\n",
      "  time_this_iter_s: 36.776520013809204\n",
      "  time_total_s: 660.2200162410736\n",
      "  timers:\n",
      "    learn_throughput: 860.681\n",
      "    learn_time_ms: 1161.87\n",
      "    load_throughput: 46320.974\n",
      "    load_time_ms: 21.588\n",
      "    sample_throughput: 47.481\n",
      "    sample_time_ms: 21061.049\n",
      "    update_time_ms: 4.463\n",
      "  timestamp: 1635283232\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 25\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">          660.22</td><td style=\"text-align: right;\">25000</td><td style=\"text-align: right;\">-4.32738</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           405.066</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 26000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-20-52\n",
      "  done: false\n",
      "  episode_len_mean: 405.234375\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.316093749999958\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 64\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.449362431632148\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011162429000397604\n",
      "          policy_loss: 0.046493505438168846\n",
      "          total_loss: 0.03313438014851676\n",
      "          vf_explained_var: 0.4939649701118469\n",
      "          vf_loss: 0.008902009242835145\n",
      "    num_agent_steps_sampled: 26000\n",
      "    num_agent_steps_trained: 26000\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 26000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.02333333333333\n",
      "    ram_util_percent: 30.473333333333336\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03930169798422336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.239139524874616\n",
      "    mean_inference_ms: 2.3126192478670795\n",
      "    mean_raw_obs_processing_ms: 0.7900703682885395\n",
      "  time_since_restore: 680.7310631275177\n",
      "  time_this_iter_s: 20.511046886444092\n",
      "  time_total_s: 680.7310631275177\n",
      "  timers:\n",
      "    learn_throughput: 859.943\n",
      "    learn_time_ms: 1162.867\n",
      "    load_throughput: 44502.466\n",
      "    load_time_ms: 22.471\n",
      "    sample_throughput: 47.422\n",
      "    sample_time_ms: 21087.362\n",
      "    update_time_ms: 4.227\n",
      "  timestamp: 1635283252\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 26\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         680.731</td><td style=\"text-align: right;\">26000</td><td style=\"text-align: right;\">-4.31609</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           405.234</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 27000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-21-11\n",
      "  done: false\n",
      "  episode_len_mean: 405.72727272727275\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.313030303030261\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 66\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4453113264507715\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011278179984771855\n",
      "          policy_loss: 0.0378076809975836\n",
      "          total_loss: 0.02216170993116167\n",
      "          vf_explained_var: 0.5464633703231812\n",
      "          vf_loss: 0.00655150698504359\n",
      "    num_agent_steps_sampled: 27000\n",
      "    num_agent_steps_trained: 27000\n",
      "    num_steps_sampled: 27000\n",
      "    num_steps_trained: 27000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.55384615384615\n",
      "    ram_util_percent: 30.12692307692308\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0392627796375079\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.961630107710942\n",
      "    mean_inference_ms: 2.3086959868833126\n",
      "    mean_raw_obs_processing_ms: 0.8106666932615502\n",
      "  time_since_restore: 699.5168797969818\n",
      "  time_this_iter_s: 18.78581666946411\n",
      "  time_total_s: 699.5168797969818\n",
      "  timers:\n",
      "    learn_throughput: 859.992\n",
      "    learn_time_ms: 1162.802\n",
      "    load_throughput: 42938.219\n",
      "    load_time_ms: 23.289\n",
      "    sample_throughput: 47.797\n",
      "    sample_time_ms: 20922.012\n",
      "    update_time_ms: 4.225\n",
      "  timestamp: 1635283271\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 27\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         699.517</td><td style=\"text-align: right;\">27000</td><td style=\"text-align: right;\">-4.31303</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           405.727</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-21-28\n",
      "  done: false\n",
      "  episode_len_mean: 407.45588235294116\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.322794117647017\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 68\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4061367564731175\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008857812622741731\n",
      "          policy_loss: -0.044620553818013935\n",
      "          total_loss: -0.057711984713872275\n",
      "          vf_explained_var: 0.529389500617981\n",
      "          vf_loss: 0.009198372737996074\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.51599999999999\n",
      "    ram_util_percent: 29.928\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03922437217100171\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.692891141555016\n",
      "    mean_inference_ms: 2.3048935471694487\n",
      "    mean_raw_obs_processing_ms: 0.8287077208391902\n",
      "  time_since_restore: 716.5164566040039\n",
      "  time_this_iter_s: 16.999576807022095\n",
      "  time_total_s: 716.5164566040039\n",
      "  timers:\n",
      "    learn_throughput: 874.701\n",
      "    learn_time_ms: 1143.248\n",
      "    load_throughput: 42944.858\n",
      "    load_time_ms: 23.286\n",
      "    sample_throughput: 48.576\n",
      "    sample_time_ms: 20586.306\n",
      "    update_time_ms: 4.228\n",
      "  timestamp: 1635283288\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 28\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         716.516</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">-4.32279</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           407.456</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 29000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-21-46\n",
      "  done: false\n",
      "  episode_len_mean: 408.95714285714286\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.330714285714243\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 70\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3726951175265842\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012609927682484694\n",
      "          policy_loss: -0.11540727615356446\n",
      "          total_loss: -0.1269681258334054\n",
      "          vf_explained_var: 0.4854773283004761\n",
      "          vf_loss: 0.009644113252741388\n",
      "    num_agent_steps_sampled: 29000\n",
      "    num_agent_steps_trained: 29000\n",
      "    num_steps_sampled: 29000\n",
      "    num_steps_trained: 29000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.392\n",
      "    ram_util_percent: 29.968000000000004\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03918691148882704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.43300916413487\n",
      "    mean_inference_ms: 2.3012385108698563\n",
      "    mean_raw_obs_processing_ms: 0.8445045104752406\n",
      "  time_since_restore: 733.903904914856\n",
      "  time_this_iter_s: 17.38744831085205\n",
      "  time_total_s: 733.903904914856\n",
      "  timers:\n",
      "    learn_throughput: 984.021\n",
      "    learn_time_ms: 1016.238\n",
      "    load_throughput: 47139.617\n",
      "    load_time_ms: 21.214\n",
      "    sample_throughput: 49.43\n",
      "    sample_time_ms: 20230.66\n",
      "    update_time_ms: 4.138\n",
      "  timestamp: 1635283306\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29000\n",
      "  training_iteration: 29\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         733.904</td><td style=\"text-align: right;\">29000</td><td style=\"text-align: right;\">-4.33071</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           408.957</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-22-04\n",
      "  done: false\n",
      "  episode_len_mean: 410.1095890410959\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.332328767123244\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 73\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2964592271380955\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010855079602665383\n",
      "          policy_loss: 0.017105329202281104\n",
      "          total_loss: 0.006037713587284088\n",
      "          vf_explained_var: 0.3646973967552185\n",
      "          vf_loss: 0.00972596295695338\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.75769230769231\n",
      "    ram_util_percent: 29.949999999999996\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03913247288675289\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.062753157509395\n",
      "    mean_inference_ms: 2.2960076627798647\n",
      "    mean_raw_obs_processing_ms: 0.8650010525032833\n",
      "  time_since_restore: 752.4897346496582\n",
      "  time_this_iter_s: 18.585829734802246\n",
      "  time_total_s: 752.4897346496582\n",
      "  timers:\n",
      "    learn_throughput: 1124.505\n",
      "    learn_time_ms: 889.281\n",
      "    load_throughput: 47075.334\n",
      "    load_time_ms: 21.243\n",
      "    sample_throughput: 49.938\n",
      "    sample_time_ms: 20024.896\n",
      "    update_time_ms: 3.71\n",
      "  timestamp: 1635283324\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 30\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">          752.49</td><td style=\"text-align: right;\">30000</td><td style=\"text-align: right;\">-4.33233</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">            410.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 31000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-22-24\n",
      "  done: false\n",
      "  episode_len_mean: 411.29333333333335\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.3379999999999574\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 75\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3256317191653784\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007388174027658619\n",
      "          policy_loss: 0.12349114186233945\n",
      "          total_loss: 0.10784915950563219\n",
      "          vf_explained_var: 0.41333329677581787\n",
      "          vf_loss: 0.006136698361014068\n",
      "    num_agent_steps_sampled: 31000\n",
      "    num_agent_steps_trained: 31000\n",
      "    num_steps_sampled: 31000\n",
      "    num_steps_trained: 31000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.59285714285714\n",
      "    ram_util_percent: 30.18571428571429\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03909966647225996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.828961107215843\n",
      "    mean_inference_ms: 2.292768984268334\n",
      "    mean_raw_obs_processing_ms: 0.8767874515390491\n",
      "  time_since_restore: 772.2327928543091\n",
      "  time_this_iter_s: 19.74305820465088\n",
      "  time_total_s: 772.2327928543091\n",
      "  timers:\n",
      "    learn_throughput: 1318.348\n",
      "    learn_time_ms: 758.525\n",
      "    load_throughput: 46573.933\n",
      "    load_time_ms: 21.471\n",
      "    sample_throughput: 50.049\n",
      "    sample_time_ms: 19980.529\n",
      "    update_time_ms: 3.678\n",
      "  timestamp: 1635283344\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31000\n",
      "  training_iteration: 31\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         772.233</td><td style=\"text-align: right;\">31000</td><td style=\"text-align: right;\">  -4.338</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           411.293</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-22-42\n",
      "  done: false\n",
      "  episode_len_mean: 413.0779220779221\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.349999999999956\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 77\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.261150670051575\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01006776460103715\n",
      "          policy_loss: 0.07695305777920616\n",
      "          total_loss: 0.06271962457233005\n",
      "          vf_explained_var: 0.06441392004489899\n",
      "          vf_loss: 0.006364521906652954\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.576\n",
      "    ram_util_percent: 30.316000000000003\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03906750334117254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.602577400010386\n",
      "    mean_inference_ms: 2.28964161497254\n",
      "    mean_raw_obs_processing_ms: 0.8870734368568081\n",
      "  time_since_restore: 789.853257894516\n",
      "  time_this_iter_s: 17.62046504020691\n",
      "  time_total_s: 789.853257894516\n",
      "  timers:\n",
      "    learn_throughput: 1318.547\n",
      "    learn_time_ms: 758.411\n",
      "    load_throughput: 46595.871\n",
      "    load_time_ms: 21.461\n",
      "    sample_throughput: 50.517\n",
      "    sample_time_ms: 19795.444\n",
      "    update_time_ms: 3.676\n",
      "  timestamp: 1635283362\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 32\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         789.853</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">   -4.35</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           413.078</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 33000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-23-00\n",
      "  done: false\n",
      "  episode_len_mean: 414.1392405063291\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.355063291139197\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 79\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.264736893441942\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012135230853571294\n",
      "          policy_loss: -0.06882172475258509\n",
      "          total_loss: -0.07726484992437893\n",
      "          vf_explained_var: 0.1795087456703186\n",
      "          vf_loss: 0.01177719769378503\n",
      "    num_agent_steps_sampled: 33000\n",
      "    num_agent_steps_trained: 33000\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 33000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.07777777777778\n",
      "    ram_util_percent: 30.300000000000004\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03903680481411457\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.383999845426988\n",
      "    mean_inference_ms: 2.2866436568572266\n",
      "    mean_raw_obs_processing_ms: 0.8960277584419707\n",
      "  time_since_restore: 808.4503545761108\n",
      "  time_this_iter_s: 18.59709668159485\n",
      "  time_total_s: 808.4503545761108\n",
      "  timers:\n",
      "    learn_throughput: 1313.773\n",
      "    learn_time_ms: 761.166\n",
      "    load_throughput: 45245.656\n",
      "    load_time_ms: 22.102\n",
      "    sample_throughput: 50.675\n",
      "    sample_time_ms: 19733.789\n",
      "    update_time_ms: 3.685\n",
      "  timestamp: 1635283380\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 33\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">          808.45</td><td style=\"text-align: right;\">33000</td><td style=\"text-align: right;\">-4.35506</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           414.139</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 34000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-23-17\n",
      "  done: false\n",
      "  episode_len_mean: 416.1604938271605\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.369999999999956\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 81\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.254399469163683\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012372651527956751\n",
      "          policy_loss: -0.06952888303332859\n",
      "          total_loss: -0.07874153653780619\n",
      "          vf_explained_var: 0.42594611644744873\n",
      "          vf_loss: 0.010856807306926284\n",
      "    num_agent_steps_sampled: 34000\n",
      "    num_agent_steps_trained: 34000\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 34000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.8\n",
      "    ram_util_percent: 30.15416666666667\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03900657737231994\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.171789926048074\n",
      "    mean_inference_ms: 2.283736488453971\n",
      "    mean_raw_obs_processing_ms: 0.9037903596277446\n",
      "  time_since_restore: 825.4780459403992\n",
      "  time_this_iter_s: 17.02769136428833\n",
      "  time_total_s: 825.4780459403992\n",
      "  timers:\n",
      "    learn_throughput: 1318.866\n",
      "    learn_time_ms: 758.227\n",
      "    load_throughput: 46850.959\n",
      "    load_time_ms: 21.344\n",
      "    sample_throughput: 51.504\n",
      "    sample_time_ms: 19415.996\n",
      "    update_time_ms: 3.673\n",
      "  timestamp: 1635283397\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 34\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         825.478</td><td style=\"text-align: right;\">34000</td><td style=\"text-align: right;\">   -4.37</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">            416.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 35000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-23-35\n",
      "  done: false\n",
      "  episode_len_mean: 417.03614457831327\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.373734939758991\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 83\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2174444437026977\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009913606287042123\n",
      "          policy_loss: -0.09640476240052118\n",
      "          total_loss: -0.1046782288286421\n",
      "          vf_explained_var: 0.2441050261259079\n",
      "          vf_loss: 0.011918259938829579\n",
      "    num_agent_steps_sampled: 35000\n",
      "    num_agent_steps_trained: 35000\n",
      "    num_steps_sampled: 35000\n",
      "    num_steps_trained: 35000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.59599999999999\n",
      "    ram_util_percent: 30.088\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03897691974816286\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.96598261566204\n",
      "    mean_inference_ms: 2.280916894365739\n",
      "    mean_raw_obs_processing_ms: 0.9104912909241941\n",
      "  time_since_restore: 842.8732862472534\n",
      "  time_this_iter_s: 17.395240306854248\n",
      "  time_total_s: 842.8732862472534\n",
      "  timers:\n",
      "    learn_throughput: 1317.168\n",
      "    learn_time_ms: 759.205\n",
      "    load_throughput: 49361.245\n",
      "    load_time_ms: 20.259\n",
      "    sample_throughput: 57.214\n",
      "    sample_time_ms: 17478.144\n",
      "    update_time_ms: 3.576\n",
      "  timestamp: 1635283415\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 35\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         842.873</td><td style=\"text-align: right;\">35000</td><td style=\"text-align: right;\">-4.37373</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           417.036</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-23-51\n",
      "  done: false\n",
      "  episode_len_mean: 418.8235294117647\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.3868235294117195\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 85\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2617368592156306\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009648960027131186\n",
      "          policy_loss: -0.1267077879773246\n",
      "          total_loss: -0.13435414565934076\n",
      "          vf_explained_var: 0.09904710948467255\n",
      "          vf_loss: 0.013041222750002312\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.75833333333334\n",
      "    ram_util_percent: 30.058333333333337\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03894784715596721\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.76583099217028\n",
      "    mean_inference_ms: 2.278181646408944\n",
      "    mean_raw_obs_processing_ms: 0.916232658574833\n",
      "  time_since_restore: 859.4839999675751\n",
      "  time_this_iter_s: 16.610713720321655\n",
      "  time_total_s: 859.4839999675751\n",
      "  timers:\n",
      "    learn_throughput: 1318.88\n",
      "    learn_time_ms: 758.219\n",
      "    load_throughput: 51535.877\n",
      "    load_time_ms: 19.404\n",
      "    sample_throughput: 58.514\n",
      "    sample_time_ms: 17089.928\n",
      "    update_time_ms: 3.605\n",
      "  timestamp: 1635283431\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 36\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         859.484</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">-4.38682</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           418.824</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 37000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-24-08\n",
      "  done: false\n",
      "  episode_len_mean: 420.85057471264366\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.4025287356321385\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 87\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1656513690948485\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013383234315499513\n",
      "          policy_loss: -0.12386591484149297\n",
      "          total_loss: -0.13113673908842935\n",
      "          vf_explained_var: 0.08924365788698196\n",
      "          vf_loss: 0.01170904511689312\n",
      "    num_agent_steps_sampled: 37000\n",
      "    num_agent_steps_trained: 37000\n",
      "    num_steps_sampled: 37000\n",
      "    num_steps_trained: 37000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.14166666666667\n",
      "    ram_util_percent: 30.054166666666664\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03891978333384014\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.571346936508448\n",
      "    mean_inference_ms: 2.275545695788213\n",
      "    mean_raw_obs_processing_ms: 0.9211280951247144\n",
      "  time_since_restore: 876.4731941223145\n",
      "  time_this_iter_s: 16.98919415473938\n",
      "  time_total_s: 876.4731941223145\n",
      "  timers:\n",
      "    learn_throughput: 1313.787\n",
      "    learn_time_ms: 761.158\n",
      "    load_throughput: 53361.628\n",
      "    load_time_ms: 18.74\n",
      "    sample_throughput: 59.144\n",
      "    sample_time_ms: 16907.979\n",
      "    update_time_ms: 3.612\n",
      "  timestamp: 1635283448\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37000\n",
      "  training_iteration: 37\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         876.473</td><td style=\"text-align: right;\">37000</td><td style=\"text-align: right;\">-4.40253</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           420.851</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 38000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-24-26\n",
      "  done: false\n",
      "  episode_len_mean: 422.5168539325843\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.414831460674112\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 89\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.135616827011108\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00944943473695494\n",
      "          policy_loss: -0.0952509940498405\n",
      "          total_loss: -0.10649778693914413\n",
      "          vf_explained_var: 0.3403535783290863\n",
      "          vf_loss: 0.008219492066483427\n",
      "    num_agent_steps_sampled: 38000\n",
      "    num_agent_steps_trained: 38000\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 38000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.548\n",
      "    ram_util_percent: 30.18\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03889252177466996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.382386948115453\n",
      "    mean_inference_ms: 2.2730093822122273\n",
      "    mean_raw_obs_processing_ms: 0.9252568511660461\n",
      "  time_since_restore: 893.5446920394897\n",
      "  time_this_iter_s: 17.071497917175293\n",
      "  time_total_s: 893.5446920394897\n",
      "  timers:\n",
      "    learn_throughput: 1309.275\n",
      "    learn_time_ms: 763.781\n",
      "    load_throughput: 53294.638\n",
      "    load_time_ms: 18.764\n",
      "    sample_throughput: 59.128\n",
      "    sample_time_ms: 16912.522\n",
      "    update_time_ms: 3.615\n",
      "  timestamp: 1635283466\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 38\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         893.545</td><td style=\"text-align: right;\">38000</td><td style=\"text-align: right;\">-4.41483</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           422.517</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 39000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-25-02\n",
      "  done: false\n",
      "  episode_len_mean: 423.67391304347825\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.420217391304303\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 92\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1636713266372682\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009122606649651043\n",
      "          policy_loss: 0.01673584704597791\n",
      "          total_loss: 0.006040062093072467\n",
      "          vf_explained_var: 0.45913106203079224\n",
      "          vf_loss: 0.009116406776593066\n",
      "    num_agent_steps_sampled: 39000\n",
      "    num_agent_steps_trained: 39000\n",
      "    num_steps_sampled: 39000\n",
      "    num_steps_trained: 39000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.77843137254901\n",
      "    ram_util_percent: 30.337254901960787\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03885286308623121\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.111286884158147\n",
      "    mean_inference_ms: 2.2693678207500003\n",
      "    mean_raw_obs_processing_ms: 0.9448307923497612\n",
      "  time_since_restore: 929.5270898342133\n",
      "  time_this_iter_s: 35.98239779472351\n",
      "  time_total_s: 929.5270898342133\n",
      "  timers:\n",
      "    learn_throughput: 1310.227\n",
      "    learn_time_ms: 763.227\n",
      "    load_throughput: 50827.419\n",
      "    load_time_ms: 19.674\n",
      "    sample_throughput: 53.272\n",
      "    sample_time_ms: 18771.67\n",
      "    update_time_ms: 3.575\n",
      "  timestamp: 1635283502\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 39\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         929.527</td><td style=\"text-align: right;\">39000</td><td style=\"text-align: right;\">-4.42022</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           423.674</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-25-18\n",
      "  done: false\n",
      "  episode_len_mean: 424.97872340425533\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.429361702127614\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 94\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0966144296858045\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007264819130787058\n",
      "          policy_loss: 0.07487137168645859\n",
      "          total_loss: 0.06162944883108139\n",
      "          vf_explained_var: 0.6552992463111877\n",
      "          vf_loss: 0.006271256596341522\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.29583333333333\n",
      "    ram_util_percent: 31.004166666666663\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03882721512301695\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.937326522866407\n",
      "    mean_inference_ms: 2.267038324082103\n",
      "    mean_raw_obs_processing_ms: 0.9564873775378875\n",
      "  time_since_restore: 946.4298865795135\n",
      "  time_this_iter_s: 16.902796745300293\n",
      "  time_total_s: 946.4298865795135\n",
      "  timers:\n",
      "    learn_throughput: 1311.911\n",
      "    learn_time_ms: 762.247\n",
      "    load_throughput: 53555.2\n",
      "    load_time_ms: 18.672\n",
      "    sample_throughput: 53.748\n",
      "    sample_time_ms: 18605.361\n",
      "    update_time_ms: 3.567\n",
      "  timestamp: 1635283518\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 40\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">          946.43</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">-4.42936</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           424.979</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 41000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-25-37\n",
      "  done: false\n",
      "  episode_len_mean: 425.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.430833333333287\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 96\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0332605242729187\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010479525744100273\n",
      "          policy_loss: 0.17163413957589202\n",
      "          total_loss: 0.15615554911394913\n",
      "          vf_explained_var: 0.6852244138717651\n",
      "          vf_loss: 0.002758111325076445\n",
      "    num_agent_steps_sampled: 41000\n",
      "    num_agent_steps_trained: 41000\n",
      "    num_steps_sampled: 41000\n",
      "    num_steps_trained: 41000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.18076923076923\n",
      "    ram_util_percent: 30.546153846153842\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03880205218886867\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.768605185723015\n",
      "    mean_inference_ms: 2.264776273710528\n",
      "    mean_raw_obs_processing_ms: 0.9670026873492349\n",
      "  time_since_restore: 964.5746531486511\n",
      "  time_this_iter_s: 18.144766569137573\n",
      "  time_total_s: 964.5746531486511\n",
      "  timers:\n",
      "    learn_throughput: 1312.951\n",
      "    learn_time_ms: 761.643\n",
      "    load_throughput: 55298.808\n",
      "    load_time_ms: 18.084\n",
      "    sample_throughput: 54.21\n",
      "    sample_time_ms: 18446.724\n",
      "    update_time_ms: 3.572\n",
      "  timestamp: 1635283537\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 41000\n",
      "  training_iteration: 41\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         964.575</td><td style=\"text-align: right;\">41000</td><td style=\"text-align: right;\">-4.43083</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">             425.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 42000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-25-54\n",
      "  done: false\n",
      "  episode_len_mean: 426.7959183673469\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.440204081632606\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 98\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0302238676283095\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0069497836353732785\n",
      "          policy_loss: 0.08858601252237956\n",
      "          total_loss: 0.07760318550798628\n",
      "          vf_explained_var: -0.5178723931312561\n",
      "          vf_loss: 0.007929457092864646\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.708\n",
      "    ram_util_percent: 30.311999999999998\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03877735362452521\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.604417525760766\n",
      "    mean_inference_ms: 2.262574354712698\n",
      "    mean_raw_obs_processing_ms: 0.9764709373171276\n",
      "  time_since_restore: 981.705502986908\n",
      "  time_this_iter_s: 17.130849838256836\n",
      "  time_total_s: 981.705502986908\n",
      "  timers:\n",
      "    learn_throughput: 1311.399\n",
      "    learn_time_ms: 762.545\n",
      "    load_throughput: 55194.166\n",
      "    load_time_ms: 18.118\n",
      "    sample_throughput: 54.357\n",
      "    sample_time_ms: 18396.844\n",
      "    update_time_ms: 3.558\n",
      "  timestamp: 1635283554\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 42\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         981.706</td><td style=\"text-align: right;\">42000</td><td style=\"text-align: right;\"> -4.4402</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">           426.796</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 43000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-26-10\n",
      "  done: false\n",
      "  episode_len_mean: 427.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.4471999999999525\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 100\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9759939604335361\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010658796190410528\n",
      "          policy_loss: 0.03126361560490396\n",
      "          total_loss: 0.02174871787428856\n",
      "          vf_explained_var: 0.20080798864364624\n",
      "          vf_loss: 0.008113281553151965\n",
      "    num_agent_steps_sampled: 43000\n",
      "    num_agent_steps_trained: 43000\n",
      "    num_steps_sampled: 43000\n",
      "    num_steps_trained: 43000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.76521739130433\n",
      "    ram_util_percent: 30.299999999999994\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03875309514827244\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.444381548018544\n",
      "    mean_inference_ms: 2.2604292007685007\n",
      "    mean_raw_obs_processing_ms: 0.9849850833049351\n",
      "  time_since_restore: 998.3650324344635\n",
      "  time_this_iter_s: 16.659529447555542\n",
      "  time_total_s: 998.3650324344635\n",
      "  timers:\n",
      "    learn_throughput: 1318.144\n",
      "    learn_time_ms: 758.642\n",
      "    load_throughput: 58786.567\n",
      "    load_time_ms: 17.011\n",
      "    sample_throughput: 54.921\n",
      "    sample_time_ms: 18208.064\n",
      "    update_time_ms: 3.548\n",
      "  timestamp: 1635283570\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 43000\n",
      "  training_iteration: 43\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         998.365</td><td style=\"text-align: right;\">43000</td><td style=\"text-align: right;\"> -4.4472</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">            427.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-26-26\n",
      "  done: false\n",
      "  episode_len_mean: 429.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.4650999999999526\n",
      "  episode_reward_min: -14.559999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 102\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9524822592735291\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0062282148513881416\n",
      "          policy_loss: 0.046789585053920744\n",
      "          total_loss: 0.037049928141964804\n",
      "          vf_explained_var: 0.13637052476406097\n",
      "          vf_loss: 0.008539520423316087\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.5304347826087\n",
      "    ram_util_percent: 30.326086956521728\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038721122649168695\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.881919947401038\n",
      "    mean_inference_ms: 2.2387312074564796\n",
      "    mean_raw_obs_processing_ms: 1.008527776964308\n",
      "  time_since_restore: 1014.2812879085541\n",
      "  time_this_iter_s: 15.916255474090576\n",
      "  time_total_s: 1014.2812879085541\n",
      "  timers:\n",
      "    learn_throughput: 1318.819\n",
      "    learn_time_ms: 758.254\n",
      "    load_throughput: 61467.883\n",
      "    load_time_ms: 16.269\n",
      "    sample_throughput: 55.255\n",
      "    sample_time_ms: 18098.023\n",
      "    update_time_ms: 3.551\n",
      "  timestamp: 1635283586\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 44\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         1014.28</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\"> -4.4651</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">              -14.56</td><td style=\"text-align: right;\">            429.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 45000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-26-44\n",
      "  done: false\n",
      "  episode_len_mean: 430.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.332999999999952\n",
      "  episode_reward_min: -6.359999999999964\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 104\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0119881351788838\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006873495412918097\n",
      "          policy_loss: -0.07099093281560474\n",
      "          total_loss: -0.07447099702225791\n",
      "          vf_explained_var: 0.04591848701238632\n",
      "          vf_loss: 0.015265121285887693\n",
      "    num_agent_steps_sampled: 45000\n",
      "    num_agent_steps_trained: 45000\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 45000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.62\n",
      "    ram_util_percent: 30.348\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03869293296322934\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.098727766921847\n",
      "    mean_inference_ms: 2.2268824274176366\n",
      "    mean_raw_obs_processing_ms: 1.0312361439131268\n",
      "  time_since_restore: 1031.8568933010101\n",
      "  time_this_iter_s: 17.575605392456055\n",
      "  time_total_s: 1031.8568933010101\n",
      "  timers:\n",
      "    learn_throughput: 1320.775\n",
      "    learn_time_ms: 757.131\n",
      "    load_throughput: 60291.488\n",
      "    load_time_ms: 16.586\n",
      "    sample_throughput: 55.197\n",
      "    sample_time_ms: 18116.872\n",
      "    update_time_ms: 3.554\n",
      "  timestamp: 1635283604\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 45\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         1031.86</td><td style=\"text-align: right;\">45000</td><td style=\"text-align: right;\">  -4.333</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">               -6.36</td><td style=\"text-align: right;\">            430.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 46000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-27-01\n",
      "  done: false\n",
      "  episode_len_mean: 432.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.352499999999951\n",
      "  episode_reward_min: -6.359999999999964\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 106\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.012096893787384\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008971980919088013\n",
      "          policy_loss: -0.062303852786620456\n",
      "          total_loss: -0.0662222935921616\n",
      "          vf_explained_var: 0.15109394490718842\n",
      "          vf_loss: 0.014408133588666614\n",
      "    num_agent_steps_sampled: 46000\n",
      "    num_agent_steps_trained: 46000\n",
      "    num_steps_sampled: 46000\n",
      "    num_steps_trained: 46000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.87083333333334\n",
      "    ram_util_percent: 30.366666666666664\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03854211905140927\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.54667415203786\n",
      "    mean_inference_ms: 2.217930729872147\n",
      "    mean_raw_obs_processing_ms: 1.053265501312188\n",
      "  time_since_restore: 1048.4888229370117\n",
      "  time_this_iter_s: 16.631929636001587\n",
      "  time_total_s: 1048.4888229370117\n",
      "  timers:\n",
      "    learn_throughput: 1322.686\n",
      "    learn_time_ms: 756.037\n",
      "    load_throughput: 58736.843\n",
      "    load_time_ms: 17.025\n",
      "    sample_throughput: 55.189\n",
      "    sample_time_ms: 18119.657\n",
      "    update_time_ms: 3.534\n",
      "  timestamp: 1635283621\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 46000\n",
      "  training_iteration: 46\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         1048.49</td><td style=\"text-align: right;\">46000</td><td style=\"text-align: right;\"> -4.3525</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">               -6.36</td><td style=\"text-align: right;\">             432.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 47000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-27-18\n",
      "  done: false\n",
      "  episode_len_mean: 434.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.364299999999952\n",
      "  episode_reward_min: -6.359999999999964\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 108\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1348640627331203\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007381682180717498\n",
      "          policy_loss: -0.07624410208728578\n",
      "          total_loss: -0.08530980240967538\n",
      "          vf_explained_var: -0.00286556757055223\n",
      "          vf_loss: 0.01080660254940287\n",
      "    num_agent_steps_sampled: 47000\n",
      "    num_agent_steps_trained: 47000\n",
      "    num_steps_sampled: 47000\n",
      "    num_steps_trained: 47000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.84400000000001\n",
      "    ram_util_percent: 30.344\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03841210062196957\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.04584627958395\n",
      "    mean_inference_ms: 2.2101769322763984\n",
      "    mean_raw_obs_processing_ms: 1.0748649816755977\n",
      "  time_since_restore: 1065.8083629608154\n",
      "  time_this_iter_s: 17.31954002380371\n",
      "  time_total_s: 1065.8083629608154\n",
      "  timers:\n",
      "    learn_throughput: 1326.545\n",
      "    learn_time_ms: 753.838\n",
      "    load_throughput: 56392.942\n",
      "    load_time_ms: 17.733\n",
      "    sample_throughput: 55.084\n",
      "    sample_time_ms: 18154.174\n",
      "    update_time_ms: 3.534\n",
      "  timestamp: 1635283638\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47000\n",
      "  training_iteration: 47\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         1065.81</td><td style=\"text-align: right;\">47000</td><td style=\"text-align: right;\"> -4.3643</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">               -6.36</td><td style=\"text-align: right;\">            434.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-27-36\n",
      "  done: false\n",
      "  episode_len_mean: 435.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.357499999999951\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 110\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0568699015511407\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008429519884158113\n",
      "          policy_loss: -0.06871056308348973\n",
      "          total_loss: -0.07452944550249312\n",
      "          vf_explained_var: 0.21179024875164032\n",
      "          vf_loss: 0.01306391263885113\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.6\n",
      "    ram_util_percent: 30.304000000000002\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038314934560026825\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.637557630176534\n",
      "    mean_inference_ms: 2.2042959920008935\n",
      "    mean_raw_obs_processing_ms: 1.096060554972337\n",
      "  time_since_restore: 1083.313987493515\n",
      "  time_this_iter_s: 17.505624532699585\n",
      "  time_total_s: 1083.313987493515\n",
      "  timers:\n",
      "    learn_throughput: 1330.708\n",
      "    learn_time_ms: 751.48\n",
      "    load_throughput: 56460.351\n",
      "    load_time_ms: 17.712\n",
      "    sample_throughput: 54.945\n",
      "    sample_time_ms: 18199.982\n",
      "    update_time_ms: 3.52\n",
      "  timestamp: 1635283656\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 48\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         1083.31</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> -4.3575</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            435.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 49000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-27-54\n",
      "  done: false\n",
      "  episode_len_mean: 436.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.368899999999951\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 112\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.20808531443278\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008178822510010737\n",
      "          policy_loss: -0.11479123598999447\n",
      "          total_loss: -0.12503869326578246\n",
      "          vf_explained_var: 0.5377601385116577\n",
      "          vf_loss: 0.010197624437407488\n",
      "    num_agent_steps_sampled: 49000\n",
      "    num_agent_steps_trained: 49000\n",
      "    num_steps_sampled: 49000\n",
      "    num_steps_trained: 49000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.01923076923077\n",
      "    ram_util_percent: 30.26153846153846\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03822947785646485\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.26919108177196\n",
      "    mean_inference_ms: 2.1990704482587766\n",
      "    mean_raw_obs_processing_ms: 1.116823146007858\n",
      "  time_since_restore: 1101.5574536323547\n",
      "  time_this_iter_s: 18.24346613883972\n",
      "  time_total_s: 1101.5574536323547\n",
      "  timers:\n",
      "    learn_throughput: 1332.438\n",
      "    learn_time_ms: 750.504\n",
      "    load_throughput: 56642.507\n",
      "    load_time_ms: 17.655\n",
      "    sample_throughput: 60.875\n",
      "    sample_time_ms: 16427.153\n",
      "    update_time_ms: 3.522\n",
      "  timestamp: 1635283674\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49000\n",
      "  training_iteration: 49\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         1101.56</td><td style=\"text-align: right;\">49000</td><td style=\"text-align: right;\"> -4.3689</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            436.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 50000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-28-12\n",
      "  done: false\n",
      "  episode_len_mean: 438.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.386899999999951\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 115\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.20339615477456\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013158183892924709\n",
      "          policy_loss: -0.015446053279770746\n",
      "          total_loss: -0.0273199243677987\n",
      "          vf_explained_var: 0.7018502950668335\n",
      "          vf_loss: 0.0075284533707114555\n",
      "    num_agent_steps_sampled: 50000\n",
      "    num_agent_steps_trained: 50000\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.57307692307693\n",
      "    ram_util_percent: 30.215384615384615\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03813294022910821\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.82943475653606\n",
      "    mean_inference_ms: 2.193036605449655\n",
      "    mean_raw_obs_processing_ms: 1.147517064670816\n",
      "  time_since_restore: 1119.6872129440308\n",
      "  time_this_iter_s: 18.129759311676025\n",
      "  time_total_s: 1119.6872129440308\n",
      "  timers:\n",
      "    learn_throughput: 1333.115\n",
      "    learn_time_ms: 750.123\n",
      "    load_throughput: 53859.58\n",
      "    load_time_ms: 18.567\n",
      "    sample_throughput: 60.426\n",
      "    sample_time_ms: 16549.3\n",
      "    update_time_ms: 3.535\n",
      "  timestamp: 1635283692\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 50\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         1119.69</td><td style=\"text-align: right;\">50000</td><td style=\"text-align: right;\"> -4.3869</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            438.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 51000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-28-32\n",
      "  done: false\n",
      "  episode_len_mean: 438.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.38469999999995\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 117\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.041082387500339\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009450865188950056\n",
      "          policy_loss: 0.0692576963454485\n",
      "          total_loss: 0.05854002253876792\n",
      "          vf_explained_var: 0.6582333445549011\n",
      "          vf_loss: 0.00780297859520134\n",
      "    num_agent_steps_sampled: 51000\n",
      "    num_agent_steps_trained: 51000\n",
      "    num_steps_sampled: 51000\n",
      "    num_steps_trained: 51000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.62142857142857\n",
      "    ram_util_percent: 30.146428571428572\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03807567604153718\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.558756138938097\n",
      "    mean_inference_ms: 2.1894267458681487\n",
      "    mean_raw_obs_processing_ms: 1.1675632816424528\n",
      "  time_since_restore: 1139.594918012619\n",
      "  time_this_iter_s: 19.907705068588257\n",
      "  time_total_s: 1139.594918012619\n",
      "  timers:\n",
      "    learn_throughput: 1332.513\n",
      "    learn_time_ms: 750.462\n",
      "    load_throughput: 53955.123\n",
      "    load_time_ms: 18.534\n",
      "    sample_throughput: 59.79\n",
      "    sample_time_ms: 16725.184\n",
      "    update_time_ms: 3.58\n",
      "  timestamp: 1635283712\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 51000\n",
      "  training_iteration: 51\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         1139.59</td><td style=\"text-align: right;\">51000</td><td style=\"text-align: right;\"> -4.3847</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            438.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-28-50\n",
      "  done: false\n",
      "  episode_len_mean: 438.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.389299999999951\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 119\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9794647494951885\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010482709002301025\n",
      "          policy_loss: -0.09127463284466002\n",
      "          total_loss: -0.09952306292123264\n",
      "          vf_explained_var: 0.6104941368103027\n",
      "          vf_loss: 0.009449675769752098\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.6888888888889\n",
      "    ram_util_percent: 30.122222222222224\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038026866992627806\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.324494126479912\n",
      "    mean_inference_ms: 2.1863153970815143\n",
      "    mean_raw_obs_processing_ms: 1.187289080038988\n",
      "  time_since_restore: 1158.2042081356049\n",
      "  time_this_iter_s: 18.60929012298584\n",
      "  time_total_s: 1158.2042081356049\n",
      "  timers:\n",
      "    learn_throughput: 1331.731\n",
      "    learn_time_ms: 750.903\n",
      "    load_throughput: 51351.321\n",
      "    load_time_ms: 19.474\n",
      "    sample_throughput: 59.271\n",
      "    sample_time_ms: 16871.636\n",
      "    update_time_ms: 3.587\n",
      "  timestamp: 1635283730\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 52\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">          1158.2</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\"> -4.3893</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            438.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 53000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-29-25\n",
      "  done: false\n",
      "  episode_len_mean: 440.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.400999999999951\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 121\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9821309937371148\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0066147436167107426\n",
      "          policy_loss: -0.12968552907307943\n",
      "          total_loss: -0.13759703172577753\n",
      "          vf_explained_var: 0.6365483999252319\n",
      "          vf_loss: 0.010586858942406251\n",
      "    num_agent_steps_sampled: 53000\n",
      "    num_agent_steps_trained: 53000\n",
      "    num_steps_sampled: 53000\n",
      "    num_steps_trained: 53000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.134693877551015\n",
      "    ram_util_percent: 30.448979591836736\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0379852851000377\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.105858098823177\n",
      "    mean_inference_ms: 2.183585945465078\n",
      "    mean_raw_obs_processing_ms: 1.2132785560899806\n",
      "  time_since_restore: 1193.0157420635223\n",
      "  time_this_iter_s: 34.81153392791748\n",
      "  time_total_s: 1193.0157420635223\n",
      "  timers:\n",
      "    learn_throughput: 1331.038\n",
      "    learn_time_ms: 751.293\n",
      "    load_throughput: 49209.225\n",
      "    load_time_ms: 20.321\n",
      "    sample_throughput: 53.517\n",
      "    sample_time_ms: 18685.556\n",
      "    update_time_ms: 3.68\n",
      "  timestamp: 1635283765\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 53000\n",
      "  training_iteration: 53\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         1193.02</td><td style=\"text-align: right;\">53000</td><td style=\"text-align: right;\">  -4.401</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            440.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 54000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-29-43\n",
      "  done: false\n",
      "  episode_len_mean: 442.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.42089999999995\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 124\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9603640423880684\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006852227737694645\n",
      "          policy_loss: 0.021181318163871764\n",
      "          total_loss: 0.013153865933418274\n",
      "          vf_explained_var: 0.6307980418205261\n",
      "          vf_loss: 0.010205738344747159\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_steps_sampled: 54000\n",
      "    num_steps_trained: 54000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.55384615384615\n",
      "    ram_util_percent: 30.526923076923076\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037927956671391615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.797496536318317\n",
      "    mean_inference_ms: 2.1797570520565346\n",
      "    mean_raw_obs_processing_ms: 1.2516043543907924\n",
      "  time_since_restore: 1210.8355615139008\n",
      "  time_this_iter_s: 17.819819450378418\n",
      "  time_total_s: 1210.8355615139008\n",
      "  timers:\n",
      "    learn_throughput: 1329.906\n",
      "    learn_time_ms: 751.933\n",
      "    load_throughput: 46265.23\n",
      "    load_time_ms: 21.615\n",
      "    sample_throughput: 52.983\n",
      "    sample_time_ms: 18874.007\n",
      "    update_time_ms: 3.685\n",
      "  timestamp: 1635283783\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 54000\n",
      "  training_iteration: 54\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         1210.84</td><td style=\"text-align: right;\">54000</td><td style=\"text-align: right;\"> -4.4209</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            442.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 55000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-30-02\n",
      "  done: false\n",
      "  episode_len_mean: 442.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.42749999999995\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 126\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7686497661802503\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009833820077159711\n",
      "          policy_loss: 0.07001753035518858\n",
      "          total_loss: 0.06028615352180269\n",
      "          vf_explained_var: 0.671104371547699\n",
      "          vf_loss: 0.0059883524544097275\n",
      "    num_agent_steps_sampled: 55000\n",
      "    num_agent_steps_trained: 55000\n",
      "    num_steps_sampled: 55000\n",
      "    num_steps_trained: 55000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.61538461538463\n",
      "    ram_util_percent: 30.35\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037893566658054476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.616979321206532\n",
      "    mean_inference_ms: 2.1774959482424134\n",
      "    mean_raw_obs_processing_ms: 1.276675451503713\n",
      "  time_since_restore: 1229.333328485489\n",
      "  time_this_iter_s: 18.497766971588135\n",
      "  time_total_s: 1229.333328485489\n",
      "  timers:\n",
      "    learn_throughput: 1321.053\n",
      "    learn_time_ms: 756.972\n",
      "    load_throughput: 45849.46\n",
      "    load_time_ms: 21.811\n",
      "    sample_throughput: 52.74\n",
      "    sample_time_ms: 18960.928\n",
      "    update_time_ms: 3.689\n",
      "  timestamp: 1635283802\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55000\n",
      "  training_iteration: 55\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         1229.33</td><td style=\"text-align: right;\">55000</td><td style=\"text-align: right;\"> -4.4275</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            442.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-30-19\n",
      "  done: false\n",
      "  episode_len_mean: 444.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.179999999999976\n",
      "  episode_reward_mean: -4.445699999999949\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 128\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7658163415061103\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01390005483931718\n",
      "          policy_loss: -0.006005084349049462\n",
      "          total_loss: -0.010460357864697773\n",
      "          vf_explained_var: 0.3914368450641632\n",
      "          vf_loss: 0.010422875977949136\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.552\n",
      "    ram_util_percent: 30.54799999999999\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037861387821946445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.4420449267054\n",
      "    mean_inference_ms: 2.1753555179031268\n",
      "    mean_raw_obs_processing_ms: 1.3013043286167405\n",
      "  time_since_restore: 1246.6578624248505\n",
      "  time_this_iter_s: 17.324533939361572\n",
      "  time_total_s: 1246.6578624248505\n",
      "  timers:\n",
      "    learn_throughput: 1318.449\n",
      "    learn_time_ms: 758.467\n",
      "    load_throughput: 45825.014\n",
      "    load_time_ms: 21.822\n",
      "    sample_throughput: 52.552\n",
      "    sample_time_ms: 19028.629\n",
      "    update_time_ms: 3.686\n",
      "  timestamp: 1635283819\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 56\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         1246.66</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\"> -4.4457</td><td style=\"text-align: right;\">               -3.18</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            444.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 57000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-30-37\n",
      "  done: false\n",
      "  episode_len_mean: 446.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.6799999999999655\n",
      "  episode_reward_mean: -4.465699999999949\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 130\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8062674972746107\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010254856603281345\n",
      "          policy_loss: -0.10252917947040664\n",
      "          total_loss: -0.10879186491171519\n",
      "          vf_explained_var: 0.6146659255027771\n",
      "          vf_loss: 0.009749019357776787\n",
      "    num_agent_steps_sampled: 57000\n",
      "    num_agent_steps_trained: 57000\n",
      "    num_steps_sampled: 57000\n",
      "    num_steps_trained: 57000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.16153846153846\n",
      "    ram_util_percent: 30.511538461538464\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037832687659911456\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.278799651248125\n",
      "    mean_inference_ms: 2.1734137783720477\n",
      "    mean_raw_obs_processing_ms: 1.3116015971545558\n",
      "  time_since_restore: 1264.6499099731445\n",
      "  time_this_iter_s: 17.992047548294067\n",
      "  time_total_s: 1264.6499099731445\n",
      "  timers:\n",
      "    learn_throughput: 1317.875\n",
      "    learn_time_ms: 758.797\n",
      "    load_throughput: 47109.332\n",
      "    load_time_ms: 21.227\n",
      "    sample_throughput: 52.367\n",
      "    sample_time_ms: 19096.144\n",
      "    update_time_ms: 3.684\n",
      "  timestamp: 1635283837\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 57000\n",
      "  training_iteration: 57\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         1264.65</td><td style=\"text-align: right;\">57000</td><td style=\"text-align: right;\"> -4.4657</td><td style=\"text-align: right;\">               -3.68</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            446.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 58000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-30-56\n",
      "  done: false\n",
      "  episode_len_mean: 447.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.6799999999999655\n",
      "  episode_reward_mean: -4.470399999999949\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 133\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.843553822570377\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007938309941479895\n",
      "          policy_loss: 0.052460870312319864\n",
      "          total_loss: 0.046676982939243314\n",
      "          vf_explained_var: 0.4853614270687103\n",
      "          vf_loss: 0.011063986170726518\n",
      "    num_agent_steps_sampled: 58000\n",
      "    num_agent_steps_trained: 58000\n",
      "    num_steps_sampled: 58000\n",
      "    num_steps_trained: 58000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.1962962962963\n",
      "    ram_util_percent: 30.496296296296297\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037792712646329676\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.04747995918298\n",
      "    mean_inference_ms: 2.170688901291725\n",
      "    mean_raw_obs_processing_ms: 1.306577341073895\n",
      "  time_since_restore: 1283.7958011627197\n",
      "  time_this_iter_s: 19.145891189575195\n",
      "  time_total_s: 1283.7958011627197\n",
      "  timers:\n",
      "    learn_throughput: 1316.734\n",
      "    learn_time_ms: 759.455\n",
      "    load_throughput: 45016.448\n",
      "    load_time_ms: 22.214\n",
      "    sample_throughput: 51.925\n",
      "    sample_time_ms: 19258.535\n",
      "    update_time_ms: 3.683\n",
      "  timestamp: 1635283856\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 58000\n",
      "  training_iteration: 58\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">          1283.8</td><td style=\"text-align: right;\">58000</td><td style=\"text-align: right;\"> -4.4704</td><td style=\"text-align: right;\">               -3.68</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">             447.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 59000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-31-15\n",
      "  done: false\n",
      "  episode_len_mean: 447.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.6799999999999655\n",
      "  episode_reward_mean: -4.471599999999949\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 135\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7300720559226141\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009527904609263269\n",
      "          policy_loss: -0.04597685717874103\n",
      "          total_loss: -0.051005705694357556\n",
      "          vf_explained_var: 0.1508496105670929\n",
      "          vf_loss: 0.010366291606462456\n",
      "    num_agent_steps_sampled: 59000\n",
      "    num_agent_steps_trained: 59000\n",
      "    num_steps_sampled: 59000\n",
      "    num_steps_trained: 59000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.67777777777776\n",
      "    ram_util_percent: 30.522222222222222\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037769245674778525\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.905460862396826\n",
      "    mean_inference_ms: 2.1690649682740006\n",
      "    mean_raw_obs_processing_ms: 1.3050150731780257\n",
      "  time_since_restore: 1302.862485408783\n",
      "  time_this_iter_s: 19.066684246063232\n",
      "  time_total_s: 1302.862485408783\n",
      "  timers:\n",
      "    learn_throughput: 1308.868\n",
      "    learn_time_ms: 764.019\n",
      "    load_throughput: 45022.391\n",
      "    load_time_ms: 22.211\n",
      "    sample_throughput: 51.716\n",
      "    sample_time_ms: 19336.264\n",
      "    update_time_ms: 3.702\n",
      "  timestamp: 1635283875\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 59000\n",
      "  training_iteration: 59\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         1302.86</td><td style=\"text-align: right;\">59000</td><td style=\"text-align: right;\"> -4.4716</td><td style=\"text-align: right;\">               -3.68</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            447.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-31-35\n",
      "  done: false\n",
      "  episode_len_mean: 446.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.6799999999999655\n",
      "  episode_reward_mean: -4.46819999999995\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 138\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8225156678093803\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0122492504059147\n",
      "          policy_loss: 0.01615407872531149\n",
      "          total_loss: 0.010866000751654307\n",
      "          vf_explained_var: 0.2725166380405426\n",
      "          vf_loss: 0.010487227847463348\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.78965517241379\n",
      "    ram_util_percent: 30.47586206896552\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037736585075434516\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.704396820118728\n",
      "    mean_inference_ms: 2.1667819412411324\n",
      "    mean_raw_obs_processing_ms: 1.3041287928632814\n",
      "  time_since_restore: 1322.6617939472198\n",
      "  time_this_iter_s: 19.79930853843689\n",
      "  time_total_s: 1322.6617939472198\n",
      "  timers:\n",
      "    learn_throughput: 1309.986\n",
      "    learn_time_ms: 763.367\n",
      "    load_throughput: 44808.977\n",
      "    load_time_ms: 22.317\n",
      "    sample_throughput: 51.272\n",
      "    sample_time_ms: 19503.768\n",
      "    update_time_ms: 3.711\n",
      "  timestamp: 1635283895\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 60\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         1322.66</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\"> -4.4682</td><td style=\"text-align: right;\">               -3.68</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            446.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 61000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-31-56\n",
      "  done: false\n",
      "  episode_len_mean: 445.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.5899999999999674\n",
      "  episode_reward_mean: -4.4587999999999495\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 140\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7802946408589682\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009370971061002544\n",
      "          policy_loss: -0.10710880971617169\n",
      "          total_loss: -0.1126515453060468\n",
      "          vf_explained_var: 0.31177884340286255\n",
      "          vf_loss: 0.010386017905289514\n",
      "    num_agent_steps_sampled: 61000\n",
      "    num_agent_steps_trained: 61000\n",
      "    num_steps_sampled: 61000\n",
      "    num_steps_trained: 61000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.68275862068967\n",
      "    ram_util_percent: 30.42068965517241\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037715972604893455\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.580551734256005\n",
      "    mean_inference_ms: 2.165327916500067\n",
      "    mean_raw_obs_processing_ms: 1.3048162828242647\n",
      "  time_since_restore: 1343.5728697776794\n",
      "  time_this_iter_s: 20.911075830459595\n",
      "  time_total_s: 1343.5728697776794\n",
      "  timers:\n",
      "    learn_throughput: 1309.277\n",
      "    learn_time_ms: 763.78\n",
      "    load_throughput: 44891.563\n",
      "    load_time_ms: 22.276\n",
      "    sample_throughput: 51.01\n",
      "    sample_time_ms: 19603.829\n",
      "    update_time_ms: 3.66\n",
      "  timestamp: 1635283916\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 61000\n",
      "  training_iteration: 61\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         1343.57</td><td style=\"text-align: right;\">61000</td><td style=\"text-align: right;\"> -4.4588</td><td style=\"text-align: right;\">               -3.59</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            445.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 62000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-32-18\n",
      "  done: false\n",
      "  episode_len_mean: 444.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.3499999999999726\n",
      "  episode_reward_mean: -4.443399999999949\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 143\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.762476544910007\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006648992690264638\n",
      "          policy_loss: -0.07242587159077327\n",
      "          total_loss: -0.07690944116976527\n",
      "          vf_explained_var: 0.4290089011192322\n",
      "          vf_loss: 0.011811398311207692\n",
      "    num_agent_steps_sampled: 62000\n",
      "    num_agent_steps_trained: 62000\n",
      "    num_steps_sampled: 62000\n",
      "    num_steps_trained: 62000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.68125\n",
      "    ram_util_percent: 30.3875\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03768583070070056\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.404885433194192\n",
      "    mean_inference_ms: 2.1632050465514996\n",
      "    mean_raw_obs_processing_ms: 1.3068809018194438\n",
      "  time_since_restore: 1365.539179801941\n",
      "  time_this_iter_s: 21.966310024261475\n",
      "  time_total_s: 1365.539179801941\n",
      "  timers:\n",
      "    learn_throughput: 1311.034\n",
      "    learn_time_ms: 762.757\n",
      "    load_throughput: 45115.13\n",
      "    load_time_ms: 22.166\n",
      "    sample_throughput: 50.149\n",
      "    sample_time_ms: 19940.645\n",
      "    update_time_ms: 3.659\n",
      "  timestamp: 1635283938\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 62000\n",
      "  training_iteration: 62\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         1365.54</td><td style=\"text-align: right;\">62000</td><td style=\"text-align: right;\"> -4.4434</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            444.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 63000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-32-40\n",
      "  done: false\n",
      "  episode_len_mean: 442.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.3499999999999726\n",
      "  episode_reward_mean: -4.42159999999995\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 146\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8402175002627903\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012371580915246403\n",
      "          policy_loss: 0.03783352259132597\n",
      "          total_loss: 0.031407994031906125\n",
      "          vf_explained_var: 0.5410161018371582\n",
      "          vf_loss: 0.009502326576815297\n",
      "    num_agent_steps_sampled: 63000\n",
      "    num_agent_steps_trained: 63000\n",
      "    num_steps_sampled: 63000\n",
      "    num_steps_trained: 63000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.64516129032258\n",
      "    ram_util_percent: 30.3258064516129\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037654067029114245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.24271707723169\n",
      "    mean_inference_ms: 2.1611366549458246\n",
      "    mean_raw_obs_processing_ms: 1.3106262577228114\n",
      "  time_since_restore: 1387.4566688537598\n",
      "  time_this_iter_s: 21.917489051818848\n",
      "  time_total_s: 1387.4566688537598\n",
      "  timers:\n",
      "    learn_throughput: 1309.674\n",
      "    learn_time_ms: 763.549\n",
      "    load_throughput: 46129.88\n",
      "    load_time_ms: 21.678\n",
      "    sample_throughput: 53.616\n",
      "    sample_time_ms: 18651.019\n",
      "    update_time_ms: 3.572\n",
      "  timestamp: 1635283960\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 63000\n",
      "  training_iteration: 63\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         1387.46</td><td style=\"text-align: right;\">63000</td><td style=\"text-align: right;\"> -4.4216</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            442.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-33-01\n",
      "  done: false\n",
      "  episode_len_mean: 441.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.3499999999999726\n",
      "  episode_reward_mean: -4.41009999999995\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 149\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8923699259757996\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005534391766837743\n",
      "          policy_loss: 0.036619051463074155\n",
      "          total_loss: 0.028495388726393383\n",
      "          vf_explained_var: 0.5749627947807312\n",
      "          vf_loss: 0.009693158788528914\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.71666666666667\n",
      "    ram_util_percent: 30.28666666666667\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0376210874181062\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.089084305220666\n",
      "    mean_inference_ms: 2.1591001634539513\n",
      "    mean_raw_obs_processing_ms: 1.3153247504958188\n",
      "  time_since_restore: 1408.4495627880096\n",
      "  time_this_iter_s: 20.992893934249878\n",
      "  time_total_s: 1408.4495627880096\n",
      "  timers:\n",
      "    learn_throughput: 1311.428\n",
      "    learn_time_ms: 762.528\n",
      "    load_throughput: 46045.46\n",
      "    load_time_ms: 21.718\n",
      "    sample_throughput: 52.717\n",
      "    sample_time_ms: 18969.225\n",
      "    update_time_ms: 3.658\n",
      "  timestamp: 1635283981\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 64\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         1408.45</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\"> -4.4101</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            441.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 65000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-33-39\n",
      "  done: false\n",
      "  episode_len_mean: 439.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.3499999999999726\n",
      "  episode_reward_mean: -4.396099999999951\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 152\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8474612368477716\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00710539944170032\n",
      "          policy_loss: -0.00047149194611443413\n",
      "          total_loss: -0.007925534082783594\n",
      "          vf_explained_var: 0.5691792964935303\n",
      "          vf_loss: 0.009599492772637556\n",
      "    num_agent_steps_sampled: 65000\n",
      "    num_agent_steps_trained: 65000\n",
      "    num_steps_sampled: 65000\n",
      "    num_steps_trained: 65000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.49818181818181\n",
      "    ram_util_percent: 30.38727272727273\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03758876888554832\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.949105417391138\n",
      "    mean_inference_ms: 2.157181687778827\n",
      "    mean_raw_obs_processing_ms: 1.3296798309976086\n",
      "  time_since_restore: 1446.646291732788\n",
      "  time_this_iter_s: 38.19672894477844\n",
      "  time_total_s: 1446.646291732788\n",
      "  timers:\n",
      "    learn_throughput: 1319.013\n",
      "    learn_time_ms: 758.142\n",
      "    load_throughput: 45159.673\n",
      "    load_time_ms: 22.144\n",
      "    sample_throughput: 47.749\n",
      "    sample_time_ms: 20943.055\n",
      "    update_time_ms: 3.738\n",
      "  timestamp: 1635284019\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 65000\n",
      "  training_iteration: 65\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         1446.65</td><td style=\"text-align: right;\">65000</td><td style=\"text-align: right;\"> -4.3961</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            439.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 66000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-34-01\n",
      "  done: false\n",
      "  episode_len_mean: 438.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.3499999999999726\n",
      "  episode_reward_mean: -4.38279999999995\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 154\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8507510979970296\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008851822530548108\n",
      "          policy_loss: -0.09902804444233576\n",
      "          total_loss: -0.1070359233352873\n",
      "          vf_explained_var: 0.714235246181488\n",
      "          vf_loss: 0.008729269199021575\n",
      "    num_agent_steps_sampled: 66000\n",
      "    num_agent_steps_trained: 66000\n",
      "    num_steps_sampled: 66000\n",
      "    num_steps_trained: 66000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.61935483870967\n",
      "    ram_util_percent: 30.590322580645164\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037567593069524674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.859882070484673\n",
      "    mean_inference_ms: 2.1559377519034495\n",
      "    mean_raw_obs_processing_ms: 1.3393433699046295\n",
      "  time_since_restore: 1468.7215323448181\n",
      "  time_this_iter_s: 22.07524061203003\n",
      "  time_total_s: 1468.7215323448181\n",
      "  timers:\n",
      "    learn_throughput: 1320.974\n",
      "    learn_time_ms: 757.017\n",
      "    load_throughput: 44442.485\n",
      "    load_time_ms: 22.501\n",
      "    sample_throughput: 46.688\n",
      "    sample_time_ms: 21418.95\n",
      "    update_time_ms: 3.748\n",
      "  timestamp: 1635284041\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 66000\n",
      "  training_iteration: 66\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         1468.72</td><td style=\"text-align: right;\">66000</td><td style=\"text-align: right;\"> -4.3828</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            438.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 67000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-34-23\n",
      "  done: false\n",
      "  episode_len_mean: 436.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.3499999999999726\n",
      "  episode_reward_mean: -4.362999999999951\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 157\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8713811371061537\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007097136614666017\n",
      "          policy_loss: -0.09518406457371181\n",
      "          total_loss: -0.0995167581571473\n",
      "          vf_explained_var: 0.5549496412277222\n",
      "          vf_loss: 0.012961685988638135\n",
      "    num_agent_steps_sampled: 67000\n",
      "    num_agent_steps_trained: 67000\n",
      "    num_steps_sampled: 67000\n",
      "    num_steps_trained: 67000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.59354838709679\n",
      "    ram_util_percent: 30.39354838709677\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037537583150684645\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.736306050952045\n",
      "    mean_inference_ms: 2.154188525163241\n",
      "    mean_raw_obs_processing_ms: 1.354660887787883\n",
      "  time_since_restore: 1490.2498049736023\n",
      "  time_this_iter_s: 21.52827262878418\n",
      "  time_total_s: 1490.2498049736023\n",
      "  timers:\n",
      "    learn_throughput: 1320.621\n",
      "    learn_time_ms: 757.22\n",
      "    load_throughput: 43256.212\n",
      "    load_time_ms: 23.118\n",
      "    sample_throughput: 45.931\n",
      "    sample_time_ms: 21771.762\n",
      "    update_time_ms: 3.743\n",
      "  timestamp: 1635284063\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 67000\n",
      "  training_iteration: 67\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         1490.25</td><td style=\"text-align: right;\">67000</td><td style=\"text-align: right;\">  -4.363</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">             436.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-34-45\n",
      "  done: false\n",
      "  episode_len_mean: 434.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.3499999999999726\n",
      "  episode_reward_mean: -4.34979999999995\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 160\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8363312416606479\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009237936790694552\n",
      "          policy_loss: 0.06432487757669555\n",
      "          total_loss: 0.05681094394789802\n",
      "          vf_explained_var: 0.7156004905700684\n",
      "          vf_loss: 0.009001788270608005\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.51935483870967\n",
      "    ram_util_percent: 30.4258064516129\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03750830110375845\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.618701376873034\n",
      "    mean_inference_ms: 2.152495458495968\n",
      "    mean_raw_obs_processing_ms: 1.3639961979343442\n",
      "  time_since_restore: 1511.9128456115723\n",
      "  time_this_iter_s: 21.66304063796997\n",
      "  time_total_s: 1511.9128456115723\n",
      "  timers:\n",
      "    learn_throughput: 1319.215\n",
      "    learn_time_ms: 758.027\n",
      "    load_throughput: 43293.136\n",
      "    load_time_ms: 23.098\n",
      "    sample_throughput: 45.408\n",
      "    sample_time_ms: 22022.599\n",
      "    update_time_ms: 3.831\n",
      "  timestamp: 1635284085\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 68\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         1511.91</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\"> -4.3498</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            434.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 69000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-35-06\n",
      "  done: false\n",
      "  episode_len_mean: 433.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.3499999999999726\n",
      "  episode_reward_mean: -4.332099999999951\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 163\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7203373339441088\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008189478509752313\n",
      "          policy_loss: 0.0703718002471659\n",
      "          total_loss: 0.0639035277068615\n",
      "          vf_explained_var: 0.4515962302684784\n",
      "          vf_loss: 0.009097204393603736\n",
      "    num_agent_steps_sampled: 69000\n",
      "    num_agent_steps_trained: 69000\n",
      "    num_steps_sampled: 69000\n",
      "    num_steps_trained: 69000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.7741935483871\n",
      "    ram_util_percent: 30.503225806451617\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03748030263590656\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.507917570807027\n",
      "    mean_inference_ms: 2.150883813368815\n",
      "    mean_raw_obs_processing_ms: 1.3617154409971493\n",
      "  time_since_restore: 1533.7974677085876\n",
      "  time_this_iter_s: 21.88462209701538\n",
      "  time_total_s: 1533.7974677085876\n",
      "  timers:\n",
      "    learn_throughput: 1326.191\n",
      "    learn_time_ms: 754.039\n",
      "    load_throughput: 43345.707\n",
      "    load_time_ms: 23.07\n",
      "    sample_throughput: 44.826\n",
      "    sample_time_ms: 22308.352\n",
      "    update_time_ms: 3.905\n",
      "  timestamp: 1635284106\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 69000\n",
      "  training_iteration: 69\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">          1533.8</td><td style=\"text-align: right;\">69000</td><td style=\"text-align: right;\"> -4.3321</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            433.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 70000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-35-29\n",
      "  done: false\n",
      "  episode_len_mean: 431.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.2999999999999736\n",
      "  episode_reward_mean: -4.310199999999952\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 166\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.661296718650394\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007379668331528835\n",
      "          policy_loss: 0.09374218020174238\n",
      "          total_loss: 0.08677808824512694\n",
      "          vf_explained_var: 0.6114711165428162\n",
      "          vf_loss: 0.00817294276979131\n",
      "    num_agent_steps_sampled: 70000\n",
      "    num_agent_steps_trained: 70000\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.58787878787881\n",
      "    ram_util_percent: 30.518181818181816\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037453316778480114\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.403757642825358\n",
      "    mean_inference_ms: 2.1493218856853864\n",
      "    mean_raw_obs_processing_ms: 1.360394421664647\n",
      "  time_since_restore: 1556.4774522781372\n",
      "  time_this_iter_s: 22.67998456954956\n",
      "  time_total_s: 1556.4774522781372\n",
      "  timers:\n",
      "    learn_throughput: 1326.678\n",
      "    learn_time_ms: 753.762\n",
      "    load_throughput: 43025.96\n",
      "    load_time_ms: 23.242\n",
      "    sample_throughput: 44.255\n",
      "    sample_time_ms: 22596.51\n",
      "    update_time_ms: 3.899\n",
      "  timestamp: 1635284129\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 70\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         1556.48</td><td style=\"text-align: right;\">70000</td><td style=\"text-align: right;\"> -4.3102</td><td style=\"text-align: right;\">                -3.3</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            431.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 71000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-35-53\n",
      "  done: false\n",
      "  episode_len_mean: 427.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.2199999999999753\n",
      "  episode_reward_mean: -4.270299999999953\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 169\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6293457905451456\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004354721471694549\n",
      "          policy_loss: 0.09049575626850129\n",
      "          total_loss: 0.08432617973950174\n",
      "          vf_explained_var: 0.5349217057228088\n",
      "          vf_loss: 0.009252936612918145\n",
      "    num_agent_steps_sampled: 71000\n",
      "    num_agent_steps_trained: 71000\n",
      "    num_steps_sampled: 71000\n",
      "    num_steps_trained: 71000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.82121212121211\n",
      "    ram_util_percent: 30.506060606060608\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03742881967832367\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.312762086785998\n",
      "    mean_inference_ms: 2.1479090946859145\n",
      "    mean_raw_obs_processing_ms: 1.3608453347170637\n",
      "  time_since_restore: 1579.9037771224976\n",
      "  time_this_iter_s: 23.42632484436035\n",
      "  time_total_s: 1579.9037771224976\n",
      "  timers:\n",
      "    learn_throughput: 1328.588\n",
      "    learn_time_ms: 752.679\n",
      "    load_throughput: 42845.67\n",
      "    load_time_ms: 23.34\n",
      "    sample_throughput: 43.766\n",
      "    sample_time_ms: 22849.034\n",
      "    update_time_ms: 3.896\n",
      "  timestamp: 1635284153\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 71000\n",
      "  training_iteration: 71\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">          1579.9</td><td style=\"text-align: right;\">71000</td><td style=\"text-align: right;\"> -4.2703</td><td style=\"text-align: right;\">               -3.22</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            427.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-36-15\n",
      "  done: false\n",
      "  episode_len_mean: 423.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.2199999999999753\n",
      "  episode_reward_mean: -4.236299999999954\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 172\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.671098158094618\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007211106322282785\n",
      "          policy_loss: 0.0702626496553421\n",
      "          total_loss: 0.06482720954550637\n",
      "          vf_explained_var: 0.48482686281204224\n",
      "          vf_loss: 0.01055443132063374\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.64375\n",
      "    ram_util_percent: 30.484375\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03740585095772442\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.2305532174875\n",
      "    mean_inference_ms: 2.1465903330535454\n",
      "    mean_raw_obs_processing_ms: 1.362416216788432\n",
      "  time_since_restore: 1602.6528158187866\n",
      "  time_this_iter_s: 22.749038696289062\n",
      "  time_total_s: 1602.6528158187866\n",
      "  timers:\n",
      "    learn_throughput: 1326.345\n",
      "    learn_time_ms: 753.952\n",
      "    load_throughput: 42683.731\n",
      "    load_time_ms: 23.428\n",
      "    sample_throughput: 43.619\n",
      "    sample_time_ms: 22925.977\n",
      "    update_time_ms: 3.892\n",
      "  timestamp: 1635284175\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 72\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         1602.65</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\"> -4.2363</td><td style=\"text-align: right;\">               -3.22</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            423.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 73000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-36-37\n",
      "  done: false\n",
      "  episode_len_mean: 421.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.2199999999999753\n",
      "  episode_reward_mean: -4.211899999999954\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 175\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.691452243593004\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008110434984915413\n",
      "          policy_loss: 0.04038001596927643\n",
      "          total_loss: 0.03286116868257523\n",
      "          vf_explained_var: 0.4983292818069458\n",
      "          vf_loss: 0.008584630594769906\n",
      "    num_agent_steps_sampled: 73000\n",
      "    num_agent_steps_trained: 73000\n",
      "    num_steps_sampled: 73000\n",
      "    num_steps_trained: 73000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.58709677419354\n",
      "    ram_util_percent: 30.390322580645154\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037382258373515\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.152865271216328\n",
      "    mean_inference_ms: 2.1452733510291138\n",
      "    mean_raw_obs_processing_ms: 1.36461174414493\n",
      "  time_since_restore: 1624.1316208839417\n",
      "  time_this_iter_s: 21.47880506515503\n",
      "  time_total_s: 1624.1316208839417\n",
      "  timers:\n",
      "    learn_throughput: 1327.324\n",
      "    learn_time_ms: 753.396\n",
      "    load_throughput: 41721.167\n",
      "    load_time_ms: 23.969\n",
      "    sample_throughput: 43.702\n",
      "    sample_time_ms: 22882.138\n",
      "    update_time_ms: 3.881\n",
      "  timestamp: 1635284197\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 73000\n",
      "  training_iteration: 73\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         1624.13</td><td style=\"text-align: right;\">73000</td><td style=\"text-align: right;\"> -4.2119</td><td style=\"text-align: right;\">               -3.22</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            421.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 74000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-36-59\n",
      "  done: false\n",
      "  episode_len_mean: 417.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.1699999999999764\n",
      "  episode_reward_mean: -4.172099999999955\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 178\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7065786507394578\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006431352449793732\n",
      "          policy_loss: 0.0033643109930886162\n",
      "          total_loss: -0.0023439201215902966\n",
      "          vf_explained_var: 0.46507126092910767\n",
      "          vf_loss: 0.010714418510906399\n",
      "    num_agent_steps_sampled: 74000\n",
      "    num_agent_steps_trained: 74000\n",
      "    num_steps_sampled: 74000\n",
      "    num_steps_trained: 74000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.7875\n",
      "    ram_util_percent: 30.340625\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0373591638535481\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.084017557085414\n",
      "    mean_inference_ms: 2.144012351499932\n",
      "    mean_raw_obs_processing_ms: 1.3680450563258164\n",
      "  time_since_restore: 1646.6479744911194\n",
      "  time_this_iter_s: 22.516353607177734\n",
      "  time_total_s: 1646.6479744911194\n",
      "  timers:\n",
      "    learn_throughput: 1327.591\n",
      "    learn_time_ms: 753.244\n",
      "    load_throughput: 41672.626\n",
      "    load_time_ms: 23.997\n",
      "    sample_throughput: 43.413\n",
      "    sample_time_ms: 23034.698\n",
      "    update_time_ms: 3.788\n",
      "  timestamp: 1635284219\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 74000\n",
      "  training_iteration: 74\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         1646.65</td><td style=\"text-align: right;\">74000</td><td style=\"text-align: right;\"> -4.1721</td><td style=\"text-align: right;\">               -3.17</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            417.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 75000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-37-41\n",
      "  done: false\n",
      "  episode_len_mean: 412.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.126099999999956\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 181\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6395066910319858\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014735081032167823\n",
      "          policy_loss: 0.005385351760519875\n",
      "          total_loss: 0.0008498146302170224\n",
      "          vf_explained_var: 0.5117055177688599\n",
      "          vf_loss: 0.010386021898981804\n",
      "    num_agent_steps_sampled: 75000\n",
      "    num_agent_steps_trained: 75000\n",
      "    num_steps_sampled: 75000\n",
      "    num_steps_trained: 75000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.25254237288135\n",
      "    ram_util_percent: 30.36271186440678\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03733676316087759\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.022917828217345\n",
      "    mean_inference_ms: 2.1428063421207844\n",
      "    mean_raw_obs_processing_ms: 1.3791081694582241\n",
      "  time_since_restore: 1687.9042644500732\n",
      "  time_this_iter_s: 41.25628995895386\n",
      "  time_total_s: 1687.9042644500732\n",
      "  timers:\n",
      "    learn_throughput: 1329.16\n",
      "    learn_time_ms: 752.355\n",
      "    load_throughput: 41669.273\n",
      "    load_time_ms: 23.998\n",
      "    sample_throughput: 42.842\n",
      "    sample_time_ms: 23341.539\n",
      "    update_time_ms: 3.77\n",
      "  timestamp: 1635284261\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 75000\n",
      "  training_iteration: 75\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">          1687.9</td><td style=\"text-align: right;\">75000</td><td style=\"text-align: right;\"> -4.1261</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            412.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-38-04\n",
      "  done: false\n",
      "  episode_len_mean: 408.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.085499999999956\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 184\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.652253630426195\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006133855740260764\n",
      "          policy_loss: 0.007081737948788537\n",
      "          total_loss: 0.0013834137055608962\n",
      "          vf_explained_var: 0.5534420609474182\n",
      "          vf_loss: 0.010210827644914388\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.07352941176471\n",
      "    ram_util_percent: 30.91176470588235\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03731597634903586\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.971041754756712\n",
      "    mean_inference_ms: 2.1416966205669596\n",
      "    mean_raw_obs_processing_ms: 1.3910686955583913\n",
      "  time_since_restore: 1711.3211269378662\n",
      "  time_this_iter_s: 23.41686248779297\n",
      "  time_total_s: 1711.3211269378662\n",
      "  timers:\n",
      "    learn_throughput: 1330.675\n",
      "    learn_time_ms: 751.498\n",
      "    load_throughput: 41280.569\n",
      "    load_time_ms: 24.224\n",
      "    sample_throughput: 42.596\n",
      "    sample_time_ms: 23476.276\n",
      "    update_time_ms: 3.822\n",
      "  timestamp: 1635284284\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 76\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         1711.32</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\"> -4.0855</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            408.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 77000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-38-27\n",
      "  done: false\n",
      "  episode_len_mean: 403.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.037899999999958\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 187\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.766915406121148\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013797709221348795\n",
      "          policy_loss: 0.0034705998169051277\n",
      "          total_loss: -0.002381066315703922\n",
      "          vf_explained_var: 0.48694202303886414\n",
      "          vf_loss: 0.010437718560246544\n",
      "    num_agent_steps_sampled: 77000\n",
      "    num_agent_steps_trained: 77000\n",
      "    num_steps_sampled: 77000\n",
      "    num_steps_trained: 77000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.21818181818183\n",
      "    ram_util_percent: 30.80909090909091\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03729635530585517\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.926547816136313\n",
      "    mean_inference_ms: 2.1406508422604382\n",
      "    mean_raw_obs_processing_ms: 1.4035610870263253\n",
      "  time_since_restore: 1734.7002522945404\n",
      "  time_this_iter_s: 23.379125356674194\n",
      "  time_total_s: 1734.7002522945404\n",
      "  timers:\n",
      "    learn_throughput: 1326.303\n",
      "    learn_time_ms: 753.976\n",
      "    load_throughput: 41422.279\n",
      "    load_time_ms: 24.142\n",
      "    sample_throughput: 42.267\n",
      "    sample_time_ms: 23658.935\n",
      "    update_time_ms: 3.831\n",
      "  timestamp: 1635284307\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 77000\n",
      "  training_iteration: 77\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">          1734.7</td><td style=\"text-align: right;\">77000</td><td style=\"text-align: right;\"> -4.0379</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            403.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 78000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-38-52\n",
      "  done: false\n",
      "  episode_len_mean: 399.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -3.9990999999999586\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 190\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8193794237242804\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017188576138266538\n",
      "          policy_loss: 0.0020415061049991186\n",
      "          total_loss: -0.004661820166640811\n",
      "          vf_explained_var: 0.5623418688774109\n",
      "          vf_loss: 0.009771610484717207\n",
      "    num_agent_steps_sampled: 78000\n",
      "    num_agent_steps_trained: 78000\n",
      "    num_steps_sampled: 78000\n",
      "    num_steps_trained: 78000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.7470588235294\n",
      "    ram_util_percent: 30.53529411764706\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03727796912324528\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.889944692424017\n",
      "    mean_inference_ms: 2.139661857932798\n",
      "    mean_raw_obs_processing_ms: 1.412320513602555\n",
      "  time_since_restore: 1758.774832725525\n",
      "  time_this_iter_s: 24.074580430984497\n",
      "  time_total_s: 1758.774832725525\n",
      "  timers:\n",
      "    learn_throughput: 1319.212\n",
      "    learn_time_ms: 758.028\n",
      "    load_throughput: 41124.377\n",
      "    load_time_ms: 24.316\n",
      "    sample_throughput: 41.848\n",
      "    sample_time_ms: 23895.836\n",
      "    update_time_ms: 3.839\n",
      "  timestamp: 1635284332\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 78000\n",
      "  training_iteration: 78\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         1758.77</td><td style=\"text-align: right;\">78000</td><td style=\"text-align: right;\"> -3.9991</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            399.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 79000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-39-16\n",
      "  done: false\n",
      "  episode_len_mean: 395.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -3.9532999999999596\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 193\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8418777704238891\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013883553774391672\n",
      "          policy_loss: 0.023801643153031668\n",
      "          total_loss: 0.015555709103743235\n",
      "          vf_explained_var: 0.6619348526000977\n",
      "          vf_loss: 0.008784491894766688\n",
      "    num_agent_steps_sampled: 79000\n",
      "    num_agent_steps_trained: 79000\n",
      "    num_steps_sampled: 79000\n",
      "    num_steps_trained: 79000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.58285714285716\n",
      "    ram_util_percent: 30.52857142857142\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03726063548919785\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.85765636439498\n",
      "    mean_inference_ms: 2.1387162092774634\n",
      "    mean_raw_obs_processing_ms: 1.4124956213119966\n",
      "  time_since_restore: 1782.8181202411652\n",
      "  time_this_iter_s: 24.04328751564026\n",
      "  time_total_s: 1782.8181202411652\n",
      "  timers:\n",
      "    learn_throughput: 1316.243\n",
      "    learn_time_ms: 759.738\n",
      "    load_throughput: 41126.837\n",
      "    load_time_ms: 24.315\n",
      "    sample_throughput: 41.476\n",
      "    sample_time_ms: 24110.062\n",
      "    update_time_ms: 3.758\n",
      "  timestamp: 1635284356\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79000\n",
      "  training_iteration: 79\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         1782.82</td><td style=\"text-align: right;\">79000</td><td style=\"text-align: right;\"> -3.9533</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            395.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-39-40\n",
      "  done: false\n",
      "  episode_len_mean: 391.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.049999999999979\n",
      "  episode_reward_mean: -3.9133999999999594\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 196\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0041314244270323\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014398503332032912\n",
      "          policy_loss: -0.01669880814022488\n",
      "          total_loss: -0.02842439826991823\n",
      "          vf_explained_var: 0.7394914627075195\n",
      "          vf_loss: 0.006875870850894393\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.7\n",
      "    ram_util_percent: 30.552941176470586\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037244313622345995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.831048548703404\n",
      "    mean_inference_ms: 2.137822582008268\n",
      "    mean_raw_obs_processing_ms: 1.4134663515735173\n",
      "  time_since_restore: 1806.8503160476685\n",
      "  time_this_iter_s: 24.032195806503296\n",
      "  time_total_s: 1806.8503160476685\n",
      "  timers:\n",
      "    learn_throughput: 1313.254\n",
      "    learn_time_ms: 761.468\n",
      "    load_throughput: 41565.089\n",
      "    load_time_ms: 24.059\n",
      "    sample_throughput: 41.248\n",
      "    sample_time_ms: 24243.809\n",
      "    update_time_ms: 3.756\n",
      "  timestamp: 1635284380\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 80\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         1806.85</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> -3.9134</td><td style=\"text-align: right;\">               -3.05</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            391.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 81000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-40-00\n",
      "  done: false\n",
      "  episode_len_mean: 389.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.049999999999979\n",
      "  episode_reward_mean: -3.8929999999999603\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 198\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2880812366803487\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016335131629321837\n",
      "          policy_loss: -0.06168805476691988\n",
      "          total_loss: -0.07700790647003386\n",
      "          vf_explained_var: 0.6165409088134766\n",
      "          vf_loss: 0.005927448108074411\n",
      "    num_agent_steps_sampled: 81000\n",
      "    num_agent_steps_trained: 81000\n",
      "    num_steps_sampled: 81000\n",
      "    num_steps_trained: 81000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.57333333333332\n",
      "    ram_util_percent: 30.52333333333333\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03723410738843982\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.81614905295314\n",
      "    mean_inference_ms: 2.1372651966323697\n",
      "    mean_raw_obs_processing_ms: 1.414608363068732\n",
      "  time_since_restore: 1827.5762617588043\n",
      "  time_this_iter_s: 20.725945711135864\n",
      "  time_total_s: 1827.5762617588043\n",
      "  timers:\n",
      "    learn_throughput: 1311.899\n",
      "    learn_time_ms: 762.254\n",
      "    load_throughput: 41598.562\n",
      "    load_time_ms: 24.039\n",
      "    sample_throughput: 41.714\n",
      "    sample_time_ms: 23973.009\n",
      "    update_time_ms: 3.762\n",
      "  timestamp: 1635284400\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 81000\n",
      "  training_iteration: 81\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         1827.58</td><td style=\"text-align: right;\">81000</td><td style=\"text-align: right;\">  -3.893</td><td style=\"text-align: right;\">               -3.05</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">             389.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 82000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-40-23\n",
      "  done: false\n",
      "  episode_len_mean: 385.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.049999999999979\n",
      "  episode_reward_mean: -3.851499999999961\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 201\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9400030122862921\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00988971363661943\n",
      "          policy_loss: 0.02242550775408745\n",
      "          total_loss: 0.009146460311280356\n",
      "          vf_explained_var: 0.8797903060913086\n",
      "          vf_loss: 0.0051320130419400005\n",
      "    num_agent_steps_sampled: 82000\n",
      "    num_agent_steps_trained: 82000\n",
      "    num_steps_sampled: 82000\n",
      "    num_steps_trained: 82000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.03750000000001\n",
      "    ram_util_percent: 30.546875\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03721973918353799\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.799250555034277\n",
      "    mean_inference_ms: 2.136485301057172\n",
      "    mean_raw_obs_processing_ms: 1.4169923259961885\n",
      "  time_since_restore: 1850.1444985866547\n",
      "  time_this_iter_s: 22.568236827850342\n",
      "  time_total_s: 1850.1444985866547\n",
      "  timers:\n",
      "    learn_throughput: 1309.708\n",
      "    learn_time_ms: 763.529\n",
      "    load_throughput: 41711.292\n",
      "    load_time_ms: 23.974\n",
      "    sample_throughput: 41.747\n",
      "    sample_time_ms: 23953.705\n",
      "    update_time_ms: 3.767\n",
      "  timestamp: 1635284423\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 82000\n",
      "  training_iteration: 82\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         1850.14</td><td style=\"text-align: right;\">82000</td><td style=\"text-align: right;\"> -3.8515</td><td style=\"text-align: right;\">               -3.05</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            385.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 83000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-40-48\n",
      "  done: false\n",
      "  episode_len_mean: 380.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.049999999999979\n",
      "  episode_reward_mean: -3.8073999999999626\n",
      "  episode_reward_min: -5.409999999999929\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 204\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7682000146971808\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03167289332838325\n",
      "          policy_loss: 0.04564874743421873\n",
      "          total_loss: 0.03649738075004683\n",
      "          vf_explained_var: 0.8937360048294067\n",
      "          vf_loss: 0.005363342875433672\n",
      "    num_agent_steps_sampled: 83000\n",
      "    num_agent_steps_trained: 83000\n",
      "    num_steps_sampled: 83000\n",
      "    num_steps_trained: 83000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.47999999999999\n",
      "    ram_util_percent: 30.56571428571429\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03720651799591189\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.788047574238366\n",
      "    mean_inference_ms: 2.1357667380846603\n",
      "    mean_raw_obs_processing_ms: 1.4199958214659876\n",
      "  time_since_restore: 1874.8660390377045\n",
      "  time_this_iter_s: 24.721540451049805\n",
      "  time_total_s: 1874.8660390377045\n",
      "  timers:\n",
      "    learn_throughput: 1307.315\n",
      "    learn_time_ms: 764.926\n",
      "    load_throughput: 42124.428\n",
      "    load_time_ms: 23.739\n",
      "    sample_throughput: 41.192\n",
      "    sample_time_ms: 24276.79\n",
      "    update_time_ms: 3.789\n",
      "  timestamp: 1635284448\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 83000\n",
      "  training_iteration: 83\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         1874.87</td><td style=\"text-align: right;\">83000</td><td style=\"text-align: right;\"> -3.8074</td><td style=\"text-align: right;\">               -3.05</td><td style=\"text-align: right;\">               -5.41</td><td style=\"text-align: right;\">            380.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-41-12\n",
      "  done: false\n",
      "  episode_len_mean: 375.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.049999999999979\n",
      "  episode_reward_mean: -3.757699999999964\n",
      "  episode_reward_min: -5.209999999999933\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 207\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7788416531350877\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008074912100920213\n",
      "          policy_loss: 0.06775343120098114\n",
      "          total_loss: 0.05634134262800217\n",
      "          vf_explained_var: 0.8978763818740845\n",
      "          vf_loss: 0.005165088304784149\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.31470588235295\n",
      "    ram_util_percent: 30.508823529411764\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03719408234596199\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.782745271725645\n",
      "    mean_inference_ms: 2.135101613786048\n",
      "    mean_raw_obs_processing_ms: 1.423807583836945\n",
      "  time_since_restore: 1898.716323852539\n",
      "  time_this_iter_s: 23.850284814834595\n",
      "  time_total_s: 1898.716323852539\n",
      "  timers:\n",
      "    learn_throughput: 1305.214\n",
      "    learn_time_ms: 766.158\n",
      "    load_throughput: 42140.087\n",
      "    load_time_ms: 23.73\n",
      "    sample_throughput: 40.969\n",
      "    sample_time_ms: 24408.952\n",
      "    update_time_ms: 3.789\n",
      "  timestamp: 1635284472\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 84\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         1898.72</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\"> -3.7577</td><td style=\"text-align: right;\">               -3.05</td><td style=\"text-align: right;\">               -5.21</td><td style=\"text-align: right;\">            375.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 85000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-41-54\n",
      "  done: false\n",
      "  episode_len_mean: 370.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.049999999999979\n",
      "  episode_reward_mean: -3.7067999999999652\n",
      "  episode_reward_min: -5.189999999999934\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 210\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7388608124521043\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009180262836580634\n",
      "          policy_loss: -0.10756052293711238\n",
      "          total_loss: -0.11585457035236889\n",
      "          vf_explained_var: 0.8316570520401001\n",
      "          vf_loss: 0.00771752144727442\n",
      "    num_agent_steps_sampled: 85000\n",
      "    num_agent_steps_trained: 85000\n",
      "    num_steps_sampled: 85000\n",
      "    num_steps_trained: 85000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.81967213114754\n",
      "    ram_util_percent: 30.62295081967213\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03718245100839572\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.781951193792086\n",
      "    mean_inference_ms: 2.1344776604906457\n",
      "    mean_raw_obs_processing_ms: 1.4343498939327803\n",
      "  time_since_restore: 1940.9250514507294\n",
      "  time_this_iter_s: 42.20872759819031\n",
      "  time_total_s: 1940.9250514507294\n",
      "  timers:\n",
      "    learn_throughput: 1301.8\n",
      "    learn_time_ms: 768.167\n",
      "    load_throughput: 43891.332\n",
      "    load_time_ms: 22.784\n",
      "    sample_throughput: 40.811\n",
      "    sample_time_ms: 24503.173\n",
      "    update_time_ms: 3.73\n",
      "  timestamp: 1635284514\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 85000\n",
      "  training_iteration: 85\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         1940.93</td><td style=\"text-align: right;\">85000</td><td style=\"text-align: right;\"> -3.7068</td><td style=\"text-align: right;\">               -3.05</td><td style=\"text-align: right;\">               -5.19</td><td style=\"text-align: right;\">            370.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 86000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-42-19\n",
      "  done: false\n",
      "  episode_len_mean: 364.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.643899999999966\n",
      "  episode_reward_min: -5.189999999999934\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 214\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.626993915769789\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008346419874965644\n",
      "          policy_loss: -0.06275139815277524\n",
      "          total_loss: -0.07085220714410147\n",
      "          vf_explained_var: 0.8280659914016724\n",
      "          vf_loss: 0.006917165178391669\n",
      "    num_agent_steps_sampled: 86000\n",
      "    num_agent_steps_trained: 86000\n",
      "    num_steps_sampled: 86000\n",
      "    num_steps_trained: 86000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.96000000000001\n",
      "    ram_util_percent: 30.911428571428573\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03716827414945295\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.78733381162425\n",
      "    mean_inference_ms: 2.133715326381702\n",
      "    mean_raw_obs_processing_ms: 1.4493558944373002\n",
      "  time_since_restore: 1965.8609056472778\n",
      "  time_this_iter_s: 24.935854196548462\n",
      "  time_total_s: 1965.8609056472778\n",
      "  timers:\n",
      "    learn_throughput: 1300.424\n",
      "    learn_time_ms: 768.98\n",
      "    load_throughput: 44156.611\n",
      "    load_time_ms: 22.647\n",
      "    sample_throughput: 40.561\n",
      "    sample_time_ms: 24654.362\n",
      "    update_time_ms: 3.754\n",
      "  timestamp: 1635284539\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 86000\n",
      "  training_iteration: 86\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         1965.86</td><td style=\"text-align: right;\">86000</td><td style=\"text-align: right;\"> -3.6439</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -5.19</td><td style=\"text-align: right;\">            364.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 87000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-42-43\n",
      "  done: false\n",
      "  episode_len_mean: 360.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.604699999999967\n",
      "  episode_reward_min: -5.189999999999934\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 217\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5788951873779298\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018548554102873125\n",
      "          policy_loss: 0.06868136417534616\n",
      "          total_loss: 0.06159850913617346\n",
      "          vf_explained_var: 0.8296195864677429\n",
      "          vf_loss: 0.005923814710048545\n",
      "    num_agent_steps_sampled: 87000\n",
      "    num_agent_steps_trained: 87000\n",
      "    num_steps_sampled: 87000\n",
      "    num_steps_trained: 87000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.56470588235295\n",
      "    ram_util_percent: 30.6\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03715809669079145\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.793664971408262\n",
      "    mean_inference_ms: 2.133170785984466\n",
      "    mean_raw_obs_processing_ms: 1.460843551974598\n",
      "  time_since_restore: 1989.7442553043365\n",
      "  time_this_iter_s: 23.883349657058716\n",
      "  time_total_s: 1989.7442553043365\n",
      "  timers:\n",
      "    learn_throughput: 1307.815\n",
      "    learn_time_ms: 764.634\n",
      "    load_throughput: 43796.234\n",
      "    load_time_ms: 22.833\n",
      "    sample_throughput: 40.471\n",
      "    sample_time_ms: 24708.967\n",
      "    update_time_ms: 3.755\n",
      "  timestamp: 1635284563\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 87000\n",
      "  training_iteration: 87\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         1989.74</td><td style=\"text-align: right;\">87000</td><td style=\"text-align: right;\"> -3.6047</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -5.19</td><td style=\"text-align: right;\">            360.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-43-05\n",
      "  done: false\n",
      "  episode_len_mean: 358.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.597599999999967\n",
      "  episode_reward_min: -5.189999999999934\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 220\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7634786155488755\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018879931254739725\n",
      "          policy_loss: -0.022022899902529185\n",
      "          total_loss: -0.008015985207425223\n",
      "          vf_explained_var: 0.6192309856414795\n",
      "          vf_loss: 0.028809713204908702\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.021875\n",
      "    ram_util_percent: 30.653125000000003\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03714874160815\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.802590562786214\n",
      "    mean_inference_ms: 2.132672485011757\n",
      "    mean_raw_obs_processing_ms: 1.4695137256049284\n",
      "  time_since_restore: 2011.6559853553772\n",
      "  time_this_iter_s: 21.91173005104065\n",
      "  time_total_s: 2011.6559853553772\n",
      "  timers:\n",
      "    learn_throughput: 1313.715\n",
      "    learn_time_ms: 761.2\n",
      "    load_throughput: 44158.331\n",
      "    load_time_ms: 22.646\n",
      "    sample_throughput: 40.822\n",
      "    sample_time_ms: 24496.384\n",
      "    update_time_ms: 3.653\n",
      "  timestamp: 1635284585\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 88\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         2011.66</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\"> -3.5976</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -5.19</td><td style=\"text-align: right;\">             358.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 89000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-43-28\n",
      "  done: false\n",
      "  episode_len_mean: 354.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.5594999999999675\n",
      "  episode_reward_min: -5.189999999999934\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 223\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7440049118465848\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010101527819164652\n",
      "          policy_loss: -0.030095893310176\n",
      "          total_loss: -0.03881876137521532\n",
      "          vf_explained_var: 0.7290900349617004\n",
      "          vf_loss: 0.007201952205246521\n",
      "    num_agent_steps_sampled: 89000\n",
      "    num_agent_steps_trained: 89000\n",
      "    num_steps_sampled: 89000\n",
      "    num_steps_trained: 89000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.74545454545455\n",
      "    ram_util_percent: 30.67575757575758\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03714013141553982\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.814725767003043\n",
      "    mean_inference_ms: 2.132215690979416\n",
      "    mean_raw_obs_processing_ms: 1.4719217159051095\n",
      "  time_since_restore: 2034.7274289131165\n",
      "  time_this_iter_s: 23.071443557739258\n",
      "  time_total_s: 2034.7274289131165\n",
      "  timers:\n",
      "    learn_throughput: 1314.552\n",
      "    learn_time_ms: 760.716\n",
      "    load_throughput: 46107.416\n",
      "    load_time_ms: 21.688\n",
      "    sample_throughput: 40.983\n",
      "    sample_time_ms: 24400.632\n",
      "    update_time_ms: 3.644\n",
      "  timestamp: 1635284608\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 89000\n",
      "  training_iteration: 89\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         2034.73</td><td style=\"text-align: right;\">89000</td><td style=\"text-align: right;\"> -3.5595</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -5.19</td><td style=\"text-align: right;\">            354.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 90000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-43-51\n",
      "  done: false\n",
      "  episode_len_mean: 351.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.5262999999999693\n",
      "  episode_reward_min: -5.189999999999934\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 226\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.727266538143158\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010966902334029546\n",
      "          policy_loss: 0.004533941547075908\n",
      "          total_loss: -0.004997111939721638\n",
      "          vf_explained_var: 0.6062621474266052\n",
      "          vf_loss: 0.006096576692976264\n",
      "    num_agent_steps_sampled: 90000\n",
      "    num_agent_steps_trained: 90000\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.53030303030302\n",
      "    ram_util_percent: 30.78787878787879\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03713212634075663\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.82906075794331\n",
      "    mean_inference_ms: 2.1317903998274765\n",
      "    mean_raw_obs_processing_ms: 1.474584644120506\n",
      "  time_since_restore: 2058.2791154384613\n",
      "  time_this_iter_s: 23.55168652534485\n",
      "  time_total_s: 2058.2791154384613\n",
      "  timers:\n",
      "    learn_throughput: 1316.578\n",
      "    learn_time_ms: 759.545\n",
      "    load_throughput: 45801.695\n",
      "    load_time_ms: 21.833\n",
      "    sample_throughput: 41.062\n",
      "    sample_time_ms: 24353.569\n",
      "    update_time_ms: 3.694\n",
      "  timestamp: 1635284631\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 90\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         2058.28</td><td style=\"text-align: right;\">90000</td><td style=\"text-align: right;\"> -3.5263</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -5.19</td><td style=\"text-align: right;\">            351.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 91000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-44-14\n",
      "  done: false\n",
      "  episode_len_mean: 347.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.48699999999997\n",
      "  episode_reward_min: -5.099999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 229\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8480321685473124\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013535840348921389\n",
      "          policy_loss: 0.0017371005482143826\n",
      "          total_loss: -0.0028262033230728573\n",
      "          vf_explained_var: 0.3054497539997101\n",
      "          vf_loss: 0.011886638636416239\n",
      "    num_agent_steps_sampled: 91000\n",
      "    num_agent_steps_trained: 91000\n",
      "    num_steps_sampled: 91000\n",
      "    num_steps_trained: 91000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.01818181818182\n",
      "    ram_util_percent: 30.809090909090912\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03712450523284433\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.846809549303646\n",
      "    mean_inference_ms: 2.13139266203399\n",
      "    mean_raw_obs_processing_ms: 1.4779352410683564\n",
      "  time_since_restore: 2081.3252885341644\n",
      "  time_this_iter_s: 23.046173095703125\n",
      "  time_total_s: 2081.3252885341644\n",
      "  timers:\n",
      "    learn_throughput: 1318.497\n",
      "    learn_time_ms: 758.439\n",
      "    load_throughput: 48079.306\n",
      "    load_time_ms: 20.799\n",
      "    sample_throughput: 40.671\n",
      "    sample_time_ms: 24587.731\n",
      "    update_time_ms: 3.682\n",
      "  timestamp: 1635284654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 91000\n",
      "  training_iteration: 91\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         2081.33</td><td style=\"text-align: right;\">91000</td><td style=\"text-align: right;\">  -3.487</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">                -5.1</td><td style=\"text-align: right;\">            347.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-44-38\n",
      "  done: false\n",
      "  episode_len_mean: 344.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.4562999999999704\n",
      "  episode_reward_min: -5.099999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 232\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6975093245506288\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015695808084929572\n",
      "          policy_loss: 0.015960905535353554\n",
      "          total_loss: 0.011365241474575467\n",
      "          vf_explained_var: 0.382171094417572\n",
      "          vf_loss: 0.010025058528279058\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.33235294117647\n",
      "    ram_util_percent: 30.81764705882353\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037117335331143454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.866910684769987\n",
      "    mean_inference_ms: 2.131028057358287\n",
      "    mean_raw_obs_processing_ms: 1.481698779303194\n",
      "  time_since_restore: 2105.194794178009\n",
      "  time_this_iter_s: 23.869505643844604\n",
      "  time_total_s: 2105.194794178009\n",
      "  timers:\n",
      "    learn_throughput: 1322.222\n",
      "    learn_time_ms: 756.302\n",
      "    load_throughput: 47008.855\n",
      "    load_time_ms: 21.273\n",
      "    sample_throughput: 40.454\n",
      "    sample_time_ms: 24719.521\n",
      "    update_time_ms: 3.681\n",
      "  timestamp: 1635284678\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 92\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         2105.19</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\"> -3.4563</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">                -5.1</td><td style=\"text-align: right;\">            344.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 93000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-45-02\n",
      "  done: false\n",
      "  episode_len_mean: 341.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.4272999999999705\n",
      "  episode_reward_min: -5.099999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 235\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6610255334112378\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011126979825156012\n",
      "          policy_loss: 0.022016379568311904\n",
      "          total_loss: 0.017805620696809556\n",
      "          vf_explained_var: 0.3916604816913605\n",
      "          vf_loss: 0.010730446472169003\n",
      "    num_agent_steps_sampled: 93000\n",
      "    num_agent_steps_trained: 93000\n",
      "    num_steps_sampled: 93000\n",
      "    num_steps_trained: 93000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.87941176470588\n",
      "    ram_util_percent: 30.741176470588236\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03711029246413732\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.88864214860005\n",
      "    mean_inference_ms: 2.1306707009699077\n",
      "    mean_raw_obs_processing_ms: 1.48565112424157\n",
      "  time_since_restore: 2128.9896376132965\n",
      "  time_this_iter_s: 23.794843435287476\n",
      "  time_total_s: 2128.9896376132965\n",
      "  timers:\n",
      "    learn_throughput: 1325.144\n",
      "    learn_time_ms: 754.635\n",
      "    load_throughput: 46397.887\n",
      "    load_time_ms: 21.553\n",
      "    sample_throughput: 40.604\n",
      "    sample_time_ms: 24628.253\n",
      "    update_time_ms: 3.662\n",
      "  timestamp: 1635284702\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 93000\n",
      "  training_iteration: 93\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         2128.99</td><td style=\"text-align: right;\">93000</td><td style=\"text-align: right;\"> -3.4273</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">                -5.1</td><td style=\"text-align: right;\">            341.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 94000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-45-27\n",
      "  done: false\n",
      "  episode_len_mean: 338.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.3992999999999722\n",
      "  episode_reward_min: -5.099999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 238\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.61178197728263\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006354728071194415\n",
      "          policy_loss: 0.057990608447127875\n",
      "          total_loss: 0.052279917067951626\n",
      "          vf_explained_var: 0.3685750365257263\n",
      "          vf_loss: 0.009453917676324232\n",
      "    num_agent_steps_sampled: 94000\n",
      "    num_agent_steps_trained: 94000\n",
      "    num_steps_sampled: 94000\n",
      "    num_steps_trained: 94000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.62222222222222\n",
      "    ram_util_percent: 30.663888888888884\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037103512342202306\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.912339022700383\n",
      "    mean_inference_ms: 2.1303299717589157\n",
      "    mean_raw_obs_processing_ms: 1.4899686211717482\n",
      "  time_since_restore: 2154.22731757164\n",
      "  time_this_iter_s: 25.237679958343506\n",
      "  time_total_s: 2154.22731757164\n",
      "  timers:\n",
      "    learn_throughput: 1325.515\n",
      "    learn_time_ms: 754.424\n",
      "    load_throughput: 46387.88\n",
      "    load_time_ms: 21.557\n",
      "    sample_throughput: 40.376\n",
      "    sample_time_ms: 24767.194\n",
      "    update_time_ms: 3.669\n",
      "  timestamp: 1635284727\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 94000\n",
      "  training_iteration: 94\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         2154.23</td><td style=\"text-align: right;\">94000</td><td style=\"text-align: right;\"> -3.3993</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">                -5.1</td><td style=\"text-align: right;\">            338.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 95000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-46-11\n",
      "  done: false\n",
      "  episode_len_mean: 336.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -3.374899999999972\n",
      "  episode_reward_min: -5.099999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 241\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5527109106381733\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005367260616789295\n",
      "          policy_loss: -0.0884154831369718\n",
      "          total_loss: -0.09110803074306911\n",
      "          vf_explained_var: 0.38739287853240967\n",
      "          vf_loss: 0.01202947199344635\n",
      "    num_agent_steps_sampled: 95000\n",
      "    num_agent_steps_trained: 95000\n",
      "    num_steps_sampled: 95000\n",
      "    num_steps_trained: 95000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.04920634920635\n",
      "    ram_util_percent: 30.784126984126985\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709709458211482\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.937434236798698\n",
      "    mean_inference_ms: 2.1300122342780567\n",
      "    mean_raw_obs_processing_ms: 1.4999597378550538\n",
      "  time_since_restore: 2198.0252389907837\n",
      "  time_this_iter_s: 43.79792141914368\n",
      "  time_total_s: 2198.0252389907837\n",
      "  timers:\n",
      "    learn_throughput: 1325.683\n",
      "    learn_time_ms: 754.328\n",
      "    load_throughput: 44371.022\n",
      "    load_time_ms: 22.537\n",
      "    sample_throughput: 40.12\n",
      "    sample_time_ms: 24925.268\n",
      "    update_time_ms: 3.665\n",
      "  timestamp: 1635284771\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 95000\n",
      "  training_iteration: 95\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         2198.03</td><td style=\"text-align: right;\">95000</td><td style=\"text-align: right;\"> -3.3749</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">                -5.1</td><td style=\"text-align: right;\">            336.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-46-35\n",
      "  done: false\n",
      "  episode_len_mean: 335.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -3.360299999999973\n",
      "  episode_reward_min: -5.099999999999957\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 245\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.616500907474094\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007017140498810903\n",
      "          policy_loss: -0.01198451088534461\n",
      "          total_loss: -0.013249536355336508\n",
      "          vf_explained_var: 0.3904201090335846\n",
      "          vf_loss: 0.013847412810557418\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.82941176470588\n",
      "    ram_util_percent: 31.320588235294114\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037088831906012594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.971097061434502\n",
      "    mean_inference_ms: 2.1296113122535463\n",
      "    mean_raw_obs_processing_ms: 1.5135202108938262\n",
      "  time_since_restore: 2222.297292470932\n",
      "  time_this_iter_s: 24.272053480148315\n",
      "  time_total_s: 2222.297292470932\n",
      "  timers:\n",
      "    learn_throughput: 1325.272\n",
      "    learn_time_ms: 754.562\n",
      "    load_throughput: 44063.74\n",
      "    load_time_ms: 22.694\n",
      "    sample_throughput: 40.228\n",
      "    sample_time_ms: 24858.587\n",
      "    update_time_ms: 3.587\n",
      "  timestamp: 1635284795\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 96\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">          2222.3</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\"> -3.3603</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">                -5.1</td><td style=\"text-align: right;\">            335.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 97000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-47-00\n",
      "  done: false\n",
      "  episode_len_mean: 333.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -3.3428999999999722\n",
      "  episode_reward_min: -5.099999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 248\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.581896612379286\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00563919220510548\n",
      "          policy_loss: 0.05140208937227726\n",
      "          total_loss: 0.04621755186882284\n",
      "          vf_explained_var: 0.47147148847579956\n",
      "          vf_loss: 0.00978855558981498\n",
      "    num_agent_steps_sampled: 97000\n",
      "    num_agent_steps_trained: 97000\n",
      "    num_steps_sampled: 97000\n",
      "    num_steps_trained: 97000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.58055555555555\n",
      "    ram_util_percent: 30.827777777777783\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708287744492536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.997003485033403\n",
      "    mean_inference_ms: 2.1293257383775406\n",
      "    mean_raw_obs_processing_ms: 1.5238446553004135\n",
      "  time_since_restore: 2247.1645333766937\n",
      "  time_this_iter_s: 24.86724090576172\n",
      "  time_total_s: 2247.1645333766937\n",
      "  timers:\n",
      "    learn_throughput: 1323.737\n",
      "    learn_time_ms: 755.437\n",
      "    load_throughput: 44161.353\n",
      "    load_time_ms: 22.644\n",
      "    sample_throughput: 40.07\n",
      "    sample_time_ms: 24956.152\n",
      "    update_time_ms: 3.585\n",
      "  timestamp: 1635284820\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 97000\n",
      "  training_iteration: 97\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         2247.16</td><td style=\"text-align: right;\">97000</td><td style=\"text-align: right;\"> -3.3429</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">                -5.1</td><td style=\"text-align: right;\">            333.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 98000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-47-26\n",
      "  done: false\n",
      "  episode_len_mean: 331.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -3.3236999999999726\n",
      "  episode_reward_min: -5.099999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 251\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4951997637748717\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005034799584732013\n",
      "          policy_loss: -0.08273711345261997\n",
      "          total_loss: -0.08423720300197601\n",
      "          vf_explained_var: 0.3778819739818573\n",
      "          vf_loss: 0.012696687721957763\n",
      "    num_agent_steps_sampled: 98000\n",
      "    num_agent_steps_trained: 98000\n",
      "    num_steps_sampled: 98000\n",
      "    num_steps_trained: 98000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.65675675675675\n",
      "    ram_util_percent: 30.79189189189189\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03707709767737583\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.024144006811706\n",
      "    mean_inference_ms: 2.129053039056493\n",
      "    mean_raw_obs_processing_ms: 1.5289093116849346\n",
      "  time_since_restore: 2273.0716087818146\n",
      "  time_this_iter_s: 25.90707540512085\n",
      "  time_total_s: 2273.0716087818146\n",
      "  timers:\n",
      "    learn_throughput: 1328.542\n",
      "    learn_time_ms: 752.705\n",
      "    load_throughput: 44140.346\n",
      "    load_time_ms: 22.655\n",
      "    sample_throughput: 39.435\n",
      "    sample_time_ms: 25358.435\n",
      "    update_time_ms: 3.592\n",
      "  timestamp: 1635284846\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 98000\n",
      "  training_iteration: 98\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         2273.07</td><td style=\"text-align: right;\">98000</td><td style=\"text-align: right;\"> -3.3237</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">                -5.1</td><td style=\"text-align: right;\">            331.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 99000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-47-52\n",
      "  done: false\n",
      "  episode_len_mean: 329.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -3.303599999999973\n",
      "  episode_reward_min: -5.099999999999957\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 255\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.631214393509759\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007233439345741348\n",
      "          policy_loss: -0.021321498519844478\n",
      "          total_loss: -0.023052970899475944\n",
      "          vf_explained_var: 0.3377542495727539\n",
      "          vf_loss: 0.013495653681457043\n",
      "    num_agent_steps_sampled: 99000\n",
      "    num_agent_steps_trained: 99000\n",
      "    num_steps_sampled: 99000\n",
      "    num_steps_trained: 99000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.69444444444443\n",
      "    ram_util_percent: 30.783333333333335\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03706999881050819\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.061021222813746\n",
      "    mean_inference_ms: 2.1287224858078213\n",
      "    mean_raw_obs_processing_ms: 1.5325882858470874\n",
      "  time_since_restore: 2298.374060153961\n",
      "  time_this_iter_s: 25.302451372146606\n",
      "  time_total_s: 2298.374060153961\n",
      "  timers:\n",
      "    learn_throughput: 1329.321\n",
      "    learn_time_ms: 752.264\n",
      "    load_throughput: 42494.534\n",
      "    load_time_ms: 23.532\n",
      "    sample_throughput: 39.091\n",
      "    sample_time_ms: 25581.113\n",
      "    update_time_ms: 3.591\n",
      "  timestamp: 1635284872\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 99000\n",
      "  training_iteration: 99\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         2298.37</td><td style=\"text-align: right;\">99000</td><td style=\"text-align: right;\"> -3.3036</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">                -5.1</td><td style=\"text-align: right;\">             329.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-48-16\n",
      "  done: false\n",
      "  episode_len_mean: 327.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -3.284699999999974\n",
      "  episode_reward_min: -5.099999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 258\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6583635846773783\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.033775696951842425\n",
      "          policy_loss: 0.09576238161987728\n",
      "          total_loss: 0.08782348740431997\n",
      "          vf_explained_var: 0.8647862672805786\n",
      "          vf_loss: 0.0035783873301827246\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.60588235294117\n",
      "    ram_util_percent: 30.729411764705887\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370649509925314\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.088868880705444\n",
      "    mean_inference_ms: 2.1284910529083554\n",
      "    mean_raw_obs_processing_ms: 1.535703372601841\n",
      "  time_since_restore: 2322.4574105739594\n",
      "  time_this_iter_s: 24.08335041999817\n",
      "  time_total_s: 2322.4574105739594\n",
      "  timers:\n",
      "    learn_throughput: 1329.113\n",
      "    learn_time_ms: 752.381\n",
      "    load_throughput: 42694.115\n",
      "    load_time_ms: 23.422\n",
      "    sample_throughput: 39.01\n",
      "    sample_time_ms: 25634.309\n",
      "    update_time_ms: 3.542\n",
      "  timestamp: 1635284896\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 100\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         2322.46</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\"> -3.2847</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">                -5.1</td><td style=\"text-align: right;\">            327.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 101000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-48-40\n",
      "  done: false\n",
      "  episode_len_mean: 327.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -3.2801999999999736\n",
      "  episode_reward_min: -5.099999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 261\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7783105055491129\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008209220919507803\n",
      "          policy_loss: 0.09342933057083024\n",
      "          total_loss: 0.08578120130631658\n",
      "          vf_explained_var: 0.23975054919719696\n",
      "          vf_loss: 0.008287903376751476\n",
      "    num_agent_steps_sampled: 101000\n",
      "    num_agent_steps_trained: 101000\n",
      "    num_steps_sampled: 101000\n",
      "    num_steps_trained: 101000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.57058823529412\n",
      "    ram_util_percent: 30.644117647058827\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03706008074989949\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.116930216648406\n",
      "    mean_inference_ms: 2.128268210273341\n",
      "    mean_raw_obs_processing_ms: 1.5389646956428265\n",
      "  time_since_restore: 2346.2753217220306\n",
      "  time_this_iter_s: 23.81791114807129\n",
      "  time_total_s: 2346.2753217220306\n",
      "  timers:\n",
      "    learn_throughput: 1326.332\n",
      "    learn_time_ms: 753.959\n",
      "    load_throughput: 41275.654\n",
      "    load_time_ms: 24.227\n",
      "    sample_throughput: 38.897\n",
      "    sample_time_ms: 25709.093\n",
      "    update_time_ms: 3.556\n",
      "  timestamp: 1635284920\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 101000\n",
      "  training_iteration: 101\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         2346.28</td><td style=\"text-align: right;\">101000</td><td style=\"text-align: right;\"> -3.2802</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">                -5.1</td><td style=\"text-align: right;\">            327.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 102000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-49-04\n",
      "  done: false\n",
      "  episode_len_mean: 325.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -3.267499999999975\n",
      "  episode_reward_min: -5.099999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 264\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7222543703185187\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0073548250486250724\n",
      "          policy_loss: 0.007151043663422267\n",
      "          total_loss: 0.0035448631478680504\n",
      "          vf_explained_var: 0.2557804584503174\n",
      "          vf_loss: 0.011961523799173947\n",
      "    num_agent_steps_sampled: 102000\n",
      "    num_agent_steps_trained: 102000\n",
      "    num_steps_sampled: 102000\n",
      "    num_steps_trained: 102000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.42222222222223\n",
      "    ram_util_percent: 30.60555555555556\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037055537546386325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.145242139193925\n",
      "    mean_inference_ms: 2.128057902517589\n",
      "    mean_raw_obs_processing_ms: 1.5423675868919482\n",
      "  time_since_restore: 2370.8575942516327\n",
      "  time_this_iter_s: 24.58227252960205\n",
      "  time_total_s: 2370.8575942516327\n",
      "  timers:\n",
      "    learn_throughput: 1325.102\n",
      "    learn_time_ms: 754.659\n",
      "    load_throughput: 42987.816\n",
      "    load_time_ms: 23.262\n",
      "    sample_throughput: 38.789\n",
      "    sample_time_ms: 25780.644\n",
      "    update_time_ms: 3.562\n",
      "  timestamp: 1635284944\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 102000\n",
      "  training_iteration: 102\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         2370.86</td><td style=\"text-align: right;\">102000</td><td style=\"text-align: right;\"> -3.2675</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">                -5.1</td><td style=\"text-align: right;\">            325.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 103000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-49-30\n",
      "  done: false\n",
      "  episode_len_mean: 324.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -3.253299999999975\n",
      "  episode_reward_min: -5.099999999999957\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 268\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6975873324606154\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006554527311066494\n",
      "          policy_loss: -0.010210533936818441\n",
      "          total_loss: -0.01064164758556419\n",
      "          vf_explained_var: 0.30479663610458374\n",
      "          vf_loss: 0.01506998875281877\n",
      "    num_agent_steps_sampled: 103000\n",
      "    num_agent_steps_trained: 103000\n",
      "    num_steps_sampled: 103000\n",
      "    num_steps_trained: 103000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.77837837837838\n",
      "    ram_util_percent: 30.58378378378378\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03704989170434056\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.183184338257835\n",
      "    mean_inference_ms: 2.1278000601396143\n",
      "    mean_raw_obs_processing_ms: 1.5472036274355576\n",
      "  time_since_restore: 2396.8830728530884\n",
      "  time_this_iter_s: 26.02547860145569\n",
      "  time_total_s: 2396.8830728530884\n",
      "  timers:\n",
      "    learn_throughput: 1320.98\n",
      "    learn_time_ms: 757.014\n",
      "    load_throughput: 43987.306\n",
      "    load_time_ms: 22.734\n",
      "    sample_throughput: 38.459\n",
      "    sample_time_ms: 26001.79\n",
      "    update_time_ms: 3.653\n",
      "  timestamp: 1635284970\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 103000\n",
      "  training_iteration: 103\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         2396.88</td><td style=\"text-align: right;\">103000</td><td style=\"text-align: right;\"> -3.2533</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">                -5.1</td><td style=\"text-align: right;\">            324.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-50-13\n",
      "  done: false\n",
      "  episode_len_mean: 322.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -3.2378999999999754\n",
      "  episode_reward_min: -5.099999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 271\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6555253744125367\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009602662969734076\n",
      "          policy_loss: 0.0243962023821142\n",
      "          total_loss: 0.019894849095079634\n",
      "          vf_explained_var: 0.4246949553489685\n",
      "          vf_loss: 0.009893300079016221\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.0983606557377\n",
      "    ram_util_percent: 30.691803278688525\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03704583993618445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.211649545433062\n",
      "    mean_inference_ms: 2.1276163956298255\n",
      "    mean_raw_obs_processing_ms: 1.5560435664527548\n",
      "  time_since_restore: 2439.7594113349915\n",
      "  time_this_iter_s: 42.876338481903076\n",
      "  time_total_s: 2439.7594113349915\n",
      "  timers:\n",
      "    learn_throughput: 1320.713\n",
      "    learn_time_ms: 757.167\n",
      "    load_throughput: 44014.587\n",
      "    load_time_ms: 22.72\n",
      "    sample_throughput: 36.016\n",
      "    sample_time_ms: 27765.52\n",
      "    update_time_ms: 3.646\n",
      "  timestamp: 1635285013\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 104\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         2439.76</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\"> -3.2379</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">                -5.1</td><td style=\"text-align: right;\">            322.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 105000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-50-40\n",
      "  done: false\n",
      "  episode_len_mean: 320.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -3.2116999999999756\n",
      "  episode_reward_min: -5.099999999999957\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 275\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.582314627700382\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006149150564390974\n",
      "          policy_loss: -0.00463731764919228\n",
      "          total_loss: -0.007306656655338075\n",
      "          vf_explained_var: 0.49279987812042236\n",
      "          vf_loss: 0.011770248273387551\n",
      "    num_agent_steps_sampled: 105000\n",
      "    num_agent_steps_trained: 105000\n",
      "    num_steps_sampled: 105000\n",
      "    num_steps_trained: 105000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.6078947368421\n",
      "    ram_util_percent: 30.905263157894744\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03704065127821419\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.250782946312754\n",
      "    mean_inference_ms: 2.1273862586236687\n",
      "    mean_raw_obs_processing_ms: 1.5679805263263638\n",
      "  time_since_restore: 2466.240907430649\n",
      "  time_this_iter_s: 26.48149609565735\n",
      "  time_total_s: 2466.240907430649\n",
      "  timers:\n",
      "    learn_throughput: 1323.202\n",
      "    learn_time_ms: 755.743\n",
      "    load_throughput: 44015.048\n",
      "    load_time_ms: 22.72\n",
      "    sample_throughput: 38.409\n",
      "    sample_time_ms: 26035.316\n",
      "    update_time_ms: 3.646\n",
      "  timestamp: 1635285040\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 105000\n",
      "  training_iteration: 105\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         2466.24</td><td style=\"text-align: right;\">105000</td><td style=\"text-align: right;\"> -3.2117</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">                -5.1</td><td style=\"text-align: right;\">            320.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 106000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-51-06\n",
      "  done: false\n",
      "  episode_len_mean: 318.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -3.1923999999999757\n",
      "  episode_reward_min: -5.099999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 278\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6211760838826497\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005530203889691501\n",
      "          policy_loss: 0.020400058726469675\n",
      "          total_loss: 0.015561754504839579\n",
      "          vf_explained_var: 0.48837241530418396\n",
      "          vf_loss: 0.01012916285544634\n",
      "    num_agent_steps_sampled: 106000\n",
      "    num_agent_steps_trained: 106000\n",
      "    num_steps_sampled: 106000\n",
      "    num_steps_trained: 106000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.7027027027027\n",
      "    ram_util_percent: 30.751351351351357\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03703708479002836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.280669132662908\n",
      "    mean_inference_ms: 2.127227978055493\n",
      "    mean_raw_obs_processing_ms: 1.5771071476785161\n",
      "  time_since_restore: 2492.5160365104675\n",
      "  time_this_iter_s: 26.275129079818726\n",
      "  time_total_s: 2492.5160365104675\n",
      "  timers:\n",
      "    learn_throughput: 1325.769\n",
      "    learn_time_ms: 754.279\n",
      "    load_throughput: 44194.251\n",
      "    load_time_ms: 22.627\n",
      "    sample_throughput: 38.114\n",
      "    sample_time_ms: 26237.087\n",
      "    update_time_ms: 3.737\n",
      "  timestamp: 1635285066\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 106000\n",
      "  training_iteration: 106\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         2492.52</td><td style=\"text-align: right;\">106000</td><td style=\"text-align: right;\"> -3.1924</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">                -5.1</td><td style=\"text-align: right;\">            318.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 107000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-51-32\n",
      "  done: false\n",
      "  episode_len_mean: 316.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -3.173799999999977\n",
      "  episode_reward_min: -5.099999999999957\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 282\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5676064160135057\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006246579472963829\n",
      "          policy_loss: -0.013791896899541219\n",
      "          total_loss: -0.014184821148713429\n",
      "          vf_explained_var: 0.38940903544425964\n",
      "          vf_loss: 0.013877660418964094\n",
      "    num_agent_steps_sampled: 107000\n",
      "    num_agent_steps_trained: 107000\n",
      "    num_steps_sampled: 107000\n",
      "    num_steps_trained: 107000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.56052631578947\n",
      "    ram_util_percent: 30.771052631578947\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03703254332782709\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.320208138463734\n",
      "    mean_inference_ms: 2.127033332635774\n",
      "    mean_raw_obs_processing_ms: 1.5804097240189783\n",
      "  time_since_restore: 2519.072371482849\n",
      "  time_this_iter_s: 26.556334972381592\n",
      "  time_total_s: 2519.072371482849\n",
      "  timers:\n",
      "    learn_throughput: 1325.171\n",
      "    learn_time_ms: 754.619\n",
      "    load_throughput: 44474.2\n",
      "    load_time_ms: 22.485\n",
      "    sample_throughput: 37.87\n",
      "    sample_time_ms: 26405.814\n",
      "    update_time_ms: 3.728\n",
      "  timestamp: 1635285092\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 107000\n",
      "  training_iteration: 107\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         2519.07</td><td style=\"text-align: right;\">107000</td><td style=\"text-align: right;\"> -3.1738</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">                -5.1</td><td style=\"text-align: right;\">            316.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-51-59\n",
      "  done: false\n",
      "  episode_len_mean: 314.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -3.158299999999977\n",
      "  episode_reward_min: -5.099999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 285\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5533149070209926\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006748259896695746\n",
      "          policy_loss: 0.020197176519367428\n",
      "          total_loss: 0.016826732125547198\n",
      "          vf_explained_var: 0.43806254863739014\n",
      "          vf_loss: 0.010644348171384384\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.58947368421052\n",
      "    ram_util_percent: 30.73157894736842\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702908788585442\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.350143897404998\n",
      "    mean_inference_ms: 2.1268904144160654\n",
      "    mean_raw_obs_processing_ms: 1.5830600534236448\n",
      "  time_since_restore: 2545.719745874405\n",
      "  time_this_iter_s: 26.647374391555786\n",
      "  time_total_s: 2545.719745874405\n",
      "  timers:\n",
      "    learn_throughput: 1322.288\n",
      "    learn_time_ms: 756.265\n",
      "    load_throughput: 44470.239\n",
      "    load_time_ms: 22.487\n",
      "    sample_throughput: 37.767\n",
      "    sample_time_ms: 26478.199\n",
      "    update_time_ms: 3.73\n",
      "  timestamp: 1635285119\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 108\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         2545.72</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\"> -3.1583</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">                -5.1</td><td style=\"text-align: right;\">            314.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 109000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-52-26\n",
      "  done: false\n",
      "  episode_len_mean: 312.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -3.133599999999977\n",
      "  episode_reward_min: -5.099999999999957\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 289\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6449105421702066\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007634539353883775\n",
      "          policy_loss: 0.02567159003681607\n",
      "          total_loss: 0.02568986780113644\n",
      "          vf_explained_var: 0.2999686896800995\n",
      "          vf_loss: 0.014749611955549982\n",
      "    num_agent_steps_sampled: 109000\n",
      "    num_agent_steps_trained: 109000\n",
      "    num_steps_sampled: 109000\n",
      "    num_steps_trained: 109000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.56842105263158\n",
      "    ram_util_percent: 30.72105263157895\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702393875775187\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.390233198468337\n",
      "    mean_inference_ms: 2.1266895800751633\n",
      "    mean_raw_obs_processing_ms: 1.5868816011010631\n",
      "  time_since_restore: 2572.4243264198303\n",
      "  time_this_iter_s: 26.704580545425415\n",
      "  time_total_s: 2572.4243264198303\n",
      "  timers:\n",
      "    learn_throughput: 1325.287\n",
      "    learn_time_ms: 754.554\n",
      "    load_throughput: 44080.504\n",
      "    load_time_ms: 22.686\n",
      "    sample_throughput: 37.566\n",
      "    sample_time_ms: 26619.925\n",
      "    update_time_ms: 3.732\n",
      "  timestamp: 1635285146\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 109000\n",
      "  training_iteration: 109\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         2572.42</td><td style=\"text-align: right;\">109000</td><td style=\"text-align: right;\"> -3.1336</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">                -5.1</td><td style=\"text-align: right;\">             312.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 110000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-52-52\n",
      "  done: false\n",
      "  episode_len_mean: 311.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -3.1202999999999776\n",
      "  episode_reward_min: -5.099999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 292\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5374998013178507\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008800734492061273\n",
      "          policy_loss: -0.05069270018074248\n",
      "          total_loss: -0.052461830857727265\n",
      "          vf_explained_var: 0.42842987179756165\n",
      "          vf_loss: 0.01162570402957499\n",
      "    num_agent_steps_sampled: 110000\n",
      "    num_agent_steps_trained: 110000\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.64999999999999\n",
      "    ram_util_percent: 30.713157894736852\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370197070456064\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.42015842916126\n",
      "    mean_inference_ms: 2.1265285538291696\n",
      "    mean_raw_obs_processing_ms: 1.5899480155506138\n",
      "  time_since_restore: 2598.734972000122\n",
      "  time_this_iter_s: 26.310645580291748\n",
      "  time_total_s: 2598.734972000122\n",
      "  timers:\n",
      "    learn_throughput: 1323.739\n",
      "    learn_time_ms: 755.436\n",
      "    load_throughput: 44045.555\n",
      "    load_time_ms: 22.704\n",
      "    sample_throughput: 37.255\n",
      "    sample_time_ms: 26841.781\n",
      "    update_time_ms: 3.73\n",
      "  timestamp: 1635285172\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 110\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         2598.73</td><td style=\"text-align: right;\">110000</td><td style=\"text-align: right;\"> -3.1203</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">                -5.1</td><td style=\"text-align: right;\">            311.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 111000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-53-18\n",
      "  done: false\n",
      "  episode_len_mean: 309.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -3.1142999999999774\n",
      "  episode_reward_min: -5.099999999999957\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 296\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6666696151097615\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004349723925397934\n",
      "          policy_loss: -0.1889729464219676\n",
      "          total_loss: -0.18266073366006216\n",
      "          vf_explained_var: 0.42624837160110474\n",
      "          vf_loss: 0.022000222084008984\n",
      "    num_agent_steps_sampled: 111000\n",
      "    num_agent_steps_trained: 111000\n",
      "    num_steps_sampled: 111000\n",
      "    num_steps_trained: 111000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.51621621621621\n",
      "    ram_util_percent: 30.651351351351344\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701383979287777\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.459897562367978\n",
      "    mean_inference_ms: 2.12631097135183\n",
      "    mean_raw_obs_processing_ms: 1.5942536204870499\n",
      "  time_since_restore: 2624.8071739673615\n",
      "  time_this_iter_s: 26.07220196723938\n",
      "  time_total_s: 2624.8071739673615\n",
      "  timers:\n",
      "    learn_throughput: 1323.32\n",
      "    learn_time_ms: 755.675\n",
      "    load_throughput: 43723.231\n",
      "    load_time_ms: 22.871\n",
      "    sample_throughput: 36.946\n",
      "    sample_time_ms: 27066.499\n",
      "    update_time_ms: 4.013\n",
      "  timestamp: 1635285198\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 111000\n",
      "  training_iteration: 111\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         2624.81</td><td style=\"text-align: right;\">111000</td><td style=\"text-align: right;\"> -3.1143</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">                -5.1</td><td style=\"text-align: right;\">            309.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-53-42\n",
      "  done: false\n",
      "  episode_len_mean: 308.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -3.236599999999978\n",
      "  episode_reward_min: -17.060000000000024\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 298\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11250000000000004\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8528739425871108\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.026208268747704523\n",
      "          policy_loss: -0.15642890754259295\n",
      "          total_loss: 0.134833996825748\n",
      "          vf_explained_var: 0.4211900532245636\n",
      "          vf_loss: 0.30684321117069985\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.45757575757575\n",
      "    ram_util_percent: 30.606060606060606\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701086265232115\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.47990725642451\n",
      "    mean_inference_ms: 2.1262015850915414\n",
      "    mean_raw_obs_processing_ms: 1.5965546500093186\n",
      "  time_since_restore: 2648.105078458786\n",
      "  time_this_iter_s: 23.29790449142456\n",
      "  time_total_s: 2648.105078458786\n",
      "  timers:\n",
      "    learn_throughput: 1324.84\n",
      "    learn_time_ms: 754.808\n",
      "    load_throughput: 42955.369\n",
      "    load_time_ms: 23.28\n",
      "    sample_throughput: 37.122\n",
      "    sample_time_ms: 26938.523\n",
      "    update_time_ms: 4.0\n",
      "  timestamp: 1635285222\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 112\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         2648.11</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\"> -3.2366</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">              -17.06</td><td style=\"text-align: right;\">            308.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 113000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-54-24\n",
      "  done: false\n",
      "  episode_len_mean: 307.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4099999999999926\n",
      "  episode_reward_mean: -3.4364999999999806\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 302\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16874999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8096281607945761\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02161639593878218\n",
      "          policy_loss: -0.09944464241464933\n",
      "          total_loss: -0.0035984674054715367\n",
      "          vf_explained_var: 0.278305321931839\n",
      "          vf_loss: 0.11029469128180709\n",
      "    num_agent_steps_sampled: 113000\n",
      "    num_agent_steps_trained: 113000\n",
      "    num_steps_sampled: 113000\n",
      "    num_steps_trained: 113000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.927868852459014\n",
      "    ram_util_percent: 30.82131147540983\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03700499276390289\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.51991500795982\n",
      "    mean_inference_ms: 2.1259848240412507\n",
      "    mean_raw_obs_processing_ms: 1.607587328193708\n",
      "  time_since_restore: 2690.7122118473053\n",
      "  time_this_iter_s: 42.60713338851929\n",
      "  time_total_s: 2690.7122118473053\n",
      "  timers:\n",
      "    learn_throughput: 1327.595\n",
      "    learn_time_ms: 753.242\n",
      "    load_throughput: 41935.198\n",
      "    load_time_ms: 23.846\n",
      "    sample_throughput: 34.968\n",
      "    sample_time_ms: 28597.755\n",
      "    update_time_ms: 3.911\n",
      "  timestamp: 1635285264\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 113000\n",
      "  training_iteration: 113\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         2690.71</td><td style=\"text-align: right;\">113000</td><td style=\"text-align: right;\"> -3.4365</td><td style=\"text-align: right;\">               -2.41</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            307.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 114000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-54-50\n",
      "  done: false\n",
      "  episode_len_mean: 306.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4099999999999926\n",
      "  episode_reward_mean: -3.51119999999998\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 305\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7342188212606642\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013732054763886087\n",
      "          policy_loss: -0.0043992525587479275\n",
      "          total_loss: 0.037056337752276\n",
      "          vf_explained_var: 0.44660699367523193\n",
      "          vf_loss: 0.055321850596616665\n",
      "    num_agent_steps_sampled: 114000\n",
      "    num_agent_steps_trained: 114000\n",
      "    num_steps_sampled: 114000\n",
      "    num_steps_trained: 114000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.4918918918919\n",
      "    ram_util_percent: 30.875675675675684\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03700050858607194\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.549523097767704\n",
      "    mean_inference_ms: 2.125819601273787\n",
      "    mean_raw_obs_processing_ms: 1.6159515001435114\n",
      "  time_since_restore: 2716.199709415436\n",
      "  time_this_iter_s: 25.487497568130493\n",
      "  time_total_s: 2716.199709415436\n",
      "  timers:\n",
      "    learn_throughput: 1327.961\n",
      "    learn_time_ms: 753.034\n",
      "    load_throughput: 42090.018\n",
      "    load_time_ms: 23.759\n",
      "    sample_throughput: 37.231\n",
      "    sample_time_ms: 26859.077\n",
      "    update_time_ms: 4.005\n",
      "  timestamp: 1635285290\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 114000\n",
      "  training_iteration: 114\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">          2716.2</td><td style=\"text-align: right;\">114000</td><td style=\"text-align: right;\"> -3.5112</td><td style=\"text-align: right;\">               -2.41</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            306.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 115000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-55-16\n",
      "  done: false\n",
      "  episode_len_mean: 305.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4099999999999926\n",
      "  episode_reward_mean: -3.492899999999981\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 309\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6214400702052647\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009527536628822691\n",
      "          policy_loss: 0.03546161527434985\n",
      "          total_loss: 0.032650427437490886\n",
      "          vf_explained_var: 0.4756033718585968\n",
      "          vf_loss: 0.010991556456105576\n",
      "    num_agent_steps_sampled: 115000\n",
      "    num_agent_steps_trained: 115000\n",
      "    num_steps_sampled: 115000\n",
      "    num_steps_trained: 115000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.66486486486485\n",
      "    ram_util_percent: 30.770270270270263\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03699454354384421\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.588861148701668\n",
      "    mean_inference_ms: 2.125604913499685\n",
      "    mean_raw_obs_processing_ms: 1.623123445397542\n",
      "  time_since_restore: 2742.683669090271\n",
      "  time_this_iter_s: 26.483959674835205\n",
      "  time_total_s: 2742.683669090271\n",
      "  timers:\n",
      "    learn_throughput: 1329.129\n",
      "    learn_time_ms: 752.373\n",
      "    load_throughput: 42107.47\n",
      "    load_time_ms: 23.749\n",
      "    sample_throughput: 37.23\n",
      "    sample_time_ms: 26859.989\n",
      "    update_time_ms: 3.999\n",
      "  timestamp: 1635285316\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 115000\n",
      "  training_iteration: 115\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         2742.68</td><td style=\"text-align: right;\">115000</td><td style=\"text-align: right;\"> -3.4929</td><td style=\"text-align: right;\">               -2.41</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            305.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-55-43\n",
      "  done: false\n",
      "  episode_len_mean: 304.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4099999999999926\n",
      "  episode_reward_mean: -3.486499999999981\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 312\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6366134537590875\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006102626439539034\n",
      "          policy_loss: -0.08681668407387204\n",
      "          total_loss: -0.09153334457013342\n",
      "          vf_explained_var: 0.36763089895248413\n",
      "          vf_loss: 0.01010474396041698\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.58717948717948\n",
      "    ram_util_percent: 30.782051282051277\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0369900954249511\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.618210931648242\n",
      "    mean_inference_ms: 2.1254481570974635\n",
      "    mean_raw_obs_processing_ms: 1.6255443175073054\n",
      "  time_since_restore: 2769.865775346756\n",
      "  time_this_iter_s: 27.182106256484985\n",
      "  time_total_s: 2769.865775346756\n",
      "  timers:\n",
      "    learn_throughput: 1327.12\n",
      "    learn_time_ms: 753.512\n",
      "    load_throughput: 42294.593\n",
      "    load_time_ms: 23.644\n",
      "    sample_throughput: 37.106\n",
      "    sample_time_ms: 26949.752\n",
      "    update_time_ms: 3.901\n",
      "  timestamp: 1635285343\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 116\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         2769.87</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\"> -3.4865</td><td style=\"text-align: right;\">               -2.41</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            304.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 117000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-56-10\n",
      "  done: false\n",
      "  episode_len_mean: 303.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4099999999999926\n",
      "  episode_reward_mean: -3.4762999999999806\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 316\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7713542858759561\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008873285662890456\n",
      "          policy_loss: 0.022573692517148122\n",
      "          total_loss: 0.01670833287967576\n",
      "          vf_explained_var: 0.47950461506843567\n",
      "          vf_loss: 0.009602128996306823\n",
      "    num_agent_steps_sampled: 117000\n",
      "    num_agent_steps_trained: 117000\n",
      "    num_steps_sampled: 117000\n",
      "    num_steps_trained: 117000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.52105263157894\n",
      "    ram_util_percent: 30.781578947368416\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03698422779643606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.657546498624484\n",
      "    mean_inference_ms: 2.1252428655823508\n",
      "    mean_raw_obs_processing_ms: 1.628816697069321\n",
      "  time_since_restore: 2796.15913438797\n",
      "  time_this_iter_s: 26.29335904121399\n",
      "  time_total_s: 2796.15913438797\n",
      "  timers:\n",
      "    learn_throughput: 1328.85\n",
      "    learn_time_ms: 752.531\n",
      "    load_throughput: 42193.968\n",
      "    load_time_ms: 23.7\n",
      "    sample_throughput: 37.141\n",
      "    sample_time_ms: 26924.365\n",
      "    update_time_ms: 3.904\n",
      "  timestamp: 1635285370\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 117000\n",
      "  training_iteration: 117\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         2796.16</td><td style=\"text-align: right;\">117000</td><td style=\"text-align: right;\"> -3.4763</td><td style=\"text-align: right;\">               -2.41</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            303.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 118000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-56-37\n",
      "  done: false\n",
      "  episode_len_mean: 300.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4099999999999926\n",
      "  episode_reward_mean: -3.436599999999982\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 320\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.680215503109826\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007444533376643846\n",
      "          policy_loss: -0.011957900267508296\n",
      "          total_loss: -0.01895484493838416\n",
      "          vf_explained_var: 0.6198225021362305\n",
      "          vf_loss: 0.00792081129944159\n",
      "    num_agent_steps_sampled: 118000\n",
      "    num_agent_steps_trained: 118000\n",
      "    num_steps_sampled: 118000\n",
      "    num_steps_trained: 118000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.49230769230768\n",
      "    ram_util_percent: 30.792307692307688\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03697856845212546\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.69776649895309\n",
      "    mean_inference_ms: 2.1250450113242505\n",
      "    mean_raw_obs_processing_ms: 1.6323891984154517\n",
      "  time_since_restore: 2823.385464668274\n",
      "  time_this_iter_s: 27.226330280303955\n",
      "  time_total_s: 2823.385464668274\n",
      "  timers:\n",
      "    learn_throughput: 1330.331\n",
      "    learn_time_ms: 751.693\n",
      "    load_throughput: 42295.659\n",
      "    load_time_ms: 23.643\n",
      "    sample_throughput: 37.06\n",
      "    sample_time_ms: 26983.159\n",
      "    update_time_ms: 3.902\n",
      "  timestamp: 1635285397\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 118000\n",
      "  training_iteration: 118\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">         2823.39</td><td style=\"text-align: right;\">118000</td><td style=\"text-align: right;\"> -3.4366</td><td style=\"text-align: right;\">               -2.41</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            300.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 119000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-57-02\n",
      "  done: false\n",
      "  episode_len_mean: 298.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4099999999999926\n",
      "  episode_reward_mean: -3.4210999999999814\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 323\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8554874857266743\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010436467334686918\n",
      "          policy_loss: -0.09372670981619093\n",
      "          total_loss: -0.10629776178134812\n",
      "          vf_explained_var: 0.8293814659118652\n",
      "          vf_loss: 0.00334209315220101\n",
      "    num_agent_steps_sampled: 119000\n",
      "    num_agent_steps_trained: 119000\n",
      "    num_steps_sampled: 119000\n",
      "    num_steps_trained: 119000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.66111111111111\n",
      "    ram_util_percent: 30.750000000000007\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036974267900691034\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.728113970740424\n",
      "    mean_inference_ms: 2.1248960262038445\n",
      "    mean_raw_obs_processing_ms: 1.6352789700284711\n",
      "  time_since_restore: 2848.9079990386963\n",
      "  time_this_iter_s: 25.522534370422363\n",
      "  time_total_s: 2848.9079990386963\n",
      "  timers:\n",
      "    learn_throughput: 1328.97\n",
      "    learn_time_ms: 752.462\n",
      "    load_throughput: 42512.926\n",
      "    load_time_ms: 23.522\n",
      "    sample_throughput: 37.224\n",
      "    sample_time_ms: 26864.235\n",
      "    update_time_ms: 3.976\n",
      "  timestamp: 1635285422\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 119000\n",
      "  training_iteration: 119\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         2848.91</td><td style=\"text-align: right;\">119000</td><td style=\"text-align: right;\"> -3.4211</td><td style=\"text-align: right;\">               -2.41</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            298.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-57-29\n",
      "  done: false\n",
      "  episode_len_mean: 297.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4099999999999926\n",
      "  episode_reward_mean: -3.4071999999999822\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 326\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.741104335255093\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005864329329521152\n",
      "          policy_loss: -0.11152531521187889\n",
      "          total_loss: -0.122040656208992\n",
      "          vf_explained_var: 0.8574399352073669\n",
      "          vf_loss: 0.005411293052343859\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.58157894736841\n",
      "    ram_util_percent: 30.686842105263167\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03696975976874124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.758792483007934\n",
      "    mean_inference_ms: 2.1247402250807754\n",
      "    mean_raw_obs_processing_ms: 1.6382594201822502\n",
      "  time_since_restore: 2875.479320049286\n",
      "  time_this_iter_s: 26.5713210105896\n",
      "  time_total_s: 2875.479320049286\n",
      "  timers:\n",
      "    learn_throughput: 1331.388\n",
      "    learn_time_ms: 751.096\n",
      "    load_throughput: 42556.233\n",
      "    load_time_ms: 23.498\n",
      "    sample_throughput: 37.186\n",
      "    sample_time_ms: 26891.673\n",
      "    update_time_ms: 3.973\n",
      "  timestamp: 1635285449\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 120\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">         2875.48</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\"> -3.4072</td><td style=\"text-align: right;\">               -2.41</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            297.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 121000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-58-11\n",
      "  done: false\n",
      "  episode_len_mean: 295.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4099999999999926\n",
      "  episode_reward_mean: -3.386299999999983\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 330\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7496587687068514\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0049601954150266655\n",
      "          policy_loss: 0.052814696398046285\n",
      "          total_loss: 0.04417626650796996\n",
      "          vf_explained_var: 0.8755493760108948\n",
      "          vf_loss: 0.007602606108412147\n",
      "    num_agent_steps_sampled: 121000\n",
      "    num_agent_steps_trained: 121000\n",
      "    num_steps_sampled: 121000\n",
      "    num_steps_trained: 121000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.64333333333334\n",
      "    ram_util_percent: 30.653333333333332\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03696377615615253\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.79946474910979\n",
      "    mean_inference_ms: 2.1245309278499516\n",
      "    mean_raw_obs_processing_ms: 1.6481239009888144\n",
      "  time_since_restore: 2917.2372345924377\n",
      "  time_this_iter_s: 41.757914543151855\n",
      "  time_total_s: 2917.2372345924377\n",
      "  timers:\n",
      "    learn_throughput: 1331.415\n",
      "    learn_time_ms: 751.081\n",
      "    load_throughput: 42593.182\n",
      "    load_time_ms: 23.478\n",
      "    sample_throughput: 35.136\n",
      "    sample_time_ms: 28460.487\n",
      "    update_time_ms: 3.674\n",
      "  timestamp: 1635285491\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 121000\n",
      "  training_iteration: 121\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         2917.24</td><td style=\"text-align: right;\">121000</td><td style=\"text-align: right;\"> -3.3863</td><td style=\"text-align: right;\">               -2.41</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            295.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 122000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-58-38\n",
      "  done: false\n",
      "  episode_len_mean: 294.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4099999999999926\n",
      "  episode_reward_mean: -3.375899999999983\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 333\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1265625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.63628830909729\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00833749184627997\n",
      "          policy_loss: -0.12838825715912713\n",
      "          total_loss: -0.1367115347749657\n",
      "          vf_explained_var: 0.7602048516273499\n",
      "          vf_loss: 0.00698439315892756\n",
      "    num_agent_steps_sampled: 122000\n",
      "    num_agent_steps_trained: 122000\n",
      "    num_steps_sampled: 122000\n",
      "    num_steps_trained: 122000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.87105263157895\n",
      "    ram_util_percent: 31.015789473684215\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03695927628301764\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.83024291486729\n",
      "    mean_inference_ms: 2.1243720291122075\n",
      "    mean_raw_obs_processing_ms: 1.6555990527587006\n",
      "  time_since_restore: 2944.208678007126\n",
      "  time_this_iter_s: 26.97144341468811\n",
      "  time_total_s: 2944.208678007126\n",
      "  timers:\n",
      "    learn_throughput: 1332.491\n",
      "    learn_time_ms: 750.474\n",
      "    load_throughput: 42197.236\n",
      "    load_time_ms: 23.698\n",
      "    sample_throughput: 34.688\n",
      "    sample_time_ms: 28828.218\n",
      "    update_time_ms: 3.68\n",
      "  timestamp: 1635285518\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 122000\n",
      "  training_iteration: 122\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         2944.21</td><td style=\"text-align: right;\">122000</td><td style=\"text-align: right;\"> -3.3759</td><td style=\"text-align: right;\">               -2.41</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            294.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 123000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-59-04\n",
      "  done: false\n",
      "  episode_len_mean: 293.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4099999999999926\n",
      "  episode_reward_mean: -3.3631999999999835\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 337\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1265625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.636862822373708\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015415753527477389\n",
      "          policy_loss: 0.05441404071946939\n",
      "          total_loss: 0.04337079752650526\n",
      "          vf_explained_var: 0.917784571647644\n",
      "          vf_loss: 0.00337432945250637\n",
      "    num_agent_steps_sampled: 123000\n",
      "    num_agent_steps_trained: 123000\n",
      "    num_steps_sampled: 123000\n",
      "    num_steps_trained: 123000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.66756756756756\n",
      "    ram_util_percent: 30.835135135135133\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03695341178606152\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.870886418264114\n",
      "    mean_inference_ms: 2.1241650214642793\n",
      "    mean_raw_obs_processing_ms: 1.6657210653651646\n",
      "  time_since_restore: 2970.0293760299683\n",
      "  time_this_iter_s: 25.820698022842407\n",
      "  time_total_s: 2970.0293760299683\n",
      "  timers:\n",
      "    learn_throughput: 1334.53\n",
      "    learn_time_ms: 749.328\n",
      "    load_throughput: 42219.876\n",
      "    load_time_ms: 23.686\n",
      "    sample_throughput: 36.831\n",
      "    sample_time_ms: 27150.755\n",
      "    update_time_ms: 3.679\n",
      "  timestamp: 1635285544\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 123000\n",
      "  training_iteration: 123\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         2970.03</td><td style=\"text-align: right;\">123000</td><td style=\"text-align: right;\"> -3.3632</td><td style=\"text-align: right;\">               -2.41</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            293.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-59-29\n",
      "  done: false\n",
      "  episode_len_mean: 293.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4099999999999926\n",
      "  episode_reward_mean: -3.3653999999999833\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 340\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1265625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4925052258703444\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.033675724076296654\n",
      "          policy_loss: -0.10125295942028363\n",
      "          total_loss: -0.10020979882942306\n",
      "          vf_explained_var: 0.6790928840637207\n",
      "          vf_loss: 0.011706130755030447\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.68611111111112\n",
      "    ram_util_percent: 30.816666666666663\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03694917435979903\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.900352859103524\n",
      "    mean_inference_ms: 2.124014263830878\n",
      "    mean_raw_obs_processing_ms: 1.669858892922818\n",
      "  time_since_restore: 2995.492772579193\n",
      "  time_this_iter_s: 25.463396549224854\n",
      "  time_total_s: 2995.492772579193\n",
      "  timers:\n",
      "    learn_throughput: 1335.299\n",
      "    learn_time_ms: 748.896\n",
      "    load_throughput: 42062.033\n",
      "    load_time_ms: 23.774\n",
      "    sample_throughput: 36.834\n",
      "    sample_time_ms: 27148.757\n",
      "    update_time_ms: 3.59\n",
      "  timestamp: 1635285569\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 124\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         2995.49</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\"> -3.3654</td><td style=\"text-align: right;\">               -2.41</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            293.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 125000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_21-59-53\n",
      "  done: false\n",
      "  episode_len_mean: 293.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4099999999999926\n",
      "  episode_reward_mean: -3.363599999999983\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 344\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6817943930625916\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011752771282642686\n",
      "          policy_loss: -0.05153635889291763\n",
      "          total_loss: -0.05361464586522844\n",
      "          vf_explained_var: 0.42639589309692383\n",
      "          vf_loss: 0.012508466777702173\n",
      "    num_agent_steps_sampled: 125000\n",
      "    num_agent_steps_trained: 125000\n",
      "    num_steps_sampled: 125000\n",
      "    num_steps_trained: 125000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.56\n",
      "    ram_util_percent: 30.845714285714283\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0369437297641241\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.93857178184741\n",
      "    mean_inference_ms: 2.1238179422865238\n",
      "    mean_raw_obs_processing_ms: 1.6731446835339912\n",
      "  time_since_restore: 3019.670153617859\n",
      "  time_this_iter_s: 24.17738103866577\n",
      "  time_total_s: 3019.670153617859\n",
      "  timers:\n",
      "    learn_throughput: 1333.283\n",
      "    learn_time_ms: 750.028\n",
      "    load_throughput: 42062.16\n",
      "    load_time_ms: 23.774\n",
      "    sample_throughput: 37.151\n",
      "    sample_time_ms: 26916.885\n",
      "    update_time_ms: 3.677\n",
      "  timestamp: 1635285593\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 125000\n",
      "  training_iteration: 125\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         3019.67</td><td style=\"text-align: right;\">125000</td><td style=\"text-align: right;\"> -3.3636</td><td style=\"text-align: right;\">               -2.41</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            293.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 126000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-00-20\n",
      "  done: false\n",
      "  episode_len_mean: 291.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4099999999999926\n",
      "  episode_reward_mean: -3.3514999999999833\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 347\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3801237927542793\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006995012194636748\n",
      "          policy_loss: -0.028521980014112262\n",
      "          total_loss: -0.03646469157603052\n",
      "          vf_explained_var: 0.8210258483886719\n",
      "          vf_loss: 0.004530567357627054\n",
      "    num_agent_steps_sampled: 126000\n",
      "    num_agent_steps_trained: 126000\n",
      "    num_steps_sampled: 126000\n",
      "    num_steps_trained: 126000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.64594594594595\n",
      "    ram_util_percent: 30.881081081081085\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036939789113919624\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.9672404355237\n",
      "    mean_inference_ms: 2.1236738244234754\n",
      "    mean_raw_obs_processing_ms: 1.6756510372477553\n",
      "  time_since_restore: 3045.928738117218\n",
      "  time_this_iter_s: 26.25858449935913\n",
      "  time_total_s: 3045.928738117218\n",
      "  timers:\n",
      "    learn_throughput: 1333.062\n",
      "    learn_time_ms: 750.153\n",
      "    load_throughput: 41744.96\n",
      "    load_time_ms: 23.955\n",
      "    sample_throughput: 37.28\n",
      "    sample_time_ms: 26824.211\n",
      "    update_time_ms: 3.677\n",
      "  timestamp: 1635285620\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 126000\n",
      "  training_iteration: 126\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">         3045.93</td><td style=\"text-align: right;\">126000</td><td style=\"text-align: right;\"> -3.3515</td><td style=\"text-align: right;\">               -2.41</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">             291.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 127000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-00-44\n",
      "  done: false\n",
      "  episode_len_mean: 292.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4099999999999926\n",
      "  episode_reward_mean: -3.352599999999984\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 350\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4917173345883687\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00549509877564421\n",
      "          policy_loss: -0.0692497486455573\n",
      "          total_loss: -0.07648200069864591\n",
      "          vf_explained_var: 0.7891548871994019\n",
      "          vf_loss: 0.006641709165544145\n",
      "    num_agent_steps_sampled: 127000\n",
      "    num_agent_steps_trained: 127000\n",
      "    num_steps_sampled: 127000\n",
      "    num_steps_trained: 127000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.57142857142856\n",
      "    ram_util_percent: 30.83714285714285\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036935974476075835\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.995007037856126\n",
      "    mean_inference_ms: 2.1235321685341835\n",
      "    mean_raw_obs_processing_ms: 1.6782365377087658\n",
      "  time_since_restore: 3069.7958147525787\n",
      "  time_this_iter_s: 23.867076635360718\n",
      "  time_total_s: 3069.7958147525787\n",
      "  timers:\n",
      "    learn_throughput: 1333.071\n",
      "    learn_time_ms: 750.148\n",
      "    load_throughput: 41789.505\n",
      "    load_time_ms: 23.929\n",
      "    sample_throughput: 37.62\n",
      "    sample_time_ms: 26581.591\n",
      "    update_time_ms: 3.701\n",
      "  timestamp: 1635285644\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 127000\n",
      "  training_iteration: 127\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">          3069.8</td><td style=\"text-align: right;\">127000</td><td style=\"text-align: right;\"> -3.3526</td><td style=\"text-align: right;\">               -2.41</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            292.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-01-09\n",
      "  done: false\n",
      "  episode_len_mean: 292.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4099999999999926\n",
      "  episode_reward_mean: -3.354499999999984\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 354\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4625033325619168\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012809826387805145\n",
      "          policy_loss: -0.09443538640108373\n",
      "          total_loss: -0.0997295603983932\n",
      "          vf_explained_var: 0.7844109535217285\n",
      "          vf_loss: 0.006898991667872501\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.70833333333333\n",
      "    ram_util_percent: 30.74444444444445\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03693087855018869\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.031290814151895\n",
      "    mean_inference_ms: 2.123342263898617\n",
      "    mean_raw_obs_processing_ms: 1.681826737443451\n",
      "  time_since_restore: 3095.360955953598\n",
      "  time_this_iter_s: 25.565141201019287\n",
      "  time_total_s: 3095.360955953598\n",
      "  timers:\n",
      "    learn_throughput: 1330.792\n",
      "    learn_time_ms: 751.432\n",
      "    load_throughput: 41734.825\n",
      "    load_time_ms: 23.961\n",
      "    sample_throughput: 37.858\n",
      "    sample_time_ms: 26414.149\n",
      "    update_time_ms: 3.699\n",
      "  timestamp: 1635285669\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 128\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">         3095.36</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\"> -3.3545</td><td style=\"text-align: right;\">               -2.41</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            292.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 129000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-01-36\n",
      "  done: false\n",
      "  episode_len_mean: 291.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4099999999999926\n",
      "  episode_reward_mean: -3.3442999999999836\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 358\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1611292534404332\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008637492703512824\n",
      "          policy_loss: -0.03706767103738255\n",
      "          total_loss: -0.042267886135313244\n",
      "          vf_explained_var: 0.8796376585960388\n",
      "          vf_loss: 0.004771301352108518\n",
      "    num_agent_steps_sampled: 129000\n",
      "    num_agent_steps_trained: 129000\n",
      "    num_steps_sampled: 129000\n",
      "    num_steps_trained: 129000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.60263157894735\n",
      "    ram_util_percent: 30.721052631578956\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03692590918234773\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.067798335229078\n",
      "    mean_inference_ms: 2.1231585852063475\n",
      "    mean_raw_obs_processing_ms: 1.6855148115973708\n",
      "  time_since_restore: 3122.1782264709473\n",
      "  time_this_iter_s: 26.817270517349243\n",
      "  time_total_s: 3122.1782264709473\n",
      "  timers:\n",
      "    learn_throughput: 1328.479\n",
      "    learn_time_ms: 752.741\n",
      "    load_throughput: 41793.17\n",
      "    load_time_ms: 23.927\n",
      "    sample_throughput: 37.676\n",
      "    sample_time_ms: 26542.418\n",
      "    update_time_ms: 3.625\n",
      "  timestamp: 1635285696\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 129000\n",
      "  training_iteration: 129\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         3122.18</td><td style=\"text-align: right;\">129000</td><td style=\"text-align: right;\"> -3.3443</td><td style=\"text-align: right;\">               -2.41</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">             291.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 130000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-02-20\n",
      "  done: false\n",
      "  episode_len_mean: 289.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.3254999999999844\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 361\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.264870580037435\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009020715044339194\n",
      "          policy_loss: 0.005826147976848814\n",
      "          total_loss: 0.0004135170744525062\n",
      "          vf_explained_var: 0.8554562926292419\n",
      "          vf_loss: 0.005523548237720712\n",
      "    num_agent_steps_sampled: 130000\n",
      "    num_agent_steps_trained: 130000\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.0203125\n",
      "    ram_util_percent: 30.807812499999997\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036922434900486516\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.09547763334845\n",
      "    mean_inference_ms: 2.1230305686320836\n",
      "    mean_raw_obs_processing_ms: 1.6923608125150402\n",
      "  time_since_restore: 3166.5108892917633\n",
      "  time_this_iter_s: 44.33266282081604\n",
      "  time_total_s: 3166.5108892917633\n",
      "  timers:\n",
      "    learn_throughput: 1327.65\n",
      "    learn_time_ms: 753.211\n",
      "    load_throughput: 41767.907\n",
      "    load_time_ms: 23.942\n",
      "    sample_throughput: 35.313\n",
      "    sample_time_ms: 28318.083\n",
      "    update_time_ms: 3.625\n",
      "  timestamp: 1635285740\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 130\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">         3166.51</td><td style=\"text-align: right;\">130000</td><td style=\"text-align: right;\"> -3.3255</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            289.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 131000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-02-47\n",
      "  done: false\n",
      "  episode_len_mean: 287.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.3106999999999847\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 365\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1588695049285889\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0065588335543138344\n",
      "          policy_loss: 0.06472476166155604\n",
      "          total_loss: 0.060164642996258205\n",
      "          vf_explained_var: 0.792820394039154\n",
      "          vf_loss: 0.005783421566916837\n",
      "    num_agent_steps_sampled: 131000\n",
      "    num_agent_steps_trained: 131000\n",
      "    num_steps_sampled: 131000\n",
      "    num_steps_trained: 131000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.75405405405404\n",
      "    ram_util_percent: 31.264864864864858\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03691771013305404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.13200323574633\n",
      "    mean_inference_ms: 2.1228632649494883\n",
      "    mean_raw_obs_processing_ms: 1.7016638103925499\n",
      "  time_since_restore: 3192.9028840065002\n",
      "  time_this_iter_s: 26.39199471473694\n",
      "  time_total_s: 3192.9028840065002\n",
      "  timers:\n",
      "    learn_throughput: 1327.478\n",
      "    learn_time_ms: 753.308\n",
      "    load_throughput: 41732.707\n",
      "    load_time_ms: 23.962\n",
      "    sample_throughput: 37.339\n",
      "    sample_time_ms: 26781.39\n",
      "    update_time_ms: 3.707\n",
      "  timestamp: 1635285767\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 131000\n",
      "  training_iteration: 131\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">          3192.9</td><td style=\"text-align: right;\">131000</td><td style=\"text-align: right;\"> -3.3107</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            287.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-03-14\n",
      "  done: false\n",
      "  episode_len_mean: 286.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.3019999999999845\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 369\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0983068227767945\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0054657040370790985\n",
      "          policy_loss: 0.020902348558108012\n",
      "          total_loss: 0.01552495550778177\n",
      "          vf_explained_var: 0.8592559099197388\n",
      "          vf_loss: 0.004568043239932093\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.67692307692307\n",
      "    ram_util_percent: 30.966666666666672\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0369130240442626\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.16826507395864\n",
      "    mean_inference_ms: 2.122697258936556\n",
      "    mean_raw_obs_processing_ms: 1.7093406010980023\n",
      "  time_since_restore: 3219.9601922035217\n",
      "  time_this_iter_s: 27.057308197021484\n",
      "  time_total_s: 3219.9601922035217\n",
      "  timers:\n",
      "    learn_throughput: 1325.191\n",
      "    learn_time_ms: 754.608\n",
      "    load_throughput: 42943.363\n",
      "    load_time_ms: 23.286\n",
      "    sample_throughput: 37.328\n",
      "    sample_time_ms: 26789.274\n",
      "    update_time_ms: 3.792\n",
      "  timestamp: 1635285794\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 132\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         3219.96</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">  -3.302</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            286.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 133000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-03-40\n",
      "  done: false\n",
      "  episode_len_mean: 286.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.300999999999984\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 372\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1458942492802937\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012472922561184024\n",
      "          policy_loss: -0.01681630959113439\n",
      "          total_loss: -0.022154872160818843\n",
      "          vf_explained_var: 0.8573592901229858\n",
      "          vf_loss: 0.0037524790360799268\n",
      "    num_agent_steps_sampled: 133000\n",
      "    num_agent_steps_trained: 133000\n",
      "    num_steps_sampled: 133000\n",
      "    num_steps_trained: 133000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.62432432432433\n",
      "    ram_util_percent: 30.913513513513518\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03690961560820472\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.194908602530678\n",
      "    mean_inference_ms: 2.12257632351446\n",
      "    mean_raw_obs_processing_ms: 1.7114135332640614\n",
      "  time_since_restore: 3245.705199956894\n",
      "  time_this_iter_s: 25.745007753372192\n",
      "  time_total_s: 3245.705199956894\n",
      "  timers:\n",
      "    learn_throughput: 1326.296\n",
      "    learn_time_ms: 753.98\n",
      "    load_throughput: 43004.829\n",
      "    load_time_ms: 23.253\n",
      "    sample_throughput: 37.338\n",
      "    sample_time_ms: 26782.273\n",
      "    update_time_ms: 3.879\n",
      "  timestamp: 1635285820\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 133000\n",
      "  training_iteration: 133\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">         3245.71</td><td style=\"text-align: right;\">133000</td><td style=\"text-align: right;\">  -3.301</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            286.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 134000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-04-05\n",
      "  done: false\n",
      "  episode_len_mean: 287.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.3038999999999845\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 376\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1589768118328518\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014492516422465289\n",
      "          policy_loss: -0.059007013258006835\n",
      "          total_loss: -0.06184819415211677\n",
      "          vf_explained_var: 0.7900031208992004\n",
      "          vf_loss: 0.005997276603658166\n",
      "    num_agent_steps_sampled: 134000\n",
      "    num_agent_steps_trained: 134000\n",
      "    num_steps_sampled: 134000\n",
      "    num_steps_trained: 134000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.69999999999999\n",
      "    ram_util_percent: 30.93428571428571\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03690521759942403\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.22950128764018\n",
      "    mean_inference_ms: 2.122419285555275\n",
      "    mean_raw_obs_processing_ms: 1.7142313362005288\n",
      "  time_since_restore: 3270.67600107193\n",
      "  time_this_iter_s: 24.97080111503601\n",
      "  time_total_s: 3270.67600107193\n",
      "  timers:\n",
      "    learn_throughput: 1325.174\n",
      "    learn_time_ms: 754.618\n",
      "    load_throughput: 42907.559\n",
      "    load_time_ms: 23.306\n",
      "    sample_throughput: 37.408\n",
      "    sample_time_ms: 26732.337\n",
      "    update_time_ms: 3.88\n",
      "  timestamp: 1635285845\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 134000\n",
      "  training_iteration: 134\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         3270.68</td><td style=\"text-align: right;\">134000</td><td style=\"text-align: right;\"> -3.3039</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            287.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 135000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-04-25\n",
      "  done: false\n",
      "  episode_len_mean: 289.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.3303999999999836\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 378\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5264506141344707\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02596486716891248\n",
      "          policy_loss: 0.10714198268122144\n",
      "          total_loss: 0.10389681690269047\n",
      "          vf_explained_var: 0.7836861610412598\n",
      "          vf_loss: 0.0070900725640563505\n",
      "    num_agent_steps_sampled: 135000\n",
      "    num_agent_steps_trained: 135000\n",
      "    num_steps_sampled: 135000\n",
      "    num_steps_trained: 135000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.56206896551724\n",
      "    ram_util_percent: 30.89999999999999\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036903036268821784\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.245763928841015\n",
      "    mean_inference_ms: 2.12234008436349\n",
      "    mean_raw_obs_processing_ms: 1.7156170739653447\n",
      "  time_since_restore: 3290.6493179798126\n",
      "  time_this_iter_s: 19.97331690788269\n",
      "  time_total_s: 3290.6493179798126\n",
      "  timers:\n",
      "    learn_throughput: 1323.973\n",
      "    learn_time_ms: 755.302\n",
      "    load_throughput: 42845.276\n",
      "    load_time_ms: 23.34\n",
      "    sample_throughput: 38.006\n",
      "    sample_time_ms: 26311.289\n",
      "    update_time_ms: 3.796\n",
      "  timestamp: 1635285865\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 135000\n",
      "  training_iteration: 135\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         3290.65</td><td style=\"text-align: right;\">135000</td><td style=\"text-align: right;\"> -3.3304</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            289.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-04-48\n",
      "  done: false\n",
      "  episode_len_mean: 290.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -3.334199999999983\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 381\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4008844918674892\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008627027649374005\n",
      "          policy_loss: 0.07326915773252646\n",
      "          total_loss: 0.06981124766170979\n",
      "          vf_explained_var: 0.6873680353164673\n",
      "          vf_loss: 0.008094252614925305\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.62058823529412\n",
      "    ram_util_percent: 30.858823529411758\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03689986567883444\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.269104765147514\n",
      "    mean_inference_ms: 2.1222234042512262\n",
      "    mean_raw_obs_processing_ms: 1.7177579423941574\n",
      "  time_since_restore: 3314.4443860054016\n",
      "  time_this_iter_s: 23.79506802558899\n",
      "  time_total_s: 3314.4443860054016\n",
      "  timers:\n",
      "    learn_throughput: 1323.707\n",
      "    learn_time_ms: 755.454\n",
      "    load_throughput: 42994.205\n",
      "    load_time_ms: 23.259\n",
      "    sample_throughput: 38.366\n",
      "    sample_time_ms: 26064.796\n",
      "    update_time_ms: 3.879\n",
      "  timestamp: 1635285888\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 136\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         3314.44</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\"> -3.3342</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            290.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 137000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-05-14\n",
      "  done: false\n",
      "  episode_len_mean: 290.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.3699999999999914\n",
      "  episode_reward_mean: -3.3183999999999845\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 385\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2663840002483793\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011740849843215094\n",
      "          policy_loss: -0.009642419384585486\n",
      "          total_loss: -0.013744571059942246\n",
      "          vf_explained_var: 0.8216759562492371\n",
      "          vf_loss: 0.0052183007795570625\n",
      "    num_agent_steps_sampled: 137000\n",
      "    num_agent_steps_trained: 137000\n",
      "    num_steps_sampled: 137000\n",
      "    num_steps_trained: 137000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.64864864864866\n",
      "    ram_util_percent: 30.81351351351351\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03689573608781456\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.29979551902979\n",
      "    mean_inference_ms: 2.1220709845688477\n",
      "    mean_raw_obs_processing_ms: 1.7205800273093959\n",
      "  time_since_restore: 3340.2039618492126\n",
      "  time_this_iter_s: 25.759575843811035\n",
      "  time_total_s: 3340.2039618492126\n",
      "  timers:\n",
      "    learn_throughput: 1323.005\n",
      "    learn_time_ms: 755.855\n",
      "    load_throughput: 42772.92\n",
      "    load_time_ms: 23.379\n",
      "    sample_throughput: 38.09\n",
      "    sample_time_ms: 26253.469\n",
      "    update_time_ms: 3.943\n",
      "  timestamp: 1635285914\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 137000\n",
      "  training_iteration: 137\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">          3340.2</td><td style=\"text-align: right;\">137000</td><td style=\"text-align: right;\"> -3.3184</td><td style=\"text-align: right;\">               -1.37</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            290.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 138000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-05-36\n",
      "  done: false\n",
      "  episode_len_mean: 292.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.2991999999999835\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 388\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7075225684377882\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02401730621124813\n",
      "          policy_loss: -0.053478807873196074\n",
      "          total_loss: 0.031384302924076714\n",
      "          vf_explained_var: -0.02827630005776882\n",
      "          vf_loss: 0.09509903024364677\n",
      "    num_agent_steps_sampled: 138000\n",
      "    num_agent_steps_trained: 138000\n",
      "    num_steps_sampled: 138000\n",
      "    num_steps_trained: 138000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.55333333333333\n",
      "    ram_util_percent: 30.779999999999994\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0368927463089477\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.32090998193126\n",
      "    mean_inference_ms: 2.121958054315737\n",
      "    mean_raw_obs_processing_ms: 1.7228556945583013\n",
      "  time_since_restore: 3361.6045756340027\n",
      "  time_this_iter_s: 21.40061378479004\n",
      "  time_total_s: 3361.6045756340027\n",
      "  timers:\n",
      "    learn_throughput: 1323.69\n",
      "    learn_time_ms: 755.464\n",
      "    load_throughput: 42734.091\n",
      "    load_time_ms: 23.401\n",
      "    sample_throughput: 38.704\n",
      "    sample_time_ms: 25837.375\n",
      "    update_time_ms: 3.954\n",
      "  timestamp: 1635285936\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 138000\n",
      "  training_iteration: 138\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">          3361.6</td><td style=\"text-align: right;\">138000</td><td style=\"text-align: right;\"> -3.2992</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            292.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 139000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-06-15\n",
      "  done: false\n",
      "  episode_len_mean: 294.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.2963999999999833\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 391\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6468330714437696\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005460101568065549\n",
      "          policy_loss: 0.07044509289165338\n",
      "          total_loss: 0.06749338329666191\n",
      "          vf_explained_var: 0.7420144081115723\n",
      "          vf_loss: 0.011184346704329882\n",
      "    num_agent_steps_sampled: 139000\n",
      "    num_agent_steps_trained: 139000\n",
      "    num_steps_sampled: 139000\n",
      "    num_steps_trained: 139000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.838596491228074\n",
      "    ram_util_percent: 30.859649122807017\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036889777049728026\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.341209285469688\n",
      "    mean_inference_ms: 2.1218446096789227\n",
      "    mean_raw_obs_processing_ms: 1.728715493537046\n",
      "  time_since_restore: 3401.0710394382477\n",
      "  time_this_iter_s: 39.466463804244995\n",
      "  time_total_s: 3401.0710394382477\n",
      "  timers:\n",
      "    learn_throughput: 1324.996\n",
      "    learn_time_ms: 754.719\n",
      "    load_throughput: 42681.081\n",
      "    load_time_ms: 23.43\n",
      "    sample_throughput: 36.896\n",
      "    sample_time_ms: 27102.995\n",
      "    update_time_ms: 3.963\n",
      "  timestamp: 1635285975\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 139000\n",
      "  training_iteration: 139\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         3401.07</td><td style=\"text-align: right;\">139000</td><td style=\"text-align: right;\"> -3.2964</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            294.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-06-35\n",
      "  done: false\n",
      "  episode_len_mean: 297.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.352699999999983\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 393\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6961319247881572\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015788029296400413\n",
      "          policy_loss: -0.0029411191741625466\n",
      "          total_loss: 0.1567590273088879\n",
      "          vf_explained_var: 0.1589200794696808\n",
      "          vf_loss: 0.169917633684559\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.9857142857143\n",
      "    ram_util_percent: 31.457142857142856\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03688785609693041\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.353714465184645\n",
      "    mean_inference_ms: 2.1217690868035284\n",
      "    mean_raw_obs_processing_ms: 1.7326043841729308\n",
      "  time_since_restore: 3420.925524711609\n",
      "  time_this_iter_s: 19.854485273361206\n",
      "  time_total_s: 3420.925524711609\n",
      "  timers:\n",
      "    learn_throughput: 1325.187\n",
      "    learn_time_ms: 754.611\n",
      "    load_throughput: 42659.723\n",
      "    load_time_ms: 23.441\n",
      "    sample_throughput: 40.559\n",
      "    sample_time_ms: 24655.274\n",
      "    update_time_ms: 3.967\n",
      "  timestamp: 1635285995\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 140\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">         3420.93</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\"> -3.3527</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            297.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 141000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-06-53\n",
      "  done: false\n",
      "  episode_len_mean: 300.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.370599999999983\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 395\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7719878686798944\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007947280785936093\n",
      "          policy_loss: -0.15494706109166145\n",
      "          total_loss: -0.1443476660384072\n",
      "          vf_explained_var: 0.21035054326057434\n",
      "          vf_loss: 0.024924604625751575\n",
      "    num_agent_steps_sampled: 141000\n",
      "    num_agent_steps_trained: 141000\n",
      "    num_steps_sampled: 141000\n",
      "    num_steps_trained: 141000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.59230769230768\n",
      "    ram_util_percent: 31.326923076923077\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03688593707481435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.36521061666675\n",
      "    mean_inference_ms: 2.121691528639643\n",
      "    mean_raw_obs_processing_ms: 1.736385770784609\n",
      "  time_since_restore: 3438.995153427124\n",
      "  time_this_iter_s: 18.069628715515137\n",
      "  time_total_s: 3438.995153427124\n",
      "  timers:\n",
      "    learn_throughput: 1328.89\n",
      "    learn_time_ms: 752.508\n",
      "    load_throughput: 42813.175\n",
      "    load_time_ms: 23.357\n",
      "    sample_throughput: 41.972\n",
      "    sample_time_ms: 23825.306\n",
      "    update_time_ms: 3.895\n",
      "  timestamp: 1635286013\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 141000\n",
      "  training_iteration: 141\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">            3439</td><td style=\"text-align: right;\">141000</td><td style=\"text-align: right;\"> -3.3706</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            300.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 142000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-07-12\n",
      "  done: false\n",
      "  episode_len_mean: 301.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.264599999999982\n",
      "  episode_reward_min: -20.29000000000017\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 398\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8287329024738737\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009301853591465913\n",
      "          policy_loss: 0.003509977294339074\n",
      "          total_loss: 0.011353949705759684\n",
      "          vf_explained_var: 0.4773760139942169\n",
      "          vf_loss: 0.02215802982553012\n",
      "    num_agent_steps_sampled: 142000\n",
      "    num_agent_steps_trained: 142000\n",
      "    num_steps_sampled: 142000\n",
      "    num_steps_trained: 142000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.53571428571429\n",
      "    ram_util_percent: 30.992857142857144\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03688308472169785\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.381542138044114\n",
      "    mean_inference_ms: 2.121574393981154\n",
      "    mean_raw_obs_processing_ms: 1.7419779376677134\n",
      "  time_since_restore: 3458.3936223983765\n",
      "  time_this_iter_s: 19.39846897125244\n",
      "  time_total_s: 3458.3936223983765\n",
      "  timers:\n",
      "    learn_throughput: 1330.192\n",
      "    learn_time_ms: 751.771\n",
      "    load_throughput: 41661.202\n",
      "    load_time_ms: 24.003\n",
      "    sample_throughput: 43.366\n",
      "    sample_time_ms: 23059.57\n",
      "    update_time_ms: 3.806\n",
      "  timestamp: 1635286032\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 142000\n",
      "  training_iteration: 142\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         3458.39</td><td style=\"text-align: right;\">142000</td><td style=\"text-align: right;\"> -3.2646</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -20.29</td><td style=\"text-align: right;\">            301.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 143000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-07-31\n",
      "  done: false\n",
      "  episode_len_mean: 304.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.1449999999999796\n",
      "  episode_reward_min: -11.549999999999937\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 400\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8273127992947897\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007110824947389361\n",
      "          policy_loss: -0.14596704178386263\n",
      "          total_loss: -0.14156670164730814\n",
      "          vf_explained_var: 0.13330630958080292\n",
      "          vf_loss: 0.019636089406493636\n",
      "    num_agent_steps_sampled: 143000\n",
      "    num_agent_steps_trained: 143000\n",
      "    num_steps_sampled: 143000\n",
      "    num_steps_trained: 143000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.55555555555556\n",
      "    ram_util_percent: 30.91481481481481\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036881214485173855\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.391297596955024\n",
      "    mean_inference_ms: 2.121496913240644\n",
      "    mean_raw_obs_processing_ms: 1.7427381661387178\n",
      "  time_since_restore: 3477.353039264679\n",
      "  time_this_iter_s: 18.95941686630249\n",
      "  time_total_s: 3477.353039264679\n",
      "  timers:\n",
      "    learn_throughput: 1327.32\n",
      "    learn_time_ms: 753.398\n",
      "    load_throughput: 41589.941\n",
      "    load_time_ms: 24.044\n",
      "    sample_throughput: 44.684\n",
      "    sample_time_ms: 22379.444\n",
      "    update_time_ms: 3.718\n",
      "  timestamp: 1635286051\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 143000\n",
      "  training_iteration: 143\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">         3477.35</td><td style=\"text-align: right;\">143000</td><td style=\"text-align: right;\">  -3.145</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.55</td><td style=\"text-align: right;\">            304.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-07-52\n",
      "  done: false\n",
      "  episode_len_mean: 305.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.04729999999998\n",
      "  episode_reward_min: -6.609999999999933\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 403\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8895086208979288\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014240508838818715\n",
      "          policy_loss: -0.12555986568331717\n",
      "          total_loss: -0.12935439894596737\n",
      "          vf_explained_var: 0.8563286066055298\n",
      "          vf_loss: 0.009017739341490798\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.48\n",
      "    ram_util_percent: 30.889999999999997\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03687833384720881\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.40528002963737\n",
      "    mean_inference_ms: 2.121378234193904\n",
      "    mean_raw_obs_processing_ms: 1.7436854280467475\n",
      "  time_since_restore: 3498.2809603214264\n",
      "  time_this_iter_s: 20.927921056747437\n",
      "  time_total_s: 3498.2809603214264\n",
      "  timers:\n",
      "    learn_throughput: 1328.83\n",
      "    learn_time_ms: 752.542\n",
      "    load_throughput: 41484.43\n",
      "    load_time_ms: 24.105\n",
      "    sample_throughput: 45.504\n",
      "    sample_time_ms: 21975.899\n",
      "    update_time_ms: 3.77\n",
      "  timestamp: 1635286072\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 144\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         3498.28</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\"> -3.0473</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">               -6.61</td><td style=\"text-align: right;\">            305.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 145000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-08-12\n",
      "  done: false\n",
      "  episode_len_mean: 307.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.152699999999979\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 406\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.004519049326579\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013435069909406986\n",
      "          policy_loss: 0.0677464731865459\n",
      "          total_loss: 0.18696191252933608\n",
      "          vf_explained_var: 0.2009071409702301\n",
      "          vf_loss: 0.13352186613612704\n",
      "    num_agent_steps_sampled: 145000\n",
      "    num_agent_steps_trained: 145000\n",
      "    num_steps_sampled: 145000\n",
      "    num_steps_trained: 145000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.54285714285713\n",
      "    ram_util_percent: 30.88214285714285\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03687550055892228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.417755512896782\n",
      "    mean_inference_ms: 2.1212594466131036\n",
      "    mean_raw_obs_processing_ms: 1.7447070645852363\n",
      "  time_since_restore: 3518.237668275833\n",
      "  time_this_iter_s: 19.95670795440674\n",
      "  time_total_s: 3518.237668275833\n",
      "  timers:\n",
      "    learn_throughput: 1328.89\n",
      "    learn_time_ms: 752.508\n",
      "    load_throughput: 41590.024\n",
      "    load_time_ms: 24.044\n",
      "    sample_throughput: 45.508\n",
      "    sample_time_ms: 21974.338\n",
      "    update_time_ms: 3.771\n",
      "  timestamp: 1635286092\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 145000\n",
      "  training_iteration: 145\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   145</td><td style=\"text-align: right;\">         3518.24</td><td style=\"text-align: right;\">145000</td><td style=\"text-align: right;\"> -3.1527</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            307.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 146000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-08-32\n",
      "  done: false\n",
      "  episode_len_mean: 310.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.1774999999999785\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 408\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9313336703512403\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010515472270533266\n",
      "          policy_loss: -0.13864523205492232\n",
      "          total_loss: -0.13116411401165856\n",
      "          vf_explained_var: 0.7621058821678162\n",
      "          vf_loss: 0.02230279156162093\n",
      "    num_agent_steps_sampled: 146000\n",
      "    num_agent_steps_trained: 146000\n",
      "    num_steps_sampled: 146000\n",
      "    num_steps_trained: 146000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.47586206896551\n",
      "    ram_util_percent: 30.88965517241379\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03687359192225685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.425230275103964\n",
      "    mean_inference_ms: 2.1211777039120943\n",
      "    mean_raw_obs_processing_ms: 1.7453431936853656\n",
      "  time_since_restore: 3538.275404691696\n",
      "  time_this_iter_s: 20.037736415863037\n",
      "  time_total_s: 3538.275404691696\n",
      "  timers:\n",
      "    learn_throughput: 1327.996\n",
      "    learn_time_ms: 753.014\n",
      "    load_throughput: 41493.048\n",
      "    load_time_ms: 24.1\n",
      "    sample_throughput: 46.3\n",
      "    sample_time_ms: 21598.125\n",
      "    update_time_ms: 3.682\n",
      "  timestamp: 1635286112\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 146000\n",
      "  training_iteration: 146\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         3538.28</td><td style=\"text-align: right;\">146000</td><td style=\"text-align: right;\"> -3.1775</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            310.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 147000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-08-54\n",
      "  done: false\n",
      "  episode_len_mean: 312.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.195099999999978\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 411\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9577445891168384\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007491656985137377\n",
      "          policy_loss: -0.07880350210600429\n",
      "          total_loss: -0.07033848547273212\n",
      "          vf_explained_var: 0.8238376975059509\n",
      "          vf_loss: 0.024842411362462572\n",
      "    num_agent_steps_sampled: 147000\n",
      "    num_agent_steps_trained: 147000\n",
      "    num_steps_sampled: 147000\n",
      "    num_steps_trained: 147000\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.69\n",
      "    ram_util_percent: 30.823333333333327\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03687073918954636\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.43534624695758\n",
      "    mean_inference_ms: 2.1210532265329163\n",
      "    mean_raw_obs_processing_ms: 1.7462422615193984\n",
      "  time_since_restore: 3559.687295436859\n",
      "  time_this_iter_s: 21.411890745162964\n",
      "  time_total_s: 3559.687295436859\n",
      "  timers:\n",
      "    learn_throughput: 1326.791\n",
      "    learn_time_ms: 753.698\n",
      "    load_throughput: 41626.512\n",
      "    load_time_ms: 24.023\n",
      "    sample_throughput: 47.253\n",
      "    sample_time_ms: 21162.733\n",
      "    update_time_ms: 3.683\n",
      "  timestamp: 1635286134\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 147000\n",
      "  training_iteration: 147\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         3559.69</td><td style=\"text-align: right;\">147000</td><td style=\"text-align: right;\"> -3.1951</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            312.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-09-15\n",
      "  done: false\n",
      "  episode_len_mean: 314.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.207699999999978\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 414\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9499300109015572\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01307893923618898\n",
      "          policy_loss: -0.02145984884765413\n",
      "          total_loss: 0.11703795327080621\n",
      "          vf_explained_var: 0.2886740267276764\n",
      "          vf_loss: 0.15241045447376866\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.59354838709677\n",
      "    ram_util_percent: 30.80967741935483\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03686792750107017\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.444078201270386\n",
      "    mean_inference_ms: 2.1209298608899503\n",
      "    mean_raw_obs_processing_ms: 1.7472093813086866\n",
      "  time_since_restore: 3580.9194979667664\n",
      "  time_this_iter_s: 21.232202529907227\n",
      "  time_total_s: 3580.9194979667664\n",
      "  timers:\n",
      "    learn_throughput: 1326.321\n",
      "    learn_time_ms: 753.965\n",
      "    load_throughput: 41583.633\n",
      "    load_time_ms: 24.048\n",
      "    sample_throughput: 47.291\n",
      "    sample_time_ms: 21145.605\n",
      "    update_time_ms: 3.671\n",
      "  timestamp: 1635286155\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 148\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">         3580.92</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\"> -3.2077</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            314.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 149000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-09-37\n",
      "  done: false\n",
      "  episode_len_mean: 316.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.229499999999977\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 417\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.992124123043484\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011304258560680106\n",
      "          policy_loss: -0.051667586093147595\n",
      "          total_loss: -0.020162198754648367\n",
      "          vf_explained_var: 0.6225396394729614\n",
      "          vf_loss: 0.0465980330740826\n",
      "    num_agent_steps_sampled: 149000\n",
      "    num_agent_steps_trained: 149000\n",
      "    num_steps_sampled: 149000\n",
      "    num_steps_trained: 149000\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.59666666666666\n",
      "    ram_util_percent: 30.756666666666668\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036865097837354886\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.451828995820062\n",
      "    mean_inference_ms: 2.12080584559377\n",
      "    mean_raw_obs_processing_ms: 1.7481174985145247\n",
      "  time_since_restore: 3602.3792808055878\n",
      "  time_this_iter_s: 21.45978283882141\n",
      "  time_total_s: 3602.3792808055878\n",
      "  timers:\n",
      "    learn_throughput: 1326.777\n",
      "    learn_time_ms: 753.706\n",
      "    load_throughput: 41543.639\n",
      "    load_time_ms: 24.071\n",
      "    sample_throughput: 51.692\n",
      "    sample_time_ms: 19345.192\n",
      "    update_time_ms: 3.661\n",
      "  timestamp: 1635286177\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 149000\n",
      "  training_iteration: 149\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         3602.38</td><td style=\"text-align: right;\">149000</td><td style=\"text-align: right;\"> -3.2295</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            316.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 150000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-10-14\n",
      "  done: false\n",
      "  episode_len_mean: 319.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.2574999999999767\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 420\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.105990489323934\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011965464386092901\n",
      "          policy_loss: -0.009267827620108922\n",
      "          total_loss: -0.009357398996750513\n",
      "          vf_explained_var: 0.7719511389732361\n",
      "          vf_loss: 0.01585930545762595\n",
      "    num_agent_steps_sampled: 150000\n",
      "    num_agent_steps_trained: 150000\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.52037037037038\n",
      "    ram_util_percent: 30.78148148148148\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03686227648496073\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.458175931881375\n",
      "    mean_inference_ms: 2.120680883531391\n",
      "    mean_raw_obs_processing_ms: 1.7525588816188913\n",
      "  time_since_restore: 3640.031299829483\n",
      "  time_this_iter_s: 37.652019023895264\n",
      "  time_total_s: 3640.031299829483\n",
      "  timers:\n",
      "    learn_throughput: 1326.103\n",
      "    learn_time_ms: 754.089\n",
      "    load_throughput: 41721.291\n",
      "    load_time_ms: 23.969\n",
      "    sample_throughput: 47.338\n",
      "    sample_time_ms: 21124.664\n",
      "    update_time_ms: 3.651\n",
      "  timestamp: 1635286214\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 150\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         3640.03</td><td style=\"text-align: right;\">150000</td><td style=\"text-align: right;\"> -3.2575</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            319.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 151000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-10-36\n",
      "  done: false\n",
      "  episode_len_mean: 320.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.269599999999976\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 422\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1331271118587916\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005507875096301085\n",
      "          policy_loss: 0.058241678391479784\n",
      "          total_loss: 0.046624735059837503\n",
      "          vf_explained_var: 0.8944633603096008\n",
      "          vf_loss: 0.0073616467700857256\n",
      "    num_agent_steps_sampled: 151000\n",
      "    num_agent_steps_trained: 151000\n",
      "    num_steps_sampled: 151000\n",
      "    num_steps_trained: 151000\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.05806451612904\n",
      "    ram_util_percent: 31.2225806451613\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036860422554760006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.461762878939563\n",
      "    mean_inference_ms: 2.120598655891669\n",
      "    mean_raw_obs_processing_ms: 1.7555484231376277\n",
      "  time_since_restore: 3661.714875936508\n",
      "  time_this_iter_s: 21.683576107025146\n",
      "  time_total_s: 3661.714875936508\n",
      "  timers:\n",
      "    learn_throughput: 1325.182\n",
      "    learn_time_ms: 754.613\n",
      "    load_throughput: 41511.774\n",
      "    load_time_ms: 24.09\n",
      "    sample_throughput: 46.543\n",
      "    sample_time_ms: 21485.349\n",
      "    update_time_ms: 3.709\n",
      "  timestamp: 1635286236\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 151000\n",
      "  training_iteration: 151\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         3661.71</td><td style=\"text-align: right;\">151000</td><td style=\"text-align: right;\"> -3.2696</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            320.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-10-58\n",
      "  done: false\n",
      "  episode_len_mean: 322.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.268699999999976\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 425\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0805513064066568\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00887743822252247\n",
      "          policy_loss: 0.027410916942689153\n",
      "          total_loss: 0.017052886759241423\n",
      "          vf_explained_var: 0.8392413854598999\n",
      "          vf_loss: 0.006655497409196363\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.76451612903226\n",
      "    ram_util_percent: 30.99354838709677\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03685767993349784\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.46630478996179\n",
      "    mean_inference_ms: 2.1204764227220534\n",
      "    mean_raw_obs_processing_ms: 1.7599566302821459\n",
      "  time_since_restore: 3683.5578615665436\n",
      "  time_this_iter_s: 21.8429856300354\n",
      "  time_total_s: 3683.5578615665436\n",
      "  timers:\n",
      "    learn_throughput: 1325.617\n",
      "    learn_time_ms: 754.366\n",
      "    load_throughput: 41622.588\n",
      "    load_time_ms: 24.025\n",
      "    sample_throughput: 46.019\n",
      "    sample_time_ms: 21730.147\n",
      "    update_time_ms: 3.716\n",
      "  timestamp: 1635286258\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 152\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         3683.56</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\"> -3.2687</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            322.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 153000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-11-19\n",
      "  done: false\n",
      "  episode_len_mean: 324.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.2833999999999754\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 428\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.122558131482866\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01179428720229357\n",
      "          policy_loss: -0.011346858574284448\n",
      "          total_loss: 0.018919939837521976\n",
      "          vf_explained_var: 0.47791579365730286\n",
      "          vf_loss: 0.04645447197318491\n",
      "    num_agent_steps_sampled: 153000\n",
      "    num_agent_steps_trained: 153000\n",
      "    num_steps_sampled: 153000\n",
      "    num_steps_trained: 153000\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.58709677419354\n",
      "    ram_util_percent: 30.880645161290317\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036855009643200765\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.46988401375251\n",
      "    mean_inference_ms: 2.120356145283615\n",
      "    mean_raw_obs_processing_ms: 1.7615913077849126\n",
      "  time_since_restore: 3704.899394750595\n",
      "  time_this_iter_s: 21.341533184051514\n",
      "  time_total_s: 3704.899394750595\n",
      "  timers:\n",
      "    learn_throughput: 1325.648\n",
      "    learn_time_ms: 754.348\n",
      "    load_throughput: 41939.853\n",
      "    load_time_ms: 23.844\n",
      "    sample_throughput: 45.52\n",
      "    sample_time_ms: 21968.537\n",
      "    update_time_ms: 3.731\n",
      "  timestamp: 1635286279\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 153000\n",
      "  training_iteration: 153\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">          3704.9</td><td style=\"text-align: right;\">153000</td><td style=\"text-align: right;\"> -3.2834</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            324.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 154000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-11-39\n",
      "  done: false\n",
      "  episode_len_mean: 327.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.3019999999999747\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 431\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2450922436184353\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.024244094058563922\n",
      "          policy_loss: -0.0067287792762120565\n",
      "          total_loss: 0.03611660272710853\n",
      "          vf_explained_var: 0.5561051964759827\n",
      "          vf_loss: 0.05494047702652299\n",
      "    num_agent_steps_sampled: 154000\n",
      "    num_agent_steps_trained: 154000\n",
      "    num_steps_sampled: 154000\n",
      "    num_steps_trained: 154000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.52142857142857\n",
      "    ram_util_percent: 30.939285714285713\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03685232865558792\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.472392871503427\n",
      "    mean_inference_ms: 2.120234305985184\n",
      "    mean_raw_obs_processing_ms: 1.761754256563536\n",
      "  time_since_restore: 3724.8106598854065\n",
      "  time_this_iter_s: 19.9112651348114\n",
      "  time_total_s: 3724.8106598854065\n",
      "  timers:\n",
      "    learn_throughput: 1325.622\n",
      "    learn_time_ms: 754.363\n",
      "    load_throughput: 41857.442\n",
      "    load_time_ms: 23.891\n",
      "    sample_throughput: 45.731\n",
      "    sample_time_ms: 21866.856\n",
      "    update_time_ms: 3.676\n",
      "  timestamp: 1635286299\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 154000\n",
      "  training_iteration: 154\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         3724.81</td><td style=\"text-align: right;\">154000</td><td style=\"text-align: right;\">  -3.302</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            327.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 155000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-12-01\n",
      "  done: false\n",
      "  episode_len_mean: 329.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.321299999999975\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 434\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2172328816519844\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008646088709338586\n",
      "          policy_loss: -0.03595864673455556\n",
      "          total_loss: -0.04804618838760588\n",
      "          vf_explained_var: 0.7718023657798767\n",
      "          vf_loss: 0.004545040466150062\n",
      "    num_agent_steps_sampled: 155000\n",
      "    num_agent_steps_trained: 155000\n",
      "    num_steps_sampled: 155000\n",
      "    num_steps_trained: 155000\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.60645161290323\n",
      "    ram_util_percent: 30.9258064516129\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036849654671058295\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.473699853325147\n",
      "    mean_inference_ms: 2.1201127056023124\n",
      "    mean_raw_obs_processing_ms: 1.7619902997096992\n",
      "  time_since_restore: 3746.353227376938\n",
      "  time_this_iter_s: 21.542567491531372\n",
      "  time_total_s: 3746.353227376938\n",
      "  timers:\n",
      "    learn_throughput: 1325.257\n",
      "    learn_time_ms: 754.571\n",
      "    load_throughput: 41744.545\n",
      "    load_time_ms: 23.955\n",
      "    sample_throughput: 45.403\n",
      "    sample_time_ms: 22025.152\n",
      "    update_time_ms: 3.679\n",
      "  timestamp: 1635286321\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 155000\n",
      "  training_iteration: 155\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">         3746.35</td><td style=\"text-align: right;\">155000</td><td style=\"text-align: right;\"> -3.3213</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">             329.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-12-21\n",
      "  done: false\n",
      "  episode_len_mean: 331.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.3195999999999746\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 436\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.224218805631002\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009127578959070308\n",
      "          policy_loss: -0.13354377816948626\n",
      "          total_loss: -0.10740843245552646\n",
      "          vf_explained_var: 0.5513020753860474\n",
      "          vf_loss: 0.04252928693571852\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.59310344827587\n",
      "    ram_util_percent: 30.92413793103448\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036847876186446187\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.473966534451854\n",
      "    mean_inference_ms: 2.1200319201468476\n",
      "    mean_raw_obs_processing_ms: 1.7621078487719979\n",
      "  time_since_restore: 3766.926285266876\n",
      "  time_this_iter_s: 20.573057889938354\n",
      "  time_total_s: 3766.926285266876\n",
      "  timers:\n",
      "    learn_throughput: 1327.189\n",
      "    learn_time_ms: 753.472\n",
      "    load_throughput: 41852.722\n",
      "    load_time_ms: 23.893\n",
      "    sample_throughput: 45.29\n",
      "    sample_time_ms: 22079.847\n",
      "    update_time_ms: 3.687\n",
      "  timestamp: 1635286341\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 156\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   156</td><td style=\"text-align: right;\">         3766.93</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\"> -3.3196</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            331.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 157000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-12-41\n",
      "  done: false\n",
      "  episode_len_mean: 334.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.387499999999974\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 439\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.250421979692247\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008698979320487549\n",
      "          policy_loss: 0.09661971082290013\n",
      "          total_loss: 0.16042745278941262\n",
      "          vf_explained_var: 0.32823580503463745\n",
      "          vf_loss: 0.08073832900780771\n",
      "    num_agent_steps_sampled: 157000\n",
      "    num_agent_steps_trained: 157000\n",
      "    num_steps_sampled: 157000\n",
      "    num_steps_trained: 157000\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.55\n",
      "    ram_util_percent: 30.91785714285714\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03684518130406125\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.473363411752583\n",
      "    mean_inference_ms: 2.119908094358521\n",
      "    mean_raw_obs_processing_ms: 1.7622374832285146\n",
      "  time_since_restore: 3786.6775522232056\n",
      "  time_this_iter_s: 19.751266956329346\n",
      "  time_total_s: 3786.6775522232056\n",
      "  timers:\n",
      "    learn_throughput: 1328.781\n",
      "    learn_time_ms: 752.569\n",
      "    load_throughput: 41912.821\n",
      "    load_time_ms: 23.859\n",
      "    sample_throughput: 45.631\n",
      "    sample_time_ms: 21914.81\n",
      "    update_time_ms: 3.607\n",
      "  timestamp: 1635286361\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 157000\n",
      "  training_iteration: 157\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">         3786.68</td><td style=\"text-align: right;\">157000</td><td style=\"text-align: right;\"> -3.3875</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            334.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 158000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-13-01\n",
      "  done: false\n",
      "  episode_len_mean: 336.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.3984999999999737\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 442\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.300723444090949\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008363813882318025\n",
      "          policy_loss: -0.047031883978181414\n",
      "          total_loss: 0.02425921360651652\n",
      "          vf_explained_var: 0.34326526522636414\n",
      "          vf_loss: 0.08893944753540886\n",
      "    num_agent_steps_sampled: 158000\n",
      "    num_agent_steps_trained: 158000\n",
      "    num_steps_sampled: 158000\n",
      "    num_steps_trained: 158000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.61\n",
      "    ram_util_percent: 30.939999999999998\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03684251406470229\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.471860947740037\n",
      "    mean_inference_ms: 2.119784715058549\n",
      "    mean_raw_obs_processing_ms: 1.7624394354627264\n",
      "  time_since_restore: 3807.0592827796936\n",
      "  time_this_iter_s: 20.381730556488037\n",
      "  time_total_s: 3807.0592827796936\n",
      "  timers:\n",
      "    learn_throughput: 1328.94\n",
      "    learn_time_ms: 752.479\n",
      "    load_throughput: 41918.937\n",
      "    load_time_ms: 23.856\n",
      "    sample_throughput: 45.809\n",
      "    sample_time_ms: 21829.853\n",
      "    update_time_ms: 3.624\n",
      "  timestamp: 1635286381\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 158000\n",
      "  training_iteration: 158\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">         3807.06</td><td style=\"text-align: right;\">158000</td><td style=\"text-align: right;\"> -3.3985</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            336.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 159000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-13-21\n",
      "  done: false\n",
      "  episode_len_mean: 337.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.4141999999999735\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 444\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.260968404346042\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012382988359573252\n",
      "          policy_loss: -0.09254080951213836\n",
      "          total_loss: -0.08948864829209116\n",
      "          vf_explained_var: 0.5106104612350464\n",
      "          vf_loss: 0.017727783932867978\n",
      "    num_agent_steps_sampled: 159000\n",
      "    num_agent_steps_trained: 159000\n",
      "    num_steps_sampled: 159000\n",
      "    num_steps_trained: 159000\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.64444444444445\n",
      "    ram_util_percent: 30.89259259259259\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03684071450911891\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.4703876172696\n",
      "    mean_inference_ms: 2.1197008096310217\n",
      "    mean_raw_obs_processing_ms: 1.7624569686406955\n",
      "  time_since_restore: 3826.4466457366943\n",
      "  time_this_iter_s: 19.387362957000732\n",
      "  time_total_s: 3826.4466457366943\n",
      "  timers:\n",
      "    learn_throughput: 1328.179\n",
      "    learn_time_ms: 752.911\n",
      "    load_throughput: 41930.503\n",
      "    load_time_ms: 23.849\n",
      "    sample_throughput: 46.249\n",
      "    sample_time_ms: 21622.198\n",
      "    update_time_ms: 3.626\n",
      "  timestamp: 1635286401\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 159000\n",
      "  training_iteration: 159\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">         3826.45</td><td style=\"text-align: right;\">159000</td><td style=\"text-align: right;\"> -3.4142</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            337.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-13-41\n",
      "  done: false\n",
      "  episode_len_mean: 340.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.4450999999999734\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 447\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.31755743821462\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007063835107683462\n",
      "          policy_loss: -0.020313807825247446\n",
      "          total_loss: -0.008380269590351317\n",
      "          vf_explained_var: 0.4026198387145996\n",
      "          vf_loss: 0.030583150257977348\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.65862068965517\n",
      "    ram_util_percent: 30.90344827586206\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03683799738722845\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.466815599381007\n",
      "    mean_inference_ms: 2.1195765337681234\n",
      "    mean_raw_obs_processing_ms: 1.7625568814369188\n",
      "  time_since_restore: 3846.351330757141\n",
      "  time_this_iter_s: 19.904685020446777\n",
      "  time_total_s: 3846.351330757141\n",
      "  timers:\n",
      "    learn_throughput: 1328.876\n",
      "    learn_time_ms: 752.516\n",
      "    load_throughput: 41855.019\n",
      "    load_time_ms: 23.892\n",
      "    sample_throughput: 50.383\n",
      "    sample_time_ms: 19847.817\n",
      "    update_time_ms: 3.633\n",
      "  timestamp: 1635286421\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 160\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         3846.35</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\"> -3.4451</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            340.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 161000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-14-18\n",
      "  done: false\n",
      "  episode_len_mean: 343.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.468899999999972\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 450\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2436168511708576\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004530881777275833\n",
      "          policy_loss: 0.005971306727992164\n",
      "          total_loss: -0.005488702240917418\n",
      "          vf_explained_var: 0.5266739726066589\n",
      "          vf_loss: 0.008073120437458985\n",
      "    num_agent_steps_sampled: 161000\n",
      "    num_agent_steps_trained: 161000\n",
      "    num_steps_sampled: 161000\n",
      "    num_steps_trained: 161000\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.53076923076923\n",
      "    ram_util_percent: 30.89807692307693\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03683529872882274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.462475832152567\n",
      "    mean_inference_ms: 2.1194543276124076\n",
      "    mean_raw_obs_processing_ms: 1.7658900793094825\n",
      "  time_since_restore: 3883.279642343521\n",
      "  time_this_iter_s: 36.928311586380005\n",
      "  time_total_s: 3883.279642343521\n",
      "  timers:\n",
      "    learn_throughput: 1329.064\n",
      "    learn_time_ms: 752.409\n",
      "    load_throughput: 41936.246\n",
      "    load_time_ms: 23.846\n",
      "    sample_throughput: 46.789\n",
      "    sample_time_ms: 21372.512\n",
      "    update_time_ms: 3.569\n",
      "  timestamp: 1635286458\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 161000\n",
      "  training_iteration: 161\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">         3883.28</td><td style=\"text-align: right;\">161000</td><td style=\"text-align: right;\"> -3.4689</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            343.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 162000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-14-37\n",
      "  done: false\n",
      "  episode_len_mean: 344.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.480499999999972\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 452\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2443522029452856\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009288977819974672\n",
      "          policy_loss: -0.017290725600388315\n",
      "          total_loss: -0.0031381814637117915\n",
      "          vf_explained_var: 0.4549788236618042\n",
      "          vf_loss: 0.03362023709859285\n",
      "    num_agent_steps_sampled: 162000\n",
      "    num_agent_steps_trained: 162000\n",
      "    num_steps_sampled: 162000\n",
      "    num_steps_trained: 162000\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.075\n",
      "    ram_util_percent: 31.4\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03683350655258632\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.458759532360833\n",
      "    mean_inference_ms: 2.1193734265852644\n",
      "    mean_raw_obs_processing_ms: 1.768139206299349\n",
      "  time_since_restore: 3902.730222225189\n",
      "  time_this_iter_s: 19.45057988166809\n",
      "  time_total_s: 3902.730222225189\n",
      "  timers:\n",
      "    learn_throughput: 1329.892\n",
      "    learn_time_ms: 751.941\n",
      "    load_throughput: 41912.528\n",
      "    load_time_ms: 23.859\n",
      "    sample_throughput: 47.318\n",
      "    sample_time_ms: 21133.722\n",
      "    update_time_ms: 3.56\n",
      "  timestamp: 1635286477\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 162000\n",
      "  training_iteration: 162\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         3902.73</td><td style=\"text-align: right;\">162000</td><td style=\"text-align: right;\"> -3.4805</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            344.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 163000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-14-57\n",
      "  done: false\n",
      "  episode_len_mean: 348.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.516799999999971\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 455\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1863873375786675\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009686057964904727\n",
      "          policy_loss: 0.015573078269759814\n",
      "          total_loss: 0.006245118172632323\n",
      "          vf_explained_var: 0.3979330360889435\n",
      "          vf_loss: 0.009432873031538393\n",
      "    num_agent_steps_sampled: 163000\n",
      "    num_agent_steps_trained: 163000\n",
      "    num_steps_sampled: 163000\n",
      "    num_steps_trained: 163000\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.66785714285716\n",
      "    ram_util_percent: 31.235714285714288\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03683076242249018\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.452325814962286\n",
      "    mean_inference_ms: 2.1192493606750378\n",
      "    mean_raw_obs_processing_ms: 1.7713311570410237\n",
      "  time_since_restore: 3922.238339662552\n",
      "  time_this_iter_s: 19.50811743736267\n",
      "  time_total_s: 3922.238339662552\n",
      "  timers:\n",
      "    learn_throughput: 1329.585\n",
      "    learn_time_ms: 752.114\n",
      "    load_throughput: 41618.003\n",
      "    load_time_ms: 24.028\n",
      "    sample_throughput: 47.733\n",
      "    sample_time_ms: 20950.049\n",
      "    update_time_ms: 3.548\n",
      "  timestamp: 1635286497\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 163000\n",
      "  training_iteration: 163\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   163</td><td style=\"text-align: right;\">         3922.24</td><td style=\"text-align: right;\">163000</td><td style=\"text-align: right;\"> -3.5168</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            348.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-15-16\n",
      "  done: false\n",
      "  episode_len_mean: 350.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.548899999999971\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 457\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.188035672240787\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012778331139882207\n",
      "          policy_loss: -0.0019674893882539535\n",
      "          total_loss: 0.0334624310127563\n",
      "          vf_explained_var: 0.22866210341453552\n",
      "          vf_loss: 0.05321659750512077\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.4857142857143\n",
      "    ram_util_percent: 30.928571428571423\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036828912891422066\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.447256162640027\n",
      "    mean_inference_ms: 2.119165285218239\n",
      "    mean_raw_obs_processing_ms: 1.7734126996310877\n",
      "  time_since_restore: 3941.732889175415\n",
      "  time_this_iter_s: 19.49454951286316\n",
      "  time_total_s: 3941.732889175415\n",
      "  timers:\n",
      "    learn_throughput: 1329.348\n",
      "    learn_time_ms: 752.249\n",
      "    load_throughput: 41948.619\n",
      "    load_time_ms: 23.839\n",
      "    sample_throughput: 47.828\n",
      "    sample_time_ms: 20908.344\n",
      "    update_time_ms: 3.643\n",
      "  timestamp: 1635286516\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 164\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         3941.73</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\"> -3.5489</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            350.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 165000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-15-36\n",
      "  done: false\n",
      "  episode_len_mean: 353.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.58219999999997\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 460\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2420010487238566\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010919387188226597\n",
      "          policy_loss: 0.011274766466683812\n",
      "          total_loss: 0.008052314445376397\n",
      "          vf_explained_var: 0.2987886965274811\n",
      "          vf_loss: 0.01569940633037024\n",
      "    num_agent_steps_sampled: 165000\n",
      "    num_agent_steps_trained: 165000\n",
      "    num_steps_sampled: 165000\n",
      "    num_steps_trained: 165000\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.43571428571428\n",
      "    ram_util_percent: 30.946428571428566\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036826055928162287\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.4384362189268\n",
      "    mean_inference_ms: 2.119036431034746\n",
      "    mean_raw_obs_processing_ms: 1.773865662489584\n",
      "  time_since_restore: 3961.4139275550842\n",
      "  time_this_iter_s: 19.68103837966919\n",
      "  time_total_s: 3961.4139275550842\n",
      "  timers:\n",
      "    learn_throughput: 1332.784\n",
      "    learn_time_ms: 750.309\n",
      "    load_throughput: 41753.105\n",
      "    load_time_ms: 23.95\n",
      "    sample_throughput: 48.253\n",
      "    sample_time_ms: 20724.033\n",
      "    update_time_ms: 3.64\n",
      "  timestamp: 1635286536\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 165000\n",
      "  training_iteration: 165\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">         3961.41</td><td style=\"text-align: right;\">165000</td><td style=\"text-align: right;\"> -3.5822</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">             353.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 166000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-15-56\n",
      "  done: false\n",
      "  episode_len_mean: 356.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.626499999999969\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 462\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2638236840565997\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010336334454159744\n",
      "          policy_loss: -0.04158201528092225\n",
      "          total_loss: 0.001509910925394959\n",
      "          vf_explained_var: -0.008582349866628647\n",
      "          vf_loss: 0.06241880040615797\n",
      "    num_agent_steps_sampled: 166000\n",
      "    num_agent_steps_trained: 166000\n",
      "    num_steps_sampled: 166000\n",
      "    num_steps_trained: 166000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.50344827586207\n",
      "    ram_util_percent: 30.97241379310345\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03682411659559055\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.43176896353114\n",
      "    mean_inference_ms: 2.118948688502743\n",
      "    mean_raw_obs_processing_ms: 1.7733016581050094\n",
      "  time_since_restore: 3981.558978319168\n",
      "  time_this_iter_s: 20.145050764083862\n",
      "  time_total_s: 3981.558978319168\n",
      "  timers:\n",
      "    learn_throughput: 1330.62\n",
      "    learn_time_ms: 751.529\n",
      "    load_throughput: 41763.124\n",
      "    load_time_ms: 23.945\n",
      "    sample_throughput: 48.356\n",
      "    sample_time_ms: 20680.01\n",
      "    update_time_ms: 3.639\n",
      "  timestamp: 1635286556\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 166000\n",
      "  training_iteration: 166\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   166</td><td style=\"text-align: right;\">         3981.56</td><td style=\"text-align: right;\">166000</td><td style=\"text-align: right;\"> -3.6265</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">             356.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 167000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-16-15\n",
      "  done: false\n",
      "  episode_len_mean: 360.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.6835999999999682\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 465\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2301726659138996\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00963376037606948\n",
      "          policy_loss: -0.0779984021352397\n",
      "          total_loss: -0.06897061864535013\n",
      "          vf_explained_var: 0.25904226303100586\n",
      "          vf_loss: 0.02824322597589344\n",
      "    num_agent_steps_sampled: 167000\n",
      "    num_agent_steps_trained: 167000\n",
      "    num_steps_sampled: 167000\n",
      "    num_steps_trained: 167000\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.4814814814815\n",
      "    ram_util_percent: 30.97407407407407\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036821145666381144\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.420755011578365\n",
      "    mean_inference_ms: 2.1188141341410374\n",
      "    mean_raw_obs_processing_ms: 1.7723549202774398\n",
      "  time_since_restore: 4000.6103513240814\n",
      "  time_this_iter_s: 19.05137300491333\n",
      "  time_total_s: 4000.6103513240814\n",
      "  timers:\n",
      "    learn_throughput: 1329.398\n",
      "    learn_time_ms: 752.22\n",
      "    load_throughput: 41507.461\n",
      "    load_time_ms: 24.092\n",
      "    sample_throughput: 48.522\n",
      "    sample_time_ms: 20609.19\n",
      "    update_time_ms: 3.643\n",
      "  timestamp: 1635286575\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 167000\n",
      "  training_iteration: 167\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         4000.61</td><td style=\"text-align: right;\">167000</td><td style=\"text-align: right;\"> -3.6836</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            360.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-16-35\n",
      "  done: false\n",
      "  episode_len_mean: 362.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.677799999999968\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 467\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1989227506849502\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014607125438644298\n",
      "          policy_loss: -0.08854588052878777\n",
      "          total_loss: -0.0108669132201208\n",
      "          vf_explained_var: 0.048544999212026596\n",
      "          vf_loss: 0.09498863479950362\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.42857142857143\n",
      "    ram_util_percent: 30.97142857142857\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036819151740158484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.412455724175306\n",
      "    mean_inference_ms: 2.118722938862688\n",
      "    mean_raw_obs_processing_ms: 1.7717700258732996\n",
      "  time_since_restore: 4020.363963842392\n",
      "  time_this_iter_s: 19.753612518310547\n",
      "  time_total_s: 4020.363963842392\n",
      "  timers:\n",
      "    learn_throughput: 1329.151\n",
      "    learn_time_ms: 752.36\n",
      "    load_throughput: 41525.583\n",
      "    load_time_ms: 24.082\n",
      "    sample_throughput: 48.671\n",
      "    sample_time_ms: 20546.178\n",
      "    update_time_ms: 3.716\n",
      "  timestamp: 1635286595\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 168\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">         4020.36</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\"> -3.6778</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            362.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 169000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-16-55\n",
      "  done: false\n",
      "  episode_len_mean: 365.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.7102999999999673\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 470\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2544280025694103\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01524596727456622\n",
      "          policy_loss: -0.002698773228459888\n",
      "          total_loss: -0.0074460671179824404\n",
      "          vf_explained_var: 0.18451470136642456\n",
      "          vf_loss: 0.012912764167413115\n",
      "    num_agent_steps_sampled: 169000\n",
      "    num_agent_steps_trained: 169000\n",
      "    num_steps_sampled: 169000\n",
      "    num_steps_trained: 169000\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.65172413793104\n",
      "    ram_util_percent: 30.96551724137931\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03681613616566565\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.39937524243487\n",
      "    mean_inference_ms: 2.1185836644880656\n",
      "    mean_raw_obs_processing_ms: 1.7707369196421725\n",
      "  time_since_restore: 4040.409574508667\n",
      "  time_this_iter_s: 20.045610666275024\n",
      "  time_total_s: 4040.409574508667\n",
      "  timers:\n",
      "    learn_throughput: 1327.751\n",
      "    learn_time_ms: 753.153\n",
      "    load_throughput: 41461.383\n",
      "    load_time_ms: 24.119\n",
      "    sample_throughput: 48.517\n",
      "    sample_time_ms: 20611.148\n",
      "    update_time_ms: 3.721\n",
      "  timestamp: 1635286615\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 169000\n",
      "  training_iteration: 169\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         4040.41</td><td style=\"text-align: right;\">169000</td><td style=\"text-align: right;\"> -3.7103</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            365.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 170000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-17-15\n",
      "  done: false\n",
      "  episode_len_mean: 369.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.7603999999999678\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 473\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.261880456076728\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006894015268936416\n",
      "          policy_loss: 0.04995582335525089\n",
      "          total_loss: 0.07086912892344925\n",
      "          vf_explained_var: 0.22745203971862793\n",
      "          vf_loss: 0.04132353206061655\n",
      "    num_agent_steps_sampled: 170000\n",
      "    num_agent_steps_trained: 170000\n",
      "    num_steps_sampled: 170000\n",
      "    num_steps_trained: 170000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.51379310344828\n",
      "    ram_util_percent: 30.948275862068964\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03681314937138678\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.38531096326485\n",
      "    mean_inference_ms: 2.1184448572396612\n",
      "    mean_raw_obs_processing_ms: 1.7697844134059355\n",
      "  time_since_restore: 4060.6405172348022\n",
      "  time_this_iter_s: 20.230942726135254\n",
      "  time_total_s: 4060.6405172348022\n",
      "  timers:\n",
      "    learn_throughput: 1326.287\n",
      "    learn_time_ms: 753.984\n",
      "    load_throughput: 41422.525\n",
      "    load_time_ms: 24.141\n",
      "    sample_throughput: 48.443\n",
      "    sample_time_ms: 20642.907\n",
      "    update_time_ms: 3.723\n",
      "  timestamp: 1635286635\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 170000\n",
      "  training_iteration: 170\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">         4060.64</td><td style=\"text-align: right;\">170000</td><td style=\"text-align: right;\"> -3.7604</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            369.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 171000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-17-35\n",
      "  done: false\n",
      "  episode_len_mean: 370.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.774799999999967\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 475\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.31441261238522\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0067483117799690285\n",
      "          policy_loss: -0.1392247050586674\n",
      "          total_loss: -0.14867750818116798\n",
      "          vf_explained_var: 0.5161355137825012\n",
      "          vf_loss: 0.011529424385581579\n",
      "    num_agent_steps_sampled: 171000\n",
      "    num_agent_steps_trained: 171000\n",
      "    num_steps_sampled: 171000\n",
      "    num_steps_trained: 171000\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.46071428571429\n",
      "    ram_util_percent: 30.939285714285713\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036811136439660955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.37542944582499\n",
      "    mean_inference_ms: 2.1183514004029664\n",
      "    mean_raw_obs_processing_ms: 1.7691189397613902\n",
      "  time_since_restore: 4080.408891916275\n",
      "  time_this_iter_s: 19.76837468147278\n",
      "  time_total_s: 4080.408891916275\n",
      "  timers:\n",
      "    learn_throughput: 1327.121\n",
      "    learn_time_ms: 753.511\n",
      "    load_throughput: 41276.344\n",
      "    load_time_ms: 24.227\n",
      "    sample_throughput: 52.834\n",
      "    sample_time_ms: 18927.303\n",
      "    update_time_ms: 3.726\n",
      "  timestamp: 1635286655\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 171000\n",
      "  training_iteration: 171\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         4080.41</td><td style=\"text-align: right;\">171000</td><td style=\"text-align: right;\"> -3.7748</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            370.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-17-55\n",
      "  done: false\n",
      "  episode_len_mean: 371.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.7935999999999668\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 478\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3719915575451322\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014526689192387466\n",
      "          policy_loss: 0.0036306160191694895\n",
      "          total_loss: -0.004525612791379293\n",
      "          vf_explained_var: 0.6538330316543579\n",
      "          vf_loss: 0.01090989682254278\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.425\n",
      "    ram_util_percent: 30.939285714285713\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036808106117560614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.360528790398288\n",
      "    mean_inference_ms: 2.118211266978655\n",
      "    mean_raw_obs_processing_ms: 1.7680895405442991\n",
      "  time_since_restore: 4099.858314990997\n",
      "  time_this_iter_s: 19.44942307472229\n",
      "  time_total_s: 4099.858314990997\n",
      "  timers:\n",
      "    learn_throughput: 1327.084\n",
      "    learn_time_ms: 753.532\n",
      "    load_throughput: 41265.339\n",
      "    load_time_ms: 24.233\n",
      "    sample_throughput: 52.834\n",
      "    sample_time_ms: 18927.137\n",
      "    update_time_ms: 3.734\n",
      "  timestamp: 1635286675\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 172\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">         4099.86</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\"> -3.7936</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            371.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 173000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-18-34\n",
      "  done: false\n",
      "  episode_len_mean: 373.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.8097999999999668\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 481\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.279130803214179\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012036955104003037\n",
      "          policy_loss: 0.0012262609269883898\n",
      "          total_loss: -0.005972775154643588\n",
      "          vf_explained_var: 0.4222016930580139\n",
      "          vf_loss: 0.011736093427882427\n",
      "    num_agent_steps_sampled: 173000\n",
      "    num_agent_steps_trained: 173000\n",
      "    num_steps_sampled: 173000\n",
      "    num_steps_trained: 173000\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.550877192982455\n",
      "    ram_util_percent: 30.95263157894737\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680511338576628\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.34569110916876\n",
      "    mean_inference_ms: 2.118072776464383\n",
      "    mean_raw_obs_processing_ms: 1.7702037812069455\n",
      "  time_since_restore: 4139.619470357895\n",
      "  time_this_iter_s: 39.76115536689758\n",
      "  time_total_s: 4139.619470357895\n",
      "  timers:\n",
      "    learn_throughput: 1327.743\n",
      "    learn_time_ms: 753.158\n",
      "    load_throughput: 41013.069\n",
      "    load_time_ms: 24.382\n",
      "    sample_throughput: 47.727\n",
      "    sample_time_ms: 20952.657\n",
      "    update_time_ms: 3.742\n",
      "  timestamp: 1635286714\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 173000\n",
      "  training_iteration: 173\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         4139.62</td><td style=\"text-align: right;\">173000</td><td style=\"text-align: right;\"> -3.8098</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            373.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 174000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-18-55\n",
      "  done: false\n",
      "  episode_len_mean: 374.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.46999999999999975\n",
      "  episode_reward_mean: -3.8340999999999665\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 483\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.290000091658698\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014206786609733473\n",
      "          policy_loss: -0.07385883050867253\n",
      "          total_loss: 0.08670181946622\n",
      "          vf_explained_var: 0.16255991160869598\n",
      "          vf_loss: 0.17890934972609912\n",
      "    num_agent_steps_sampled: 174000\n",
      "    num_agent_steps_trained: 174000\n",
      "    num_steps_sampled: 174000\n",
      "    num_steps_trained: 174000\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.35000000000001\n",
      "    ram_util_percent: 31.639285714285716\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680312369579257\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.33513553340941\n",
      "    mean_inference_ms: 2.1179802268174925\n",
      "    mean_raw_obs_processing_ms: 1.7716454798110057\n",
      "  time_since_restore: 4159.8507652282715\n",
      "  time_this_iter_s: 20.231294870376587\n",
      "  time_total_s: 4159.8507652282715\n",
      "  timers:\n",
      "    learn_throughput: 1326.713\n",
      "    learn_time_ms: 753.743\n",
      "    load_throughput: 40785.15\n",
      "    load_time_ms: 24.519\n",
      "    sample_throughput: 47.561\n",
      "    sample_time_ms: 21025.607\n",
      "    update_time_ms: 3.738\n",
      "  timestamp: 1635286735\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 174000\n",
      "  training_iteration: 174\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   174</td><td style=\"text-align: right;\">         4159.85</td><td style=\"text-align: right;\">174000</td><td style=\"text-align: right;\"> -3.8341</td><td style=\"text-align: right;\">                0.47</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            374.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 175000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-19-14\n",
      "  done: false\n",
      "  episode_len_mean: 377.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.8499999999999929\n",
      "  episode_reward_mean: -3.8812999999999653\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 486\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4096783743964303\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015681376014350965\n",
      "          policy_loss: -0.0019990531934632196\n",
      "          total_loss: 0.011109435392750635\n",
      "          vf_explained_var: 0.17970234155654907\n",
      "          vf_loss: 0.0321815675124526\n",
      "    num_agent_steps_sampled: 175000\n",
      "    num_agent_steps_trained: 175000\n",
      "    num_steps_sampled: 175000\n",
      "    num_steps_trained: 175000\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.64827586206897\n",
      "    ram_util_percent: 31.2448275862069\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680010631878874\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.318964151479594\n",
      "    mean_inference_ms: 2.117840295076857\n",
      "    mean_raw_obs_processing_ms: 1.7736458808062594\n",
      "  time_since_restore: 4179.54155254364\n",
      "  time_this_iter_s: 19.690787315368652\n",
      "  time_total_s: 4179.54155254364\n",
      "  timers:\n",
      "    learn_throughput: 1326.166\n",
      "    learn_time_ms: 754.053\n",
      "    load_throughput: 41047.145\n",
      "    load_time_ms: 24.362\n",
      "    sample_throughput: 47.559\n",
      "    sample_time_ms: 21026.423\n",
      "    update_time_ms: 3.732\n",
      "  timestamp: 1635286754\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 175000\n",
      "  training_iteration: 175\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         4179.54</td><td style=\"text-align: right;\">175000</td><td style=\"text-align: right;\"> -3.8813</td><td style=\"text-align: right;\">               -0.85</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            377.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-19-36\n",
      "  done: false\n",
      "  episode_len_mean: 377.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.8499999999999929\n",
      "  episode_reward_mean: -3.8721999999999652\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 488\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3334773010677763\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01242059922487518\n",
      "          policy_loss: -0.0895013684199916\n",
      "          total_loss: -0.0891623714317878\n",
      "          vf_explained_var: 0.7985448837280273\n",
      "          vf_loss: 0.01969468700699508\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.77666666666667\n",
      "    ram_util_percent: 30.939999999999998\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679810023253207\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.308220786652456\n",
      "    mean_inference_ms: 2.1177475450088776\n",
      "    mean_raw_obs_processing_ms: 1.774943176504039\n",
      "  time_since_restore: 4200.909015655518\n",
      "  time_this_iter_s: 21.36746311187744\n",
      "  time_total_s: 4200.909015655518\n",
      "  timers:\n",
      "    learn_throughput: 1327.343\n",
      "    learn_time_ms: 753.385\n",
      "    load_throughput: 40905.951\n",
      "    load_time_ms: 24.446\n",
      "    sample_throughput: 47.283\n",
      "    sample_time_ms: 21149.256\n",
      "    update_time_ms: 3.722\n",
      "  timestamp: 1635286776\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 176\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   176</td><td style=\"text-align: right;\">         4200.91</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\"> -3.8722</td><td style=\"text-align: right;\">               -0.85</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            377.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 177000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-19-57\n",
      "  done: false\n",
      "  episode_len_mean: 378.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.39000000000002766\n",
      "  episode_reward_mean: -3.8584999999999643\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 491\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2272895680533513\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.04539407430571324\n",
      "          policy_loss: -0.15747499089274142\n",
      "          total_loss: 0.01940955865300364\n",
      "          vf_explained_var: 0.6163669228553772\n",
      "          vf_loss: 0.184614937286824\n",
      "    num_agent_steps_sampled: 177000\n",
      "    num_agent_steps_trained: 177000\n",
      "    num_steps_sampled: 177000\n",
      "    num_steps_trained: 177000\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.43333333333334\n",
      "    ram_util_percent: 30.906666666666663\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036795096478084405\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.291947744335697\n",
      "    mean_inference_ms: 2.117608737208363\n",
      "    mean_raw_obs_processing_ms: 1.7732975560757152\n",
      "  time_since_restore: 4221.927193164825\n",
      "  time_this_iter_s: 21.01817750930786\n",
      "  time_total_s: 4221.927193164825\n",
      "  timers:\n",
      "    learn_throughput: 1326.858\n",
      "    learn_time_ms: 753.66\n",
      "    load_throughput: 41170.961\n",
      "    load_time_ms: 24.289\n",
      "    sample_throughput: 46.848\n",
      "    sample_time_ms: 21345.826\n",
      "    update_time_ms: 3.716\n",
      "  timestamp: 1635286797\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 177000\n",
      "  training_iteration: 177\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         4221.93</td><td style=\"text-align: right;\">177000</td><td style=\"text-align: right;\"> -3.8585</td><td style=\"text-align: right;\">                0.39</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            378.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 178000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-20-18\n",
      "  done: false\n",
      "  episode_len_mean: 376.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.39000000000002766\n",
      "  episode_reward_mean: -3.8828999999999643\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 494\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3329594850540163\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01217493513649502\n",
      "          policy_loss: -0.03504777948061625\n",
      "          total_loss: 0.013175926854213078\n",
      "          vf_explained_var: 0.5372480154037476\n",
      "          vf_loss: 0.06570273438572055\n",
      "    num_agent_steps_sampled: 178000\n",
      "    num_agent_steps_trained: 178000\n",
      "    num_steps_sampled: 178000\n",
      "    num_steps_trained: 178000\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.4\n",
      "    ram_util_percent: 30.970967741935485\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679213519611467\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.27652546155822\n",
      "    mean_inference_ms: 2.1174728024310996\n",
      "    mean_raw_obs_processing_ms: 1.7718481332010585\n",
      "  time_since_restore: 4243.534805774689\n",
      "  time_this_iter_s: 21.60761260986328\n",
      "  time_total_s: 4243.534805774689\n",
      "  timers:\n",
      "    learn_throughput: 1326.666\n",
      "    learn_time_ms: 753.769\n",
      "    load_throughput: 41150.199\n",
      "    load_time_ms: 24.301\n",
      "    sample_throughput: 46.444\n",
      "    sample_time_ms: 21531.193\n",
      "    update_time_ms: 3.624\n",
      "  timestamp: 1635286818\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 178000\n",
      "  training_iteration: 178\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   178</td><td style=\"text-align: right;\">         4243.53</td><td style=\"text-align: right;\">178000</td><td style=\"text-align: right;\"> -3.8829</td><td style=\"text-align: right;\">                0.39</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">             376.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 179000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-20-39\n",
      "  done: false\n",
      "  episode_len_mean: 375.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.39000000000002766\n",
      "  episode_reward_mean: -3.886899999999965\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 497\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3067545784844294\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017929536704290108\n",
      "          policy_loss: -0.03888000225027402\n",
      "          total_loss: 0.10749236308038235\n",
      "          vf_explained_var: 0.4049966633319855\n",
      "          vf_loss: 0.16082401523987452\n",
      "    num_agent_steps_sampled: 179000\n",
      "    num_agent_steps_trained: 179000\n",
      "    num_steps_sampled: 179000\n",
      "    num_steps_trained: 179000\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.4793103448276\n",
      "    ram_util_percent: 31.003448275862073\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678920441192895\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.261937232137583\n",
      "    mean_inference_ms: 2.117340055635959\n",
      "    mean_raw_obs_processing_ms: 1.7705943111078994\n",
      "  time_since_restore: 4264.0268201828\n",
      "  time_this_iter_s: 20.492014408111572\n",
      "  time_total_s: 4264.0268201828\n",
      "  timers:\n",
      "    learn_throughput: 1328.597\n",
      "    learn_time_ms: 752.674\n",
      "    load_throughput: 41203.6\n",
      "    load_time_ms: 24.27\n",
      "    sample_throughput: 46.346\n",
      "    sample_time_ms: 21576.948\n",
      "    update_time_ms: 3.625\n",
      "  timestamp: 1635286839\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 179000\n",
      "  training_iteration: 179\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         4264.03</td><td style=\"text-align: right;\">179000</td><td style=\"text-align: right;\"> -3.8869</td><td style=\"text-align: right;\">                0.39</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            375.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-21-00\n",
      "  done: false\n",
      "  episode_len_mean: 374.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.39000000000002766\n",
      "  episode_reward_mean: -3.9005999999999648\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 499\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.258635762002733\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012708582410564074\n",
      "          policy_loss: -0.01198444426473644\n",
      "          total_loss: 0.10606287601921294\n",
      "          vf_explained_var: 0.6823891401290894\n",
      "          vf_loss: 0.13452667316628827\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.63666666666667\n",
      "    ram_util_percent: 31.02333333333333\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678725430207147\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.252506656292223\n",
      "    mean_inference_ms: 2.117252695909938\n",
      "    mean_raw_obs_processing_ms: 1.7697713134238138\n",
      "  time_since_restore: 4285.128143548965\n",
      "  time_this_iter_s: 21.10132336616516\n",
      "  time_total_s: 4285.128143548965\n",
      "  timers:\n",
      "    learn_throughput: 1331.734\n",
      "    learn_time_ms: 750.901\n",
      "    load_throughput: 40929.303\n",
      "    load_time_ms: 24.432\n",
      "    sample_throughput: 46.156\n",
      "    sample_time_ms: 21665.607\n",
      "    update_time_ms: 3.62\n",
      "  timestamp: 1635286860\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 180\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         4285.13</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\"> -3.9006</td><td style=\"text-align: right;\">                0.39</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">             374.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 181000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-21-21\n",
      "  done: false\n",
      "  episode_len_mean: 374.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.39000000000002766\n",
      "  episode_reward_mean: -3.9367999999999648\n",
      "  episode_reward_min: -11.809999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 502\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3250065777036877\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0106948583207553\n",
      "          policy_loss: 0.07895064685079786\n",
      "          total_loss: 0.1417471756744716\n",
      "          vf_explained_var: 0.5769373774528503\n",
      "          vf_loss: 0.0809072665249308\n",
      "    num_agent_steps_sampled: 181000\n",
      "    num_agent_steps_trained: 181000\n",
      "    num_steps_sampled: 181000\n",
      "    num_steps_trained: 181000\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.43333333333334\n",
      "    ram_util_percent: 30.98\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036784382403276944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.238744903482072\n",
      "    mean_inference_ms: 2.11712405480858\n",
      "    mean_raw_obs_processing_ms: 1.768670825589611\n",
      "  time_since_restore: 4305.863177776337\n",
      "  time_this_iter_s: 20.735034227371216\n",
      "  time_total_s: 4305.863177776337\n",
      "  timers:\n",
      "    learn_throughput: 1329.097\n",
      "    learn_time_ms: 752.391\n",
      "    load_throughput: 41074.358\n",
      "    load_time_ms: 24.346\n",
      "    sample_throughput: 45.954\n",
      "    sample_time_ms: 21760.859\n",
      "    update_time_ms: 3.619\n",
      "  timestamp: 1635286881\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 181000\n",
      "  training_iteration: 181\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   181</td><td style=\"text-align: right;\">         4305.86</td><td style=\"text-align: right;\">181000</td><td style=\"text-align: right;\"> -3.9368</td><td style=\"text-align: right;\">                0.39</td><td style=\"text-align: right;\">              -11.81</td><td style=\"text-align: right;\">            374.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 182000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-21-42\n",
      "  done: false\n",
      "  episode_len_mean: 374.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.39000000000002766\n",
      "  episode_reward_mean: -3.842899999999966\n",
      "  episode_reward_min: -9.50999999999994\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 505\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2717790921529133\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011950098698582224\n",
      "          policy_loss: -0.0493016988866859\n",
      "          total_loss: 0.080889289929635\n",
      "          vf_explained_var: 0.46030837297439575\n",
      "          vf_loss: 0.1471662582208713\n",
      "    num_agent_steps_sampled: 182000\n",
      "    num_agent_steps_trained: 182000\n",
      "    num_steps_sampled: 182000\n",
      "    num_steps_trained: 182000\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.48666666666666\n",
      "    ram_util_percent: 30.986666666666668\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036781503507024686\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.225313126436046\n",
      "    mean_inference_ms: 2.1169960386446203\n",
      "    mean_raw_obs_processing_ms: 1.7676418173336124\n",
      "  time_since_restore: 4327.06414437294\n",
      "  time_this_iter_s: 21.200966596603394\n",
      "  time_total_s: 4327.06414437294\n",
      "  timers:\n",
      "    learn_throughput: 1326.129\n",
      "    learn_time_ms: 754.075\n",
      "    load_throughput: 40994.108\n",
      "    load_time_ms: 24.394\n",
      "    sample_throughput: 45.591\n",
      "    sample_time_ms: 21934.297\n",
      "    update_time_ms: 3.618\n",
      "  timestamp: 1635286902\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 182000\n",
      "  training_iteration: 182\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">         4327.06</td><td style=\"text-align: right;\">182000</td><td style=\"text-align: right;\"> -3.8429</td><td style=\"text-align: right;\">                0.39</td><td style=\"text-align: right;\">               -9.51</td><td style=\"text-align: right;\">            374.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 183000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-22-03\n",
      "  done: false\n",
      "  episode_len_mean: 374.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.39000000000002766\n",
      "  episode_reward_mean: -3.848999999999967\n",
      "  episode_reward_min: -9.50999999999994\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 507\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.358477012316386\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008634620526751884\n",
      "          policy_loss: -0.2565384913235903\n",
      "          total_loss: -0.21339114242129856\n",
      "          vf_explained_var: 0.4352845549583435\n",
      "          vf_loss: 0.062582820146862\n",
      "    num_agent_steps_sampled: 183000\n",
      "    num_agent_steps_trained: 183000\n",
      "    num_steps_sampled: 183000\n",
      "    num_steps_trained: 183000\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.48666666666666\n",
      "    ram_util_percent: 30.966666666666665\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367795747495043\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.216544554161707\n",
      "    mean_inference_ms: 2.116911290477702\n",
      "    mean_raw_obs_processing_ms: 1.7669670409876936\n",
      "  time_since_restore: 4348.176927089691\n",
      "  time_this_iter_s: 21.1127827167511\n",
      "  time_total_s: 4348.176927089691\n",
      "  timers:\n",
      "    learn_throughput: 1327.792\n",
      "    learn_time_ms: 753.13\n",
      "    load_throughput: 41274.72\n",
      "    load_time_ms: 24.228\n",
      "    sample_throughput: 49.824\n",
      "    sample_time_ms: 20070.585\n",
      "    update_time_ms: 3.611\n",
      "  timestamp: 1635286923\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 183000\n",
      "  training_iteration: 183\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">         4348.18</td><td style=\"text-align: right;\">183000</td><td style=\"text-align: right;\">  -3.849</td><td style=\"text-align: right;\">                0.39</td><td style=\"text-align: right;\">               -9.51</td><td style=\"text-align: right;\">            374.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-22-43\n",
      "  done: false\n",
      "  episode_len_mean: 373.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.39000000000002766\n",
      "  episode_reward_mean: -3.9012999999999662\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 510\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3476116206910875\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007596225792022141\n",
      "          policy_loss: -0.12164669334888459\n",
      "          total_loss: -0.04499437045305967\n",
      "          vf_explained_var: 0.4617255628108978\n",
      "          vf_loss: 0.09647813369002607\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.310526315789474\n",
      "    ram_util_percent: 31.021052631578947\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677671943919878\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.20395053855731\n",
      "    mean_inference_ms: 2.1167879431740078\n",
      "    mean_raw_obs_processing_ms: 1.7688105892922974\n",
      "  time_since_restore: 4387.923334121704\n",
      "  time_this_iter_s: 39.74640703201294\n",
      "  time_total_s: 4387.923334121704\n",
      "  timers:\n",
      "    learn_throughput: 1326.996\n",
      "    learn_time_ms: 753.582\n",
      "    load_throughput: 41410.01\n",
      "    load_time_ms: 24.149\n",
      "    sample_throughput: 45.41\n",
      "    sample_time_ms: 22021.823\n",
      "    update_time_ms: 3.529\n",
      "  timestamp: 1635286963\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 184\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">         4387.92</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\"> -3.9013</td><td style=\"text-align: right;\">                0.39</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            373.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 185000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-23-05\n",
      "  done: false\n",
      "  episode_len_mean: 372.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.39000000000002766\n",
      "  episode_reward_mean: -3.9765999999999666\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 513\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2971659077538384\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011014162183964792\n",
      "          policy_loss: -0.05017374969191021\n",
      "          total_loss: 0.059851547620362704\n",
      "          vf_explained_var: 0.2668967843055725\n",
      "          vf_loss: 0.12770418785512447\n",
      "    num_agent_steps_sampled: 185000\n",
      "    num_agent_steps_trained: 185000\n",
      "    num_steps_sampled: 185000\n",
      "    num_steps_trained: 185000\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.79032258064515\n",
      "    ram_util_percent: 31.20645161290323\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677387068011751\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.191564240120357\n",
      "    mean_inference_ms: 2.1166651119463067\n",
      "    mean_raw_obs_processing_ms: 1.7707092241362552\n",
      "  time_since_restore: 4409.728825092316\n",
      "  time_this_iter_s: 21.805490970611572\n",
      "  time_total_s: 4409.728825092316\n",
      "  timers:\n",
      "    learn_throughput: 1328.01\n",
      "    learn_time_ms: 753.006\n",
      "    load_throughput: 41575.471\n",
      "    load_time_ms: 24.053\n",
      "    sample_throughput: 44.976\n",
      "    sample_time_ms: 22233.973\n",
      "    update_time_ms: 3.531\n",
      "  timestamp: 1635286985\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 185000\n",
      "  training_iteration: 185\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   185</td><td style=\"text-align: right;\">         4409.73</td><td style=\"text-align: right;\">185000</td><td style=\"text-align: right;\"> -3.9766</td><td style=\"text-align: right;\">                0.39</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            372.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 186000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-23-27\n",
      "  done: false\n",
      "  episode_len_mean: 372.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.39000000000002766\n",
      "  episode_reward_mean: -4.053899999999967\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 516\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.317429116037157\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011392729937054943\n",
      "          policy_loss: -0.12237265701923106\n",
      "          total_loss: 0.053942916852732496\n",
      "          vf_explained_var: 0.386945515871048\n",
      "          vf_loss: 0.19401517907778423\n",
      "    num_agent_steps_sampled: 186000\n",
      "    num_agent_steps_trained: 186000\n",
      "    num_steps_sampled: 186000\n",
      "    num_steps_trained: 186000\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.35\n",
      "    ram_util_percent: 30.875\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036771039164654025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.17938500416518\n",
      "    mean_inference_ms: 2.1165429061829086\n",
      "    mean_raw_obs_processing_ms: 1.7726622374159877\n",
      "  time_since_restore: 4431.6893658638\n",
      "  time_this_iter_s: 21.960540771484375\n",
      "  time_total_s: 4431.6893658638\n",
      "  timers:\n",
      "    learn_throughput: 1327.918\n",
      "    learn_time_ms: 753.058\n",
      "    load_throughput: 41766.618\n",
      "    load_time_ms: 23.943\n",
      "    sample_throughput: 44.856\n",
      "    sample_time_ms: 22293.315\n",
      "    update_time_ms: 3.545\n",
      "  timestamp: 1635287007\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 186000\n",
      "  training_iteration: 186\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   186</td><td style=\"text-align: right;\">         4431.69</td><td style=\"text-align: right;\">186000</td><td style=\"text-align: right;\"> -4.0539</td><td style=\"text-align: right;\">                0.39</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            372.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 187000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-23-50\n",
      "  done: false\n",
      "  episode_len_mean: 371.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.39000000000002766\n",
      "  episode_reward_mean: -4.083899999999966\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 519\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3067133797539605\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01096910906279906\n",
      "          policy_loss: -0.09385113327039613\n",
      "          total_loss: -0.03925350838237339\n",
      "          vf_explained_var: 0.34588077664375305\n",
      "          vf_loss: 0.0723936401721504\n",
      "    num_agent_steps_sampled: 187000\n",
      "    num_agent_steps_trained: 187000\n",
      "    num_steps_sampled: 187000\n",
      "    num_steps_trained: 187000\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.50909090909092\n",
      "    ram_util_percent: 30.884848484848487\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367681980848928\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.167877515486897\n",
      "    mean_inference_ms: 2.1164227673873155\n",
      "    mean_raw_obs_processing_ms: 1.7722740062753368\n",
      "  time_since_restore: 4455.190635919571\n",
      "  time_this_iter_s: 23.501270055770874\n",
      "  time_total_s: 4455.190635919571\n",
      "  timers:\n",
      "    learn_throughput: 1330.317\n",
      "    learn_time_ms: 751.7\n",
      "    load_throughput: 41788.631\n",
      "    load_time_ms: 23.93\n",
      "    sample_throughput: 44.36\n",
      "    sample_time_ms: 22542.993\n",
      "    update_time_ms: 3.542\n",
      "  timestamp: 1635287030\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 187000\n",
      "  training_iteration: 187\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">         4455.19</td><td style=\"text-align: right;\">187000</td><td style=\"text-align: right;\"> -4.0839</td><td style=\"text-align: right;\">                0.39</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            371.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-24-58\n",
      "  done: false\n",
      "  episode_len_mean: 371.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -4.0341999999999665\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 522\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2656992382473415\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019615406809779322\n",
      "          policy_loss: -0.039596232606305015\n",
      "          total_loss: 0.37991645245088473\n",
      "          vf_explained_var: 0.47088441252708435\n",
      "          vf_loss: 0.432743650343683\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.83711340206185\n",
      "    ram_util_percent: 30.989690721649488\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036765377838220245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.156924395291508\n",
      "    mean_inference_ms: 2.1163041301476793\n",
      "    mean_raw_obs_processing_ms: 1.7778264881457653\n",
      "  time_since_restore: 4523.237153768539\n",
      "  time_this_iter_s: 68.0465178489685\n",
      "  time_total_s: 4523.237153768539\n",
      "  timers:\n",
      "    learn_throughput: 1332.842\n",
      "    learn_time_ms: 750.277\n",
      "    load_throughput: 41776.436\n",
      "    load_time_ms: 23.937\n",
      "    sample_throughput: 36.781\n",
      "    sample_time_ms: 27188.243\n",
      "    update_time_ms: 3.59\n",
      "  timestamp: 1635287098\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 188\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   188</td><td style=\"text-align: right;\">         4523.24</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\"> -4.0342</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            371.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 189000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-25-23\n",
      "  done: false\n",
      "  episode_len_mean: 370.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -4.085199999999967\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 525\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2647224108378095\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015595998042305591\n",
      "          policy_loss: -0.05416837069723341\n",
      "          total_loss: 0.04078332525160577\n",
      "          vf_explained_var: 0.41937583684921265\n",
      "          vf_loss: 0.11010438886781533\n",
      "    num_agent_steps_sampled: 189000\n",
      "    num_agent_steps_trained: 189000\n",
      "    num_steps_sampled: 189000\n",
      "    num_steps_trained: 189000\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.70833333333331\n",
      "    ram_util_percent: 31.466666666666665\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036762819526666005\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.14659110017686\n",
      "    mean_inference_ms: 2.1162001016651955\n",
      "    mean_raw_obs_processing_ms: 1.7835242114073753\n",
      "  time_since_restore: 4548.227462053299\n",
      "  time_this_iter_s: 24.99030828475952\n",
      "  time_total_s: 4548.227462053299\n",
      "  timers:\n",
      "    learn_throughput: 1329.03\n",
      "    learn_time_ms: 752.428\n",
      "    load_throughput: 41867.804\n",
      "    load_time_ms: 23.885\n",
      "    sample_throughput: 36.185\n",
      "    sample_time_ms: 27635.431\n",
      "    update_time_ms: 4.087\n",
      "  timestamp: 1635287123\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 189000\n",
      "  training_iteration: 189\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">         4548.23</td><td style=\"text-align: right;\">189000</td><td style=\"text-align: right;\"> -4.0852</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            370.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 190000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-25-47\n",
      "  done: false\n",
      "  episode_len_mean: 370.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -4.131699999999966\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 528\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.209772239791022\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012813097350702357\n",
      "          policy_loss: 0.0011765957706504398\n",
      "          total_loss: 0.12260570956601037\n",
      "          vf_explained_var: 0.43081361055374146\n",
      "          vf_loss: 0.13736960738897325\n",
      "    num_agent_steps_sampled: 190000\n",
      "    num_agent_steps_trained: 190000\n",
      "    num_steps_sampled: 190000\n",
      "    num_steps_trained: 190000\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.92571428571429\n",
      "    ram_util_percent: 31.10285714285714\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676079209691761\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.136787182838397\n",
      "    mean_inference_ms: 2.1161094805257754\n",
      "    mean_raw_obs_processing_ms: 1.7892547980246332\n",
      "  time_since_restore: 4572.370881319046\n",
      "  time_this_iter_s: 24.14341926574707\n",
      "  time_total_s: 4572.370881319046\n",
      "  timers:\n",
      "    learn_throughput: 1327.644\n",
      "    learn_time_ms: 753.214\n",
      "    load_throughput: 42098.975\n",
      "    load_time_ms: 23.754\n",
      "    sample_throughput: 35.792\n",
      "    sample_time_ms: 27938.871\n",
      "    update_time_ms: 4.181\n",
      "  timestamp: 1635287147\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 190000\n",
      "  training_iteration: 190\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   190</td><td style=\"text-align: right;\">         4572.37</td><td style=\"text-align: right;\">190000</td><td style=\"text-align: right;\"> -4.1317</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            370.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 191000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-26-11\n",
      "  done: false\n",
      "  episode_len_mean: 369.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -4.150699999999967\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 531\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.326027923160129\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010881531996858367\n",
      "          policy_loss: 0.026702500383059182\n",
      "          total_loss: 0.049789409504996406\n",
      "          vf_explained_var: 0.5300736427307129\n",
      "          vf_loss: 0.041118158120661975\n",
      "    num_agent_steps_sampled: 191000\n",
      "    num_agent_steps_trained: 191000\n",
      "    num_steps_sampled: 191000\n",
      "    num_steps_trained: 191000\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.35454545454546\n",
      "    ram_util_percent: 31.109090909090913\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675889599265097\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.127675107628033\n",
      "    mean_inference_ms: 2.116024730980379\n",
      "    mean_raw_obs_processing_ms: 1.7950222993151295\n",
      "  time_since_restore: 4595.809967279434\n",
      "  time_this_iter_s: 23.439085960388184\n",
      "  time_total_s: 4595.809967279434\n",
      "  timers:\n",
      "    learn_throughput: 1327.385\n",
      "    learn_time_ms: 753.361\n",
      "    load_throughput: 42068.615\n",
      "    load_time_ms: 23.771\n",
      "    sample_throughput: 35.45\n",
      "    sample_time_ms: 28209.12\n",
      "    update_time_ms: 4.178\n",
      "  timestamp: 1635287171\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 191000\n",
      "  training_iteration: 191\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">         4595.81</td><td style=\"text-align: right;\">191000</td><td style=\"text-align: right;\"> -4.1507</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            369.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-26-34\n",
      "  done: false\n",
      "  episode_len_mean: 368.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -4.092999999999966\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 534\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3287371423509384\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01076391709500782\n",
      "          policy_loss: -0.08995789285335276\n",
      "          total_loss: 0.08156086363726192\n",
      "          vf_explained_var: 0.1319604068994522\n",
      "          vf_loss: 0.18963361146549385\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.32727272727273\n",
      "    ram_util_percent: 31.075757575757574\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675709583201184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.118899216064463\n",
      "    mean_inference_ms: 2.1159430629962035\n",
      "    mean_raw_obs_processing_ms: 1.8008250997890516\n",
      "  time_since_restore: 4619.03076171875\n",
      "  time_this_iter_s: 23.220794439315796\n",
      "  time_total_s: 4619.03076171875\n",
      "  timers:\n",
      "    learn_throughput: 1326.147\n",
      "    learn_time_ms: 754.064\n",
      "    load_throughput: 42122.524\n",
      "    load_time_ms: 23.74\n",
      "    sample_throughput: 35.198\n",
      "    sample_time_ms: 28410.409\n",
      "    update_time_ms: 4.193\n",
      "  timestamp: 1635287194\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 192\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">         4619.03</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">  -4.093</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            368.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 193000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-26-58\n",
      "  done: false\n",
      "  episode_len_mean: 368.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -4.097099999999966\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 537\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3529561281204225\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011997428407114017\n",
      "          policy_loss: -0.08751395667592685\n",
      "          total_loss: -0.054398456464211146\n",
      "          vf_explained_var: 0.5230932235717773\n",
      "          vf_loss: 0.050879791068534054\n",
      "    num_agent_steps_sampled: 193000\n",
      "    num_agent_steps_trained: 193000\n",
      "    num_steps_sampled: 193000\n",
      "    num_steps_trained: 193000\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.70294117647059\n",
      "    ram_util_percent: 31.247058823529407\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675562706206481\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.11082990711901\n",
      "    mean_inference_ms: 2.1158773213833357\n",
      "    mean_raw_obs_processing_ms: 1.8067616574896892\n",
      "  time_since_restore: 4642.440516471863\n",
      "  time_this_iter_s: 23.409754753112793\n",
      "  time_total_s: 4642.440516471863\n",
      "  timers:\n",
      "    learn_throughput: 1318.182\n",
      "    learn_time_ms: 758.621\n",
      "    load_throughput: 42999.142\n",
      "    load_time_ms: 23.256\n",
      "    sample_throughput: 34.921\n",
      "    sample_time_ms: 28636.027\n",
      "    update_time_ms: 4.193\n",
      "  timestamp: 1635287218\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 193000\n",
      "  training_iteration: 193\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   193</td><td style=\"text-align: right;\">         4642.44</td><td style=\"text-align: right;\">193000</td><td style=\"text-align: right;\"> -4.0971</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">             368.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 194000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-27-19\n",
      "  done: false\n",
      "  episode_len_mean: 367.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -4.090499999999967\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 540\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2978264649709064\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013837889903458404\n",
      "          policy_loss: 0.020679524085587927\n",
      "          total_loss: 0.09658315773639414\n",
      "          vf_explained_var: 0.5921259522438049\n",
      "          vf_loss: 0.09223221130669117\n",
      "    num_agent_steps_sampled: 194000\n",
      "    num_agent_steps_trained: 194000\n",
      "    num_steps_sampled: 194000\n",
      "    num_steps_trained: 194000\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.26129032258065\n",
      "    ram_util_percent: 31.261290322580653\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675418684042183\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.103175756145653\n",
      "    mean_inference_ms: 2.1158151860740566\n",
      "    mean_raw_obs_processing_ms: 1.8127255693323092\n",
      "  time_since_restore: 4664.277743816376\n",
      "  time_this_iter_s: 21.83722734451294\n",
      "  time_total_s: 4664.277743816376\n",
      "  timers:\n",
      "    learn_throughput: 1316.126\n",
      "    learn_time_ms: 759.806\n",
      "    load_throughput: 42981.737\n",
      "    load_time_ms: 23.266\n",
      "    sample_throughput: 37.253\n",
      "    sample_time_ms: 26843.344\n",
      "    update_time_ms: 4.4\n",
      "  timestamp: 1635287239\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 194000\n",
      "  training_iteration: 194\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   194</td><td style=\"text-align: right;\">         4664.28</td><td style=\"text-align: right;\">194000</td><td style=\"text-align: right;\"> -4.0905</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            367.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 195000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-27-43\n",
      "  done: false\n",
      "  episode_len_mean: 366.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -4.086699999999968\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 543\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2708101352055867\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010794993380585993\n",
      "          policy_loss: -0.16321659949090744\n",
      "          total_loss: -0.16141261474953758\n",
      "          vf_explained_var: 0.6841334700584412\n",
      "          vf_loss: 0.01932463672839933\n",
      "    num_agent_steps_sampled: 195000\n",
      "    num_agent_steps_trained: 195000\n",
      "    num_steps_sampled: 195000\n",
      "    num_steps_trained: 195000\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.63030303030303\n",
      "    ram_util_percent: 31.236363636363635\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675281400720929\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.09615805903193\n",
      "    mean_inference_ms: 2.1157564576809014\n",
      "    mean_raw_obs_processing_ms: 1.8187149740967195\n",
      "  time_since_restore: 4687.677192926407\n",
      "  time_this_iter_s: 23.399449110031128\n",
      "  time_total_s: 4687.677192926407\n",
      "  timers:\n",
      "    learn_throughput: 1311.424\n",
      "    learn_time_ms: 762.53\n",
      "    load_throughput: 42789.72\n",
      "    load_time_ms: 23.37\n",
      "    sample_throughput: 37.037\n",
      "    sample_time_ms: 26999.869\n",
      "    update_time_ms: 4.404\n",
      "  timestamp: 1635287263\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 195000\n",
      "  training_iteration: 195\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   195</td><td style=\"text-align: right;\">         4687.68</td><td style=\"text-align: right;\">195000</td><td style=\"text-align: right;\"> -4.0867</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            366.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-28-06\n",
      "  done: false\n",
      "  episode_len_mean: 365.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -4.077699999999967\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 545\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.152109220292833\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011511431184763991\n",
      "          policy_loss: -0.048033137950632306\n",
      "          total_loss: -0.055042925808164805\n",
      "          vf_explained_var: 0.8246409893035889\n",
      "          vf_loss: 0.008979575294587348\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.22647058823529\n",
      "    ram_util_percent: 31.37058823529412\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675200902496176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.092009694701012\n",
      "    mean_inference_ms: 2.115721312229342\n",
      "    mean_raw_obs_processing_ms: 1.8227557873962446\n",
      "  time_since_restore: 4711.065784215927\n",
      "  time_this_iter_s: 23.388591289520264\n",
      "  time_total_s: 4711.065784215927\n",
      "  timers:\n",
      "    learn_throughput: 1306.098\n",
      "    learn_time_ms: 765.639\n",
      "    load_throughput: 42747.07\n",
      "    load_time_ms: 23.393\n",
      "    sample_throughput: 36.847\n",
      "    sample_time_ms: 27139.506\n",
      "    update_time_ms: 4.447\n",
      "  timestamp: 1635287286\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 196\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   196</td><td style=\"text-align: right;\">         4711.07</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\"> -4.0777</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            365.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 197000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-28-28\n",
      "  done: false\n",
      "  episode_len_mean: 364.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -4.071899999999967\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 548\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.069192265139686\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011110005383036508\n",
      "          policy_loss: -0.026423815058337317\n",
      "          total_loss: -0.03271177320016755\n",
      "          vf_explained_var: 0.7799867391586304\n",
      "          vf_loss: 0.009065139619633556\n",
      "    num_agent_steps_sampled: 197000\n",
      "    num_agent_steps_trained: 197000\n",
      "    num_steps_sampled: 197000\n",
      "    num_steps_trained: 197000\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.31612903225808\n",
      "    ram_util_percent: 31.454838709677418\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036750983066967995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.086079848934354\n",
      "    mean_inference_ms: 2.115674359762805\n",
      "    mean_raw_obs_processing_ms: 1.8277426272057329\n",
      "  time_since_restore: 4732.768331050873\n",
      "  time_this_iter_s: 21.70254683494568\n",
      "  time_total_s: 4732.768331050873\n",
      "  timers:\n",
      "    learn_throughput: 1297.572\n",
      "    learn_time_ms: 770.67\n",
      "    load_throughput: 42631.539\n",
      "    load_time_ms: 23.457\n",
      "    sample_throughput: 37.101\n",
      "    sample_time_ms: 26953.579\n",
      "    update_time_ms: 5.154\n",
      "  timestamp: 1635287308\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 197000\n",
      "  training_iteration: 197\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   197</td><td style=\"text-align: right;\">         4732.77</td><td style=\"text-align: right;\">197000</td><td style=\"text-align: right;\"> -4.0719</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            364.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 198000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-29-08\n",
      "  done: false\n",
      "  episode_len_mean: 364.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -4.0683999999999685\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 551\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1695056200027465\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014873751040050405\n",
      "          policy_loss: 0.10862586134009891\n",
      "          total_loss: 0.10106382320324579\n",
      "          vf_explained_var: 0.8153120279312134\n",
      "          vf_loss: 0.0069855597361715305\n",
      "    num_agent_steps_sampled: 198000\n",
      "    num_agent_steps_trained: 198000\n",
      "    num_steps_sampled: 198000\n",
      "    num_steps_trained: 198000\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.63859649122807\n",
      "    ram_util_percent: 31.236842105263158\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036749980837807734\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.080798749688924\n",
      "    mean_inference_ms: 2.11562832442884\n",
      "    mean_raw_obs_processing_ms: 1.8331683314251195\n",
      "  time_since_restore: 4772.732040643692\n",
      "  time_this_iter_s: 39.963709592819214\n",
      "  time_total_s: 4772.732040643692\n",
      "  timers:\n",
      "    learn_throughput: 1287.053\n",
      "    learn_time_ms: 776.969\n",
      "    load_throughput: 42622.572\n",
      "    load_time_ms: 23.462\n",
      "    sample_throughput: 41.427\n",
      "    sample_time_ms: 24139.036\n",
      "    update_time_ms: 5.114\n",
      "  timestamp: 1635287348\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 198000\n",
      "  training_iteration: 198\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   198</td><td style=\"text-align: right;\">         4772.73</td><td style=\"text-align: right;\">198000</td><td style=\"text-align: right;\"> -4.0684</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            364.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 199000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-29-30\n",
      "  done: false\n",
      "  episode_len_mean: 362.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -4.053099999999969\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 554\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.100461739963955\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014333649627081554\n",
      "          policy_loss: 0.06556548807356093\n",
      "          total_loss: 0.056568961052431\n",
      "          vf_explained_var: 0.7184872031211853\n",
      "          vf_loss: 0.005120167087039186\n",
      "    num_agent_steps_sampled: 199000\n",
      "    num_agent_steps_trained: 199000\n",
      "    num_steps_sampled: 199000\n",
      "    num_steps_trained: 199000\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.02903225806452\n",
      "    ram_util_percent: 31.645161290322584\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674903201506386\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.07613691021441\n",
      "    mean_inference_ms: 2.1155838512625085\n",
      "    mean_raw_obs_processing_ms: 1.8387273110374622\n",
      "  time_since_restore: 4794.330426692963\n",
      "  time_this_iter_s: 21.59838604927063\n",
      "  time_total_s: 4794.330426692963\n",
      "  timers:\n",
      "    learn_throughput: 1290.577\n",
      "    learn_time_ms: 774.847\n",
      "    load_throughput: 42603.609\n",
      "    load_time_ms: 23.472\n",
      "    sample_throughput: 42.012\n",
      "    sample_time_ms: 23802.533\n",
      "    update_time_ms: 4.61\n",
      "  timestamp: 1635287370\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 199000\n",
      "  training_iteration: 199\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         4794.33</td><td style=\"text-align: right;\">199000</td><td style=\"text-align: right;\"> -4.0531</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">             362.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-29-52\n",
      "  done: false\n",
      "  episode_len_mean: 361.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -4.027499999999968\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 557\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.141997146606445\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011658645740282204\n",
      "          policy_loss: 0.03930358356899685\n",
      "          total_loss: 0.032903994454277885\n",
      "          vf_explained_var: 0.3105916678905487\n",
      "          vf_loss: 0.009417909244075418\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.58125000000001\n",
      "    ram_util_percent: 31.271875\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036748103788499915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.072035015714754\n",
      "    mean_inference_ms: 2.1155404386473777\n",
      "    mean_raw_obs_processing_ms: 1.8443157202322111\n",
      "  time_since_restore: 4816.801217556\n",
      "  time_this_iter_s: 22.47079086303711\n",
      "  time_total_s: 4816.801217556\n",
      "  timers:\n",
      "    learn_throughput: 1288.83\n",
      "    learn_time_ms: 775.897\n",
      "    load_throughput: 42741.45\n",
      "    load_time_ms: 23.396\n",
      "    sample_throughput: 42.311\n",
      "    sample_time_ms: 23634.418\n",
      "    update_time_ms: 4.517\n",
      "  timestamp: 1635287392\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 200\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">          4816.8</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\"> -4.0275</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            361.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 201000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-30-14\n",
      "  done: false\n",
      "  episode_len_mean: 360.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -4.016699999999969\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 560\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0675991906060114\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007807518450098256\n",
      "          policy_loss: 0.03675085372394986\n",
      "          total_loss: 0.027453429996967316\n",
      "          vf_explained_var: 0.47630763053894043\n",
      "          vf_loss: 0.007626721603770016\n",
      "    num_agent_steps_sampled: 201000\n",
      "    num_agent_steps_trained: 201000\n",
      "    num_steps_sampled: 201000\n",
      "    num_steps_trained: 201000\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.8258064516129\n",
      "    ram_util_percent: 31.038709677419355\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036747255676132955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.0685919741053\n",
      "    mean_inference_ms: 2.1154995604146363\n",
      "    mean_raw_obs_processing_ms: 1.8500284359040813\n",
      "  time_since_restore: 4839.030797481537\n",
      "  time_this_iter_s: 22.22957992553711\n",
      "  time_total_s: 4839.030797481537\n",
      "  timers:\n",
      "    learn_throughput: 1289.893\n",
      "    learn_time_ms: 775.258\n",
      "    load_throughput: 42807.189\n",
      "    load_time_ms: 23.361\n",
      "    sample_throughput: 42.528\n",
      "    sample_time_ms: 23514.122\n",
      "    update_time_ms: 4.528\n",
      "  timestamp: 1635287414\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 201000\n",
      "  training_iteration: 201\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   201</td><td style=\"text-align: right;\">         4839.03</td><td style=\"text-align: right;\">201000</td><td style=\"text-align: right;\"> -4.0167</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            360.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 202000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-30-37\n",
      "  done: false\n",
      "  episode_len_mean: 358.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -3.991599999999969\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 563\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1341961834165786\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014468822069443298\n",
      "          policy_loss: -0.006849668795863787\n",
      "          total_loss: 0.1357834299819337\n",
      "          vf_explained_var: 0.20611029863357544\n",
      "          vf_loss: 0.15702218379721875\n",
      "    num_agent_steps_sampled: 202000\n",
      "    num_agent_steps_trained: 202000\n",
      "    num_steps_sampled: 202000\n",
      "    num_steps_trained: 202000\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.73125000000002\n",
      "    ram_util_percent: 31.01875\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674648627844027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.065765587798367\n",
      "    mean_inference_ms: 2.115462062042923\n",
      "    mean_raw_obs_processing_ms: 1.8558629342431754\n",
      "  time_since_restore: 4861.4428191185\n",
      "  time_this_iter_s: 22.41202163696289\n",
      "  time_total_s: 4861.4428191185\n",
      "  timers:\n",
      "    learn_throughput: 1291.939\n",
      "    learn_time_ms: 774.03\n",
      "    load_throughput: 42981.913\n",
      "    load_time_ms: 23.266\n",
      "    sample_throughput: 42.672\n",
      "    sample_time_ms: 23434.593\n",
      "    update_time_ms: 4.51\n",
      "  timestamp: 1635287437\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 202000\n",
      "  training_iteration: 202\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   202</td><td style=\"text-align: right;\">         4861.44</td><td style=\"text-align: right;\">202000</td><td style=\"text-align: right;\"> -3.9916</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            358.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 203000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-30-59\n",
      "  done: false\n",
      "  episode_len_mean: 356.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -3.9513999999999694\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 566\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1342698203192816\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011440754328114621\n",
      "          policy_loss: 0.015735490454567805\n",
      "          total_loss: 0.013093497355779011\n",
      "          vf_explained_var: 0.3893946707248688\n",
      "          vf_loss: 0.01320294319302775\n",
      "    num_agent_steps_sampled: 203000\n",
      "    num_agent_steps_trained: 203000\n",
      "    num_steps_sampled: 203000\n",
      "    num_steps_trained: 203000\n",
      "  iterations_since_restore: 203\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.97272727272728\n",
      "    ram_util_percent: 31.07878787878788\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674576441774526\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.063550438727383\n",
      "    mean_inference_ms: 2.11542579591414\n",
      "    mean_raw_obs_processing_ms: 1.8617201930988252\n",
      "  time_since_restore: 4884.13144159317\n",
      "  time_this_iter_s: 22.68862247467041\n",
      "  time_total_s: 4884.13144159317\n",
      "  timers:\n",
      "    learn_throughput: 1298.732\n",
      "    learn_time_ms: 769.982\n",
      "    load_throughput: 41979.646\n",
      "    load_time_ms: 23.821\n",
      "    sample_throughput: 42.797\n",
      "    sample_time_ms: 23365.993\n",
      "    update_time_ms: 4.498\n",
      "  timestamp: 1635287459\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 203000\n",
      "  training_iteration: 203\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   203</td><td style=\"text-align: right;\">         4884.13</td><td style=\"text-align: right;\">203000</td><td style=\"text-align: right;\"> -3.9514</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            356.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-31-22\n",
      "  done: false\n",
      "  episode_len_mean: 355.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -3.9683999999999684\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 569\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.074854935540093\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006103807952001164\n",
      "          policy_loss: 0.02047874712281757\n",
      "          total_loss: 0.010403881470362345\n",
      "          vf_explained_var: -0.06300951540470123\n",
      "          vf_loss: 0.0077405487122531565\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.9121212121212\n",
      "    ram_util_percent: 31.106060606060606\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674510172525202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.061995665909745\n",
      "    mean_inference_ms: 2.1153927243712154\n",
      "    mean_raw_obs_processing_ms: 1.8676913716356354\n",
      "  time_since_restore: 4906.9697251319885\n",
      "  time_this_iter_s: 22.83828353881836\n",
      "  time_total_s: 4906.9697251319885\n",
      "  timers:\n",
      "    learn_throughput: 1302.422\n",
      "    learn_time_ms: 767.8\n",
      "    load_throughput: 42039.858\n",
      "    load_time_ms: 23.787\n",
      "    sample_throughput: 42.61\n",
      "    sample_time_ms: 23468.888\n",
      "    update_time_ms: 4.278\n",
      "  timestamp: 1635287482\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 204\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   204</td><td style=\"text-align: right;\">         4906.97</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\"> -3.9684</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            355.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 205000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-31-45\n",
      "  done: false\n",
      "  episode_len_mean: 353.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -3.9317999999999693\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 572\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1357473373413085\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008508033445097313\n",
      "          policy_loss: 0.0495820391509268\n",
      "          total_loss: 0.03989131409260962\n",
      "          vf_explained_var: 0.3005598783493042\n",
      "          vf_loss: 0.007578277089891748\n",
      "    num_agent_steps_sampled: 205000\n",
      "    num_agent_steps_trained: 205000\n",
      "    num_steps_sampled: 205000\n",
      "    num_steps_trained: 205000\n",
      "  iterations_since_restore: 205\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.765625\n",
      "    ram_util_percent: 31.08125\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674445350942066\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.060890987671694\n",
      "    mean_inference_ms: 2.115360365832009\n",
      "    mean_raw_obs_processing_ms: 1.8736788096938983\n",
      "  time_since_restore: 4929.703625917435\n",
      "  time_this_iter_s: 22.733900785446167\n",
      "  time_total_s: 4929.703625917435\n",
      "  timers:\n",
      "    learn_throughput: 1303.53\n",
      "    learn_time_ms: 767.148\n",
      "    load_throughput: 42021.031\n",
      "    load_time_ms: 23.798\n",
      "    sample_throughput: 42.73\n",
      "    sample_time_ms: 23403.019\n",
      "    update_time_ms: 4.277\n",
      "  timestamp: 1635287505\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 205000\n",
      "  training_iteration: 205\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   205</td><td style=\"text-align: right;\">          4929.7</td><td style=\"text-align: right;\">205000</td><td style=\"text-align: right;\"> -3.9318</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            353.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 206000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-32-08\n",
      "  done: false\n",
      "  episode_len_mean: 352.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -3.9172999999999694\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 575\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1773431115680273\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008277449572305941\n",
      "          policy_loss: 0.024604091048240663\n",
      "          total_loss: 0.016373431268665525\n",
      "          vf_explained_var: -0.08250058442354202\n",
      "          vf_loss: 0.009565108163385756\n",
      "    num_agent_steps_sampled: 206000\n",
      "    num_agent_steps_trained: 206000\n",
      "    num_steps_sampled: 206000\n",
      "    num_steps_trained: 206000\n",
      "  iterations_since_restore: 206\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.75454545454545\n",
      "    ram_util_percent: 31.075757575757574\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036743841390998004\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.060291671649985\n",
      "    mean_inference_ms: 2.11533065733347\n",
      "    mean_raw_obs_processing_ms: 1.8796862106253622\n",
      "  time_since_restore: 4952.605139017105\n",
      "  time_this_iter_s: 22.90151309967041\n",
      "  time_total_s: 4952.605139017105\n",
      "  timers:\n",
      "    learn_throughput: 1309.734\n",
      "    learn_time_ms: 763.514\n",
      "    load_throughput: 42006.764\n",
      "    load_time_ms: 23.806\n",
      "    sample_throughput: 42.812\n",
      "    sample_time_ms: 23357.911\n",
      "    update_time_ms: 4.305\n",
      "  timestamp: 1635287528\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 206000\n",
      "  training_iteration: 206\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   206</td><td style=\"text-align: right;\">         4952.61</td><td style=\"text-align: right;\">206000</td><td style=\"text-align: right;\"> -3.9173</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            352.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 207000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-32-30\n",
      "  done: false\n",
      "  episode_len_mean: 350.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -3.89049999999997\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 578\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.099393873744541\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010654267721008795\n",
      "          policy_loss: 0.029064346849918366\n",
      "          total_loss: 0.024096226278278562\n",
      "          vf_explained_var: 0.4127701222896576\n",
      "          vf_loss: 0.010905994738762578\n",
      "    num_agent_steps_sampled: 207000\n",
      "    num_agent_steps_trained: 207000\n",
      "    num_steps_sampled: 207000\n",
      "    num_steps_trained: 207000\n",
      "  iterations_since_restore: 207\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.771875\n",
      "    ram_util_percent: 31.0375\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367432943203538\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.0603493863423\n",
      "    mean_inference_ms: 2.115303149532919\n",
      "    mean_raw_obs_processing_ms: 1.8857987389462787\n",
      "  time_since_restore: 4975.010113477707\n",
      "  time_this_iter_s: 22.404974460601807\n",
      "  time_total_s: 4975.010113477707\n",
      "  timers:\n",
      "    learn_throughput: 1316.111\n",
      "    learn_time_ms: 759.814\n",
      "    load_throughput: 41856.355\n",
      "    load_time_ms: 23.891\n",
      "    sample_throughput: 42.675\n",
      "    sample_time_ms: 23432.722\n",
      "    update_time_ms: 3.601\n",
      "  timestamp: 1635287550\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 207000\n",
      "  training_iteration: 207\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   207</td><td style=\"text-align: right;\">         4975.01</td><td style=\"text-align: right;\">207000</td><td style=\"text-align: right;\"> -3.8905</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            350.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-33-11\n",
      "  done: false\n",
      "  episode_len_mean: 349.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -3.8802999999999703\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 581\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0656408031781512\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00850700425383807\n",
      "          policy_loss: 0.03999806154105398\n",
      "          total_loss: 0.028642282138268152\n",
      "          vf_explained_var: 0.5763236284255981\n",
      "          vf_loss: 0.005212655550955484\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.543103448275865\n",
      "    ram_util_percent: 31.11896551724138\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674278061345296\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.060496212441507\n",
      "    mean_inference_ms: 2.1152767148651646\n",
      "    mean_raw_obs_processing_ms: 1.891462003242477\n",
      "  time_since_restore: 5015.708595275879\n",
      "  time_this_iter_s: 40.698481798172\n",
      "  time_total_s: 5015.708595275879\n",
      "  timers:\n",
      "    learn_throughput: 1327.152\n",
      "    learn_time_ms: 753.493\n",
      "    load_throughput: 41994.273\n",
      "    load_time_ms: 23.813\n",
      "    sample_throughput: 42.531\n",
      "    sample_time_ms: 23512.43\n",
      "    update_time_ms: 3.6\n",
      "  timestamp: 1635287591\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 208\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   208</td><td style=\"text-align: right;\">         5015.71</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\"> -3.8803</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            349.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 209000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-33-34\n",
      "  done: false\n",
      "  episode_len_mean: 347.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -3.8760999999999695\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 584\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.192266578144497\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016402153856111646\n",
      "          policy_loss: -0.013703177703751459\n",
      "          total_loss: -0.01582478168937895\n",
      "          vf_explained_var: 0.4648977518081665\n",
      "          vf_loss: 0.011919133911012776\n",
      "    num_agent_steps_sampled: 209000\n",
      "    num_agent_steps_trained: 209000\n",
      "    num_steps_sampled: 209000\n",
      "    num_steps_trained: 209000\n",
      "  iterations_since_restore: 209\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.97878787878787\n",
      "    ram_util_percent: 31.309090909090912\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674230845874337\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.061257151550823\n",
      "    mean_inference_ms: 2.115254490049847\n",
      "    mean_raw_obs_processing_ms: 1.8972380931894874\n",
      "  time_since_restore: 5038.842716217041\n",
      "  time_this_iter_s: 23.13412094116211\n",
      "  time_total_s: 5038.842716217041\n",
      "  timers:\n",
      "    learn_throughput: 1325.701\n",
      "    learn_time_ms: 754.318\n",
      "    load_throughput: 41969.187\n",
      "    load_time_ms: 23.827\n",
      "    sample_throughput: 42.256\n",
      "    sample_time_ms: 23665.139\n",
      "    update_time_ms: 3.596\n",
      "  timestamp: 1635287614\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 209000\n",
      "  training_iteration: 209\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   209</td><td style=\"text-align: right;\">         5038.84</td><td style=\"text-align: right;\">209000</td><td style=\"text-align: right;\"> -3.8761</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            347.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 210000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-33-58\n",
      "  done: false\n",
      "  episode_len_mean: 345.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -3.87419999999997\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 587\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.065255833996667\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0065728630602395295\n",
      "          policy_loss: 0.04987748016913732\n",
      "          total_loss: 0.03852434915800889\n",
      "          vf_explained_var: 0.6650003790855408\n",
      "          vf_loss: 0.006140889134258032\n",
      "    num_agent_steps_sampled: 210000\n",
      "    num_agent_steps_trained: 210000\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.70294117647059\n",
      "    ram_util_percent: 30.970588235294116\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036741894018212776\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.062602203839404\n",
      "    mean_inference_ms: 2.1152337091536637\n",
      "    mean_raw_obs_processing_ms: 1.9030323885633198\n",
      "  time_since_restore: 5062.816965341568\n",
      "  time_this_iter_s: 23.974249124526978\n",
      "  time_total_s: 5062.816965341568\n",
      "  timers:\n",
      "    learn_throughput: 1329.296\n",
      "    learn_time_ms: 752.278\n",
      "    load_throughput: 41828.18\n",
      "    load_time_ms: 23.907\n",
      "    sample_throughput: 41.986\n",
      "    sample_time_ms: 23817.432\n",
      "    update_time_ms: 3.598\n",
      "  timestamp: 1635287638\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 210\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   210</td><td style=\"text-align: right;\">         5062.82</td><td style=\"text-align: right;\">210000</td><td style=\"text-align: right;\"> -3.8742</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            345.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 211000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-34-20\n",
      "  done: false\n",
      "  episode_len_mean: 344.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -3.8790999999999696\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 590\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.114531644185384\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00869534239371773\n",
      "          policy_loss: 0.06513691188560591\n",
      "          total_loss: 0.058855331357982425\n",
      "          vf_explained_var: 0.237808495759964\n",
      "          vf_loss: 0.010685259517696168\n",
      "    num_agent_steps_sampled: 211000\n",
      "    num_agent_steps_trained: 211000\n",
      "    num_steps_sampled: 211000\n",
      "    num_steps_trained: 211000\n",
      "  iterations_since_restore: 211\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.7032258064516\n",
      "    ram_util_percent: 31.074193548387097\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674155593804145\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.064053458145505\n",
      "    mean_inference_ms: 2.115215066889528\n",
      "    mean_raw_obs_processing_ms: 1.908935811170961\n",
      "  time_since_restore: 5084.20814371109\n",
      "  time_this_iter_s: 21.391178369522095\n",
      "  time_total_s: 5084.20814371109\n",
      "  timers:\n",
      "    learn_throughput: 1329.526\n",
      "    learn_time_ms: 752.148\n",
      "    load_throughput: 41820.131\n",
      "    load_time_ms: 23.912\n",
      "    sample_throughput: 42.134\n",
      "    sample_time_ms: 23733.723\n",
      "    update_time_ms: 3.591\n",
      "  timestamp: 1635287660\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 211000\n",
      "  training_iteration: 211\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   211</td><td style=\"text-align: right;\">         5084.21</td><td style=\"text-align: right;\">211000</td><td style=\"text-align: right;\"> -3.8791</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            344.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-34-42\n",
      "  done: false\n",
      "  episode_len_mean: 344.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -3.8433999999999715\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 593\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2367808394961886\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004763234768003195\n",
      "          policy_loss: 0.04652092845903503\n",
      "          total_loss: 0.03233851504822572\n",
      "          vf_explained_var: 0.7468599677085876\n",
      "          vf_loss: 0.005896460440837675\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 212\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.75483870967743\n",
      "    ram_util_percent: 31.061290322580643\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036741259666897334\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.065638056941538\n",
      "    mean_inference_ms: 2.1151983500880402\n",
      "    mean_raw_obs_processing_ms: 1.9148526356074884\n",
      "  time_since_restore: 5106.437030076981\n",
      "  time_this_iter_s: 22.228886365890503\n",
      "  time_total_s: 5106.437030076981\n",
      "  timers:\n",
      "    learn_throughput: 1330.952\n",
      "    learn_time_ms: 751.342\n",
      "    load_throughput: 41521.02\n",
      "    load_time_ms: 24.084\n",
      "    sample_throughput: 42.166\n",
      "    sample_time_ms: 23716.043\n",
      "    update_time_ms: 3.594\n",
      "  timestamp: 1635287682\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 212\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   212</td><td style=\"text-align: right;\">         5106.44</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\"> -3.8434</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">             344.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 213000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-35-04\n",
      "  done: false\n",
      "  episode_len_mean: 343.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -3.8284999999999707\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 596\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2402709960937499\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.158386935128106\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0077107715212253285\n",
      "          policy_loss: -0.0009628848483165105\n",
      "          total_loss: -0.004381926957931784\n",
      "          vf_explained_var: 0.30280423164367676\n",
      "          vf_loss: 0.016312153632235198\n",
      "    num_agent_steps_sampled: 213000\n",
      "    num_agent_steps_trained: 213000\n",
      "    num_steps_sampled: 213000\n",
      "    num_steps_trained: 213000\n",
      "  iterations_since_restore: 213\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.8375\n",
      "    ram_util_percent: 31.118750000000002\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036741022272174136\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.06745240759077\n",
      "    mean_inference_ms: 2.1151845011559285\n",
      "    mean_raw_obs_processing_ms: 1.9207817142649108\n",
      "  time_since_restore: 5128.713548660278\n",
      "  time_this_iter_s: 22.27651858329773\n",
      "  time_total_s: 5128.713548660278\n",
      "  timers:\n",
      "    learn_throughput: 1332.624\n",
      "    learn_time_ms: 750.399\n",
      "    load_throughput: 41596.376\n",
      "    load_time_ms: 24.041\n",
      "    sample_throughput: 42.237\n",
      "    sample_time_ms: 23675.804\n",
      "    update_time_ms: 3.607\n",
      "  timestamp: 1635287704\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 213000\n",
      "  training_iteration: 213\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   213</td><td style=\"text-align: right;\">         5128.71</td><td style=\"text-align: right;\">213000</td><td style=\"text-align: right;\"> -3.8285</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            343.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 214000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-35-25\n",
      "  done: false\n",
      "  episode_len_mean: 342.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -3.7952999999999713\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 598\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2402709960937499\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.101566770341661\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0177845626443281\n",
      "          policy_loss: -0.12184589323070315\n",
      "          total_loss: -0.1274376965645287\n",
      "          vf_explained_var: 0.37691056728363037\n",
      "          vf_loss: 0.011150745762926009\n",
      "    num_agent_steps_sampled: 214000\n",
      "    num_agent_steps_trained: 214000\n",
      "    num_steps_sampled: 214000\n",
      "    num_steps_trained: 214000\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.52333333333333\n",
      "    ram_util_percent: 31.14\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674088114082192\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.068670078763745\n",
      "    mean_inference_ms: 2.115175282530462\n",
      "    mean_raw_obs_processing_ms: 1.924712540980048\n",
      "  time_since_restore: 5149.698427915573\n",
      "  time_this_iter_s: 20.9848792552948\n",
      "  time_total_s: 5149.698427915573\n",
      "  timers:\n",
      "    learn_throughput: 1332.986\n",
      "    learn_time_ms: 750.195\n",
      "    load_throughput: 41459.17\n",
      "    load_time_ms: 24.12\n",
      "    sample_throughput: 42.57\n",
      "    sample_time_ms: 23490.572\n",
      "    update_time_ms: 3.614\n",
      "  timestamp: 1635287725\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 214000\n",
      "  training_iteration: 214\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   214</td><td style=\"text-align: right;\">          5149.7</td><td style=\"text-align: right;\">214000</td><td style=\"text-align: right;\"> -3.7953</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            342.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 215000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-35-48\n",
      "  done: false\n",
      "  episode_len_mean: 341.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -3.7081999999999713\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 601\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2402709960937499\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.06307536760966\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008189988800047605\n",
      "          policy_loss: -0.10643710676166747\n",
      "          total_loss: -0.11444124852617582\n",
      "          vf_explained_var: 0.5458593368530273\n",
      "          vf_loss: 0.010658795200288295\n",
      "    num_agent_steps_sampled: 215000\n",
      "    num_agent_steps_trained: 215000\n",
      "    num_steps_sampled: 215000\n",
      "    num_steps_trained: 215000\n",
      "  iterations_since_restore: 215\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.690625\n",
      "    ram_util_percent: 31.146875\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036740735904321545\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.070780243601085\n",
      "    mean_inference_ms: 2.1151630203879663\n",
      "    mean_raw_obs_processing_ms: 1.9306655418520475\n",
      "  time_since_restore: 5172.179250001907\n",
      "  time_this_iter_s: 22.48082208633423\n",
      "  time_total_s: 5172.179250001907\n",
      "  timers:\n",
      "    learn_throughput: 1336.012\n",
      "    learn_time_ms: 748.496\n",
      "    load_throughput: 41275.816\n",
      "    load_time_ms: 24.227\n",
      "    sample_throughput: 42.613\n",
      "    sample_time_ms: 23466.835\n",
      "    update_time_ms: 3.609\n",
      "  timestamp: 1635287748\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 215000\n",
      "  training_iteration: 215\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   215</td><td style=\"text-align: right;\">         5172.18</td><td style=\"text-align: right;\">215000</td><td style=\"text-align: right;\"> -3.7082</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            341.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-36-11\n",
      "  done: false\n",
      "  episode_len_mean: 340.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -3.7069999999999728\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 605\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2402709960937499\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9984996610217625\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014352168694954036\n",
      "          policy_loss: -0.05995497206846873\n",
      "          total_loss: -0.06667070537805557\n",
      "          vf_explained_var: 0.6363044381141663\n",
      "          vf_loss: 0.009820852149277925\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.83529411764707\n",
      "    ram_util_percent: 31.105882352941173\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674064754521494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.07404240491165\n",
      "    mean_inference_ms: 2.115149734722516\n",
      "    mean_raw_obs_processing_ms: 1.9386563658148646\n",
      "  time_since_restore: 5195.460777997971\n",
      "  time_this_iter_s: 23.281527996063232\n",
      "  time_total_s: 5195.460777997971\n",
      "  timers:\n",
      "    learn_throughput: 1333.831\n",
      "    learn_time_ms: 749.72\n",
      "    load_throughput: 41328.689\n",
      "    load_time_ms: 24.196\n",
      "    sample_throughput: 42.546\n",
      "    sample_time_ms: 23503.724\n",
      "    update_time_ms: 3.524\n",
      "  timestamp: 1635287771\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 216\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   216</td><td style=\"text-align: right;\">         5195.46</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">  -3.707</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">             340.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 217000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-36-34\n",
      "  done: false\n",
      "  episode_len_mean: 339.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -3.6921999999999717\n",
      "  episode_reward_min: -10.289999999999965\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 607\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2402709960937499\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0160121334923637\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004150191670655337\n",
      "          policy_loss: -0.1354136394129859\n",
      "          total_loss: -0.14934945305188496\n",
      "          vf_explained_var: 0.8271165490150452\n",
      "          vf_loss: 0.005227137463710581\n",
      "    num_agent_steps_sampled: 217000\n",
      "    num_agent_steps_trained: 217000\n",
      "    num_steps_sampled: 217000\n",
      "    num_steps_trained: 217000\n",
      "  iterations_since_restore: 217\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.84375\n",
      "    ram_util_percent: 31.0125\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674066529130329\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.075855376183906\n",
      "    mean_inference_ms: 2.1151450037432253\n",
      "    mean_raw_obs_processing_ms: 1.9427010762352988\n",
      "  time_since_restore: 5218.15278840065\n",
      "  time_this_iter_s: 22.692010402679443\n",
      "  time_total_s: 5218.15278840065\n",
      "  timers:\n",
      "    learn_throughput: 1334.508\n",
      "    learn_time_ms: 749.34\n",
      "    load_throughput: 41372.635\n",
      "    load_time_ms: 24.171\n",
      "    sample_throughput: 42.494\n",
      "    sample_time_ms: 23532.826\n",
      "    update_time_ms: 3.524\n",
      "  timestamp: 1635287794\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 217000\n",
      "  training_iteration: 217\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   217</td><td style=\"text-align: right;\">         5218.15</td><td style=\"text-align: right;\">217000</td><td style=\"text-align: right;\"> -3.6922</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">              -10.29</td><td style=\"text-align: right;\">            339.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 218000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-37-14\n",
      "  done: false\n",
      "  episode_len_mean: 339.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -3.5666999999999716\n",
      "  episode_reward_min: -8.71999999999995\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 611\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9210202813148498\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010373790971761564\n",
      "          policy_loss: -0.05387344252732065\n",
      "          total_loss: -0.05567632226480378\n",
      "          vf_explained_var: 0.5382281541824341\n",
      "          vf_loss: 0.01616105933466719\n",
      "    num_agent_steps_sampled: 218000\n",
      "    num_agent_steps_trained: 218000\n",
      "    num_steps_sampled: 218000\n",
      "    num_steps_trained: 218000\n",
      "  iterations_since_restore: 218\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.04912280701755\n",
      "    ram_util_percent: 31.114035087719298\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674079078589182\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.079527157275958\n",
      "    mean_inference_ms: 2.115137069433602\n",
      "    mean_raw_obs_processing_ms: 1.9503370105271935\n",
      "  time_since_restore: 5258.3635931015015\n",
      "  time_this_iter_s: 40.21080470085144\n",
      "  time_total_s: 5258.3635931015015\n",
      "  timers:\n",
      "    learn_throughput: 1331.48\n",
      "    learn_time_ms: 751.044\n",
      "    load_throughput: 41358.887\n",
      "    load_time_ms: 24.179\n",
      "    sample_throughput: 42.585\n",
      "    sample_time_ms: 23482.519\n",
      "    update_time_ms: 3.523\n",
      "  timestamp: 1635287834\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 218000\n",
      "  training_iteration: 218\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   218</td><td style=\"text-align: right;\">         5258.36</td><td style=\"text-align: right;\">218000</td><td style=\"text-align: right;\"> -3.5667</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">               -8.72</td><td style=\"text-align: right;\">            339.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 219000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-37-35\n",
      "  done: false\n",
      "  episode_len_mean: 339.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -3.606499999999972\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 613\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9815361579259236\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018087265455498276\n",
      "          policy_loss: -0.04815838618410958\n",
      "          total_loss: 0.03957497767276234\n",
      "          vf_explained_var: 0.45640936493873596\n",
      "          vf_loss: 0.10537580531090499\n",
      "    num_agent_steps_sampled: 219000\n",
      "    num_agent_steps_trained: 219000\n",
      "    num_steps_sampled: 219000\n",
      "    num_steps_trained: 219000\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.10645161290323\n",
      "    ram_util_percent: 31.416129032258056\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674088352754918\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.08129390994328\n",
      "    mean_inference_ms: 2.1151331967323626\n",
      "    mean_raw_obs_processing_ms: 1.9541175956174284\n",
      "  time_since_restore: 5279.514890909195\n",
      "  time_this_iter_s: 21.15129780769348\n",
      "  time_total_s: 5279.514890909195\n",
      "  timers:\n",
      "    learn_throughput: 1334.403\n",
      "    learn_time_ms: 749.399\n",
      "    load_throughput: 40932.778\n",
      "    load_time_ms: 24.43\n",
      "    sample_throughput: 42.945\n",
      "    sample_time_ms: 23285.652\n",
      "    update_time_ms: 3.522\n",
      "  timestamp: 1635287855\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 219000\n",
      "  training_iteration: 219\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   219</td><td style=\"text-align: right;\">         5279.51</td><td style=\"text-align: right;\">219000</td><td style=\"text-align: right;\"> -3.6065</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            339.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-37-59\n",
      "  done: false\n",
      "  episode_len_mean: 339.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7900000000000116\n",
      "  episode_reward_mean: -3.5334999999999726\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 616\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8415015008714464\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01318392714093785\n",
      "          policy_loss: -0.08345170766115188\n",
      "          total_loss: -0.09378802428642909\n",
      "          vf_explained_var: 0.8162569403648376\n",
      "          vf_loss: 0.006494837498757988\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.90882352941175\n",
      "    ram_util_percent: 31.123529411764707\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674109763118234\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.084179750150284\n",
      "    mean_inference_ms: 2.11512911301466\n",
      "    mean_raw_obs_processing_ms: 1.9597997402381306\n",
      "  time_since_restore: 5303.259326696396\n",
      "  time_this_iter_s: 23.744435787200928\n",
      "  time_total_s: 5303.259326696396\n",
      "  timers:\n",
      "    learn_throughput: 1329.331\n",
      "    learn_time_ms: 752.258\n",
      "    load_throughput: 41041.281\n",
      "    load_time_ms: 24.366\n",
      "    sample_throughput: 42.993\n",
      "    sample_time_ms: 23259.723\n",
      "    update_time_ms: 3.678\n",
      "  timestamp: 1635287879\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 220\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   220</td><td style=\"text-align: right;\">         5303.26</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\"> -3.5335</td><td style=\"text-align: right;\">                2.79</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            339.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 221000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-38-23\n",
      "  done: false\n",
      "  episode_len_mean: 338.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.1799999999999879\n",
      "  episode_reward_mean: -3.5480999999999723\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 620\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.809896128707462\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011859217803880048\n",
      "          policy_loss: -0.052185601037409574\n",
      "          total_loss: -0.06372814464072386\n",
      "          vf_explained_var: 0.8661167621612549\n",
      "          vf_loss: 0.005131701385188434\n",
      "    num_agent_steps_sampled: 221000\n",
      "    num_agent_steps_trained: 221000\n",
      "    num_steps_sampled: 221000\n",
      "    num_steps_trained: 221000\n",
      "  iterations_since_restore: 221\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.60588235294118\n",
      "    ram_util_percent: 31.11176470588235\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674147055876634\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.087959738766237\n",
      "    mean_inference_ms: 2.115124348132839\n",
      "    mean_raw_obs_processing_ms: 1.9651260388995346\n",
      "  time_since_restore: 5327.208005905151\n",
      "  time_this_iter_s: 23.948679208755493\n",
      "  time_total_s: 5327.208005905151\n",
      "  timers:\n",
      "    learn_throughput: 1330.379\n",
      "    learn_time_ms: 751.666\n",
      "    load_throughput: 40901.683\n",
      "    load_time_ms: 24.449\n",
      "    sample_throughput: 42.524\n",
      "    sample_time_ms: 23516.003\n",
      "    update_time_ms: 3.677\n",
      "  timestamp: 1635287903\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 221000\n",
      "  training_iteration: 221\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   221</td><td style=\"text-align: right;\">         5327.21</td><td style=\"text-align: right;\">221000</td><td style=\"text-align: right;\"> -3.5481</td><td style=\"text-align: right;\">               -0.18</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            338.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 222000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-38-46\n",
      "  done: false\n",
      "  episode_len_mean: 337.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.1799999999999879\n",
      "  episode_reward_mean: -3.5074999999999714\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 623\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7324645876884461\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010830905519136439\n",
      "          policy_loss: -0.020608162652287217\n",
      "          total_loss: -0.027802445677419504\n",
      "          vf_explained_var: 0.3982659578323364\n",
      "          vf_loss: 0.008829185810625657\n",
      "    num_agent_steps_sampled: 222000\n",
      "    num_agent_steps_trained: 222000\n",
      "    num_steps_sampled: 222000\n",
      "    num_steps_trained: 222000\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.68124999999999\n",
      "    ram_util_percent: 31.168750000000003\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674168047952053\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.090524633476544\n",
      "    mean_inference_ms: 2.11511744346937\n",
      "    mean_raw_obs_processing_ms: 1.9638627655642653\n",
      "  time_since_restore: 5349.924620389938\n",
      "  time_this_iter_s: 22.716614484786987\n",
      "  time_total_s: 5349.924620389938\n",
      "  timers:\n",
      "    learn_throughput: 1328.68\n",
      "    learn_time_ms: 752.627\n",
      "    load_throughput: 41079.508\n",
      "    load_time_ms: 24.343\n",
      "    sample_throughput: 42.438\n",
      "    sample_time_ms: 23563.92\n",
      "    update_time_ms: 3.676\n",
      "  timestamp: 1635287926\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 222000\n",
      "  training_iteration: 222\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   222</td><td style=\"text-align: right;\">         5349.92</td><td style=\"text-align: right;\">222000</td><td style=\"text-align: right;\"> -3.5075</td><td style=\"text-align: right;\">               -0.18</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            337.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 223000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-39-09\n",
      "  done: false\n",
      "  episode_len_mean: 337.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.1799999999999879\n",
      "  episode_reward_mean: -3.463099999999973\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 626\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.978050336572859\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020047072447940194\n",
      "          policy_loss: 0.0717071005039745\n",
      "          total_loss: 0.05793831340140766\n",
      "          vf_explained_var: 0.9238448143005371\n",
      "          vf_loss: 0.0036033517758672436\n",
      "    num_agent_steps_sampled: 223000\n",
      "    num_agent_steps_trained: 223000\n",
      "    num_steps_sampled: 223000\n",
      "    num_steps_trained: 223000\n",
      "  iterations_since_restore: 223\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.91176470588235\n",
      "    ram_util_percent: 31.202941176470592\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674157980796191\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.09288981305964\n",
      "    mean_inference_ms: 2.115098601861964\n",
      "    mean_raw_obs_processing_ms: 1.9626433422087703\n",
      "  time_since_restore: 5373.5389931201935\n",
      "  time_this_iter_s: 23.614372730255127\n",
      "  time_total_s: 5373.5389931201935\n",
      "  timers:\n",
      "    learn_throughput: 1325.725\n",
      "    learn_time_ms: 754.305\n",
      "    load_throughput: 41026.267\n",
      "    load_time_ms: 24.375\n",
      "    sample_throughput: 42.201\n",
      "    sample_time_ms: 23695.978\n",
      "    update_time_ms: 3.672\n",
      "  timestamp: 1635287949\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 223000\n",
      "  training_iteration: 223\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   223</td><td style=\"text-align: right;\">         5373.54</td><td style=\"text-align: right;\">223000</td><td style=\"text-align: right;\"> -3.4631</td><td style=\"text-align: right;\">               -0.18</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            337.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-39-31\n",
      "  done: false\n",
      "  episode_len_mean: 337.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.1799999999999879\n",
      "  episode_reward_mean: -3.4467999999999734\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 629\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18020324707031254\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.024118537373013\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020033680257757917\n",
      "          policy_loss: 0.015619089951117833\n",
      "          total_loss: 0.01850550700392988\n",
      "          vf_explained_var: 0.7445630431175232\n",
      "          vf_loss: 0.019517471152357756\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.75806451612904\n",
      "    ram_util_percent: 31.225806451612907\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674110151478959\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.094971089684204\n",
      "    mean_inference_ms: 2.115071219991733\n",
      "    mean_raw_obs_processing_ms: 1.961467093888734\n",
      "  time_since_restore: 5395.6030440330505\n",
      "  time_this_iter_s: 22.064050912857056\n",
      "  time_total_s: 5395.6030440330505\n",
      "  timers:\n",
      "    learn_throughput: 1324.17\n",
      "    learn_time_ms: 755.19\n",
      "    load_throughput: 41220.284\n",
      "    load_time_ms: 24.26\n",
      "    sample_throughput: 42.011\n",
      "    sample_time_ms: 23803.122\n",
      "    update_time_ms: 3.664\n",
      "  timestamp: 1635287971\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 224\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   224</td><td style=\"text-align: right;\">          5395.6</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\"> -3.4468</td><td style=\"text-align: right;\">               -0.18</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            337.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 225000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-39-53\n",
      "  done: false\n",
      "  episode_len_mean: 338.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.4500000000000026\n",
      "  episode_reward_mean: -3.4552999999999723\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 632\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048706054688\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9279865529802112\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014843102293075934\n",
      "          policy_loss: -0.0748923088527388\n",
      "          total_loss: -0.04322464296387302\n",
      "          vf_explained_var: 0.4696248769760132\n",
      "          vf_loss: 0.04693536836033066\n",
      "    num_agent_steps_sampled: 225000\n",
      "    num_agent_steps_trained: 225000\n",
      "    num_steps_sampled: 225000\n",
      "    num_steps_trained: 225000\n",
      "  iterations_since_restore: 225\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.56129032258065\n",
      "    ram_util_percent: 31.183870967741946\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674053344455222\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.09673430820583\n",
      "    mean_inference_ms: 2.115040436102693\n",
      "    mean_raw_obs_processing_ms: 1.9603311958255625\n",
      "  time_since_restore: 5416.759079456329\n",
      "  time_this_iter_s: 21.15603542327881\n",
      "  time_total_s: 5416.759079456329\n",
      "  timers:\n",
      "    learn_throughput: 1323.225\n",
      "    learn_time_ms: 755.729\n",
      "    load_throughput: 41397.585\n",
      "    load_time_ms: 24.156\n",
      "    sample_throughput: 42.247\n",
      "    sample_time_ms: 23670.21\n",
      "    update_time_ms: 3.675\n",
      "  timestamp: 1635287993\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 225000\n",
      "  training_iteration: 225\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   225</td><td style=\"text-align: right;\">         5416.76</td><td style=\"text-align: right;\">225000</td><td style=\"text-align: right;\"> -3.4553</td><td style=\"text-align: right;\">               -1.45</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            338.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 226000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-40-15\n",
      "  done: false\n",
      "  episode_len_mean: 338.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.199999999999985\n",
      "  episode_reward_mean: -3.466899999999973\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 634\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048706054688\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.899841276804606\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011333741658115572\n",
      "          policy_loss: -0.11672495942976739\n",
      "          total_loss: -0.07001879453245137\n",
      "          vf_explained_var: 0.3532034158706665\n",
      "          vf_loss: 0.0626410121512082\n",
      "    num_agent_steps_sampled: 226000\n",
      "    num_agent_steps_trained: 226000\n",
      "    num_steps_sampled: 226000\n",
      "    num_steps_trained: 226000\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.66129032258063\n",
      "    ram_util_percent: 31.174193548387098\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674014481780601\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.09782691885495\n",
      "    mean_inference_ms: 2.1150192206899963\n",
      "    mean_raw_obs_processing_ms: 1.9595392812334629\n",
      "  time_since_restore: 5438.813255548477\n",
      "  time_this_iter_s: 22.054176092147827\n",
      "  time_total_s: 5438.813255548477\n",
      "  timers:\n",
      "    learn_throughput: 1324.465\n",
      "    learn_time_ms: 755.022\n",
      "    load_throughput: 41373.452\n",
      "    load_time_ms: 24.17\n",
      "    sample_throughput: 42.466\n",
      "    sample_time_ms: 23548.168\n",
      "    update_time_ms: 3.68\n",
      "  timestamp: 1635288015\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 226000\n",
      "  training_iteration: 226\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   226</td><td style=\"text-align: right;\">         5438.81</td><td style=\"text-align: right;\">226000</td><td style=\"text-align: right;\"> -3.4669</td><td style=\"text-align: right;\">                -2.2</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            338.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 227000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-40-37\n",
      "  done: false\n",
      "  episode_len_mean: 338.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.199999999999985\n",
      "  episode_reward_mean: -3.4777999999999722\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 637\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048706054688\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.883231516679128\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013760982272449181\n",
      "          policy_loss: -0.1041947594533364\n",
      "          total_loss: -0.09048558734357356\n",
      "          vf_explained_var: 0.7040407657623291\n",
      "          vf_loss: 0.02882182974782255\n",
      "    num_agent_steps_sampled: 227000\n",
      "    num_agent_steps_trained: 227000\n",
      "    num_steps_sampled: 227000\n",
      "    num_steps_trained: 227000\n",
      "  iterations_since_restore: 227\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.728125\n",
      "    ram_util_percent: 31.096875000000004\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036739293043230474\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.09929761413176\n",
      "    mean_inference_ms: 2.114974177769828\n",
      "    mean_raw_obs_processing_ms: 1.9583966891047302\n",
      "  time_since_restore: 5461.061591863632\n",
      "  time_this_iter_s: 22.24833631515503\n",
      "  time_total_s: 5461.061591863632\n",
      "  timers:\n",
      "    learn_throughput: 1323.68\n",
      "    learn_time_ms: 755.47\n",
      "    load_throughput: 41361.987\n",
      "    load_time_ms: 24.177\n",
      "    sample_throughput: 42.547\n",
      "    sample_time_ms: 23503.282\n",
      "    update_time_ms: 3.758\n",
      "  timestamp: 1635288037\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 227000\n",
      "  training_iteration: 227\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   227</td><td style=\"text-align: right;\">         5461.06</td><td style=\"text-align: right;\">227000</td><td style=\"text-align: right;\"> -3.4778</td><td style=\"text-align: right;\">                -2.2</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            338.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-41-13\n",
      "  done: false\n",
      "  episode_len_mean: 338.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.199999999999985\n",
      "  episode_reward_mean: -3.449599999999972\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 640\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048706054688\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9977255331145392\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.024578442273072633\n",
      "          policy_loss: 0.07152785174548626\n",
      "          total_loss: 0.08231454040441248\n",
      "          vf_explained_var: 0.44066542387008667\n",
      "          vf_loss: 0.024120271214956624\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.14230769230768\n",
      "    ram_util_percent: 31.03653846153846\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673845599469833\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.10050032752162\n",
      "    mean_inference_ms: 2.1149282969927787\n",
      "    mean_raw_obs_processing_ms: 1.9594967222141348\n",
      "  time_since_restore: 5497.495193719864\n",
      "  time_this_iter_s: 36.43360185623169\n",
      "  time_total_s: 5497.495193719864\n",
      "  timers:\n",
      "    learn_throughput: 1326.075\n",
      "    learn_time_ms: 754.105\n",
      "    load_throughput: 41898.627\n",
      "    load_time_ms: 23.867\n",
      "    sample_throughput: 43.239\n",
      "    sample_time_ms: 23127.154\n",
      "    update_time_ms: 3.845\n",
      "  timestamp: 1635288073\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 228\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   228</td><td style=\"text-align: right;\">          5497.5</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -3.4496</td><td style=\"text-align: right;\">                -2.2</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            338.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 229000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-41-37\n",
      "  done: false\n",
      "  episode_len_mean: 338.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.199999999999985\n",
      "  episode_reward_mean: -3.5127999999999724\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 643\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9479670166969298\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008876115895641324\n",
      "          policy_loss: -0.012914443098836475\n",
      "          total_loss: 0.08555543824202484\n",
      "          vf_explained_var: 0.3197985887527466\n",
      "          vf_loss: 0.11435066221488846\n",
      "    num_agent_steps_sampled: 229000\n",
      "    num_agent_steps_trained: 229000\n",
      "    num_steps_sampled: 229000\n",
      "    num_steps_trained: 229000\n",
      "  iterations_since_restore: 229\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.05882352941178\n",
      "    ram_util_percent: 31.573529411764714\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673755295423636\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.101694609967968\n",
      "    mean_inference_ms: 2.114881333613969\n",
      "    mean_raw_obs_processing_ms: 1.960634144052191\n",
      "  time_since_restore: 5521.054846763611\n",
      "  time_this_iter_s: 23.55965304374695\n",
      "  time_total_s: 5521.054846763611\n",
      "  timers:\n",
      "    learn_throughput: 1325.7\n",
      "    learn_time_ms: 754.319\n",
      "    load_throughput: 42385.031\n",
      "    load_time_ms: 23.593\n",
      "    sample_throughput: 42.793\n",
      "    sample_time_ms: 23368.042\n",
      "    update_time_ms: 3.842\n",
      "  timestamp: 1635288097\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 229000\n",
      "  training_iteration: 229\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   229</td><td style=\"text-align: right;\">         5521.05</td><td style=\"text-align: right;\">229000</td><td style=\"text-align: right;\"> -3.5128</td><td style=\"text-align: right;\">                -2.2</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            338.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 230000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-41-59\n",
      "  done: false\n",
      "  episode_len_mean: 339.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.199999999999985\n",
      "  episode_reward_mean: -3.523599999999972\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 646\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.035753643512726\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008894434671686016\n",
      "          policy_loss: 0.07952289026644495\n",
      "          total_loss: 0.10574816134240893\n",
      "          vf_explained_var: 0.26480183005332947\n",
      "          vf_loss: 0.0429764933573703\n",
      "    num_agent_steps_sampled: 230000\n",
      "    num_agent_steps_trained: 230000\n",
      "    num_steps_sampled: 230000\n",
      "    num_steps_trained: 230000\n",
      "  iterations_since_restore: 230\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.81935483870969\n",
      "    ram_util_percent: 31.293548387096774\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673648172624615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.102695497391313\n",
      "    mean_inference_ms: 2.1148292510869253\n",
      "    mean_raw_obs_processing_ms: 1.96189647230278\n",
      "  time_since_restore: 5542.900686740875\n",
      "  time_this_iter_s: 21.845839977264404\n",
      "  time_total_s: 5542.900686740875\n",
      "  timers:\n",
      "    learn_throughput: 1328.971\n",
      "    learn_time_ms: 752.462\n",
      "    load_throughput: 42166.438\n",
      "    load_time_ms: 23.716\n",
      "    sample_throughput: 43.141\n",
      "    sample_time_ms: 23180.032\n",
      "    update_time_ms: 3.721\n",
      "  timestamp: 1635288119\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 230000\n",
      "  training_iteration: 230\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   230</td><td style=\"text-align: right;\">          5542.9</td><td style=\"text-align: right;\">230000</td><td style=\"text-align: right;\"> -3.5236</td><td style=\"text-align: right;\">                -2.2</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            339.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 231000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-42-19\n",
      "  done: false\n",
      "  episode_len_mean: 338.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.199999999999985\n",
      "  episode_reward_mean: -3.5400999999999727\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 648\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8303581568929883\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008790738995752929\n",
      "          policy_loss: -0.078284507110301\n",
      "          total_loss: -0.05286247158009145\n",
      "          vf_explained_var: 0.4416642189025879\n",
      "          vf_loss: 0.040161346043977475\n",
      "    num_agent_steps_sampled: 231000\n",
      "    num_agent_steps_trained: 231000\n",
      "    num_steps_sampled: 231000\n",
      "    num_steps_trained: 231000\n",
      "  iterations_since_restore: 231\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.7448275862069\n",
      "    ram_util_percent: 31.117241379310347\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036735680151622825\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.103252323520195\n",
      "    mean_inference_ms: 2.1147913963812393\n",
      "    mean_raw_obs_processing_ms: 1.9626989689022434\n",
      "  time_since_restore: 5563.358732700348\n",
      "  time_this_iter_s: 20.458045959472656\n",
      "  time_total_s: 5563.358732700348\n",
      "  timers:\n",
      "    learn_throughput: 1329.059\n",
      "    learn_time_ms: 752.412\n",
      "    load_throughput: 42129.886\n",
      "    load_time_ms: 23.736\n",
      "    sample_throughput: 43.8\n",
      "    sample_time_ms: 22830.985\n",
      "    update_time_ms: 3.724\n",
      "  timestamp: 1635288139\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 231000\n",
      "  training_iteration: 231\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   231</td><td style=\"text-align: right;\">         5563.36</td><td style=\"text-align: right;\">231000</td><td style=\"text-align: right;\"> -3.5401</td><td style=\"text-align: right;\">                -2.2</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            338.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-42-39\n",
      "  done: false\n",
      "  episode_len_mean: 340.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.849999999999978\n",
      "  episode_reward_mean: -3.5508999999999724\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 651\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.893789972199334\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015066755564951961\n",
      "          policy_loss: -0.0578368893927998\n",
      "          total_loss: -0.0034803330898284914\n",
      "          vf_explained_var: 0.16017389297485352\n",
      "          vf_loss: 0.06718552800723249\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "  iterations_since_restore: 232\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.56551724137931\n",
      "    ram_util_percent: 31.124137931034486\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673446924512454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.103653910520194\n",
      "    mean_inference_ms: 2.1147333257387775\n",
      "    mean_raw_obs_processing_ms: 1.9614218638491545\n",
      "  time_since_restore: 5583.5171575546265\n",
      "  time_this_iter_s: 20.158424854278564\n",
      "  time_total_s: 5583.5171575546265\n",
      "  timers:\n",
      "    learn_throughput: 1329.462\n",
      "    learn_time_ms: 752.184\n",
      "    load_throughput: 42155.377\n",
      "    load_time_ms: 23.722\n",
      "    sample_throughput: 44.296\n",
      "    sample_time_ms: 22575.405\n",
      "    update_time_ms: 3.725\n",
      "  timestamp: 1635288159\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 232\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   232</td><td style=\"text-align: right;\">         5583.52</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\"> -3.5509</td><td style=\"text-align: right;\">               -1.85</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            340.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 233000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-42-57\n",
      "  done: false\n",
      "  episode_len_mean: 342.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.849999999999978\n",
      "  episode_reward_mean: -3.577799999999972\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 653\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5944139745500352\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013888559555523313\n",
      "          policy_loss: 0.006040028731028239\n",
      "          total_loss: 0.03479443697465791\n",
      "          vf_explained_var: -0.0043597472831606865\n",
      "          vf_loss: 0.039067333581139486\n",
      "    num_agent_steps_sampled: 233000\n",
      "    num_agent_steps_trained: 233000\n",
      "    num_steps_sampled: 233000\n",
      "    num_steps_trained: 233000\n",
      "  iterations_since_restore: 233\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.73333333333333\n",
      "    ram_util_percent: 31.14166666666667\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036733661681850084\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.103558113068114\n",
      "    mean_inference_ms: 2.114694161597234\n",
      "    mean_raw_obs_processing_ms: 1.9605963211286213\n",
      "  time_since_restore: 5600.720546007156\n",
      "  time_this_iter_s: 17.203388452529907\n",
      "  time_total_s: 5600.720546007156\n",
      "  timers:\n",
      "    learn_throughput: 1329.787\n",
      "    learn_time_ms: 752.0\n",
      "    load_throughput: 44011.677\n",
      "    load_time_ms: 22.721\n",
      "    sample_throughput: 45.588\n",
      "    sample_time_ms: 21935.495\n",
      "    update_time_ms: 3.734\n",
      "  timestamp: 1635288177\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 233000\n",
      "  training_iteration: 233\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   233</td><td style=\"text-align: right;\">         5600.72</td><td style=\"text-align: right;\">233000</td><td style=\"text-align: right;\"> -3.5778</td><td style=\"text-align: right;\">               -1.85</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            342.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 234000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-43-16\n",
      "  done: false\n",
      "  episode_len_mean: 344.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.849999999999978\n",
      "  episode_reward_mean: -3.581099999999972\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 656\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8008521132998996\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014970227584410958\n",
      "          policy_loss: -0.07508861124515534\n",
      "          total_loss: 0.021968484307742783\n",
      "          vf_explained_var: 0.28931280970573425\n",
      "          vf_loss: 0.10899582744265596\n",
      "    num_agent_steps_sampled: 234000\n",
      "    num_agent_steps_trained: 234000\n",
      "    num_steps_sampled: 234000\n",
      "    num_steps_trained: 234000\n",
      "  iterations_since_restore: 234\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.62758620689657\n",
      "    ram_util_percent: 31.151724137931037\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673244562539353\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.10306881602047\n",
      "    mean_inference_ms: 2.114634374067281\n",
      "    mean_raw_obs_processing_ms: 1.9593131209808647\n",
      "  time_since_restore: 5620.422827720642\n",
      "  time_this_iter_s: 19.702281713485718\n",
      "  time_total_s: 5620.422827720642\n",
      "  timers:\n",
      "    learn_throughput: 1331.253\n",
      "    learn_time_ms: 751.172\n",
      "    load_throughput: 44181.961\n",
      "    load_time_ms: 22.634\n",
      "    sample_throughput: 46.082\n",
      "    sample_time_ms: 21700.23\n",
      "    update_time_ms: 3.739\n",
      "  timestamp: 1635288196\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 234000\n",
      "  training_iteration: 234\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   234</td><td style=\"text-align: right;\">         5620.42</td><td style=\"text-align: right;\">234000</td><td style=\"text-align: right;\"> -3.5811</td><td style=\"text-align: right;\">               -1.85</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            344.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 235000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-43-35\n",
      "  done: false\n",
      "  episode_len_mean: 345.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.849999999999978\n",
      "  episode_reward_mean: -3.5906999999999716\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 658\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.855696631802453\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015119419447556827\n",
      "          policy_loss: -0.025825649251540503\n",
      "          total_loss: 0.01495686024427414\n",
      "          vf_explained_var: 0.5931426286697388\n",
      "          vf_loss: 0.053209202809052336\n",
      "    num_agent_steps_sampled: 235000\n",
      "    num_agent_steps_trained: 235000\n",
      "    num_steps_sampled: 235000\n",
      "    num_steps_trained: 235000\n",
      "  iterations_since_restore: 235\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.53703703703705\n",
      "    ram_util_percent: 31.15925925925926\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036731622975454875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.102469216731432\n",
      "    mean_inference_ms: 2.1145939888109813\n",
      "    mean_raw_obs_processing_ms: 1.9584550841667732\n",
      "  time_since_restore: 5639.471528530121\n",
      "  time_this_iter_s: 19.04870080947876\n",
      "  time_total_s: 5639.471528530121\n",
      "  timers:\n",
      "    learn_throughput: 1331.542\n",
      "    learn_time_ms: 751.009\n",
      "    load_throughput: 44239.7\n",
      "    load_time_ms: 22.604\n",
      "    sample_throughput: 46.534\n",
      "    sample_time_ms: 21489.701\n",
      "    update_time_ms: 3.73\n",
      "  timestamp: 1635288215\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 235000\n",
      "  training_iteration: 235\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   235</td><td style=\"text-align: right;\">         5639.47</td><td style=\"text-align: right;\">235000</td><td style=\"text-align: right;\"> -3.5907</td><td style=\"text-align: right;\">               -1.85</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            345.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-44-36\n",
      "  done: false\n",
      "  episode_len_mean: 345.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.552599999999971\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 661\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7938179095586142\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01852198924015276\n",
      "          policy_loss: -0.07435986383093728\n",
      "          total_loss: 0.007390204403135512\n",
      "          vf_explained_var: 0.3033483028411865\n",
      "          vf_loss: 0.09217837183839744\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_agent_steps_trained: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "  iterations_since_restore: 236\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.689534883720945\n",
      "    ram_util_percent: 31.23837209302325\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673037855798402\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.10118462696959\n",
      "    mean_inference_ms: 2.114533353887087\n",
      "    mean_raw_obs_processing_ms: 1.9624367049094054\n",
      "  time_since_restore: 5700.150351285934\n",
      "  time_this_iter_s: 60.6788227558136\n",
      "  time_total_s: 5700.150351285934\n",
      "  timers:\n",
      "    learn_throughput: 1332.976\n",
      "    learn_time_ms: 750.201\n",
      "    load_throughput: 43980.895\n",
      "    load_time_ms: 22.737\n",
      "    sample_throughput: 39.443\n",
      "    sample_time_ms: 25352.827\n",
      "    update_time_ms: 3.739\n",
      "  timestamp: 1635288276\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 236\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   236</td><td style=\"text-align: right;\">         5700.15</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\"> -3.5526</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            345.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 237000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-44-54\n",
      "  done: false\n",
      "  episode_len_mean: 347.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.5903999999999705\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 663\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9026661025153266\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01262270338643224\n",
      "          policy_loss: -0.016675270017650394\n",
      "          total_loss: 0.05781119283702638\n",
      "          vf_explained_var: 0.4044830799102783\n",
      "          vf_loss: 0.08839515590419372\n",
      "    num_agent_steps_sampled: 237000\n",
      "    num_agent_steps_trained: 237000\n",
      "    num_steps_sampled: 237000\n",
      "    num_steps_trained: 237000\n",
      "  iterations_since_restore: 237\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.79615384615384\n",
      "    ram_util_percent: 31.646153846153847\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672953900868024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.09995539718192\n",
      "    mean_inference_ms: 2.1144918548194958\n",
      "    mean_raw_obs_processing_ms: 1.9650453618986734\n",
      "  time_since_restore: 5718.012275934219\n",
      "  time_this_iter_s: 17.861924648284912\n",
      "  time_total_s: 5718.012275934219\n",
      "  timers:\n",
      "    learn_throughput: 1335.104\n",
      "    learn_time_ms: 749.005\n",
      "    load_throughput: 45373.602\n",
      "    load_time_ms: 22.039\n",
      "    sample_throughput: 40.135\n",
      "    sample_time_ms: 24916.153\n",
      "    update_time_ms: 3.669\n",
      "  timestamp: 1635288294\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 237000\n",
      "  training_iteration: 237\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   237</td><td style=\"text-align: right;\">         5718.01</td><td style=\"text-align: right;\">237000</td><td style=\"text-align: right;\"> -3.5904</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            347.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 238000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-45-13\n",
      "  done: false\n",
      "  episode_len_mean: 349.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.6075999999999704\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 665\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8114957796202766\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00701565209127389\n",
      "          policy_loss: -0.05926168113946915\n",
      "          total_loss: -0.06172801098889775\n",
      "          vf_explained_var: 0.38781118392944336\n",
      "          vf_loss: 0.012804080188895265\n",
      "    num_agent_steps_sampled: 238000\n",
      "    num_agent_steps_trained: 238000\n",
      "    num_steps_sampled: 238000\n",
      "    num_steps_trained: 238000\n",
      "  iterations_since_restore: 238\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.59615384615384\n",
      "    ram_util_percent: 31.269230769230766\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672870570343092\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.098371000531436\n",
      "    mean_inference_ms: 2.114450660335482\n",
      "    mean_raw_obs_processing_ms: 1.9676651629528221\n",
      "  time_since_restore: 5736.43630194664\n",
      "  time_this_iter_s: 18.424026012420654\n",
      "  time_total_s: 5736.43630194664\n",
      "  timers:\n",
      "    learn_throughput: 1336.277\n",
      "    learn_time_ms: 748.348\n",
      "    load_throughput: 44676.576\n",
      "    load_time_ms: 22.383\n",
      "    sample_throughput: 43.261\n",
      "    sample_time_ms: 23115.591\n",
      "    update_time_ms: 3.587\n",
      "  timestamp: 1635288313\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 238000\n",
      "  training_iteration: 238\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   238</td><td style=\"text-align: right;\">         5736.44</td><td style=\"text-align: right;\">238000</td><td style=\"text-align: right;\"> -3.6076</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            349.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 239000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-45-30\n",
      "  done: false\n",
      "  episode_len_mean: 352.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.64229999999997\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 668\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0215247366163465\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008950883002519896\n",
      "          policy_loss: -0.05788168762293127\n",
      "          total_loss: 0.016433541932039792\n",
      "          vf_explained_var: 0.23435591161251068\n",
      "          vf_loss: 0.09090127493772242\n",
      "    num_agent_steps_sampled: 239000\n",
      "    num_agent_steps_trained: 239000\n",
      "    num_steps_sampled: 239000\n",
      "    num_steps_trained: 239000\n",
      "  iterations_since_restore: 239\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.5\n",
      "    ram_util_percent: 31.208\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672742339723892\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.095360149808865\n",
      "    mean_inference_ms: 2.1143875128769967\n",
      "    mean_raw_obs_processing_ms: 1.9715307189802982\n",
      "  time_since_restore: 5754.002090930939\n",
      "  time_this_iter_s: 17.565788984298706\n",
      "  time_total_s: 5754.002090930939\n",
      "  timers:\n",
      "    learn_throughput: 1335.94\n",
      "    learn_time_ms: 748.536\n",
      "    load_throughput: 45667.598\n",
      "    load_time_ms: 21.897\n",
      "    sample_throughput: 44.412\n",
      "    sample_time_ms: 22516.514\n",
      "    update_time_ms: 3.59\n",
      "  timestamp: 1635288330\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 239000\n",
      "  training_iteration: 239\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   239</td><td style=\"text-align: right;\">            5754</td><td style=\"text-align: right;\">239000</td><td style=\"text-align: right;\"> -3.6423</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            352.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-45-48\n",
      "  done: false\n",
      "  episode_len_mean: 355.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.6571999999999694\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 670\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.970040085580614\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012209607921028681\n",
      "          policy_loss: -0.034501176327466965\n",
      "          total_loss: 0.021816291991207333\n",
      "          vf_explained_var: 0.26587730646133423\n",
      "          vf_loss: 0.07106739067369038\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 240\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.59615384615384\n",
      "    ram_util_percent: 31.20384615384615\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036726566103475446\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.092943679229606\n",
      "    mean_inference_ms: 2.1143443342753163\n",
      "    mean_raw_obs_processing_ms: 1.9740914850547457\n",
      "  time_since_restore: 5771.68665766716\n",
      "  time_this_iter_s: 17.684566736221313\n",
      "  time_total_s: 5771.68665766716\n",
      "  timers:\n",
      "    learn_throughput: 1337.959\n",
      "    learn_time_ms: 747.407\n",
      "    load_throughput: 47980.471\n",
      "    load_time_ms: 20.842\n",
      "    sample_throughput: 45.244\n",
      "    sample_time_ms: 22102.622\n",
      "    update_time_ms: 3.551\n",
      "  timestamp: 1635288348\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 240\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   240</td><td style=\"text-align: right;\">         5771.69</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\"> -3.6572</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            355.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 241000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-46-05\n",
      "  done: false\n",
      "  episode_len_mean: 358.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.682199999999969\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 672\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.937909444173177\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009266464388253315\n",
      "          policy_loss: -0.027876437620984185\n",
      "          total_loss: -0.004898440796467993\n",
      "          vf_explained_var: 0.1942243129014969\n",
      "          vf_loss: 0.03859993639505572\n",
      "    num_agent_steps_sampled: 241000\n",
      "    num_agent_steps_trained: 241000\n",
      "    num_steps_sampled: 241000\n",
      "    num_steps_trained: 241000\n",
      "  iterations_since_restore: 241\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.51666666666667\n",
      "    ram_util_percent: 31.2\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367257080146463\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.090101981806914\n",
      "    mean_inference_ms: 2.114300049241058\n",
      "    mean_raw_obs_processing_ms: 1.9765798921045894\n",
      "  time_since_restore: 5788.920197486877\n",
      "  time_this_iter_s: 17.233539819717407\n",
      "  time_total_s: 5788.920197486877\n",
      "  timers:\n",
      "    learn_throughput: 1337.355\n",
      "    learn_time_ms: 747.745\n",
      "    load_throughput: 49696.899\n",
      "    load_time_ms: 20.122\n",
      "    sample_throughput: 45.913\n",
      "    sample_time_ms: 21780.552\n",
      "    update_time_ms: 3.552\n",
      "  timestamp: 1635288365\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 241000\n",
      "  training_iteration: 241\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   241</td><td style=\"text-align: right;\">         5788.92</td><td style=\"text-align: right;\">241000</td><td style=\"text-align: right;\"> -3.6822</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            358.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 242000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-46-27\n",
      "  done: false\n",
      "  episode_len_mean: 358.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.688099999999969\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 675\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8410458445549012\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009917044729784935\n",
      "          policy_loss: -0.003293954332669576\n",
      "          total_loss: -0.0073127454353703394\n",
      "          vf_explained_var: 0.7656556963920593\n",
      "          vf_loss: 0.01037072787552865\n",
      "    num_agent_steps_sampled: 242000\n",
      "    num_agent_steps_trained: 242000\n",
      "    num_steps_sampled: 242000\n",
      "    num_steps_trained: 242000\n",
      "  iterations_since_restore: 242\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.58709677419355\n",
      "    ram_util_percent: 31.293548387096767\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672442630843158\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.08567541463322\n",
      "    mean_inference_ms: 2.114232031215873\n",
      "    mean_raw_obs_processing_ms: 1.9803344236837819\n",
      "  time_since_restore: 5810.456981182098\n",
      "  time_this_iter_s: 21.536783695220947\n",
      "  time_total_s: 5810.456981182098\n",
      "  timers:\n",
      "    learn_throughput: 1337.445\n",
      "    learn_time_ms: 747.694\n",
      "    load_throughput: 49383.214\n",
      "    load_time_ms: 20.25\n",
      "    sample_throughput: 45.624\n",
      "    sample_time_ms: 21918.311\n",
      "    update_time_ms: 3.553\n",
      "  timestamp: 1635288387\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 242000\n",
      "  training_iteration: 242\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   242</td><td style=\"text-align: right;\">         5810.46</td><td style=\"text-align: right;\">242000</td><td style=\"text-align: right;\"> -3.6881</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            358.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 243000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-46-45\n",
      "  done: false\n",
      "  episode_len_mean: 360.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.6835999999999682\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 677\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8078768836127388\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010272192665223044\n",
      "          policy_loss: -0.08172532220681508\n",
      "          total_loss: 0.006545289419591427\n",
      "          vf_explained_var: 0.4513828754425049\n",
      "          vf_loss: 0.10218444439686007\n",
      "    num_agent_steps_sampled: 243000\n",
      "    num_agent_steps_trained: 243000\n",
      "    num_steps_sampled: 243000\n",
      "    num_steps_trained: 243000\n",
      "  iterations_since_restore: 243\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.95384615384616\n",
      "    ram_util_percent: 31.396153846153844\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672361076876517\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.082417828871794\n",
      "    mean_inference_ms: 2.114188377065298\n",
      "    mean_raw_obs_processing_ms: 1.982849639310038\n",
      "  time_since_restore: 5829.004685163498\n",
      "  time_this_iter_s: 18.547703981399536\n",
      "  time_total_s: 5829.004685163498\n",
      "  timers:\n",
      "    learn_throughput: 1337.166\n",
      "    learn_time_ms: 747.851\n",
      "    load_throughput: 47028.357\n",
      "    load_time_ms: 21.264\n",
      "    sample_throughput: 45.348\n",
      "    sample_time_ms: 22051.592\n",
      "    update_time_ms: 3.543\n",
      "  timestamp: 1635288405\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 243000\n",
      "  training_iteration: 243\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   243</td><td style=\"text-align: right;\">            5829</td><td style=\"text-align: right;\">243000</td><td style=\"text-align: right;\"> -3.6836</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            360.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-47-05\n",
      "  done: false\n",
      "  episode_len_mean: 362.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.6842999999999675\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 680\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9069171918763055\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02162707065403146\n",
      "          policy_loss: -0.11482556571977007\n",
      "          total_loss: 0.0290543626062572\n",
      "          vf_explained_var: 0.6183449029922485\n",
      "          vf_loss: 0.15418024179525674\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_agent_steps_trained: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "  iterations_since_restore: 244\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.07931034482759\n",
      "    ram_util_percent: 31.38965517241379\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672240794895745\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.077127927325936\n",
      "    mean_inference_ms: 2.11412367403488\n",
      "    mean_raw_obs_processing_ms: 1.9849060858933691\n",
      "  time_since_restore: 5848.81333398819\n",
      "  time_this_iter_s: 19.808648824691772\n",
      "  time_total_s: 5848.81333398819\n",
      "  timers:\n",
      "    learn_throughput: 1332.857\n",
      "    learn_time_ms: 750.268\n",
      "    load_throughput: 46146.781\n",
      "    load_time_ms: 21.67\n",
      "    sample_throughput: 45.332\n",
      "    sample_time_ms: 22059.36\n",
      "    update_time_ms: 3.592\n",
      "  timestamp: 1635288425\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 244\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   244</td><td style=\"text-align: right;\">         5848.81</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\"> -3.6843</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            362.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 245000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-47-25\n",
      "  done: false\n",
      "  episode_len_mean: 364.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.660899999999968\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 682\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.089949515130785\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009874281095368096\n",
      "          policy_loss: -0.1280183591776424\n",
      "          total_loss: -0.08721468026439348\n",
      "          vf_explained_var: 0.374420702457428\n",
      "          vf_loss: 0.05569776897836062\n",
      "    num_agent_steps_sampled: 245000\n",
      "    num_agent_steps_trained: 245000\n",
      "    num_steps_sampled: 245000\n",
      "    num_steps_trained: 245000\n",
      "  iterations_since_restore: 245\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.96896551724137\n",
      "    ram_util_percent: 31.468965517241376\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672166234547824\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.073375116671727\n",
      "    mean_inference_ms: 2.1140834146169167\n",
      "    mean_raw_obs_processing_ms: 1.9857139655565894\n",
      "  time_since_restore: 5869.129363298416\n",
      "  time_this_iter_s: 20.31602931022644\n",
      "  time_total_s: 5869.129363298416\n",
      "  timers:\n",
      "    learn_throughput: 1330.693\n",
      "    learn_time_ms: 751.488\n",
      "    load_throughput: 46413.803\n",
      "    load_time_ms: 21.545\n",
      "    sample_throughput: 45.076\n",
      "    sample_time_ms: 22184.956\n",
      "    update_time_ms: 3.641\n",
      "  timestamp: 1635288445\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 245000\n",
      "  training_iteration: 245\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   245</td><td style=\"text-align: right;\">         5869.13</td><td style=\"text-align: right;\">245000</td><td style=\"text-align: right;\"> -3.6609</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            364.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 246000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-47-44\n",
      "  done: false\n",
      "  episode_len_mean: 367.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.688499999999968\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 685\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1094284150335523\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010876534560299007\n",
      "          policy_loss: 0.01648150355451637\n",
      "          total_loss: 0.020132199219531483\n",
      "          vf_explained_var: 0.37188538908958435\n",
      "          vf_loss: 0.0181300260230071\n",
      "    num_agent_steps_sampled: 246000\n",
      "    num_agent_steps_trained: 246000\n",
      "    num_steps_sampled: 246000\n",
      "    num_steps_trained: 246000\n",
      "  iterations_since_restore: 246\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.51153846153846\n",
      "    ram_util_percent: 31.388461538461534\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036720556540419794\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.06715472312455\n",
      "    mean_inference_ms: 2.114023182820323\n",
      "    mean_raw_obs_processing_ms: 1.9869164440757918\n",
      "  time_since_restore: 5887.569487333298\n",
      "  time_this_iter_s: 18.440124034881592\n",
      "  time_total_s: 5887.569487333298\n",
      "  timers:\n",
      "    learn_throughput: 1324.415\n",
      "    learn_time_ms: 755.05\n",
      "    load_throughput: 47381.434\n",
      "    load_time_ms: 21.105\n",
      "    sample_throughput: 55.686\n",
      "    sample_time_ms: 17957.982\n",
      "    update_time_ms: 3.635\n",
      "  timestamp: 1635288464\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 246000\n",
      "  training_iteration: 246\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   246</td><td style=\"text-align: right;\">         5887.57</td><td style=\"text-align: right;\">246000</td><td style=\"text-align: right;\"> -3.6885</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            367.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 247000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-48-02\n",
      "  done: false\n",
      "  episode_len_mean: 370.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.7281999999999673\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 687\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8958124412430657\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010902524351551562\n",
      "          policy_loss: 0.032059089342753096\n",
      "          total_loss: 0.17802880006945795\n",
      "          vf_explained_var: 0.4830828011035919\n",
      "          vf_loss: 0.15829706920517816\n",
      "    num_agent_steps_sampled: 247000\n",
      "    num_agent_steps_trained: 247000\n",
      "    num_steps_sampled: 247000\n",
      "    num_steps_trained: 247000\n",
      "  iterations_since_restore: 247\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.99230769230769\n",
      "    ram_util_percent: 31.39230769230769\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03671980967126464\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.06255297993042\n",
      "    mean_inference_ms: 2.1139821117217146\n",
      "    mean_raw_obs_processing_ms: 1.9876836346492615\n",
      "  time_since_restore: 5905.355558633804\n",
      "  time_this_iter_s: 17.786071300506592\n",
      "  time_total_s: 5905.355558633804\n",
      "  timers:\n",
      "    learn_throughput: 1321.957\n",
      "    learn_time_ms: 756.454\n",
      "    load_throughput: 46014.343\n",
      "    load_time_ms: 21.732\n",
      "    sample_throughput: 55.715\n",
      "    sample_time_ms: 17948.352\n",
      "    update_time_ms: 3.624\n",
      "  timestamp: 1635288482\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 247000\n",
      "  training_iteration: 247\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   247</td><td style=\"text-align: right;\">         5905.36</td><td style=\"text-align: right;\">247000</td><td style=\"text-align: right;\"> -3.7282</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            370.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-48-20\n",
      "  done: false\n",
      "  episode_len_mean: 372.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.753799999999967\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 689\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.01229987276925\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013248487601995046\n",
      "          policy_loss: -0.0204683985768093\n",
      "          total_loss: 0.033724678080115054\n",
      "          vf_explained_var: 0.5422191023826599\n",
      "          vf_loss: 0.06625853049465352\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "  iterations_since_restore: 248\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.83076923076923\n",
      "    ram_util_percent: 31.338461538461537\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03671909511136196\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.057774009100783\n",
      "    mean_inference_ms: 2.1139427203765155\n",
      "    mean_raw_obs_processing_ms: 1.9884710611772567\n",
      "  time_since_restore: 5924.173134326935\n",
      "  time_this_iter_s: 18.817575693130493\n",
      "  time_total_s: 5924.173134326935\n",
      "  timers:\n",
      "    learn_throughput: 1318.685\n",
      "    learn_time_ms: 758.331\n",
      "    load_throughput: 46082.846\n",
      "    load_time_ms: 21.7\n",
      "    sample_throughput: 55.599\n",
      "    sample_time_ms: 17985.877\n",
      "    update_time_ms: 3.619\n",
      "  timestamp: 1635288500\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 248\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   248</td><td style=\"text-align: right;\">         5924.17</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\"> -3.7538</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            372.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 249000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-48-58\n",
      "  done: false\n",
      "  episode_len_mean: 373.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.7596999999999663\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 691\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8642124149534438\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008924068280047947\n",
      "          policy_loss: -0.12895106942289405\n",
      "          total_loss: -0.09348558224737644\n",
      "          vf_explained_var: 0.6128149032592773\n",
      "          vf_loss: 0.04868011558428407\n",
      "    num_agent_steps_sampled: 249000\n",
      "    num_agent_steps_trained: 249000\n",
      "    num_steps_sampled: 249000\n",
      "    num_steps_trained: 249000\n",
      "  iterations_since_restore: 249\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.53703703703702\n",
      "    ram_util_percent: 31.412962962962958\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036718374568826866\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.05287994459923\n",
      "    mean_inference_ms: 2.113902846552888\n",
      "    mean_raw_obs_processing_ms: 1.9905461320296047\n",
      "  time_since_restore: 5961.54486823082\n",
      "  time_this_iter_s: 37.37173390388489\n",
      "  time_total_s: 5961.54486823082\n",
      "  timers:\n",
      "    learn_throughput: 1319.43\n",
      "    learn_time_ms: 757.903\n",
      "    load_throughput: 44420.599\n",
      "    load_time_ms: 22.512\n",
      "    sample_throughput: 50.085\n",
      "    sample_time_ms: 19965.982\n",
      "    update_time_ms: 3.661\n",
      "  timestamp: 1635288538\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 249000\n",
      "  training_iteration: 249\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   249</td><td style=\"text-align: right;\">         5961.54</td><td style=\"text-align: right;\">249000</td><td style=\"text-align: right;\"> -3.7597</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            373.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 250000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-49-59\n",
      "  done: false\n",
      "  episode_len_mean: 374.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.687499999999966\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 694\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9541585021548802\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02429959009591243\n",
      "          policy_loss: -0.12467897459864616\n",
      "          total_loss: 0.02547127016716533\n",
      "          vf_explained_var: 0.37317797541618347\n",
      "          vf_loss: 0.15491316169500352\n",
      "    num_agent_steps_sampled: 250000\n",
      "    num_agent_steps_trained: 250000\n",
      "    num_steps_sampled: 250000\n",
      "    num_steps_trained: 250000\n",
      "  iterations_since_restore: 250\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.079310344827604\n",
      "    ram_util_percent: 31.80114942528736\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03671729329356608\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.045030859409692\n",
      "    mean_inference_ms: 2.1138414245297166\n",
      "    mean_raw_obs_processing_ms: 1.9988141033417746\n",
      "  time_since_restore: 6022.4031291008\n",
      "  time_this_iter_s: 60.85826086997986\n",
      "  time_total_s: 6022.4031291008\n",
      "  timers:\n",
      "    learn_throughput: 1317.785\n",
      "    learn_time_ms: 758.849\n",
      "    load_throughput: 42532.714\n",
      "    load_time_ms: 23.511\n",
      "    sample_throughput: 41.184\n",
      "    sample_time_ms: 24281.33\n",
      "    update_time_ms: 3.735\n",
      "  timestamp: 1635288599\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 250000\n",
      "  training_iteration: 250\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   250</td><td style=\"text-align: right;\">          6022.4</td><td style=\"text-align: right;\">250000</td><td style=\"text-align: right;\"> -3.6875</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            374.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 251000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-50-18\n",
      "  done: false\n",
      "  episode_len_mean: 376.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.7069999999999665\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 697\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9122789382934571\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8123698936568367\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007469793183118867\n",
      "          policy_loss: 0.05636573632558187\n",
      "          total_loss: 0.06500961755712827\n",
      "          vf_explained_var: 0.5512741804122925\n",
      "          vf_loss: 0.019953043262163798\n",
      "    num_agent_steps_sampled: 251000\n",
      "    num_agent_steps_trained: 251000\n",
      "    num_steps_sampled: 251000\n",
      "    num_steps_trained: 251000\n",
      "  iterations_since_restore: 251\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.93333333333334\n",
      "    ram_util_percent: 31.944444444444443\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036716252824215936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.036916139779624\n",
      "    mean_inference_ms: 2.113783594304797\n",
      "    mean_raw_obs_processing_ms: 2.007088961584058\n",
      "  time_since_restore: 6041.759988069534\n",
      "  time_this_iter_s: 19.35685896873474\n",
      "  time_total_s: 6041.759988069534\n",
      "  timers:\n",
      "    learn_throughput: 1316.062\n",
      "    learn_time_ms: 759.843\n",
      "    load_throughput: 41399.874\n",
      "    load_time_ms: 24.155\n",
      "    sample_throughput: 40.83\n",
      "    sample_time_ms: 24492.026\n",
      "    update_time_ms: 3.731\n",
      "  timestamp: 1635288618\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 251000\n",
      "  training_iteration: 251\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   251</td><td style=\"text-align: right;\">         6041.76</td><td style=\"text-align: right;\">251000</td><td style=\"text-align: right;\">  -3.707</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            376.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-50-35\n",
      "  done: false\n",
      "  episode_len_mean: 378.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.7442999999999658\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 699\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9122789382934571\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9543842448128594\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014370910585459602\n",
      "          policy_loss: -0.0012163687083456252\n",
      "          total_loss: 0.14266624657644167\n",
      "          vf_explained_var: 0.4830244183540344\n",
      "          vf_loss: 0.15031618169612354\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_agent_steps_trained: 252000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "  iterations_since_restore: 252\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.856\n",
      "    ram_util_percent: 31.363999999999997\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03671557047493843\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.031199251447077\n",
      "    mean_inference_ms: 2.1137455285792073\n",
      "    mean_raw_obs_processing_ms: 2.01263494821419\n",
      "  time_since_restore: 6059.00524353981\n",
      "  time_this_iter_s: 17.24525547027588\n",
      "  time_total_s: 6059.00524353981\n",
      "  timers:\n",
      "    learn_throughput: 1315.956\n",
      "    learn_time_ms: 759.904\n",
      "    load_throughput: 43310.437\n",
      "    load_time_ms: 23.089\n",
      "    sample_throughput: 41.556\n",
      "    sample_time_ms: 24063.805\n",
      "    update_time_ms: 3.805\n",
      "  timestamp: 1635288635\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 252\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   252</td><td style=\"text-align: right;\">         6059.01</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\"> -3.7443</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            378.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 253000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-50-55\n",
      "  done: false\n",
      "  episode_len_mean: 379.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.7739999999999663\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 701\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9122789382934571\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8602267543474833\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007891239731276818\n",
      "          policy_loss: -0.08838021068109406\n",
      "          total_loss: -0.009950821474194526\n",
      "          vf_explained_var: 0.49021753668785095\n",
      "          vf_loss: 0.08983264503379663\n",
      "    num_agent_steps_sampled: 253000\n",
      "    num_agent_steps_trained: 253000\n",
      "    num_steps_sampled: 253000\n",
      "    num_steps_trained: 253000\n",
      "  iterations_since_restore: 253\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.91785714285716\n",
      "    ram_util_percent: 31.16071428571429\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03671487805320167\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.02528985089938\n",
      "    mean_inference_ms: 2.1137069151032755\n",
      "    mean_raw_obs_processing_ms: 2.018101560314828\n",
      "  time_since_restore: 6078.660069465637\n",
      "  time_this_iter_s: 19.654825925827026\n",
      "  time_total_s: 6078.660069465637\n",
      "  timers:\n",
      "    learn_throughput: 1317.83\n",
      "    learn_time_ms: 758.823\n",
      "    load_throughput: 43442.681\n",
      "    load_time_ms: 23.019\n",
      "    sample_throughput: 41.364\n",
      "    sample_time_ms: 24175.641\n",
      "    update_time_ms: 3.802\n",
      "  timestamp: 1635288655\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 253000\n",
      "  training_iteration: 253\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   253</td><td style=\"text-align: right;\">         6078.66</td><td style=\"text-align: right;\">253000</td><td style=\"text-align: right;\">  -3.774</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            379.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 254000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-51-10\n",
      "  done: false\n",
      "  episode_len_mean: 383.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.838099999999966\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 703\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9122789382934571\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.425760895676083\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007025114400166347\n",
      "          policy_loss: 0.03910549332698186\n",
      "          total_loss: 0.05671714639498128\n",
      "          vf_explained_var: 0.22049586474895477\n",
      "          vf_loss: 0.02546039745470302\n",
      "    num_agent_steps_sampled: 254000\n",
      "    num_agent_steps_trained: 254000\n",
      "    num_steps_sampled: 254000\n",
      "    num_steps_trained: 254000\n",
      "  iterations_since_restore: 254\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.51363636363637\n",
      "    ram_util_percent: 31.254545454545465\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03671416631923427\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.018779512102437\n",
      "    mean_inference_ms: 2.113666920982157\n",
      "    mean_raw_obs_processing_ms: 2.02356367585561\n",
      "  time_since_restore: 6094.057904481888\n",
      "  time_this_iter_s: 15.39783501625061\n",
      "  time_total_s: 6094.057904481888\n",
      "  timers:\n",
      "    learn_throughput: 1321.057\n",
      "    learn_time_ms: 756.97\n",
      "    load_throughput: 46770.609\n",
      "    load_time_ms: 21.381\n",
      "    sample_throughput: 42.126\n",
      "    sample_time_ms: 23738.119\n",
      "    update_time_ms: 3.746\n",
      "  timestamp: 1635288670\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 254000\n",
      "  training_iteration: 254\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   254</td><td style=\"text-align: right;\">         6094.06</td><td style=\"text-align: right;\">254000</td><td style=\"text-align: right;\"> -3.8381</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            383.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 255000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-51-30\n",
      "  done: false\n",
      "  episode_len_mean: 386.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.864299999999965\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 706\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9122789382934571\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8441607793172201\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00969095361802018\n",
      "          policy_loss: 0.05074947261148029\n",
      "          total_loss: 0.06656337860557768\n",
      "          vf_explained_var: 0.41905632615089417\n",
      "          vf_loss: 0.025414659620987046\n",
      "    num_agent_steps_sampled: 255000\n",
      "    num_agent_steps_trained: 255000\n",
      "    num_steps_sampled: 255000\n",
      "    num_steps_trained: 255000\n",
      "  iterations_since_restore: 255\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.55714285714286\n",
      "    ram_util_percent: 31.214285714285715\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036713068274045114\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.00867583499575\n",
      "    mean_inference_ms: 2.113605238076041\n",
      "    mean_raw_obs_processing_ms: 2.031602691944717\n",
      "  time_since_restore: 6113.30158162117\n",
      "  time_this_iter_s: 19.243677139282227\n",
      "  time_total_s: 6113.30158162117\n",
      "  timers:\n",
      "    learn_throughput: 1321.777\n",
      "    learn_time_ms: 756.557\n",
      "    load_throughput: 46556.304\n",
      "    load_time_ms: 21.479\n",
      "    sample_throughput: 42.317\n",
      "    sample_time_ms: 23631.215\n",
      "    update_time_ms: 3.723\n",
      "  timestamp: 1635288690\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 255000\n",
      "  training_iteration: 255\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   255</td><td style=\"text-align: right;\">          6113.3</td><td style=\"text-align: right;\">255000</td><td style=\"text-align: right;\"> -3.8643</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            386.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-51-48\n",
      "  done: false\n",
      "  episode_len_mean: 388.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.885299999999964\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 708\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9122789382934571\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.885148576895396\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005972760169916608\n",
      "          policy_loss: 0.02638040797577964\n",
      "          total_loss: 0.02272107783291075\n",
      "          vf_explained_var: 0.5931234359741211\n",
      "          vf_loss: 0.00974333418222765\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "  iterations_since_restore: 256\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.268\n",
      "    ram_util_percent: 31.355999999999998\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03671237131776261\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.001537737792574\n",
      "    mean_inference_ms: 2.113565945950061\n",
      "    mean_raw_obs_processing_ms: 2.0362179092697046\n",
      "  time_since_restore: 6131.056265592575\n",
      "  time_this_iter_s: 17.75468397140503\n",
      "  time_total_s: 6131.056265592575\n",
      "  timers:\n",
      "    learn_throughput: 1326.918\n",
      "    learn_time_ms: 753.626\n",
      "    load_throughput: 45857.48\n",
      "    load_time_ms: 21.807\n",
      "    sample_throughput: 42.435\n",
      "    sample_time_ms: 23565.279\n",
      "    update_time_ms: 3.718\n",
      "  timestamp: 1635288708\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 256\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   256</td><td style=\"text-align: right;\">         6131.06</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\"> -3.8853</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            388.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 257000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-52-06\n",
      "  done: false\n",
      "  episode_len_mean: 390.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.9161999999999635\n",
      "  episode_reward_min: -8.779999999999964\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 710\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9122789382934571\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9614636937777201\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0035978840114192006\n",
      "          policy_loss: 0.098604522107376\n",
      "          total_loss: 0.1149626531948646\n",
      "          vf_explained_var: 0.4172840118408203\n",
      "          vf_loss: 0.032690493928061594\n",
      "    num_agent_steps_sampled: 257000\n",
      "    num_agent_steps_trained: 257000\n",
      "    num_steps_sampled: 257000\n",
      "    num_steps_trained: 257000\n",
      "  iterations_since_restore: 257\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.54615384615386\n",
      "    ram_util_percent: 31.388461538461534\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03671165307928737\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.994069553919633\n",
      "    mean_inference_ms: 2.1135253788130424\n",
      "    mean_raw_obs_processing_ms: 2.0399835345756236\n",
      "  time_since_restore: 6149.345054388046\n",
      "  time_this_iter_s: 18.28878879547119\n",
      "  time_total_s: 6149.345054388046\n",
      "  timers:\n",
      "    learn_throughput: 1327.34\n",
      "    learn_time_ms: 753.386\n",
      "    load_throughput: 45714.086\n",
      "    load_time_ms: 21.875\n",
      "    sample_throughput: 42.345\n",
      "    sample_time_ms: 23615.719\n",
      "    update_time_ms: 3.722\n",
      "  timestamp: 1635288726\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 257000\n",
      "  training_iteration: 257\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   257</td><td style=\"text-align: right;\">         6149.35</td><td style=\"text-align: right;\">257000</td><td style=\"text-align: right;\"> -3.9162</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.78</td><td style=\"text-align: right;\">            390.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 258000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-52-24\n",
      "  done: false\n",
      "  episode_len_mean: 392.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.9058999999999635\n",
      "  episode_reward_min: -8.329999999999952\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 712\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.910293632083469\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012614739617290946\n",
      "          policy_loss: -0.030181219345993465\n",
      "          total_loss: 0.18634410372210874\n",
      "          vf_explained_var: 0.07750091701745987\n",
      "          vf_loss: 0.22987417558001147\n",
      "    num_agent_steps_sampled: 258000\n",
      "    num_agent_steps_trained: 258000\n",
      "    num_steps_sampled: 258000\n",
      "    num_steps_trained: 258000\n",
      "  iterations_since_restore: 258\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.37692307692308\n",
      "    ram_util_percent: 31.376923076923074\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036710907631883755\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.98634414896881\n",
      "    mean_inference_ms: 2.113483576761613\n",
      "    mean_raw_obs_processing_ms: 2.043677524994057\n",
      "  time_since_restore: 6167.248914480209\n",
      "  time_this_iter_s: 17.903860092163086\n",
      "  time_total_s: 6167.248914480209\n",
      "  timers:\n",
      "    learn_throughput: 1330.606\n",
      "    learn_time_ms: 751.537\n",
      "    load_throughput: 45401.155\n",
      "    load_time_ms: 22.026\n",
      "    sample_throughput: 42.506\n",
      "    sample_time_ms: 23526.033\n",
      "    update_time_ms: 3.725\n",
      "  timestamp: 1635288744\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 258000\n",
      "  training_iteration: 258\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   258</td><td style=\"text-align: right;\">         6167.25</td><td style=\"text-align: right;\">258000</td><td style=\"text-align: right;\"> -3.9059</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.33</td><td style=\"text-align: right;\">             392.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 259000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-52-42\n",
      "  done: false\n",
      "  episode_len_mean: 394.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.868399999999964\n",
      "  episode_reward_min: -8.329999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 715\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8667590883043077\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011190218574325783\n",
      "          policy_loss: 0.05217740784088771\n",
      "          total_loss: 0.07368081203765339\n",
      "          vf_explained_var: 0.5124814510345459\n",
      "          vf_loss: 0.035066696608232126\n",
      "    num_agent_steps_sampled: 259000\n",
      "    num_agent_steps_trained: 259000\n",
      "    num_steps_sampled: 259000\n",
      "    num_steps_trained: 259000\n",
      "  iterations_since_restore: 259\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.44999999999999\n",
      "    ram_util_percent: 31.319230769230767\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670975334023473\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.9743218534304\n",
      "    mean_inference_ms: 2.1134201258880383\n",
      "    mean_raw_obs_processing_ms: 2.0492769584972166\n",
      "  time_since_restore: 6185.865623474121\n",
      "  time_this_iter_s: 18.616708993911743\n",
      "  time_total_s: 6185.865623474121\n",
      "  timers:\n",
      "    learn_throughput: 1330.358\n",
      "    learn_time_ms: 751.677\n",
      "    load_throughput: 46104.223\n",
      "    load_time_ms: 21.69\n",
      "    sample_throughput: 46.188\n",
      "    sample_time_ms: 21650.817\n",
      "    update_time_ms: 3.689\n",
      "  timestamp: 1635288762\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 259000\n",
      "  training_iteration: 259\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   259</td><td style=\"text-align: right;\">         6185.87</td><td style=\"text-align: right;\">259000</td><td style=\"text-align: right;\"> -3.8684</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.33</td><td style=\"text-align: right;\">            394.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-53-00\n",
      "  done: false\n",
      "  episode_len_mean: 398.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.901399999999963\n",
      "  episode_reward_min: -8.329999999999952\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 717\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8253410591019525\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00981389831878027\n",
      "          policy_loss: -0.032609401270747185\n",
      "          total_loss: -0.035648805937833254\n",
      "          vf_explained_var: -0.2469576746225357\n",
      "          vf_loss: 0.010737497980395952\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_agent_steps_trained: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "  iterations_since_restore: 260\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.692\n",
      "    ram_util_percent: 31.308000000000003\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670897502292428\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.965832968346962\n",
      "    mean_inference_ms: 2.1133769588783324\n",
      "    mean_raw_obs_processing_ms: 2.052989463084089\n",
      "  time_since_restore: 6203.065958976746\n",
      "  time_this_iter_s: 17.20033550262451\n",
      "  time_total_s: 6203.065958976746\n",
      "  timers:\n",
      "    learn_throughput: 1330.42\n",
      "    learn_time_ms: 751.642\n",
      "    load_throughput: 48290.876\n",
      "    load_time_ms: 20.708\n",
      "    sample_throughput: 57.85\n",
      "    sample_time_ms: 17286.122\n",
      "    update_time_ms: 3.608\n",
      "  timestamp: 1635288780\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 260\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   260</td><td style=\"text-align: right;\">         6203.07</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> -3.9014</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.33</td><td style=\"text-align: right;\">            398.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 261000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-53-20\n",
      "  done: false\n",
      "  episode_len_mean: 400.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.895399999999963\n",
      "  episode_reward_min: -8.329999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 720\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0028325902091133\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012398245969264544\n",
      "          policy_loss: -0.031659469174014195\n",
      "          total_loss: 0.015408214016093148\n",
      "          vf_explained_var: 0.42138850688934326\n",
      "          vf_loss: 0.06144067717509137\n",
      "    num_agent_steps_sampled: 261000\n",
      "    num_agent_steps_trained: 261000\n",
      "    num_steps_sampled: 261000\n",
      "    num_steps_trained: 261000\n",
      "  iterations_since_restore: 261\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.56785714285714\n",
      "    ram_util_percent: 31.23571428571429\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670778585423312\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.9527879450883\n",
      "    mean_inference_ms: 2.1133110952266825\n",
      "    mean_raw_obs_processing_ms: 2.0584535542017557\n",
      "  time_since_restore: 6223.190372467041\n",
      "  time_this_iter_s: 20.12441349029541\n",
      "  time_total_s: 6223.190372467041\n",
      "  timers:\n",
      "    learn_throughput: 1331.715\n",
      "    learn_time_ms: 750.911\n",
      "    load_throughput: 48095.404\n",
      "    load_time_ms: 20.792\n",
      "    sample_throughput: 57.592\n",
      "    sample_time_ms: 17363.543\n",
      "    update_time_ms: 3.606\n",
      "  timestamp: 1635288800\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 261000\n",
      "  training_iteration: 261\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   261</td><td style=\"text-align: right;\">         6223.19</td><td style=\"text-align: right;\">261000</td><td style=\"text-align: right;\"> -3.8954</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.33</td><td style=\"text-align: right;\">            400.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 262000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-53-38\n",
      "  done: false\n",
      "  episode_len_mean: 402.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.8684999999999627\n",
      "  episode_reward_min: -8.329999999999952\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 722\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9665798637602063\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012052317955105958\n",
      "          policy_loss: -0.0024587040146191916\n",
      "          total_loss: 0.10545194769899051\n",
      "          vf_explained_var: 0.4113612174987793\n",
      "          vf_loss: 0.1220789129121436\n",
      "    num_agent_steps_sampled: 262000\n",
      "    num_agent_steps_trained: 262000\n",
      "    num_steps_sampled: 262000\n",
      "    num_steps_trained: 262000\n",
      "  iterations_since_restore: 262\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.34814814814816\n",
      "    ram_util_percent: 31.20740740740741\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367069892819535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.943808981275023\n",
      "    mean_inference_ms: 2.1132662535050586\n",
      "    mean_raw_obs_processing_ms: 2.0621056027161693\n",
      "  time_since_restore: 6241.818462133408\n",
      "  time_this_iter_s: 18.628089666366577\n",
      "  time_total_s: 6241.818462133408\n",
      "  timers:\n",
      "    learn_throughput: 1331.933\n",
      "    learn_time_ms: 750.788\n",
      "    load_throughput: 45893.256\n",
      "    load_time_ms: 21.79\n",
      "    sample_throughput: 57.14\n",
      "    sample_time_ms: 17500.936\n",
      "    update_time_ms: 3.616\n",
      "  timestamp: 1635288818\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 262000\n",
      "  training_iteration: 262\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   262</td><td style=\"text-align: right;\">         6241.82</td><td style=\"text-align: right;\">262000</td><td style=\"text-align: right;\"> -3.8685</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">               -8.33</td><td style=\"text-align: right;\">            402.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 263000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-54-18\n",
      "  done: false\n",
      "  episode_len_mean: 403.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.9483999999999626\n",
      "  episode_reward_min: -10.379999999999972\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 725\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.91213495598899\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004972552163748054\n",
      "          policy_loss: -0.25723773058917787\n",
      "          total_loss: -0.2632149542371432\n",
      "          vf_explained_var: 0.470896452665329\n",
      "          vf_loss: 0.010875948673735062\n",
      "    num_agent_steps_sampled: 263000\n",
      "    num_agent_steps_trained: 263000\n",
      "    num_steps_sampled: 263000\n",
      "    num_steps_trained: 263000\n",
      "  iterations_since_restore: 263\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.533928571428575\n",
      "    ram_util_percent: 31.273214285714285\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670579094900069\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.93026799240158\n",
      "    mean_inference_ms: 2.1131985966503213\n",
      "    mean_raw_obs_processing_ms: 2.06945900187805\n",
      "  time_since_restore: 6281.037337303162\n",
      "  time_this_iter_s: 39.21887516975403\n",
      "  time_total_s: 6281.037337303162\n",
      "  timers:\n",
      "    learn_throughput: 1333.734\n",
      "    learn_time_ms: 749.775\n",
      "    load_throughput: 45902.799\n",
      "    load_time_ms: 21.785\n",
      "    sample_throughput: 51.392\n",
      "    sample_time_ms: 19458.312\n",
      "    update_time_ms: 3.684\n",
      "  timestamp: 1635288858\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 263000\n",
      "  training_iteration: 263\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   263</td><td style=\"text-align: right;\">         6281.04</td><td style=\"text-align: right;\">263000</td><td style=\"text-align: right;\"> -3.9484</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">              -10.38</td><td style=\"text-align: right;\">            403.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-54-37\n",
      "  done: false\n",
      "  episode_len_mean: 404.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.921399999999963\n",
      "  episode_reward_min: -10.379999999999972\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 727\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22806973457336427\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.040134225951301\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014463737427225709\n",
      "          policy_loss: -0.004636909564336141\n",
      "          total_loss: 0.019881784584787156\n",
      "          vf_explained_var: 0.5108748078346252\n",
      "          vf_loss: 0.04162129515575038\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "  iterations_since_restore: 264\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.9857142857143\n",
      "    ram_util_percent: 31.88214285714285\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670499943064346\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.92102828834941\n",
      "    mean_inference_ms: 2.113153117791977\n",
      "    mean_raw_obs_processing_ms: 2.074338677673134\n",
      "  time_since_restore: 6300.41859126091\n",
      "  time_this_iter_s: 19.381253957748413\n",
      "  time_total_s: 6300.41859126091\n",
      "  timers:\n",
      "    learn_throughput: 1334.394\n",
      "    learn_time_ms: 749.404\n",
      "    load_throughput: 43213.339\n",
      "    load_time_ms: 23.141\n",
      "    sample_throughput: 50.363\n",
      "    sample_time_ms: 19855.671\n",
      "    update_time_ms: 3.694\n",
      "  timestamp: 1635288877\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 264\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   264</td><td style=\"text-align: right;\">         6300.42</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\"> -3.9214</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">              -10.38</td><td style=\"text-align: right;\">             404.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 265000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-54-56\n",
      "  done: false\n",
      "  episode_len_mean: 406.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -3.9107999999999628\n",
      "  episode_reward_min: -10.379999999999972\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 730\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22806973457336427\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9754609160953098\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014018235291322432\n",
      "          policy_loss: 0.027198329981830385\n",
      "          total_loss: 0.08400211764706506\n",
      "          vf_explained_var: 0.5273119211196899\n",
      "          vf_loss: 0.07336126486253407\n",
      "    num_agent_steps_sampled: 265000\n",
      "    num_agent_steps_trained: 265000\n",
      "    num_steps_sampled: 265000\n",
      "    num_steps_trained: 265000\n",
      "  iterations_since_restore: 265\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.80740740740741\n",
      "    ram_util_percent: 31.599999999999994\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670382504426555\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.906955674806774\n",
      "    mean_inference_ms: 2.1130841878216757\n",
      "    mean_raw_obs_processing_ms: 2.0816298857775837\n",
      "  time_since_restore: 6319.798755645752\n",
      "  time_this_iter_s: 19.38016438484192\n",
      "  time_total_s: 6319.798755645752\n",
      "  timers:\n",
      "    learn_throughput: 1334.945\n",
      "    learn_time_ms: 749.095\n",
      "    load_throughput: 43046.847\n",
      "    load_time_ms: 23.231\n",
      "    sample_throughput: 50.328\n",
      "    sample_time_ms: 19869.482\n",
      "    update_time_ms: 3.757\n",
      "  timestamp: 1635288896\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 265000\n",
      "  training_iteration: 265\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   265</td><td style=\"text-align: right;\">          6319.8</td><td style=\"text-align: right;\">265000</td><td style=\"text-align: right;\"> -3.9108</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">              -10.38</td><td style=\"text-align: right;\">            406.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 266000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-55-18\n",
      "  done: false\n",
      "  episode_len_mean: 406.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -4.0274999999999626\n",
      "  episode_reward_min: -12.609999999999967\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 733\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22806973457336427\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8424313902854919\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.025099126991818467\n",
      "          policy_loss: -0.03620023348679145\n",
      "          total_loss: 0.1786708252090547\n",
      "          vf_explained_var: 0.3510389029979706\n",
      "          vf_loss: 0.2275710203167465\n",
      "    num_agent_steps_sampled: 266000\n",
      "    num_agent_steps_trained: 266000\n",
      "    num_steps_sampled: 266000\n",
      "    num_steps_trained: 266000\n",
      "  iterations_since_restore: 266\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.5625\n",
      "    ram_util_percent: 31.234375\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670264175120826\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.893011838576548\n",
      "    mean_inference_ms: 2.113015289544106\n",
      "    mean_raw_obs_processing_ms: 2.0889323908105704\n",
      "  time_since_restore: 6341.809923171997\n",
      "  time_this_iter_s: 22.011167526245117\n",
      "  time_total_s: 6341.809923171997\n",
      "  timers:\n",
      "    learn_throughput: 1333.489\n",
      "    learn_time_ms: 749.913\n",
      "    load_throughput: 42921.171\n",
      "    load_time_ms: 23.299\n",
      "    sample_throughput: 49.275\n",
      "    sample_time_ms: 20294.233\n",
      "    update_time_ms: 3.758\n",
      "  timestamp: 1635288918\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 266000\n",
      "  training_iteration: 266\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   266</td><td style=\"text-align: right;\">         6341.81</td><td style=\"text-align: right;\">266000</td><td style=\"text-align: right;\"> -4.0275</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">              -12.61</td><td style=\"text-align: right;\">            406.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 267000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-55-37\n",
      "  done: false\n",
      "  episode_len_mean: 408.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -4.091899999999963\n",
      "  episode_reward_min: -12.609999999999967\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 735\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3421046018600464\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7969429506195915\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018449371903025577\n",
      "          policy_loss: -0.05598214409417576\n",
      "          total_loss: 0.00937354779905743\n",
      "          vf_explained_var: 0.6551483869552612\n",
      "          vf_loss: 0.07701351340446207\n",
      "    num_agent_steps_sampled: 267000\n",
      "    num_agent_steps_trained: 267000\n",
      "    num_steps_sampled: 267000\n",
      "    num_steps_trained: 267000\n",
      "  iterations_since_restore: 267\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.49230769230769\n",
      "    ram_util_percent: 31.276923076923076\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036701851877740464\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.883457369496682\n",
      "    mean_inference_ms: 2.112968913149284\n",
      "    mean_raw_obs_processing_ms: 2.0938306758862506\n",
      "  time_since_restore: 6359.94371509552\n",
      "  time_this_iter_s: 18.13379192352295\n",
      "  time_total_s: 6359.94371509552\n",
      "  timers:\n",
      "    learn_throughput: 1336.418\n",
      "    learn_time_ms: 748.269\n",
      "    load_throughput: 42143.263\n",
      "    load_time_ms: 23.729\n",
      "    sample_throughput: 49.31\n",
      "    sample_time_ms: 20279.961\n",
      "    update_time_ms: 3.76\n",
      "  timestamp: 1635288937\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 267000\n",
      "  training_iteration: 267\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   267</td><td style=\"text-align: right;\">         6359.94</td><td style=\"text-align: right;\">267000</td><td style=\"text-align: right;\"> -4.0919</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">              -12.61</td><td style=\"text-align: right;\">            408.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 268000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-55-57\n",
      "  done: false\n",
      "  episode_len_mean: 408.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -4.189199999999962\n",
      "  episode_reward_min: -12.609999999999967\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 737\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3421046018600464\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9179462697770862\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015334260596213293\n",
      "          policy_loss: -0.06360326028532452\n",
      "          total_loss: 0.0020181768470340306\n",
      "          vf_explained_var: 0.6209217309951782\n",
      "          vf_loss: 0.07955497944106658\n",
      "    num_agent_steps_sampled: 268000\n",
      "    num_agent_steps_trained: 268000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "  iterations_since_restore: 268\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.5551724137931\n",
      "    ram_util_percent: 31.286206896551718\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670104601544519\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.873811780792813\n",
      "    mean_inference_ms: 2.112921980490599\n",
      "    mean_raw_obs_processing_ms: 2.0986571032635384\n",
      "  time_since_restore: 6380.489533901215\n",
      "  time_this_iter_s: 20.54581880569458\n",
      "  time_total_s: 6380.489533901215\n",
      "  timers:\n",
      "    learn_throughput: 1334.355\n",
      "    learn_time_ms: 749.426\n",
      "    load_throughput: 42278.861\n",
      "    load_time_ms: 23.652\n",
      "    sample_throughput: 48.678\n",
      "    sample_time_ms: 20542.998\n",
      "    update_time_ms: 3.847\n",
      "  timestamp: 1635288957\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 268\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   268</td><td style=\"text-align: right;\">         6380.49</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\"> -4.1892</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">              -12.61</td><td style=\"text-align: right;\">            408.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 269000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-56-16\n",
      "  done: false\n",
      "  episode_len_mean: 410.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -4.293199999999962\n",
      "  episode_reward_min: -12.609999999999967\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 740\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3421046018600464\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9494385997454324\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01332436167707694\n",
      "          policy_loss: -0.02363744436038865\n",
      "          total_loss: 0.009957784165938696\n",
      "          vf_explained_var: 0.5847480893135071\n",
      "          vf_loss: 0.04853129001955191\n",
      "    num_agent_steps_sampled: 269000\n",
      "    num_agent_steps_trained: 269000\n",
      "    num_steps_sampled: 269000\n",
      "    num_steps_trained: 269000\n",
      "  iterations_since_restore: 269\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.49259259259259\n",
      "    ram_util_percent: 31.299999999999997\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669981470058328\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.85936470159946\n",
      "    mean_inference_ms: 2.112850737437171\n",
      "    mean_raw_obs_processing_ms: 2.1037067355548036\n",
      "  time_since_restore: 6399.471520185471\n",
      "  time_this_iter_s: 18.98198628425598\n",
      "  time_total_s: 6399.471520185471\n",
      "  timers:\n",
      "    learn_throughput: 1332.079\n",
      "    learn_time_ms: 750.706\n",
      "    load_throughput: 42473.233\n",
      "    load_time_ms: 23.544\n",
      "    sample_throughput: 48.595\n",
      "    sample_time_ms: 20578.361\n",
      "    update_time_ms: 3.847\n",
      "  timestamp: 1635288976\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 269000\n",
      "  training_iteration: 269\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   269</td><td style=\"text-align: right;\">         6399.47</td><td style=\"text-align: right;\">269000</td><td style=\"text-align: right;\"> -4.2932</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">              -12.61</td><td style=\"text-align: right;\">            410.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 270000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-56-35\n",
      "  done: false\n",
      "  episode_len_mean: 411.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -4.356999999999962\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 742\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3421046018600464\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.026923867066701\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015635892380925418\n",
      "          policy_loss: -0.00742742990454038\n",
      "          total_loss: 0.26456136379597917\n",
      "          vf_explained_var: 0.2662096619606018\n",
      "          vf_loss: 0.2869089209371143\n",
      "    num_agent_steps_sampled: 270000\n",
      "    num_agent_steps_trained: 270000\n",
      "    num_steps_sampled: 270000\n",
      "    num_steps_trained: 270000\n",
      "  iterations_since_restore: 270\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.95357142857144\n",
      "    ram_util_percent: 31.39642857142857\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669901766629608\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.84942079382934\n",
      "    mean_inference_ms: 2.1128033322470077\n",
      "    mean_raw_obs_processing_ms: 2.1070815836688688\n",
      "  time_since_restore: 6418.5416712760925\n",
      "  time_this_iter_s: 19.07015109062195\n",
      "  time_total_s: 6418.5416712760925\n",
      "  timers:\n",
      "    learn_throughput: 1328.645\n",
      "    learn_time_ms: 752.646\n",
      "    load_throughput: 40696.664\n",
      "    load_time_ms: 24.572\n",
      "    sample_throughput: 48.164\n",
      "    sample_time_ms: 20762.358\n",
      "    update_time_ms: 3.867\n",
      "  timestamp: 1635288995\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 270000\n",
      "  training_iteration: 270\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   270</td><td style=\"text-align: right;\">         6418.54</td><td style=\"text-align: right;\">270000</td><td style=\"text-align: right;\">  -4.357</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            411.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 271000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-57-34\n",
      "  done: false\n",
      "  episode_len_mean: 412.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -4.383799999999962\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 745\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3421046018600464\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1083424515194364\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013526562636547378\n",
      "          policy_loss: 0.01741482557521926\n",
      "          total_loss: 0.17562568961746164\n",
      "          vf_explained_var: 0.3792422413825989\n",
      "          vf_loss: 0.17466678449677098\n",
      "    num_agent_steps_sampled: 271000\n",
      "    num_agent_steps_trained: 271000\n",
      "    num_steps_sampled: 271000\n",
      "    num_steps_trained: 271000\n",
      "  iterations_since_restore: 271\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.26904761904761\n",
      "    ram_util_percent: 31.502380952380957\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669816178351805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.83441805081816\n",
      "    mean_inference_ms: 2.1127333129678383\n",
      "    mean_raw_obs_processing_ms: 2.1163029214643716\n",
      "  time_since_restore: 6477.358819246292\n",
      "  time_this_iter_s: 58.817147970199585\n",
      "  time_total_s: 6477.358819246292\n",
      "  timers:\n",
      "    learn_throughput: 1323.207\n",
      "    learn_time_ms: 755.739\n",
      "    load_throughput: 40877.606\n",
      "    load_time_ms: 24.463\n",
      "    sample_throughput: 40.603\n",
      "    sample_time_ms: 24628.634\n",
      "    update_time_ms: 3.875\n",
      "  timestamp: 1635289054\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 271000\n",
      "  training_iteration: 271\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   271</td><td style=\"text-align: right;\">         6477.36</td><td style=\"text-align: right;\">271000</td><td style=\"text-align: right;\"> -4.3838</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            412.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-57-56\n",
      "  done: false\n",
      "  episode_len_mean: 413.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -4.366999999999962\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 747\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3421046018600464\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.085288353761037\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012734075690632531\n",
      "          policy_loss: -0.11864920092953576\n",
      "          total_loss: -0.047000334742996426\n",
      "          vf_explained_var: 0.14363747835159302\n",
      "          vf_loss: 0.08814536564879948\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_agent_steps_trained: 272000\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "  iterations_since_restore: 272\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.77666666666666\n",
      "    ram_util_percent: 32.05\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669776126064654\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.824479651915514\n",
      "    mean_inference_ms: 2.112694693666495\n",
      "    mean_raw_obs_processing_ms: 2.1224258830369647\n",
      "  time_since_restore: 6498.882945537567\n",
      "  time_this_iter_s: 21.524126291275024\n",
      "  time_total_s: 6498.882945537567\n",
      "  timers:\n",
      "    learn_throughput: 1322.945\n",
      "    learn_time_ms: 755.889\n",
      "    load_throughput: 40900.327\n",
      "    load_time_ms: 24.45\n",
      "    sample_throughput: 40.131\n",
      "    sample_time_ms: 24918.176\n",
      "    update_time_ms: 3.806\n",
      "  timestamp: 1635289076\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 272\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   272</td><td style=\"text-align: right;\">         6498.88</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\">  -4.367</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            413.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 273000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-58-44\n",
      "  done: false\n",
      "  episode_len_mean: 414.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -4.272099999999963\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 750\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3421046018600464\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.061022871070438\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009200073988038238\n",
      "          policy_loss: 0.08785094080699815\n",
      "          total_loss: 0.16654431712296275\n",
      "          vf_explained_var: 0.31881552934646606\n",
      "          vf_loss: 0.09615621488127443\n",
      "    num_agent_steps_sampled: 273000\n",
      "    num_agent_steps_trained: 273000\n",
      "    num_steps_sampled: 273000\n",
      "    num_steps_trained: 273000\n",
      "  iterations_since_restore: 273\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.77536231884057\n",
      "    ram_util_percent: 32.04637681159421\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036697404754478885\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.80958249143755\n",
      "    mean_inference_ms: 2.1126469226694145\n",
      "    mean_raw_obs_processing_ms: 2.134833030575534\n",
      "  time_since_restore: 6547.093708992004\n",
      "  time_this_iter_s: 48.210763454437256\n",
      "  time_total_s: 6547.093708992004\n",
      "  timers:\n",
      "    learn_throughput: 1320.305\n",
      "    learn_time_ms: 757.401\n",
      "    load_throughput: 40752.18\n",
      "    load_time_ms: 24.539\n",
      "    sample_throughput: 38.736\n",
      "    sample_time_ms: 25815.828\n",
      "    update_time_ms: 3.744\n",
      "  timestamp: 1635289124\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 273000\n",
      "  training_iteration: 273\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   273</td><td style=\"text-align: right;\">         6547.09</td><td style=\"text-align: right;\">273000</td><td style=\"text-align: right;\"> -4.2721</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            414.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 274000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-59-05\n",
      "  done: false\n",
      "  episode_len_mean: 413.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -4.304399999999962\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 752\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3421046018600464\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1149016247855292\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016844171839534298\n",
      "          policy_loss: -0.06253790474600263\n",
      "          total_loss: -0.008436658978462219\n",
      "          vf_explained_var: 0.3061247169971466\n",
      "          vf_loss: 0.06948779109451506\n",
      "    num_agent_steps_sampled: 274000\n",
      "    num_agent_steps_trained: 274000\n",
      "    num_steps_sampled: 274000\n",
      "    num_steps_trained: 274000\n",
      "  iterations_since_restore: 274\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.12666666666668\n",
      "    ram_util_percent: 31.97666666666667\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669722537222561\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.799868661089036\n",
      "    mean_inference_ms: 2.112618545623398\n",
      "    mean_raw_obs_processing_ms: 2.1430716643312437\n",
      "  time_since_restore: 6568.106800556183\n",
      "  time_this_iter_s: 21.013091564178467\n",
      "  time_total_s: 6568.106800556183\n",
      "  timers:\n",
      "    learn_throughput: 1315.932\n",
      "    learn_time_ms: 759.918\n",
      "    load_throughput: 41220.932\n",
      "    load_time_ms: 24.26\n",
      "    sample_throughput: 38.496\n",
      "    sample_time_ms: 25976.742\n",
      "    update_time_ms: 3.735\n",
      "  timestamp: 1635289145\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 274000\n",
      "  training_iteration: 274\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   274</td><td style=\"text-align: right;\">         6568.11</td><td style=\"text-align: right;\">274000</td><td style=\"text-align: right;\"> -4.3044</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            413.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 275000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-59-25\n",
      "  done: false\n",
      "  episode_len_mean: 414.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -4.323599999999962\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 755\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3421046018600464\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2609165959888036\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01292171791602706\n",
      "          policy_loss: 0.08125495430496003\n",
      "          total_loss: 0.08065439106689559\n",
      "          vf_explained_var: 0.3710860311985016\n",
      "          vf_loss: 0.017588021657947036\n",
      "    num_agent_steps_sampled: 275000\n",
      "    num_agent_steps_trained: 275000\n",
      "    num_steps_sampled: 275000\n",
      "    num_steps_trained: 275000\n",
      "  iterations_since_restore: 275\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.25714285714287\n",
      "    ram_util_percent: 32.142857142857146\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0366971679854238\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.78562141822348\n",
      "    mean_inference_ms: 2.1125843173875567\n",
      "    mean_raw_obs_processing_ms: 2.1554587300871826\n",
      "  time_since_restore: 6587.978578329086\n",
      "  time_this_iter_s: 19.871777772903442\n",
      "  time_total_s: 6587.978578329086\n",
      "  timers:\n",
      "    learn_throughput: 1314.494\n",
      "    learn_time_ms: 760.749\n",
      "    load_throughput: 41348.123\n",
      "    load_time_ms: 24.185\n",
      "    sample_throughput: 38.424\n",
      "    sample_time_ms: 26025.232\n",
      "    update_time_ms: 3.644\n",
      "  timestamp: 1635289165\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 275000\n",
      "  training_iteration: 275\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   275</td><td style=\"text-align: right;\">         6587.98</td><td style=\"text-align: right;\">275000</td><td style=\"text-align: right;\"> -4.3236</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">             414.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 276000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_22-59-44\n",
      "  done: false\n",
      "  episode_len_mean: 414.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -4.325099999999962\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 757\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3421046018600464\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.262235821617974\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010574287414886927\n",
      "          policy_loss: 0.05236445168654124\n",
      "          total_loss: 0.04548815555042691\n",
      "          vf_explained_var: 0.3101809620857239\n",
      "          vf_loss: 0.012128552807391517\n",
      "    num_agent_steps_sampled: 276000\n",
      "    num_agent_steps_trained: 276000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "  iterations_since_restore: 276\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.24999999999999\n",
      "    ram_util_percent: 31.678571428571434\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036697216806379246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.776164124426327\n",
      "    mean_inference_ms: 2.1125646585201907\n",
      "    mean_raw_obs_processing_ms: 2.163683957892988\n",
      "  time_since_restore: 6607.486602067947\n",
      "  time_this_iter_s: 19.508023738861084\n",
      "  time_total_s: 6607.486602067947\n",
      "  timers:\n",
      "    learn_throughput: 1314.744\n",
      "    learn_time_ms: 760.605\n",
      "    load_throughput: 41108.255\n",
      "    load_time_ms: 24.326\n",
      "    sample_throughput: 38.798\n",
      "    sample_time_ms: 25774.856\n",
      "    update_time_ms: 3.717\n",
      "  timestamp: 1635289184\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 276\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   276</td><td style=\"text-align: right;\">         6607.49</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\"> -4.3251</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            414.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 277000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-00-04\n",
      "  done: false\n",
      "  episode_len_mean: 414.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.080000000000001\n",
      "  episode_reward_mean: -4.355499999999962\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 759\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3421046018600464\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.36558833916982\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006075056504457748\n",
      "          policy_loss: 0.06074025712700354\n",
      "          total_loss: 0.07384498756792811\n",
      "          vf_explained_var: 0.34371039271354675\n",
      "          vf_loss: 0.034682307219029304\n",
      "    num_agent_steps_sampled: 277000\n",
      "    num_agent_steps_trained: 277000\n",
      "    num_steps_sampled: 277000\n",
      "    num_steps_trained: 277000\n",
      "  iterations_since_restore: 277\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.95862068965518\n",
      "    ram_util_percent: 31.617241379310347\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036697368353930095\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.76683433903544\n",
      "    mean_inference_ms: 2.112550040627839\n",
      "    mean_raw_obs_processing_ms: 2.1701454158178914\n",
      "  time_since_restore: 6627.476576805115\n",
      "  time_this_iter_s: 19.98997473716736\n",
      "  time_total_s: 6627.476576805115\n",
      "  timers:\n",
      "    learn_throughput: 1311.358\n",
      "    learn_time_ms: 762.568\n",
      "    load_throughput: 41807.917\n",
      "    load_time_ms: 23.919\n",
      "    sample_throughput: 38.522\n",
      "    sample_time_ms: 25958.866\n",
      "    update_time_ms: 3.724\n",
      "  timestamp: 1635289204\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 277000\n",
      "  training_iteration: 277\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   277</td><td style=\"text-align: right;\">         6627.48</td><td style=\"text-align: right;\">277000</td><td style=\"text-align: right;\"> -4.3555</td><td style=\"text-align: right;\">                4.08</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            414.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 278000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-00-24\n",
      "  done: false\n",
      "  episode_len_mean: 416.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.69000000000002\n",
      "  episode_reward_mean: -4.477699999999961\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 762\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3421046018600464\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.143610715866089\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011857775884385694\n",
      "          policy_loss: -0.06358208076821434\n",
      "          total_loss: 0.12568037857611974\n",
      "          vf_explained_var: 0.32505881786346436\n",
      "          vf_loss: 0.20664197184766334\n",
      "    num_agent_steps_sampled: 278000\n",
      "    num_agent_steps_trained: 278000\n",
      "    num_steps_sampled: 278000\n",
      "    num_steps_trained: 278000\n",
      "  iterations_since_restore: 278\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.75925925925925\n",
      "    ram_util_percent: 31.485185185185188\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036697606193024326\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.752940005523893\n",
      "    mean_inference_ms: 2.1125286297598374\n",
      "    mean_raw_obs_processing_ms: 2.177164446010886\n",
      "  time_since_restore: 6646.72346162796\n",
      "  time_this_iter_s: 19.24688482284546\n",
      "  time_total_s: 6646.72346162796\n",
      "  timers:\n",
      "    learn_throughput: 1311.872\n",
      "    learn_time_ms: 762.269\n",
      "    load_throughput: 41924.97\n",
      "    load_time_ms: 23.852\n",
      "    sample_throughput: 38.716\n",
      "    sample_time_ms: 25829.424\n",
      "    update_time_ms: 3.628\n",
      "  timestamp: 1635289224\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 278000\n",
      "  training_iteration: 278\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   278</td><td style=\"text-align: right;\">         6646.72</td><td style=\"text-align: right;\">278000</td><td style=\"text-align: right;\"> -4.4777</td><td style=\"text-align: right;\">                3.69</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            416.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 279000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-00-43\n",
      "  done: false\n",
      "  episode_len_mean: 415.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.69000000000002\n",
      "  episode_reward_mean: -4.487799999999961\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 764\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3421046018600464\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2037213113572864\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011455450938850555\n",
      "          policy_loss: -0.12783972695469856\n",
      "          total_loss: -0.11114887620011965\n",
      "          vf_explained_var: 0.3968803286552429\n",
      "          vf_loss: 0.03480910582260953\n",
      "    num_agent_steps_sampled: 279000\n",
      "    num_agent_steps_trained: 279000\n",
      "    num_steps_sampled: 279000\n",
      "    num_steps_trained: 279000\n",
      "  iterations_since_restore: 279\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.42857142857143\n",
      "    ram_util_percent: 31.47857142857143\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669777067750253\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.743912870024257\n",
      "    mean_inference_ms: 2.1125151269893037\n",
      "    mean_raw_obs_processing_ms: 2.1818781704038765\n",
      "  time_since_restore: 6666.160675764084\n",
      "  time_this_iter_s: 19.437214136123657\n",
      "  time_total_s: 6666.160675764084\n",
      "  timers:\n",
      "    learn_throughput: 1314.068\n",
      "    learn_time_ms: 760.995\n",
      "    load_throughput: 41714.943\n",
      "    load_time_ms: 23.972\n",
      "    sample_throughput: 38.646\n",
      "    sample_time_ms: 25876.088\n",
      "    update_time_ms: 3.639\n",
      "  timestamp: 1635289243\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 279000\n",
      "  training_iteration: 279\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   279</td><td style=\"text-align: right;\">         6666.16</td><td style=\"text-align: right;\">279000</td><td style=\"text-align: right;\"> -4.4878</td><td style=\"text-align: right;\">                3.69</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">             415.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-01-04\n",
      "  done: false\n",
      "  episode_len_mean: 413.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.69000000000002\n",
      "  episode_reward_mean: -4.491599999999961\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 767\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3421046018600464\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.114264946513706\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013760541701974856\n",
      "          policy_loss: 0.06507652501265208\n",
      "          total_loss: 0.21103116306993697\n",
      "          vf_explained_var: 0.3139573931694031\n",
      "          vf_loss: 0.16238974610136614\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "  iterations_since_restore: 280\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.79666666666667\n",
      "    ram_util_percent: 31.49\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669803166050432\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.73087596810825\n",
      "    mean_inference_ms: 2.1124959122139244\n",
      "    mean_raw_obs_processing_ms: 2.1890022680504506\n",
      "  time_since_restore: 6686.826966762543\n",
      "  time_this_iter_s: 20.666290998458862\n",
      "  time_total_s: 6686.826966762543\n",
      "  timers:\n",
      "    learn_throughput: 1318.856\n",
      "    learn_time_ms: 758.233\n",
      "    load_throughput: 41534.383\n",
      "    load_time_ms: 24.076\n",
      "    sample_throughput: 38.405\n",
      "    sample_time_ms: 26038.355\n",
      "    update_time_ms: 3.628\n",
      "  timestamp: 1635289264\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 280\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   280</td><td style=\"text-align: right;\">         6686.83</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\"> -4.4916</td><td style=\"text-align: right;\">                3.69</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            413.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 281000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-01-24\n",
      "  done: false\n",
      "  episode_len_mean: 413.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.69000000000002\n",
      "  episode_reward_mean: -4.454799999999961\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 769\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3421046018600464\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.327785841623942\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017241766989858666\n",
      "          policy_loss: -0.02067667328649097\n",
      "          total_loss: 0.15838075263632667\n",
      "          vf_explained_var: 0.4464995861053467\n",
      "          vf_loss: 0.19643679811722703\n",
      "    num_agent_steps_sampled: 281000\n",
      "    num_agent_steps_trained: 281000\n",
      "    num_steps_sampled: 281000\n",
      "    num_steps_trained: 281000\n",
      "  iterations_since_restore: 281\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.72857142857143\n",
      "    ram_util_percent: 31.442857142857143\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036698219476724095\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.72235358107137\n",
      "    mean_inference_ms: 2.1124842870427303\n",
      "    mean_raw_obs_processing_ms: 2.193731918937768\n",
      "  time_since_restore: 6706.5478484630585\n",
      "  time_this_iter_s: 19.720881700515747\n",
      "  time_total_s: 6706.5478484630585\n",
      "  timers:\n",
      "    learn_throughput: 1321.804\n",
      "    learn_time_ms: 756.542\n",
      "    load_throughput: 41517.321\n",
      "    load_time_ms: 24.086\n",
      "    sample_throughput: 45.187\n",
      "    sample_time_ms: 22130.409\n",
      "    update_time_ms: 3.629\n",
      "  timestamp: 1635289284\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 281000\n",
      "  training_iteration: 281\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   281</td><td style=\"text-align: right;\">         6706.55</td><td style=\"text-align: right;\">281000</td><td style=\"text-align: right;\"> -4.4548</td><td style=\"text-align: right;\">                3.69</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            413.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 282000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-01-44\n",
      "  done: false\n",
      "  episode_len_mean: 411.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.69000000000002\n",
      "  episode_reward_mean: -4.443099999999962\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 772\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3421046018600464\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.232400001419915\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017553872293619478\n",
      "          policy_loss: -5.6584676106770836e-05\n",
      "          total_loss: 0.13456944906049303\n",
      "          vf_explained_var: 0.5227845311164856\n",
      "          vf_loss: 0.15094476826488973\n",
      "    num_agent_steps_sampled: 282000\n",
      "    num_agent_steps_trained: 282000\n",
      "    num_steps_sampled: 282000\n",
      "    num_steps_trained: 282000\n",
      "  iterations_since_restore: 282\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.46551724137932\n",
      "    ram_util_percent: 31.410344827586208\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669851749732098\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.710068427925904\n",
      "    mean_inference_ms: 2.1124686195502873\n",
      "    mean_raw_obs_processing_ms: 2.200881470398292\n",
      "  time_since_restore: 6726.60644364357\n",
      "  time_this_iter_s: 20.058595180511475\n",
      "  time_total_s: 6726.60644364357\n",
      "  timers:\n",
      "    learn_throughput: 1322.029\n",
      "    learn_time_ms: 756.413\n",
      "    load_throughput: 41470.238\n",
      "    load_time_ms: 24.114\n",
      "    sample_throughput: 45.488\n",
      "    sample_time_ms: 21983.967\n",
      "    update_time_ms: 3.61\n",
      "  timestamp: 1635289304\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 282000\n",
      "  training_iteration: 282\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   282</td><td style=\"text-align: right;\">         6726.61</td><td style=\"text-align: right;\">282000</td><td style=\"text-align: right;\"> -4.4431</td><td style=\"text-align: right;\">                3.69</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            411.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 283000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-02-04\n",
      "  done: false\n",
      "  episode_len_mean: 411.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.69000000000002\n",
      "  episode_reward_mean: -4.414899999999963\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 774\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3421046018600464\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9256677124235364\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010596474722818853\n",
      "          policy_loss: -0.05300898287031386\n",
      "          total_loss: 0.06538092721667554\n",
      "          vf_explained_var: 0.49981245398521423\n",
      "          vf_loss: 0.13402148072297373\n",
      "    num_agent_steps_sampled: 283000\n",
      "    num_agent_steps_trained: 283000\n",
      "    num_steps_sampled: 283000\n",
      "    num_steps_trained: 283000\n",
      "  iterations_since_restore: 283\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.63448275862069\n",
      "    ram_util_percent: 31.375862068965514\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036698726622213115\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.70196120175161\n",
      "    mean_inference_ms: 2.1124589976806907\n",
      "    mean_raw_obs_processing_ms: 2.2057047966182255\n",
      "  time_since_restore: 6746.899708271027\n",
      "  time_this_iter_s: 20.293264627456665\n",
      "  time_total_s: 6746.899708271027\n",
      "  timers:\n",
      "    learn_throughput: 1323.506\n",
      "    learn_time_ms: 755.569\n",
      "    load_throughput: 41635.975\n",
      "    load_time_ms: 24.018\n",
      "    sample_throughput: 52.102\n",
      "    sample_time_ms: 19193.066\n",
      "    update_time_ms: 3.682\n",
      "  timestamp: 1635289324\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 283000\n",
      "  training_iteration: 283\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   283</td><td style=\"text-align: right;\">          6746.9</td><td style=\"text-align: right;\">283000</td><td style=\"text-align: right;\"> -4.4149</td><td style=\"text-align: right;\">                3.69</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            411.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 284000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-02-26\n",
      "  done: false\n",
      "  episode_len_mean: 410.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.69000000000002\n",
      "  episode_reward_mean: -4.483699999999962\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 777\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3421046018600464\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1507389730877344\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.030161412134528772\n",
      "          policy_loss: 0.00881035245127148\n",
      "          total_loss: 0.2328876337243451\n",
      "          vf_explained_var: -0.006822675000876188\n",
      "          vf_loss: 0.23526631519198418\n",
      "    num_agent_steps_sampled: 284000\n",
      "    num_agent_steps_trained: 284000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "  iterations_since_restore: 284\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.91666666666667\n",
      "    ram_util_percent: 31.270000000000003\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0366989901921739\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.69008315534797\n",
      "    mean_inference_ms: 2.11244299188495\n",
      "    mean_raw_obs_processing_ms: 2.21287589291401\n",
      "  time_since_restore: 6768.5300097465515\n",
      "  time_this_iter_s: 21.630301475524902\n",
      "  time_total_s: 6768.5300097465515\n",
      "  timers:\n",
      "    learn_throughput: 1327.921\n",
      "    learn_time_ms: 753.057\n",
      "    load_throughput: 41173.063\n",
      "    load_time_ms: 24.288\n",
      "    sample_throughput: 51.929\n",
      "    sample_time_ms: 19257.05\n",
      "    update_time_ms: 3.688\n",
      "  timestamp: 1635289346\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 284\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   284</td><td style=\"text-align: right;\">         6768.53</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\"> -4.4837</td><td style=\"text-align: right;\">                3.69</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            410.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 285000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-03-22\n",
      "  done: false\n",
      "  episode_len_mean: 405.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.530000000000005\n",
      "  episode_reward_mean: -4.3647999999999625\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 781\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3145055294036867\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011686353649677588\n",
      "          policy_loss: 0.1080525173081292\n",
      "          total_loss: 0.20162430248326726\n",
      "          vf_explained_var: 0.37640902400016785\n",
      "          vf_loss: 0.11071990426215861\n",
      "    num_agent_steps_sampled: 285000\n",
      "    num_agent_steps_trained: 285000\n",
      "    num_steps_sampled: 285000\n",
      "    num_steps_trained: 285000\n",
      "  iterations_since_restore: 285\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.18780487804879\n",
      "    ram_util_percent: 31.3109756097561\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669928051422066\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.67469902923014\n",
      "    mean_inference_ms: 2.112418096543071\n",
      "    mean_raw_obs_processing_ms: 2.2277568739156326\n",
      "  time_since_restore: 6825.394624471664\n",
      "  time_this_iter_s: 56.864614725112915\n",
      "  time_total_s: 6825.394624471664\n",
      "  timers:\n",
      "    learn_throughput: 1329.261\n",
      "    learn_time_ms: 752.297\n",
      "    load_throughput: 41070.899\n",
      "    load_time_ms: 24.348\n",
      "    sample_throughput: 43.56\n",
      "    sample_time_ms: 22957.018\n",
      "    update_time_ms: 3.695\n",
      "  timestamp: 1635289402\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 285000\n",
      "  training_iteration: 285\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   285</td><td style=\"text-align: right;\">         6825.39</td><td style=\"text-align: right;\">285000</td><td style=\"text-align: right;\"> -4.3648</td><td style=\"text-align: right;\">                5.53</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            405.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 286000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-03-43\n",
      "  done: false\n",
      "  episode_len_mean: 404.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.530000000000005\n",
      "  episode_reward_mean: -4.425499999999962\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 784\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1942942089504665\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01282050867935559\n",
      "          policy_loss: -0.19532947623067431\n",
      "          total_loss: -0.1833234128024843\n",
      "          vf_explained_var: 0.7427030205726624\n",
      "          vf_loss: 0.027370068617165087\n",
      "    num_agent_steps_sampled: 286000\n",
      "    num_agent_steps_trained: 286000\n",
      "    num_steps_sampled: 286000\n",
      "    num_steps_trained: 286000\n",
      "  iterations_since_restore: 286\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.10357142857143\n",
      "    ram_util_percent: 31.878571428571426\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036699433524416236\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.663465866872965\n",
      "    mean_inference_ms: 2.1123951384313107\n",
      "    mean_raw_obs_processing_ms: 2.239006612994788\n",
      "  time_since_restore: 6845.584669113159\n",
      "  time_this_iter_s: 20.19004464149475\n",
      "  time_total_s: 6845.584669113159\n",
      "  timers:\n",
      "    learn_throughput: 1329.338\n",
      "    learn_time_ms: 752.254\n",
      "    load_throughput: 41571.474\n",
      "    load_time_ms: 24.055\n",
      "    sample_throughput: 43.43\n",
      "    sample_time_ms: 23025.517\n",
      "    update_time_ms: 3.718\n",
      "  timestamp: 1635289423\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 286000\n",
      "  training_iteration: 286\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   286</td><td style=\"text-align: right;\">         6845.58</td><td style=\"text-align: right;\">286000</td><td style=\"text-align: right;\"> -4.4255</td><td style=\"text-align: right;\">                5.53</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            404.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 287000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-04-04\n",
      "  done: false\n",
      "  episode_len_mean: 403.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.530000000000005\n",
      "  episode_reward_mean: -4.388699999999963\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 786\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.825083765718672\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008159453256955808\n",
      "          policy_loss: -0.0025841747721036274\n",
      "          total_loss: -0.013921920789612664\n",
      "          vf_explained_var: 0.8637131452560425\n",
      "          vf_loss: 0.0027260143814298015\n",
      "    num_agent_steps_sampled: 287000\n",
      "    num_agent_steps_trained: 287000\n",
      "    num_steps_sampled: 287000\n",
      "    num_steps_trained: 287000\n",
      "  iterations_since_restore: 287\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.16333333333333\n",
      "    ram_util_percent: 31.96333333333333\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669956220581595\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.656188137305016\n",
      "    mean_inference_ms: 2.112380871807035\n",
      "    mean_raw_obs_processing_ms: 2.2464762280689516\n",
      "  time_since_restore: 6866.416944026947\n",
      "  time_this_iter_s: 20.832274913787842\n",
      "  time_total_s: 6866.416944026947\n",
      "  timers:\n",
      "    learn_throughput: 1328.38\n",
      "    learn_time_ms: 752.796\n",
      "    load_throughput: 41605.164\n",
      "    load_time_ms: 24.035\n",
      "    sample_throughput: 43.273\n",
      "    sample_time_ms: 23109.232\n",
      "    update_time_ms: 3.748\n",
      "  timestamp: 1635289444\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 287000\n",
      "  training_iteration: 287\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   287</td><td style=\"text-align: right;\">         6866.42</td><td style=\"text-align: right;\">287000</td><td style=\"text-align: right;\"> -4.3887</td><td style=\"text-align: right;\">                5.53</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            403.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-04-24\n",
      "  done: false\n",
      "  episode_len_mean: 400.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.530000000000005\n",
      "  episode_reward_mean: -4.3765999999999625\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 789\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8612268580330742\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010640879851243628\n",
      "          policy_loss: 0.03984911185171869\n",
      "          total_loss: 0.0310187641531229\n",
      "          vf_explained_var: 0.5930296182632446\n",
      "          vf_loss: 0.004321479018674129\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_agent_steps_trained: 288000\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "  iterations_since_restore: 288\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.00666666666667\n",
      "    ram_util_percent: 32.120000000000005\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036699790487487685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.64570491497428\n",
      "    mean_inference_ms: 2.112361084649659\n",
      "    mean_raw_obs_processing_ms: 2.257712115227992\n",
      "  time_since_restore: 6887.3017818927765\n",
      "  time_this_iter_s: 20.884837865829468\n",
      "  time_total_s: 6887.3017818927765\n",
      "  timers:\n",
      "    learn_throughput: 1328.251\n",
      "    learn_time_ms: 752.87\n",
      "    load_throughput: 41554.999\n",
      "    load_time_ms: 24.064\n",
      "    sample_throughput: 42.969\n",
      "    sample_time_ms: 23272.819\n",
      "    update_time_ms: 3.852\n",
      "  timestamp: 1635289464\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 288\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   288</td><td style=\"text-align: right;\">          6887.3</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\"> -4.3766</td><td style=\"text-align: right;\">                5.53</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            400.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 289000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-04-45\n",
      "  done: false\n",
      "  episode_len_mean: 400.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.530000000000005\n",
      "  episode_reward_mean: -4.379699999999962\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 791\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9465351170963712\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006667166961030027\n",
      "          policy_loss: -0.10067998394370078\n",
      "          total_loss: -0.11261441343360477\n",
      "          vf_explained_var: 0.46143850684165955\n",
      "          vf_loss: 0.004109620527985195\n",
      "    num_agent_steps_sampled: 289000\n",
      "    num_agent_steps_trained: 289000\n",
      "    num_steps_sampled: 289000\n",
      "    num_steps_trained: 289000\n",
      "  iterations_since_restore: 289\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.41034482758621\n",
      "    ram_util_percent: 32.24482758620689\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0366999377049323\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.638824178373426\n",
      "    mean_inference_ms: 2.1123478995942078\n",
      "    mean_raw_obs_processing_ms: 2.2638952612286056\n",
      "  time_since_restore: 6907.596386671066\n",
      "  time_this_iter_s: 20.294604778289795\n",
      "  time_total_s: 6907.596386671066\n",
      "  timers:\n",
      "    learn_throughput: 1329.154\n",
      "    learn_time_ms: 752.358\n",
      "    load_throughput: 41481.188\n",
      "    load_time_ms: 24.107\n",
      "    sample_throughput: 42.81\n",
      "    sample_time_ms: 23359.034\n",
      "    update_time_ms: 3.835\n",
      "  timestamp: 1635289485\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 289000\n",
      "  training_iteration: 289\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   289</td><td style=\"text-align: right;\">          6907.6</td><td style=\"text-align: right;\">289000</td><td style=\"text-align: right;\"> -4.3797</td><td style=\"text-align: right;\">                5.53</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">             400.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 290000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-05-04\n",
      "  done: false\n",
      "  episode_len_mean: 401.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.530000000000005\n",
      "  episode_reward_mean: -4.470699999999962\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 794\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9530833707915412\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006398008893495898\n",
      "          policy_loss: 0.06941820846663581\n",
      "          total_loss: 0.05690849853886498\n",
      "          vf_explained_var: 0.08836641162633896\n",
      "          vf_loss: 0.0037379444622072495\n",
      "    num_agent_steps_sampled: 290000\n",
      "    num_agent_steps_trained: 290000\n",
      "    num_steps_sampled: 290000\n",
      "    num_steps_trained: 290000\n",
      "  iterations_since_restore: 290\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.0962962962963\n",
      "    ram_util_percent: 32.28148148148148\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670016031714695\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.628724695229202\n",
      "    mean_inference_ms: 2.1123290554136864\n",
      "    mean_raw_obs_processing_ms: 2.2679991612884787\n",
      "  time_since_restore: 6926.506669044495\n",
      "  time_this_iter_s: 18.910282373428345\n",
      "  time_total_s: 6926.506669044495\n",
      "  timers:\n",
      "    learn_throughput: 1326.353\n",
      "    learn_time_ms: 753.947\n",
      "    load_throughput: 41765.994\n",
      "    load_time_ms: 23.943\n",
      "    sample_throughput: 43.137\n",
      "    sample_time_ms: 23181.887\n",
      "    update_time_ms: 3.97\n",
      "  timestamp: 1635289504\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 290000\n",
      "  training_iteration: 290\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   290</td><td style=\"text-align: right;\">         6926.51</td><td style=\"text-align: right;\">290000</td><td style=\"text-align: right;\"> -4.4707</td><td style=\"text-align: right;\">                5.53</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            401.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 291000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-05-24\n",
      "  done: false\n",
      "  episode_len_mean: 400.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.530000000000005\n",
      "  episode_reward_mean: -4.4615999999999625\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 797\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9099188566207885\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00507121164515504\n",
      "          policy_loss: 0.120610336224652\n",
      "          total_loss: 0.1066594530103935\n",
      "          vf_explained_var: 0.9112955927848816\n",
      "          vf_loss: 0.0025459759299539856\n",
      "    num_agent_steps_sampled: 291000\n",
      "    num_agent_steps_trained: 291000\n",
      "    num_steps_sampled: 291000\n",
      "    num_steps_trained: 291000\n",
      "  iterations_since_restore: 291\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.58620689655172\n",
      "    ram_util_percent: 32.32413793103447\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036700346052134236\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.618801492844597\n",
      "    mean_inference_ms: 2.11230622155994\n",
      "    mean_raw_obs_processing_ms: 2.272123446150425\n",
      "  time_since_restore: 6946.7726521492\n",
      "  time_this_iter_s: 20.26598310470581\n",
      "  time_total_s: 6946.7726521492\n",
      "  timers:\n",
      "    learn_throughput: 1327.88\n",
      "    learn_time_ms: 753.08\n",
      "    load_throughput: 41723.449\n",
      "    load_time_ms: 23.967\n",
      "    sample_throughput: 43.034\n",
      "    sample_time_ms: 23237.239\n",
      "    update_time_ms: 3.965\n",
      "  timestamp: 1635289524\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 291000\n",
      "  training_iteration: 291\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   291</td><td style=\"text-align: right;\">         6946.77</td><td style=\"text-align: right;\">291000</td><td style=\"text-align: right;\"> -4.4616</td><td style=\"text-align: right;\">                5.53</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            400.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 292000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-05-44\n",
      "  done: false\n",
      "  episode_len_mean: 398.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.530000000000005\n",
      "  episode_reward_mean: -4.424899999999962\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 799\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8926772541469998\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008056046385927433\n",
      "          policy_loss: -0.09302623122930527\n",
      "          total_loss: -0.1031689499815305\n",
      "          vf_explained_var: 0.5272597074508667\n",
      "          vf_loss: 0.004650035964570836\n",
      "    num_agent_steps_sampled: 292000\n",
      "    num_agent_steps_trained: 292000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "  iterations_since_restore: 292\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.66551724137932\n",
      "    ram_util_percent: 32.34482758620689\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670047854229563\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.61245153208219\n",
      "    mean_inference_ms: 2.1122927329556522\n",
      "    mean_raw_obs_processing_ms: 2.2748855710723497\n",
      "  time_since_restore: 6966.836881637573\n",
      "  time_this_iter_s: 20.064229488372803\n",
      "  time_total_s: 6966.836881637573\n",
      "  timers:\n",
      "    learn_throughput: 1328.667\n",
      "    learn_time_ms: 752.634\n",
      "    load_throughput: 41743.008\n",
      "    load_time_ms: 23.956\n",
      "    sample_throughput: 43.032\n",
      "    sample_time_ms: 23238.268\n",
      "    update_time_ms: 3.964\n",
      "  timestamp: 1635289544\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 292\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   292</td><td style=\"text-align: right;\">         6966.84</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\"> -4.4249</td><td style=\"text-align: right;\">                5.53</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            398.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 293000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-06-03\n",
      "  done: false\n",
      "  episode_len_mean: 398.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.530000000000005\n",
      "  episode_reward_mean: -4.367999999999963\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 802\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9535734401808844\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006973852037221009\n",
      "          policy_loss: 0.0486263867881563\n",
      "          total_loss: 0.03887723419401381\n",
      "          vf_explained_var: 0.286708801984787\n",
      "          vf_loss: 0.006207904286889566\n",
      "    num_agent_steps_sampled: 293000\n",
      "    num_agent_steps_trained: 293000\n",
      "    num_steps_sampled: 293000\n",
      "    num_steps_trained: 293000\n",
      "  iterations_since_restore: 293\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.64814814814815\n",
      "    ram_util_percent: 32.35185185185184\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670070015771582\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.60316643283197\n",
      "    mean_inference_ms: 2.11227408276418\n",
      "    mean_raw_obs_processing_ms: 2.2791302121316277\n",
      "  time_since_restore: 6985.677314281464\n",
      "  time_this_iter_s: 18.84043264389038\n",
      "  time_total_s: 6985.677314281464\n",
      "  timers:\n",
      "    learn_throughput: 1327.746\n",
      "    learn_time_ms: 753.156\n",
      "    load_throughput: 41773.565\n",
      "    load_time_ms: 23.939\n",
      "    sample_throughput: 43.304\n",
      "    sample_time_ms: 23092.539\n",
      "    update_time_ms: 3.891\n",
      "  timestamp: 1635289563\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 293000\n",
      "  training_iteration: 293\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   293</td><td style=\"text-align: right;\">         6985.68</td><td style=\"text-align: right;\">293000</td><td style=\"text-align: right;\">  -4.368</td><td style=\"text-align: right;\">                5.53</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">             398.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 294000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-06-22\n",
      "  done: false\n",
      "  episode_len_mean: 395.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.530000000000005\n",
      "  episode_reward_mean: -4.345299999999963\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 804\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9411897646056282\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014047587843896843\n",
      "          policy_loss: -0.0471730910655525\n",
      "          total_loss: 0.05076198784841431\n",
      "          vf_explained_var: -0.1574135720729828\n",
      "          vf_loss: 0.11013835924354175\n",
      "    num_agent_steps_sampled: 294000\n",
      "    num_agent_steps_trained: 294000\n",
      "    num_steps_sampled: 294000\n",
      "    num_steps_trained: 294000\n",
      "  iterations_since_restore: 294\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.57407407407408\n",
      "    ram_util_percent: 32.34444444444444\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036700862948678005\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.597255022925896\n",
      "    mean_inference_ms: 2.1122627530411116\n",
      "    mean_raw_obs_processing_ms: 2.281997690926869\n",
      "  time_since_restore: 7004.821128845215\n",
      "  time_this_iter_s: 19.14381456375122\n",
      "  time_total_s: 7004.821128845215\n",
      "  timers:\n",
      "    learn_throughput: 1326.201\n",
      "    learn_time_ms: 754.033\n",
      "    load_throughput: 41776.81\n",
      "    load_time_ms: 23.937\n",
      "    sample_throughput: 43.777\n",
      "    sample_time_ms: 22843.014\n",
      "    update_time_ms: 3.894\n",
      "  timestamp: 1635289582\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 294000\n",
      "  training_iteration: 294\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   294</td><td style=\"text-align: right;\">         7004.82</td><td style=\"text-align: right;\">294000</td><td style=\"text-align: right;\"> -4.3453</td><td style=\"text-align: right;\">                5.53</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            395.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 295000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-06-42\n",
      "  done: false\n",
      "  episode_len_mean: 395.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.530000000000005\n",
      "  episode_reward_mean: -4.343099999999963\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 807\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.849352737267812\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007770031934407272\n",
      "          policy_loss: 0.0019649676978588104\n",
      "          total_loss: -0.00537921032971806\n",
      "          vf_explained_var: 0.5006734728813171\n",
      "          vf_loss: 0.007162103408740627\n",
      "    num_agent_steps_sampled: 295000\n",
      "    num_agent_steps_trained: 295000\n",
      "    num_steps_sampled: 295000\n",
      "    num_steps_trained: 295000\n",
      "  iterations_since_restore: 295\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.85357142857143\n",
      "    ram_util_percent: 32.30714285714286\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670108614507888\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.58848481075355\n",
      "    mean_inference_ms: 2.112244742067262\n",
      "    mean_raw_obs_processing_ms: 2.286279644460028\n",
      "  time_since_restore: 7024.270357370377\n",
      "  time_this_iter_s: 19.449228525161743\n",
      "  time_total_s: 7024.270357370377\n",
      "  timers:\n",
      "    learn_throughput: 1325.723\n",
      "    learn_time_ms: 754.306\n",
      "    load_throughput: 41781.638\n",
      "    load_time_ms: 23.934\n",
      "    sample_throughput: 52.353\n",
      "    sample_time_ms: 19101.216\n",
      "    update_time_ms: 3.889\n",
      "  timestamp: 1635289602\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 295000\n",
      "  training_iteration: 295\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   295</td><td style=\"text-align: right;\">         7024.27</td><td style=\"text-align: right;\">295000</td><td style=\"text-align: right;\"> -4.3431</td><td style=\"text-align: right;\">                5.53</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            395.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-07-04\n",
      "  done: false\n",
      "  episode_len_mean: 393.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.530000000000005\n",
      "  episode_reward_mean: -4.310099999999964\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 810\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.615348948372735\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004506457111841142\n",
      "          policy_loss: 0.006541941232151455\n",
      "          total_loss: -0.0012124780151579115\n",
      "          vf_explained_var: 0.7153885960578918\n",
      "          vf_loss: 0.006086549610416922\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "  iterations_since_restore: 296\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.01612903225808\n",
      "    ram_util_percent: 32.280645161290316\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670130020120274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.58036662280739\n",
      "    mean_inference_ms: 2.112226223810769\n",
      "    mean_raw_obs_processing_ms: 2.290660697419383\n",
      "  time_since_restore: 7046.357697725296\n",
      "  time_this_iter_s: 22.087340354919434\n",
      "  time_total_s: 7046.357697725296\n",
      "  timers:\n",
      "    learn_throughput: 1328.003\n",
      "    learn_time_ms: 753.011\n",
      "    load_throughput: 41908.507\n",
      "    load_time_ms: 23.862\n",
      "    sample_throughput: 51.834\n",
      "    sample_time_ms: 19292.404\n",
      "    update_time_ms: 3.797\n",
      "  timestamp: 1635289624\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 296\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   296</td><td style=\"text-align: right;\">         7046.36</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\"> -4.3101</td><td style=\"text-align: right;\">                5.53</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            393.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 297000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-07-43\n",
      "  done: false\n",
      "  episode_len_mean: 391.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.530000000000005\n",
      "  episode_reward_mean: -4.274299999999964\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 812\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2565784513950349\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7971637792057462\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009564549643077699\n",
      "          policy_loss: -0.11525553382105297\n",
      "          total_loss: -0.1245700791478157\n",
      "          vf_explained_var: 0.21835371851921082\n",
      "          vf_loss: 0.0062030380145491415\n",
      "    num_agent_steps_sampled: 297000\n",
      "    num_agent_steps_trained: 297000\n",
      "    num_steps_sampled: 297000\n",
      "    num_steps_trained: 297000\n",
      "  iterations_since_restore: 297\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.916071428571435\n",
      "    ram_util_percent: 32.1375\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036701476837724636\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.575396373547598\n",
      "    mean_inference_ms: 2.1122160237219534\n",
      "    mean_raw_obs_processing_ms: 2.2947664558472813\n",
      "  time_since_restore: 7085.260987281799\n",
      "  time_this_iter_s: 38.903289556503296\n",
      "  time_total_s: 7085.260987281799\n",
      "  timers:\n",
      "    learn_throughput: 1332.087\n",
      "    learn_time_ms: 750.702\n",
      "    load_throughput: 41981.789\n",
      "    load_time_ms: 23.82\n",
      "    sample_throughput: 47.389\n",
      "    sample_time_ms: 21101.923\n",
      "    update_time_ms: 3.745\n",
      "  timestamp: 1635289663\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 297000\n",
      "  training_iteration: 297\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   297</td><td style=\"text-align: right;\">         7085.26</td><td style=\"text-align: right;\">297000</td><td style=\"text-align: right;\"> -4.2743</td><td style=\"text-align: right;\">                5.53</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            391.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 298000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-08-05\n",
      "  done: false\n",
      "  episode_len_mean: 388.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.530000000000005\n",
      "  episode_reward_mean: -4.298199999999963\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 815\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2565784513950349\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4879438241322835\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008276456345901965\n",
      "          policy_loss: -0.05867435874210464\n",
      "          total_loss: -0.05704856717752086\n",
      "          vf_explained_var: 0.2995237112045288\n",
      "          vf_loss: 0.014381670703490576\n",
      "    num_agent_steps_sampled: 298000\n",
      "    num_agent_steps_trained: 298000\n",
      "    num_steps_sampled: 298000\n",
      "    num_steps_trained: 298000\n",
      "  iterations_since_restore: 298\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.0225806451613\n",
      "    ram_util_percent: 32.1967741935484\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367017649445578\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.568373426245707\n",
      "    mean_inference_ms: 2.1122025103799533\n",
      "    mean_raw_obs_processing_ms: 2.300933901437407\n",
      "  time_since_restore: 7107.449439287186\n",
      "  time_this_iter_s: 22.188452005386353\n",
      "  time_total_s: 7107.449439287186\n",
      "  timers:\n",
      "    learn_throughput: 1332.615\n",
      "    learn_time_ms: 750.404\n",
      "    load_throughput: 41953.99\n",
      "    load_time_ms: 23.836\n",
      "    sample_throughput: 47.097\n",
      "    sample_time_ms: 21232.671\n",
      "    update_time_ms: 3.644\n",
      "  timestamp: 1635289685\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 298000\n",
      "  training_iteration: 298\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   298</td><td style=\"text-align: right;\">         7107.45</td><td style=\"text-align: right;\">298000</td><td style=\"text-align: right;\"> -4.2982</td><td style=\"text-align: right;\">                5.53</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            388.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 299000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-08-31\n",
      "  done: false\n",
      "  episode_len_mean: 383.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.530000000000005\n",
      "  episode_reward_mean: -4.266999999999964\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 819\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2565784513950349\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4925471001201205\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005721037658847116\n",
      "          policy_loss: 0.1458091297083431\n",
      "          total_loss: 0.1409029988778962\n",
      "          vf_explained_var: 0.6800746321678162\n",
      "          vf_loss: 0.008551444066688419\n",
      "    num_agent_steps_sampled: 299000\n",
      "    num_agent_steps_trained: 299000\n",
      "    num_steps_sampled: 299000\n",
      "    num_steps_trained: 299000\n",
      "  iterations_since_restore: 299\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.64864864864865\n",
      "    ram_util_percent: 32.208108108108114\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367022171121584\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.56039313053902\n",
      "    mean_inference_ms: 2.1121895546700937\n",
      "    mean_raw_obs_processing_ms: 2.3093285761728177\n",
      "  time_since_restore: 7133.196984291077\n",
      "  time_this_iter_s: 25.74754500389099\n",
      "  time_total_s: 7133.196984291077\n",
      "  timers:\n",
      "    learn_throughput: 1328.593\n",
      "    learn_time_ms: 752.676\n",
      "    load_throughput: 41885.657\n",
      "    load_time_ms: 23.875\n",
      "    sample_throughput: 45.923\n",
      "    sample_time_ms: 21775.671\n",
      "    update_time_ms: 3.646\n",
      "  timestamp: 1635289711\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 299000\n",
      "  training_iteration: 299\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">          7133.2</td><td style=\"text-align: right;\">299000</td><td style=\"text-align: right;\">  -4.267</td><td style=\"text-align: right;\">                5.53</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            383.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-08-56\n",
      "  done: false\n",
      "  episode_len_mean: 380.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.530000000000005\n",
      "  episode_reward_mean: -4.283499999999965\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 822\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2565784513950349\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7226395183139378\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02123436528893655\n",
      "          policy_loss: 0.01916178365548452\n",
      "          total_loss: 0.1620778633074628\n",
      "          vf_explained_var: -5.930529732722789e-05\n",
      "          vf_loss: 0.15469419401552942\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_agent_steps_trained: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "  iterations_since_restore: 300\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.82222222222222\n",
      "    ram_util_percent: 32.24444444444445\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036702604987699027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.55507770995157\n",
      "    mean_inference_ms: 2.1121823632060845\n",
      "    mean_raw_obs_processing_ms: 2.315672055068019\n",
      "  time_since_restore: 7158.136536836624\n",
      "  time_this_iter_s: 24.939552545547485\n",
      "  time_total_s: 7158.136536836624\n",
      "  timers:\n",
      "    learn_throughput: 1330.618\n",
      "    learn_time_ms: 751.531\n",
      "    load_throughput: 41901.055\n",
      "    load_time_ms: 23.866\n",
      "    sample_throughput: 44.683\n",
      "    sample_time_ms: 22379.781\n",
      "    update_time_ms: 3.615\n",
      "  timestamp: 1635289736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 300\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   300</td><td style=\"text-align: right;\">         7158.14</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\"> -4.2835</td><td style=\"text-align: right;\">                5.53</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            380.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 301000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-09-20\n",
      "  done: false\n",
      "  episode_len_mean: 379.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.530000000000005\n",
      "  episode_reward_mean: -4.163199999999963\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 825\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5638733943303427\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.030222635850644625\n",
      "          policy_loss: -0.03605732247233391\n",
      "          total_loss: 0.05672811439467801\n",
      "          vf_explained_var: 0.43455442786216736\n",
      "          vf_loss: 0.09679245972074568\n",
      "    num_agent_steps_sampled: 301000\n",
      "    num_agent_steps_trained: 301000\n",
      "    num_steps_sampled: 301000\n",
      "    num_steps_trained: 301000\n",
      "  iterations_since_restore: 301\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.66857142857143\n",
      "    ram_util_percent: 32.30857142857142\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670300461850163\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.55015227263767\n",
      "    mean_inference_ms: 2.112181600013908\n",
      "    mean_raw_obs_processing_ms: 2.320159582615018\n",
      "  time_since_restore: 7182.999018907547\n",
      "  time_this_iter_s: 24.86248207092285\n",
      "  time_total_s: 7182.999018907547\n",
      "  timers:\n",
      "    learn_throughput: 1333.197\n",
      "    learn_time_ms: 750.077\n",
      "    load_throughput: 41907.502\n",
      "    load_time_ms: 23.862\n",
      "    sample_throughput: 43.781\n",
      "    sample_time_ms: 22840.884\n",
      "    update_time_ms: 3.618\n",
      "  timestamp: 1635289760\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 301000\n",
      "  training_iteration: 301\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   301</td><td style=\"text-align: right;\">            7183</td><td style=\"text-align: right;\">301000</td><td style=\"text-align: right;\"> -4.1632</td><td style=\"text-align: right;\">                5.53</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            379.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 302000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-10-29\n",
      "  done: false\n",
      "  episode_len_mean: 372.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.530000000000005\n",
      "  episode_reward_mean: -4.129699999999965\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 829\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5773015156388283\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.495784666803148\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.022934917194812995\n",
      "          policy_loss: 0.016746337546242607\n",
      "          total_loss: 0.4383596905403667\n",
      "          vf_explained_var: 0.3194805681705475\n",
      "          vf_loss: 0.4233308404684067\n",
      "    num_agent_steps_sampled: 302000\n",
      "    num_agent_steps_trained: 302000\n",
      "    num_steps_sampled: 302000\n",
      "    num_steps_trained: 302000\n",
      "  iterations_since_restore: 302\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.92448979591836\n",
      "    ram_util_percent: 32.30204081632654\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036703535639321466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.54515002129533\n",
      "    mean_inference_ms: 2.1121839093592154\n",
      "    mean_raw_obs_processing_ms: 2.3314897479761005\n",
      "  time_since_restore: 7251.400958538055\n",
      "  time_this_iter_s: 68.40193963050842\n",
      "  time_total_s: 7251.400958538055\n",
      "  timers:\n",
      "    learn_throughput: 1332.267\n",
      "    learn_time_ms: 750.6\n",
      "    load_throughput: 41946.312\n",
      "    load_time_ms: 23.84\n",
      "    sample_throughput: 36.135\n",
      "    sample_time_ms: 27674.151\n",
      "    update_time_ms: 3.617\n",
      "  timestamp: 1635289829\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 302000\n",
      "  training_iteration: 302\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   302</td><td style=\"text-align: right;\">          7251.4</td><td style=\"text-align: right;\">302000</td><td style=\"text-align: right;\"> -4.1297</td><td style=\"text-align: right;\">                5.53</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            372.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 303000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-11-35\n",
      "  done: false\n",
      "  episode_len_mean: 367.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.530000000000005\n",
      "  episode_reward_mean: -3.967899999999965\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 833\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8659522734582428\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.657802669207255\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0068204147468965935\n",
      "          policy_loss: -0.0724066817926036\n",
      "          total_loss: 0.10688570845458242\n",
      "          vf_explained_var: 0.4857064485549927\n",
      "          vf_loss: 0.18996426600755917\n",
      "    num_agent_steps_sampled: 303000\n",
      "    num_agent_steps_trained: 303000\n",
      "    num_steps_sampled: 303000\n",
      "    num_steps_trained: 303000\n",
      "  iterations_since_restore: 303\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.46736842105264\n",
      "    ram_util_percent: 32.23789473684209\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036704089738956676\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.541011146856896\n",
      "    mean_inference_ms: 2.1121891340178127\n",
      "    mean_raw_obs_processing_ms: 2.348067820505278\n",
      "  time_since_restore: 7317.606793165207\n",
      "  time_this_iter_s: 66.20583462715149\n",
      "  time_total_s: 7317.606793165207\n",
      "  timers:\n",
      "    learn_throughput: 1331.61\n",
      "    learn_time_ms: 750.971\n",
      "    load_throughput: 41829.723\n",
      "    load_time_ms: 23.906\n",
      "    sample_throughput: 30.854\n",
      "    sample_time_ms: 32410.261\n",
      "    update_time_ms: 3.619\n",
      "  timestamp: 1635289895\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 303000\n",
      "  training_iteration: 303\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   303</td><td style=\"text-align: right;\">         7317.61</td><td style=\"text-align: right;\">303000</td><td style=\"text-align: right;\"> -3.9679</td><td style=\"text-align: right;\">                5.53</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">            367.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-12-03\n",
      "  done: false\n",
      "  episode_len_mean: 363.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.530000000000005\n",
      "  episode_reward_mean: -3.7486999999999653\n",
      "  episode_reward_min: -15.389999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 837\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8659522734582428\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7287214159965516\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009294639405474411\n",
      "          policy_loss: -0.05952277382214864\n",
      "          total_loss: 0.13886750555700725\n",
      "          vf_explained_var: 0.2859959304332733\n",
      "          vf_loss: 0.20762878213491703\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_agent_steps_trained: 304000\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "  iterations_since_restore: 304\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.09487179487179\n",
      "    ram_util_percent: 32.26410256410257\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036704717386141485\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.538267030482476\n",
      "    mean_inference_ms: 2.112199425805146\n",
      "    mean_raw_obs_processing_ms: 2.364859450434786\n",
      "  time_since_restore: 7345.177613019943\n",
      "  time_this_iter_s: 27.570819854736328\n",
      "  time_total_s: 7345.177613019943\n",
      "  timers:\n",
      "    learn_throughput: 1333.343\n",
      "    learn_time_ms: 749.995\n",
      "    load_throughput: 41776.644\n",
      "    load_time_ms: 23.937\n",
      "    sample_throughput: 30.072\n",
      "    sample_time_ms: 33253.913\n",
      "    update_time_ms: 3.613\n",
      "  timestamp: 1635289923\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 304\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   304</td><td style=\"text-align: right;\">         7345.18</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\"> -3.7487</td><td style=\"text-align: right;\">                5.53</td><td style=\"text-align: right;\">              -15.39</td><td style=\"text-align: right;\">               363</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 305000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-12-33\n",
      "  done: false\n",
      "  episode_len_mean: 356.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.530000000000005\n",
      "  episode_reward_mean: -3.4083999999999666\n",
      "  episode_reward_min: -9.809999999999963\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 841\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8659522734582428\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.517671322822571\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014122943401950019\n",
      "          policy_loss: -0.027784924623039033\n",
      "          total_loss: 0.33420864186353155\n",
      "          vf_explained_var: 0.5018125176429749\n",
      "          vf_loss: 0.36494048257461853\n",
      "    num_agent_steps_sampled: 305000\n",
      "    num_agent_steps_trained: 305000\n",
      "    num_steps_sampled: 305000\n",
      "    num_steps_trained: 305000\n",
      "  iterations_since_restore: 305\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.55813953488374\n",
      "    ram_util_percent: 32.16511627906977\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670540637121184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.537262897333356\n",
      "    mean_inference_ms: 2.11221503283022\n",
      "    mean_raw_obs_processing_ms: 2.381852580393678\n",
      "  time_since_restore: 7375.5455503463745\n",
      "  time_this_iter_s: 30.367937326431274\n",
      "  time_total_s: 7375.5455503463745\n",
      "  timers:\n",
      "    learn_throughput: 1333.118\n",
      "    learn_time_ms: 750.121\n",
      "    load_throughput: 41887.455\n",
      "    load_time_ms: 23.873\n",
      "    sample_throughput: 29.116\n",
      "    sample_time_ms: 34345.69\n",
      "    update_time_ms: 3.633\n",
      "  timestamp: 1635289953\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 305000\n",
      "  training_iteration: 305\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   305</td><td style=\"text-align: right;\">         7375.55</td><td style=\"text-align: right;\">305000</td><td style=\"text-align: right;\"> -3.4084</td><td style=\"text-align: right;\">                5.53</td><td style=\"text-align: right;\">               -9.81</td><td style=\"text-align: right;\">            356.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 306000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-13-06\n",
      "  done: false\n",
      "  episode_len_mean: 351.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.6700000000000275\n",
      "  episode_reward_mean: -3.092699999999967\n",
      "  episode_reward_min: -9.519999999999941\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 845\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8659522734582428\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6393849094708761\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0171993155779395\n",
      "          policy_loss: -0.0429238885641098\n",
      "          total_loss: 0.9499171717299355\n",
      "          vf_explained_var: 0.6941211819648743\n",
      "          vf_loss: 0.9943411296440495\n",
      "    num_agent_steps_sampled: 306000\n",
      "    num_agent_steps_trained: 306000\n",
      "    num_steps_sampled: 306000\n",
      "    num_steps_trained: 306000\n",
      "  iterations_since_restore: 306\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.6531914893617\n",
      "    ram_util_percent: 32.17872340425532\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367057784371534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.538103514530754\n",
      "    mean_inference_ms: 2.112232282233716\n",
      "    mean_raw_obs_processing_ms: 2.3947560483342345\n",
      "  time_since_restore: 7408.244171380997\n",
      "  time_this_iter_s: 32.69862103462219\n",
      "  time_total_s: 7408.244171380997\n",
      "  timers:\n",
      "    learn_throughput: 1327.774\n",
      "    learn_time_ms: 753.14\n",
      "    load_throughput: 41588.787\n",
      "    load_time_ms: 24.045\n",
      "    sample_throughput: 28.246\n",
      "    sample_time_ms: 35403.647\n",
      "    update_time_ms: 3.629\n",
      "  timestamp: 1635289986\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 306000\n",
      "  training_iteration: 306\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   306</td><td style=\"text-align: right;\">         7408.24</td><td style=\"text-align: right;\">306000</td><td style=\"text-align: right;\"> -3.0927</td><td style=\"text-align: right;\">                5.67</td><td style=\"text-align: right;\">               -9.52</td><td style=\"text-align: right;\">            351.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 307000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-13-36\n",
      "  done: false\n",
      "  episode_len_mean: 346.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.6700000000000275\n",
      "  episode_reward_mean: -3.0256999999999663\n",
      "  episode_reward_min: -9.519999999999941\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 848\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8659522734582428\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6577290693918865\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012392794389461514\n",
      "          policy_loss: -0.05182886082265112\n",
      "          total_loss: 1.0884597649176915\n",
      "          vf_explained_var: 0.4481945335865021\n",
      "          vf_loss: 1.146134360631307\n",
      "    num_agent_steps_sampled: 307000\n",
      "    num_agent_steps_trained: 307000\n",
      "    num_steps_sampled: 307000\n",
      "    num_steps_trained: 307000\n",
      "  iterations_since_restore: 307\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.36046511627906\n",
      "    ram_util_percent: 32.30930232558139\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670566848409387\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.539709362236\n",
      "    mean_inference_ms: 2.112232198248399\n",
      "    mean_raw_obs_processing_ms: 2.4024471530171714\n",
      "  time_since_restore: 7438.329444646835\n",
      "  time_this_iter_s: 30.085273265838623\n",
      "  time_total_s: 7438.329444646835\n",
      "  timers:\n",
      "    learn_throughput: 1328.099\n",
      "    learn_time_ms: 752.956\n",
      "    load_throughput: 41203.519\n",
      "    load_time_ms: 24.27\n",
      "    sample_throughput: 28.967\n",
      "    sample_time_ms: 34521.781\n",
      "    update_time_ms: 3.633\n",
      "  timestamp: 1635290016\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 307000\n",
      "  training_iteration: 307\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   307</td><td style=\"text-align: right;\">         7438.33</td><td style=\"text-align: right;\">307000</td><td style=\"text-align: right;\"> -3.0257</td><td style=\"text-align: right;\">                5.67</td><td style=\"text-align: right;\">               -9.52</td><td style=\"text-align: right;\">            346.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 308000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-15-05\n",
      "  done: false\n",
      "  episode_len_mean: 333.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.6700000000000275\n",
      "  episode_reward_mean: -2.7198999999999676\n",
      "  episode_reward_min: -9.519999999999941\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 854\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8659522734582428\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.524781756930881\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00876284837470773\n",
      "          policy_loss: -0.060213596125443775\n",
      "          total_loss: 0.3694758315467172\n",
      "          vf_explained_var: 0.6546421647071838\n",
      "          vf_loss: 0.4373490405579408\n",
      "    num_agent_steps_sampled: 308000\n",
      "    num_agent_steps_trained: 308000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "  iterations_since_restore: 308\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.534645669291336\n",
      "    ram_util_percent: 32.274015748031495\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367050317849377\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.545468294771577\n",
      "    mean_inference_ms: 2.112216808483093\n",
      "    mean_raw_obs_processing_ms: 2.4250815210525065\n",
      "  time_since_restore: 7527.329427957535\n",
      "  time_this_iter_s: 88.99998331069946\n",
      "  time_total_s: 7527.329427957535\n",
      "  timers:\n",
      "    learn_throughput: 1326.531\n",
      "    learn_time_ms: 753.846\n",
      "    load_throughput: 41397.381\n",
      "    load_time_ms: 24.156\n",
      "    sample_throughput: 24.271\n",
      "    sample_time_ms: 41202.067\n",
      "    update_time_ms: 3.718\n",
      "  timestamp: 1635290105\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 308\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   308</td><td style=\"text-align: right;\">         7527.33</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\"> -2.7199</td><td style=\"text-align: right;\">                5.67</td><td style=\"text-align: right;\">               -9.52</td><td style=\"text-align: right;\">            333.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 309000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-16-48\n",
      "  done: false\n",
      "  episode_len_mean: 316.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.6700000000000275\n",
      "  episode_reward_mean: -2.4187999999999694\n",
      "  episode_reward_min: -8.479999999999952\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 860\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8659522734582428\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5014404389593337\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01438916499897885\n",
      "          policy_loss: -0.00786817396680514\n",
      "          total_loss: 1.0190998600588905\n",
      "          vf_explained_var: 0.43308427929878235\n",
      "          vf_loss: 1.0295220977730222\n",
      "    num_agent_steps_sampled: 309000\n",
      "    num_agent_steps_trained: 309000\n",
      "    num_steps_sampled: 309000\n",
      "    num_steps_trained: 309000\n",
      "  iterations_since_restore: 309\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.59315068493151\n",
      "    ram_util_percent: 32.21232876712328\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670388585026135\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.55458419602753\n",
      "    mean_inference_ms: 2.112185137692093\n",
      "    mean_raw_obs_processing_ms: 2.4617662719325346\n",
      "  time_since_restore: 7629.9423360824585\n",
      "  time_this_iter_s: 102.6129081249237\n",
      "  time_total_s: 7629.9423360824585\n",
      "  timers:\n",
      "    learn_throughput: 1329.597\n",
      "    learn_time_ms: 752.108\n",
      "    load_throughput: 41458.678\n",
      "    load_time_ms: 24.12\n",
      "    sample_throughput: 20.454\n",
      "    sample_time_ms: 48890.365\n",
      "    update_time_ms: 3.723\n",
      "  timestamp: 1635290208\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 309000\n",
      "  training_iteration: 309\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   309</td><td style=\"text-align: right;\">         7629.94</td><td style=\"text-align: right;\">309000</td><td style=\"text-align: right;\"> -2.4188</td><td style=\"text-align: right;\">                5.67</td><td style=\"text-align: right;\">               -8.48</td><td style=\"text-align: right;\">            316.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 310000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-19-24\n",
      "  done: false\n",
      "  episode_len_mean: 295.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.6700000000000275\n",
      "  episode_reward_mean: -1.963199999999973\n",
      "  episode_reward_min: -10.49999999999995\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 868\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8659522734582428\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7602985779444376\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012552140034904388\n",
      "          policy_loss: 0.13646701922019322\n",
      "          total_loss: 0.9084712713956833\n",
      "          vf_explained_var: 0.5574313998222351\n",
      "          vf_loss: 0.7787376791238785\n",
      "    num_agent_steps_sampled: 310000\n",
      "    num_agent_steps_trained: 310000\n",
      "    num_steps_sampled: 310000\n",
      "    num_steps_trained: 310000\n",
      "  iterations_since_restore: 310\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.363392857142856\n",
      "    ram_util_percent: 32.21696428571428\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670231795173507\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.571101613522593\n",
      "    mean_inference_ms: 2.112143788008475\n",
      "    mean_raw_obs_processing_ms: 2.543827092209302\n",
      "  time_since_restore: 7786.574425935745\n",
      "  time_this_iter_s: 156.63208985328674\n",
      "  time_total_s: 7786.574425935745\n",
      "  timers:\n",
      "    learn_throughput: 1330.605\n",
      "    learn_time_ms: 751.538\n",
      "    load_throughput: 41351.099\n",
      "    load_time_ms: 24.183\n",
      "    sample_throughput: 16.113\n",
      "    sample_time_ms: 62060.149\n",
      "    update_time_ms: 3.7\n",
      "  timestamp: 1635290364\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 310000\n",
      "  training_iteration: 310\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   310</td><td style=\"text-align: right;\">         7786.57</td><td style=\"text-align: right;\">310000</td><td style=\"text-align: right;\"> -1.9632</td><td style=\"text-align: right;\">                5.67</td><td style=\"text-align: right;\">               -10.5</td><td style=\"text-align: right;\">            295.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 311000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-20-32\n",
      "  done: false\n",
      "  episode_len_mean: 287.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.6700000000000275\n",
      "  episode_reward_mean: -1.8035999999999734\n",
      "  episode_reward_min: -11.399999999999954\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 873\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8659522734582428\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.635550113519033\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007795471351937246\n",
      "          policy_loss: 0.06725499565816588\n",
      "          total_loss: 0.9926906256212129\n",
      "          vf_explained_var: 0.5127375721931458\n",
      "          vf_loss: 0.9350406378507614\n",
      "    num_agent_steps_sampled: 311000\n",
      "    num_agent_steps_trained: 311000\n",
      "    num_steps_sampled: 311000\n",
      "    num_steps_trained: 311000\n",
      "  iterations_since_restore: 311\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.49278350515464\n",
      "    ram_util_percent: 32.246391752577324\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670138688014449\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.583982633221495\n",
      "    mean_inference_ms: 2.1121238532414726\n",
      "    mean_raw_obs_processing_ms: 2.601253381070035\n",
      "  time_since_restore: 7854.663127660751\n",
      "  time_this_iter_s: 68.0887017250061\n",
      "  time_total_s: 7854.663127660751\n",
      "  timers:\n",
      "    learn_throughput: 1330.02\n",
      "    learn_time_ms: 751.868\n",
      "    load_throughput: 41424.161\n",
      "    load_time_ms: 24.141\n",
      "    sample_throughput: 15.064\n",
      "    sample_time_ms: 66382.384\n",
      "    update_time_ms: 3.789\n",
      "  timestamp: 1635290432\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 311000\n",
      "  training_iteration: 311\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   311</td><td style=\"text-align: right;\">         7854.66</td><td style=\"text-align: right;\">311000</td><td style=\"text-align: right;\"> -1.8036</td><td style=\"text-align: right;\">                5.67</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            287.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-21-43\n",
      "  done: false\n",
      "  episode_len_mean: 278.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.6700000000000275\n",
      "  episode_reward_mean: -1.622099999999974\n",
      "  episode_reward_min: -11.399999999999954\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 878\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8659522734582428\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.562542121940189\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011262756202566138\n",
      "          policy_loss: 0.046200666245487\n",
      "          total_loss: 0.8416742321517733\n",
      "          vf_explained_var: 0.6184872984886169\n",
      "          vf_loss: 0.8013459649350908\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "  iterations_since_restore: 312\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.279207920792075\n",
      "    ram_util_percent: 32.22475247524752\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670052294014905\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.5985899359414\n",
      "    mean_inference_ms: 2.112109315183046\n",
      "    mean_raw_obs_processing_ms: 2.6640432187936423\n",
      "  time_since_restore: 7925.542543411255\n",
      "  time_this_iter_s: 70.87941575050354\n",
      "  time_total_s: 7925.542543411255\n",
      "  timers:\n",
      "    learn_throughput: 1331.455\n",
      "    learn_time_ms: 751.058\n",
      "    load_throughput: 41670.391\n",
      "    load_time_ms: 23.998\n",
      "    sample_throughput: 15.008\n",
      "    sample_time_ms: 66631.078\n",
      "    update_time_ms: 3.793\n",
      "  timestamp: 1635290503\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 312\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   312</td><td style=\"text-align: right;\">         7925.54</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\"> -1.6221</td><td style=\"text-align: right;\">                5.67</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            278.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 313000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-22-13\n",
      "  done: false\n",
      "  episode_len_mean: 277.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.6700000000000275\n",
      "  episode_reward_mean: -1.6299999999999744\n",
      "  episode_reward_min: -11.399999999999954\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 882\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8659522734582428\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6745590209960937\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009859013515385623\n",
      "          policy_loss: -0.0608590135557784\n",
      "          total_loss: 0.6879018487201797\n",
      "          vf_explained_var: 0.4018242657184601\n",
      "          vf_loss: 0.7569690161695083\n",
      "    num_agent_steps_sampled: 313000\n",
      "    num_agent_steps_trained: 313000\n",
      "    num_steps_sampled: 313000\n",
      "    num_steps_trained: 313000\n",
      "  iterations_since_restore: 313\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.01904761904763\n",
      "    ram_util_percent: 32.26428571428571\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036699875397626966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.611435249772224\n",
      "    mean_inference_ms: 2.1121014829113722\n",
      "    mean_raw_obs_processing_ms: 2.7101135458145778\n",
      "  time_since_restore: 7954.993030309677\n",
      "  time_this_iter_s: 29.45048689842224\n",
      "  time_total_s: 7954.993030309677\n",
      "  timers:\n",
      "    learn_throughput: 1333.663\n",
      "    learn_time_ms: 749.814\n",
      "    load_throughput: 41548.043\n",
      "    load_time_ms: 24.069\n",
      "    sample_throughput: 15.884\n",
      "    sample_time_ms: 62956.735\n",
      "    update_time_ms: 3.793\n",
      "  timestamp: 1635290533\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 313000\n",
      "  training_iteration: 313\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   313</td><td style=\"text-align: right;\">         7954.99</td><td style=\"text-align: right;\">313000</td><td style=\"text-align: right;\">   -1.63</td><td style=\"text-align: right;\">                5.67</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            277.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 314000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-23-53\n",
      "  done: false\n",
      "  episode_len_mean: 271.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.6700000000000275\n",
      "  episode_reward_mean: -1.4787999999999755\n",
      "  episode_reward_min: -11.399999999999954\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 886\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8659522734582428\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9423490444819131\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009599529089615227\n",
      "          policy_loss: -0.04572170310550266\n",
      "          total_loss: 0.35057620170215764\n",
      "          vf_explained_var: 0.34275686740875244\n",
      "          vf_loss: 0.4074086618092325\n",
      "    num_agent_steps_sampled: 314000\n",
      "    num_agent_steps_trained: 314000\n",
      "    num_steps_sampled: 314000\n",
      "    num_steps_trained: 314000\n",
      "  iterations_since_restore: 314\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.96223776223776\n",
      "    ram_util_percent: 32.21748251748252\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036699227279876824\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.625540260633333\n",
      "    mean_inference_ms: 2.112095587342841\n",
      "    mean_raw_obs_processing_ms: 2.7650148919345003\n",
      "  time_since_restore: 8054.841680288315\n",
      "  time_this_iter_s: 99.8486499786377\n",
      "  time_total_s: 8054.841680288315\n",
      "  timers:\n",
      "    learn_throughput: 1333.461\n",
      "    learn_time_ms: 749.928\n",
      "    load_throughput: 41496.25\n",
      "    load_time_ms: 24.099\n",
      "    sample_throughput: 14.248\n",
      "    sample_time_ms: 70184.367\n",
      "    update_time_ms: 3.8\n",
      "  timestamp: 1635290633\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 314000\n",
      "  training_iteration: 314\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   314</td><td style=\"text-align: right;\">         8054.84</td><td style=\"text-align: right;\">314000</td><td style=\"text-align: right;\"> -1.4788</td><td style=\"text-align: right;\">                5.67</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            271.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 315000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-25-05\n",
      "  done: false\n",
      "  episode_len_mean: 265.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.230000000000006\n",
      "  episode_reward_mean: -1.3234999999999764\n",
      "  episode_reward_min: -11.399999999999954\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 890\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8659522734582428\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7277367326948379\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016150402014468688\n",
      "          policy_loss: -0.0207147516310215\n",
      "          total_loss: 0.630356514453888\n",
      "          vf_explained_var: 0.7180262207984924\n",
      "          vf_loss: 0.654363154206011\n",
      "    num_agent_steps_sampled: 315000\n",
      "    num_agent_steps_trained: 315000\n",
      "    num_steps_sampled: 315000\n",
      "    num_steps_trained: 315000\n",
      "  iterations_since_restore: 315\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.057281553398056\n",
      "    ram_util_percent: 32.240776699029134\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669851439537235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.64124529927955\n",
      "    mean_inference_ms: 2.1120910006451648\n",
      "    mean_raw_obs_processing_ms: 2.8249732293905847\n",
      "  time_since_restore: 8126.932604312897\n",
      "  time_this_iter_s: 72.09092402458191\n",
      "  time_total_s: 8126.932604312897\n",
      "  timers:\n",
      "    learn_throughput: 1334.19\n",
      "    learn_time_ms: 749.518\n",
      "    load_throughput: 41443.644\n",
      "    load_time_ms: 24.129\n",
      "    sample_throughput: 13.449\n",
      "    sample_time_ms: 74357.073\n",
      "    update_time_ms: 3.787\n",
      "  timestamp: 1635290705\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 315000\n",
      "  training_iteration: 315\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   315</td><td style=\"text-align: right;\">         8126.93</td><td style=\"text-align: right;\">315000</td><td style=\"text-align: right;\"> -1.3235</td><td style=\"text-align: right;\">                8.23</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            265.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 316000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-25-35\n",
      "  done: false\n",
      "  episode_len_mean: 260.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.230000000000006\n",
      "  episode_reward_mean: -1.303799999999977\n",
      "  episode_reward_min: -11.399999999999954\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 894\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8659522734582428\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9678904983732435\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017835923082631825\n",
      "          policy_loss: 0.06409372107850181\n",
      "          total_loss: 0.834533616900444\n",
      "          vf_explained_var: 0.3068963885307312\n",
      "          vf_loss: 0.7746737440427144\n",
      "    num_agent_steps_sampled: 316000\n",
      "    num_agent_steps_trained: 316000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "  iterations_since_restore: 316\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.6093023255814\n",
      "    ram_util_percent: 32.26744186046512\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669783592266417\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.658370677673705\n",
      "    mean_inference_ms: 2.1120900618339435\n",
      "    mean_raw_obs_processing_ms: 2.8849252733989124\n",
      "  time_since_restore: 8156.892930984497\n",
      "  time_this_iter_s: 29.960326671600342\n",
      "  time_total_s: 8156.892930984497\n",
      "  timers:\n",
      "    learn_throughput: 1338.083\n",
      "    learn_time_ms: 747.338\n",
      "    load_throughput: 41547.014\n",
      "    load_time_ms: 24.069\n",
      "    sample_throughput: 13.498\n",
      "    sample_time_ms: 74085.475\n",
      "    update_time_ms: 3.791\n",
      "  timestamp: 1635290735\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 316\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   316</td><td style=\"text-align: right;\">         8156.89</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\"> -1.3038</td><td style=\"text-align: right;\">                8.23</td><td style=\"text-align: right;\">               -11.4</td><td style=\"text-align: right;\">            260.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 317000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-26-43\n",
      "  done: false\n",
      "  episode_len_mean: 252.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.230000000000006\n",
      "  episode_reward_mean: -1.2887999999999773\n",
      "  episode_reward_min: -14.34999999999998\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 899\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8659522734582428\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0209280305438573\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007394002487694588\n",
      "          policy_loss: -0.2106351002636883\n",
      "          total_loss: 0.26002531413816743\n",
      "          vf_explained_var: 0.5658034682273865\n",
      "          vf_loss: 0.48446683941615953\n",
      "    num_agent_steps_sampled: 317000\n",
      "    num_agent_steps_trained: 317000\n",
      "    num_steps_sampled: 317000\n",
      "    num_steps_trained: 317000\n",
      "  iterations_since_restore: 317\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.156122448979595\n",
      "    ram_util_percent: 32.19183673469388\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0366970768257372\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.681371867910485\n",
      "    mean_inference_ms: 2.112095669812707\n",
      "    mean_raw_obs_processing_ms: 2.9661331413260905\n",
      "  time_since_restore: 8225.666313886642\n",
      "  time_this_iter_s: 68.77338290214539\n",
      "  time_total_s: 8225.666313886642\n",
      "  timers:\n",
      "    learn_throughput: 1335.999\n",
      "    learn_time_ms: 748.504\n",
      "    load_throughput: 41782.636\n",
      "    load_time_ms: 23.933\n",
      "    sample_throughput: 12.828\n",
      "    sample_time_ms: 77953.177\n",
      "    update_time_ms: 3.891\n",
      "  timestamp: 1635290803\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 317000\n",
      "  training_iteration: 317\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   317</td><td style=\"text-align: right;\">         8225.67</td><td style=\"text-align: right;\">317000</td><td style=\"text-align: right;\"> -1.2888</td><td style=\"text-align: right;\">                8.23</td><td style=\"text-align: right;\">              -14.35</td><td style=\"text-align: right;\">             252.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 318000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-28-44\n",
      "  done: false\n",
      "  episode_len_mean: 237.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.230000000000006\n",
      "  episode_reward_mean: -1.191299999999979\n",
      "  episode_reward_min: -14.34999999999998\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 905\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8659522734582428\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7260056853294372\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021938684452900623\n",
      "          policy_loss: 0.11025371799866358\n",
      "          total_loss: 1.6850134485297732\n",
      "          vf_explained_var: 0.5399739742279053\n",
      "          vf_loss: 1.573021951980061\n",
      "    num_agent_steps_sampled: 318000\n",
      "    num_agent_steps_trained: 318000\n",
      "    num_steps_sampled: 318000\n",
      "    num_steps_trained: 318000\n",
      "  iterations_since_restore: 318\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.45813953488373\n",
      "    ram_util_percent: 32.26279069767441\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669625601234269\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.711510901857704\n",
      "    mean_inference_ms: 2.1121099526328226\n",
      "    mean_raw_obs_processing_ms: 3.0809823087083457\n",
      "  time_since_restore: 8346.099916696548\n",
      "  time_this_iter_s: 120.433602809906\n",
      "  time_total_s: 8346.099916696548\n",
      "  timers:\n",
      "    learn_throughput: 1339.148\n",
      "    learn_time_ms: 746.744\n",
      "    load_throughput: 41625.975\n",
      "    load_time_ms: 24.023\n",
      "    sample_throughput: 12.331\n",
      "    sample_time_ms: 81098.291\n",
      "    update_time_ms: 3.81\n",
      "  timestamp: 1635290924\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 318000\n",
      "  training_iteration: 318\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   318</td><td style=\"text-align: right;\">          8346.1</td><td style=\"text-align: right;\">318000</td><td style=\"text-align: right;\"> -1.1913</td><td style=\"text-align: right;\">                8.23</td><td style=\"text-align: right;\">              -14.35</td><td style=\"text-align: right;\">            237.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 319000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-29-15\n",
      "  done: false\n",
      "  episode_len_mean: 234.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.230000000000006\n",
      "  episode_reward_mean: -1.1509999999999792\n",
      "  episode_reward_min: -14.34999999999998\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 908\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2989284101873633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.915290449725257\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011515005193097573\n",
      "          policy_loss: 0.03390086396700806\n",
      "          total_loss: 1.0153373209138712\n",
      "          vf_explained_var: 0.4146987795829773\n",
      "          vf_loss: 0.9856322026915021\n",
      "    num_agent_steps_sampled: 319000\n",
      "    num_agent_steps_trained: 319000\n",
      "    num_steps_sampled: 319000\n",
      "    num_steps_trained: 319000\n",
      "  iterations_since_restore: 319\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.06444444444445\n",
      "    ram_util_percent: 32.31777777777778\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669587439693925\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.727753467346503\n",
      "    mean_inference_ms: 2.1121212717411426\n",
      "    mean_raw_obs_processing_ms: 3.1383894473281266\n",
      "  time_since_restore: 8377.597911834717\n",
      "  time_this_iter_s: 31.497995138168335\n",
      "  time_total_s: 8377.597911834717\n",
      "  timers:\n",
      "    learn_throughput: 1336.809\n",
      "    learn_time_ms: 748.05\n",
      "    load_throughput: 41579.923\n",
      "    load_time_ms: 24.05\n",
      "    sample_throughput: 13.516\n",
      "    sample_time_ms: 73985.455\n",
      "    update_time_ms: 3.823\n",
      "  timestamp: 1635290955\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 319000\n",
      "  training_iteration: 319\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   319</td><td style=\"text-align: right;\">          8377.6</td><td style=\"text-align: right;\">319000</td><td style=\"text-align: right;\">  -1.151</td><td style=\"text-align: right;\">                8.23</td><td style=\"text-align: right;\">              -14.35</td><td style=\"text-align: right;\">            234.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-30-59\n",
      "  done: false\n",
      "  episode_len_mean: 224.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.230000000000006\n",
      "  episode_reward_mean: -1.0313999999999799\n",
      "  episode_reward_min: -14.34999999999998\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 914\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2989284101873633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.745900321006775\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0113457283552814\n",
      "          policy_loss: 0.029807388285795846\n",
      "          total_loss: 0.9541744271914164\n",
      "          vf_explained_var: 0.4578646421432495\n",
      "          vf_loss: 0.927088760998514\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_agent_steps_trained: 320000\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "  iterations_since_restore: 320\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.92770270270271\n",
      "    ram_util_percent: 32.260135135135144\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036695111668574996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.76147479020044\n",
      "    mean_inference_ms: 2.112146810521902\n",
      "    mean_raw_obs_processing_ms: 3.2647692508834125\n",
      "  time_since_restore: 8481.202205181122\n",
      "  time_this_iter_s: 103.60429334640503\n",
      "  time_total_s: 8481.202205181122\n",
      "  timers:\n",
      "    learn_throughput: 1334.211\n",
      "    learn_time_ms: 749.507\n",
      "    load_throughput: 41646.186\n",
      "    load_time_ms: 24.012\n",
      "    sample_throughput: 14.56\n",
      "    sample_time_ms: 68681.332\n",
      "    update_time_ms: 3.738\n",
      "  timestamp: 1635291059\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 320\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   320</td><td style=\"text-align: right;\">          8481.2</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\"> -1.0314</td><td style=\"text-align: right;\">                8.23</td><td style=\"text-align: right;\">              -14.35</td><td style=\"text-align: right;\">            224.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 321000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-31-32\n",
      "  done: false\n",
      "  episode_len_mean: 222.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.230000000000006\n",
      "  episode_reward_mean: -0.95899999999998\n",
      "  episode_reward_min: -14.34999999999998\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 918\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2989284101873633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9016142037179735\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005817505895396735\n",
      "          policy_loss: -0.01833261458410157\n",
      "          total_loss: 0.6841111601226859\n",
      "          vf_explained_var: 0.4798163175582886\n",
      "          vf_loss: 0.7139033940103319\n",
      "    num_agent_steps_sampled: 321000\n",
      "    num_agent_steps_trained: 321000\n",
      "    num_steps_sampled: 321000\n",
      "    num_steps_trained: 321000\n",
      "  iterations_since_restore: 321\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.77234042553192\n",
      "    ram_util_percent: 32.270212765957446\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669460902213561\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.784848680745057\n",
      "    mean_inference_ms: 2.112165353809198\n",
      "    mean_raw_obs_processing_ms: 3.3482519198776504\n",
      "  time_since_restore: 8514.48630309105\n",
      "  time_this_iter_s: 33.28409790992737\n",
      "  time_total_s: 8514.48630309105\n",
      "  timers:\n",
      "    learn_throughput: 1333.74\n",
      "    learn_time_ms: 749.772\n",
      "    load_throughput: 41659.133\n",
      "    load_time_ms: 24.004\n",
      "    sample_throughput: 15.337\n",
      "    sample_time_ms: 65200.721\n",
      "    update_time_ms: 3.649\n",
      "  timestamp: 1635291092\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 321000\n",
      "  training_iteration: 321\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   321</td><td style=\"text-align: right;\">         8514.49</td><td style=\"text-align: right;\">321000</td><td style=\"text-align: right;\">  -0.959</td><td style=\"text-align: right;\">                8.23</td><td style=\"text-align: right;\">              -14.35</td><td style=\"text-align: right;\">             222.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 322000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-33-10\n",
      "  done: false\n",
      "  episode_len_mean: 219.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.230000000000006\n",
      "  episode_reward_mean: -0.7645999999999805\n",
      "  episode_reward_min: -14.34999999999998\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 922\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2989284101873633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.975496346420712\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010209151243705734\n",
      "          policy_loss: -0.11179977125591702\n",
      "          total_loss: 0.7454488298959202\n",
      "          vf_explained_var: 0.36232680082321167\n",
      "          vf_loss: 0.8637426071696811\n",
      "    num_agent_steps_sampled: 322000\n",
      "    num_agent_steps_trained: 322000\n",
      "    num_steps_sampled: 322000\n",
      "    num_steps_trained: 322000\n",
      "  iterations_since_restore: 322\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.77214285714285\n",
      "    ram_util_percent: 32.28285714285715\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669408795494797\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.808870386343333\n",
      "    mean_inference_ms: 2.1121844074776215\n",
      "    mean_raw_obs_processing_ms: 3.4397967036143178\n",
      "  time_since_restore: 8612.528998613358\n",
      "  time_this_iter_s: 98.04269552230835\n",
      "  time_total_s: 8612.528998613358\n",
      "  timers:\n",
      "    learn_throughput: 1330.412\n",
      "    learn_time_ms: 751.647\n",
      "    load_throughput: 41457.818\n",
      "    load_time_ms: 24.121\n",
      "    sample_throughput: 14.724\n",
      "    sample_time_ms: 67915.066\n",
      "    update_time_ms: 3.651\n",
      "  timestamp: 1635291190\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 322000\n",
      "  training_iteration: 322\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   322</td><td style=\"text-align: right;\">         8612.53</td><td style=\"text-align: right;\">322000</td><td style=\"text-align: right;\"> -0.7646</td><td style=\"text-align: right;\">                8.23</td><td style=\"text-align: right;\">              -14.35</td><td style=\"text-align: right;\">            219.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 323000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-34-53\n",
      "  done: false\n",
      "  episode_len_mean: 216.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.230000000000006\n",
      "  episode_reward_mean: -0.6752999999999807\n",
      "  episode_reward_min: -14.34999999999998\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 927\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2989284101873633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9467139720916748\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007680057893747892\n",
      "          policy_loss: -0.07311358758144909\n",
      "          total_loss: 0.5101654244793786\n",
      "          vf_explained_var: 0.23416048288345337\n",
      "          vf_loss: 0.5927702991498841\n",
      "    num_agent_steps_sampled: 323000\n",
      "    num_agent_steps_trained: 323000\n",
      "    num_steps_sampled: 323000\n",
      "    num_steps_trained: 323000\n",
      "  iterations_since_restore: 323\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.009589041095886\n",
      "    ram_util_percent: 32.32328767123288\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036693527459578414\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.83896451544066\n",
      "    mean_inference_ms: 2.1122034167413886\n",
      "    mean_raw_obs_processing_ms: 3.5626588360653386\n",
      "  time_since_restore: 8714.60702252388\n",
      "  time_this_iter_s: 102.07802391052246\n",
      "  time_total_s: 8714.60702252388\n",
      "  timers:\n",
      "    learn_throughput: 1328.38\n",
      "    learn_time_ms: 752.797\n",
      "    load_throughput: 41800.042\n",
      "    load_time_ms: 23.923\n",
      "    sample_throughput: 13.302\n",
      "    sample_time_ms: 75176.873\n",
      "    update_time_ms: 3.649\n",
      "  timestamp: 1635291293\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 323000\n",
      "  training_iteration: 323\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   323</td><td style=\"text-align: right;\">         8714.61</td><td style=\"text-align: right;\">323000</td><td style=\"text-align: right;\"> -0.6753</td><td style=\"text-align: right;\">                8.23</td><td style=\"text-align: right;\">              -14.35</td><td style=\"text-align: right;\">            216.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 324000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-35-23\n",
      "  done: false\n",
      "  episode_len_mean: 216.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.230000000000006\n",
      "  episode_reward_mean: -0.8559999999999803\n",
      "  episode_reward_min: -14.539999999999953\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 930\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2989284101873633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.937814512517717\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011601897051466123\n",
      "          policy_loss: -0.008710888193713294\n",
      "          total_loss: 0.6176155045628547\n",
      "          vf_explained_var: 0.5242556929588318\n",
      "          vf_loss: 0.6306345005830128\n",
      "    num_agent_steps_sampled: 324000\n",
      "    num_agent_steps_trained: 324000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "  iterations_since_restore: 324\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.9860465116279\n",
      "    ram_util_percent: 32.31162790697674\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669321601838802\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.85713530373787\n",
      "    mean_inference_ms: 2.112215294856404\n",
      "    mean_raw_obs_processing_ms: 3.632526545155729\n",
      "  time_since_restore: 8744.957284927368\n",
      "  time_this_iter_s: 30.35026240348816\n",
      "  time_total_s: 8744.957284927368\n",
      "  timers:\n",
      "    learn_throughput: 1328.286\n",
      "    learn_time_ms: 752.85\n",
      "    load_throughput: 42044.283\n",
      "    load_time_ms: 23.784\n",
      "    sample_throughput: 14.657\n",
      "    sample_time_ms: 68227.145\n",
      "    update_time_ms: 3.641\n",
      "  timestamp: 1635291323\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 324\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   324</td><td style=\"text-align: right;\">         8744.96</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">  -0.856</td><td style=\"text-align: right;\">                8.23</td><td style=\"text-align: right;\">              -14.54</td><td style=\"text-align: right;\">            216.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 325000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-36-10\n",
      "  done: false\n",
      "  episode_len_mean: 217.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.230000000000006\n",
      "  episode_reward_mean: -0.8076999999999802\n",
      "  episode_reward_min: -14.539999999999953\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 934\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2989284101873633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.154096500078837\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009115917694957634\n",
      "          policy_loss: -0.10064737002054851\n",
      "          total_loss: 0.3994911861916383\n",
      "          vf_explained_var: 0.6671806573867798\n",
      "          vf_loss: 0.5098386016156938\n",
      "    num_agent_steps_sampled: 325000\n",
      "    num_agent_steps_trained: 325000\n",
      "    num_steps_sampled: 325000\n",
      "    num_steps_trained: 325000\n",
      "  iterations_since_restore: 325\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.756716417910454\n",
      "    ram_util_percent: 32.22985074626867\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669282441741615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.881595699281327\n",
      "    mean_inference_ms: 2.1122324638083247\n",
      "    mean_raw_obs_processing_ms: 3.72403698853667\n",
      "  time_since_restore: 8791.568610668182\n",
      "  time_this_iter_s: 46.61132574081421\n",
      "  time_total_s: 8791.568610668182\n",
      "  timers:\n",
      "    learn_throughput: 1326.969\n",
      "    learn_time_ms: 753.597\n",
      "    load_throughput: 42067.222\n",
      "    load_time_ms: 23.771\n",
      "    sample_throughput: 15.226\n",
      "    sample_time_ms: 65678.382\n",
      "    update_time_ms: 3.71\n",
      "  timestamp: 1635291370\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 325000\n",
      "  training_iteration: 325\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   325</td><td style=\"text-align: right;\">         8791.57</td><td style=\"text-align: right;\">325000</td><td style=\"text-align: right;\"> -0.8077</td><td style=\"text-align: right;\">                8.23</td><td style=\"text-align: right;\">              -14.54</td><td style=\"text-align: right;\">            217.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 326000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-36-39\n",
      "  done: false\n",
      "  episode_len_mean: 216.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.230000000000006\n",
      "  episode_reward_mean: -0.83839999999998\n",
      "  episode_reward_min: -14.539999999999953\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 938\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2989284101873633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8146284699440003\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006998862030337517\n",
      "          policy_loss: -0.09340387909776635\n",
      "          total_loss: 0.7610313458575143\n",
      "          vf_explained_var: 0.5119386911392212\n",
      "          vf_loss: 0.8634904911120732\n",
      "    num_agent_steps_sampled: 326000\n",
      "    num_agent_steps_trained: 326000\n",
      "    num_steps_sampled: 326000\n",
      "    num_steps_trained: 326000\n",
      "  iterations_since_restore: 326\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.56904761904761\n",
      "    ram_util_percent: 32.34047619047619\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669243763533272\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.906110522696864\n",
      "    mean_inference_ms: 2.11224952657473\n",
      "    mean_raw_obs_processing_ms: 3.8152870123119214\n",
      "  time_since_restore: 8821.352174520493\n",
      "  time_this_iter_s: 29.78356385231018\n",
      "  time_total_s: 8821.352174520493\n",
      "  timers:\n",
      "    learn_throughput: 1328.14\n",
      "    learn_time_ms: 752.933\n",
      "    load_throughput: 41975.739\n",
      "    load_time_ms: 23.823\n",
      "    sample_throughput: 15.23\n",
      "    sample_time_ms: 65661.227\n",
      "    update_time_ms: 3.799\n",
      "  timestamp: 1635291399\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 326000\n",
      "  training_iteration: 326\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   326</td><td style=\"text-align: right;\">         8821.35</td><td style=\"text-align: right;\">326000</td><td style=\"text-align: right;\"> -0.8384</td><td style=\"text-align: right;\">                8.23</td><td style=\"text-align: right;\">              -14.54</td><td style=\"text-align: right;\">            216.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 327000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-37-50\n",
      "  done: false\n",
      "  episode_len_mean: 215.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.230000000000006\n",
      "  episode_reward_mean: -0.7827999999999798\n",
      "  episode_reward_min: -14.539999999999953\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 942\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2989284101873633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7351612501674227\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008081156088031113\n",
      "          policy_loss: -0.036660920745796625\n",
      "          total_loss: 0.34547441572778753\n",
      "          vf_explained_var: 0.828098475933075\n",
      "          vf_loss: 0.38899010320504507\n",
      "    num_agent_steps_sampled: 327000\n",
      "    num_agent_steps_trained: 327000\n",
      "    num_steps_sampled: 327000\n",
      "    num_steps_trained: 327000\n",
      "  iterations_since_restore: 327\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.9069306930693\n",
      "    ram_util_percent: 32.312871287128715\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669207928816328\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.93045819857293\n",
      "    mean_inference_ms: 2.112266588402108\n",
      "    mean_raw_obs_processing_ms: 3.911136385742333\n",
      "  time_since_restore: 8891.903529405594\n",
      "  time_this_iter_s: 70.55135488510132\n",
      "  time_total_s: 8891.903529405594\n",
      "  timers:\n",
      "    learn_throughput: 1327.153\n",
      "    learn_time_ms: 753.493\n",
      "    load_throughput: 43269.555\n",
      "    load_time_ms: 23.111\n",
      "    sample_throughput: 15.189\n",
      "    sample_time_ms: 65839.244\n",
      "    update_time_ms: 3.711\n",
      "  timestamp: 1635291470\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 327000\n",
      "  training_iteration: 327\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   327</td><td style=\"text-align: right;\">          8891.9</td><td style=\"text-align: right;\">327000</td><td style=\"text-align: right;\"> -0.7828</td><td style=\"text-align: right;\">                8.23</td><td style=\"text-align: right;\">              -14.54</td><td style=\"text-align: right;\">            215.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-38-56\n",
      "  done: false\n",
      "  episode_len_mean: 214.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.230000000000006\n",
      "  episode_reward_mean: -1.00279999999998\n",
      "  episode_reward_min: -14.539999999999953\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 947\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2989284101873633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0509749386045666\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0074542789384505126\n",
      "          policy_loss: 0.009323303070333269\n",
      "          total_loss: 0.5510249165818095\n",
      "          vf_explained_var: 0.5335423946380615\n",
      "          vf_loss: 0.5525287934475475\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "  iterations_since_restore: 328\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.649473684210534\n",
      "    ram_util_percent: 32.33052631578948\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036691628954275836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.960257214519455\n",
      "    mean_inference_ms: 2.11228702633075\n",
      "    mean_raw_obs_processing_ms: 4.036150104213061\n",
      "  time_since_restore: 8958.401260137558\n",
      "  time_this_iter_s: 66.49773073196411\n",
      "  time_total_s: 8958.401260137558\n",
      "  timers:\n",
      "    learn_throughput: 1324.308\n",
      "    learn_time_ms: 755.111\n",
      "    load_throughput: 44392.813\n",
      "    load_time_ms: 22.526\n",
      "    sample_throughput: 16.544\n",
      "    sample_time_ms: 60444.615\n",
      "    update_time_ms: 3.712\n",
      "  timestamp: 1635291536\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 328\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   328</td><td style=\"text-align: right;\">          8958.4</td><td style=\"text-align: right;\">328000</td><td style=\"text-align: right;\"> -1.0028</td><td style=\"text-align: right;\">                8.23</td><td style=\"text-align: right;\">              -14.54</td><td style=\"text-align: right;\">             214.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 329000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-40-07\n",
      "  done: false\n",
      "  episode_len_mean: 212.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.230000000000006\n",
      "  episode_reward_mean: -1.0016999999999798\n",
      "  episode_reward_min: -14.539999999999953\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 951\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2989284101873633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8260688794983757\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00540293078492854\n",
      "          policy_loss: -0.06122268819146686\n",
      "          total_loss: 0.5768072531869014\n",
      "          vf_explained_var: 0.6662530303001404\n",
      "          vf_loss: 0.6492726150486204\n",
      "    num_agent_steps_sampled: 329000\n",
      "    num_agent_steps_trained: 329000\n",
      "    num_steps_sampled: 329000\n",
      "    num_steps_trained: 329000\n",
      "  iterations_since_restore: 329\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.479207920792085\n",
      "    ram_util_percent: 32.309900990099\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669130314699759\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.983567955933598\n",
      "    mean_inference_ms: 2.1123033105804967\n",
      "    mean_raw_obs_processing_ms: 4.135230825186652\n",
      "  time_since_restore: 9029.397719860077\n",
      "  time_this_iter_s: 70.99645972251892\n",
      "  time_total_s: 9029.397719860077\n",
      "  timers:\n",
      "    learn_throughput: 1326.155\n",
      "    learn_time_ms: 754.059\n",
      "    load_throughput: 44139.046\n",
      "    load_time_ms: 22.656\n",
      "    sample_throughput: 15.529\n",
      "    sample_time_ms: 64395.382\n",
      "    update_time_ms: 3.704\n",
      "  timestamp: 1635291607\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 329000\n",
      "  training_iteration: 329\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   329</td><td style=\"text-align: right;\">          9029.4</td><td style=\"text-align: right;\">329000</td><td style=\"text-align: right;\"> -1.0017</td><td style=\"text-align: right;\">                8.23</td><td style=\"text-align: right;\">              -14.54</td><td style=\"text-align: right;\">            212.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 330000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-40-35\n",
      "  done: false\n",
      "  episode_len_mean: 217.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.230000000000006\n",
      "  episode_reward_mean: -1.495399999999979\n",
      "  episode_reward_min: -14.539999999999953\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 955\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2989284101873633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2195588005913627\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011522818712168379\n",
      "          policy_loss: 0.04900864755941762\n",
      "          total_loss: 0.3945848715802034\n",
      "          vf_explained_var: 0.853367805480957\n",
      "          vf_loss: 0.3528044874469439\n",
      "    num_agent_steps_sampled: 330000\n",
      "    num_agent_steps_trained: 330000\n",
      "    num_steps_sampled: 330000\n",
      "    num_steps_trained: 330000\n",
      "  iterations_since_restore: 330\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.33076923076923\n",
      "    ram_util_percent: 32.22051282051283\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036690958962977006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.00675434157346\n",
      "    mean_inference_ms: 2.1123184884271673\n",
      "    mean_raw_obs_processing_ms: 4.2297839734721965\n",
      "  time_since_restore: 9056.65480709076\n",
      "  time_this_iter_s: 27.257087230682373\n",
      "  time_total_s: 9056.65480709076\n",
      "  timers:\n",
      "    learn_throughput: 1327.907\n",
      "    learn_time_ms: 753.065\n",
      "    load_throughput: 44496.8\n",
      "    load_time_ms: 22.474\n",
      "    sample_throughput: 17.617\n",
      "    sample_time_ms: 56761.76\n",
      "    update_time_ms: 3.785\n",
      "  timestamp: 1635291635\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 330000\n",
      "  training_iteration: 330\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   330</td><td style=\"text-align: right;\">         9056.65</td><td style=\"text-align: right;\">330000</td><td style=\"text-align: right;\"> -1.4954</td><td style=\"text-align: right;\">                8.23</td><td style=\"text-align: right;\">              -14.54</td><td style=\"text-align: right;\">            217.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 331000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-42-07\n",
      "  done: false\n",
      "  episode_len_mean: 222.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.230000000000006\n",
      "  episode_reward_mean: -1.4252999999999794\n",
      "  episode_reward_min: -14.539999999999953\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 959\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2989284101873633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0436540100309584\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011709143817145279\n",
      "          policy_loss: 0.15002288379602963\n",
      "          total_loss: 0.4591095358961158\n",
      "          vf_explained_var: 0.43372759222984314\n",
      "          vf_loss: 0.3143138537804286\n",
      "    num_agent_steps_sampled: 331000\n",
      "    num_agent_steps_trained: 331000\n",
      "    num_steps_sampled: 331000\n",
      "    num_steps_trained: 331000\n",
      "  iterations_since_restore: 331\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.20916030534351\n",
      "    ram_util_percent: 32.3351145038168\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036690605761419484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.029411843003203\n",
      "    mean_inference_ms: 2.1123324209196035\n",
      "    mean_raw_obs_processing_ms: 4.325053622289606\n",
      "  time_since_restore: 9148.456026554108\n",
      "  time_this_iter_s: 91.80121946334839\n",
      "  time_total_s: 9148.456026554108\n",
      "  timers:\n",
      "    learn_throughput: 1325.249\n",
      "    learn_time_ms: 754.575\n",
      "    load_throughput: 44280.52\n",
      "    load_time_ms: 22.583\n",
      "    sample_throughput: 15.971\n",
      "    sample_time_ms: 62611.841\n",
      "    update_time_ms: 3.784\n",
      "  timestamp: 1635291727\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 331000\n",
      "  training_iteration: 331\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   331</td><td style=\"text-align: right;\">         9148.46</td><td style=\"text-align: right;\">331000</td><td style=\"text-align: right;\"> -1.4253</td><td style=\"text-align: right;\">                8.23</td><td style=\"text-align: right;\">              -14.54</td><td style=\"text-align: right;\">            222.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 332000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-42-52\n",
      "  done: false\n",
      "  episode_len_mean: 227.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.73\n",
      "  episode_reward_mean: -1.5012999999999785\n",
      "  episode_reward_min: -14.539999999999953\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 963\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2989284101873633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.028140510453118\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0052076740640790336\n",
      "          policy_loss: 0.08467664569616318\n",
      "          total_loss: 0.45701972511079575\n",
      "          vf_explained_var: 0.4637264609336853\n",
      "          vf_loss: 0.38586009177896713\n",
      "    num_agent_steps_sampled: 332000\n",
      "    num_agent_steps_trained: 332000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "  iterations_since_restore: 332\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.84615384615385\n",
      "    ram_util_percent: 32.39846153846153\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669024577381718\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.051796376536913\n",
      "    mean_inference_ms: 2.112346501544902\n",
      "    mean_raw_obs_processing_ms: 4.409963073390212\n",
      "  time_since_restore: 9193.816502332687\n",
      "  time_this_iter_s: 45.36047577857971\n",
      "  time_total_s: 9193.816502332687\n",
      "  timers:\n",
      "    learn_throughput: 1328.689\n",
      "    learn_time_ms: 752.622\n",
      "    load_throughput: 44096.029\n",
      "    load_time_ms: 22.678\n",
      "    sample_throughput: 17.438\n",
      "    sample_time_ms: 57345.45\n",
      "    update_time_ms: 3.79\n",
      "  timestamp: 1635291772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 332\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   332</td><td style=\"text-align: right;\">         9193.82</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\"> -1.5013</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -14.54</td><td style=\"text-align: right;\">            227.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 333000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-43-22\n",
      "  done: false\n",
      "  episode_len_mean: 229.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.73\n",
      "  episode_reward_mean: -1.4664999999999788\n",
      "  episode_reward_min: -14.539999999999953\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 966\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2989284101873633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.803689800368415\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005158267124021501\n",
      "          policy_loss: -0.11793016940355301\n",
      "          total_loss: 0.30786538268956876\n",
      "          vf_explained_var: 0.3943372964859009\n",
      "          vf_loss: 0.43713223276038965\n",
      "    num_agent_steps_sampled: 333000\n",
      "    num_agent_steps_trained: 333000\n",
      "    num_steps_sampled: 333000\n",
      "    num_steps_trained: 333000\n",
      "  iterations_since_restore: 333\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.89302325581396\n",
      "    ram_util_percent: 32.372093023255815\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036689956523749935\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.06903165787465\n",
      "    mean_inference_ms: 2.1123585886001073\n",
      "    mean_raw_obs_processing_ms: 4.470239092278973\n",
      "  time_since_restore: 9224.086336135864\n",
      "  time_this_iter_s: 30.26983380317688\n",
      "  time_total_s: 9224.086336135864\n",
      "  timers:\n",
      "    learn_throughput: 1330.259\n",
      "    learn_time_ms: 751.733\n",
      "    load_throughput: 43899.739\n",
      "    load_time_ms: 22.779\n",
      "    sample_throughput: 19.934\n",
      "    sample_time_ms: 50165.419\n",
      "    update_time_ms: 3.782\n",
      "  timestamp: 1635291802\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 333000\n",
      "  training_iteration: 333\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   333</td><td style=\"text-align: right;\">         9224.09</td><td style=\"text-align: right;\">333000</td><td style=\"text-align: right;\"> -1.4665</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -14.54</td><td style=\"text-align: right;\">             229.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 334000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-43-47\n",
      "  done: false\n",
      "  episode_len_mean: 235.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.73\n",
      "  episode_reward_mean: -1.7011999999999776\n",
      "  episode_reward_min: -14.539999999999953\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 969\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2989284101873633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0371588667233786\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006565731734337849\n",
      "          policy_loss: -0.01466605994436476\n",
      "          total_loss: 0.15767426838477452\n",
      "          vf_explained_var: 0.5131674408912659\n",
      "          vf_loss: 0.18418349909285706\n",
      "    num_agent_steps_sampled: 334000\n",
      "    num_agent_steps_trained: 334000\n",
      "    num_steps_sampled: 334000\n",
      "    num_steps_trained: 334000\n",
      "  iterations_since_restore: 334\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.38055555555555\n",
      "    ram_util_percent: 32.3361111111111\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036689689650643405\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.086133540689474\n",
      "    mean_inference_ms: 2.1123694467326875\n",
      "    mean_raw_obs_processing_ms: 4.529023520092155\n",
      "  time_since_restore: 9249.047460317612\n",
      "  time_this_iter_s: 24.961124181747437\n",
      "  time_total_s: 9249.047460317612\n",
      "  timers:\n",
      "    learn_throughput: 1330.297\n",
      "    learn_time_ms: 751.712\n",
      "    load_throughput: 43736.408\n",
      "    load_time_ms: 22.864\n",
      "    sample_throughput: 20.151\n",
      "    sample_time_ms: 49626.415\n",
      "    update_time_ms: 3.789\n",
      "  timestamp: 1635291827\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 334000\n",
      "  training_iteration: 334\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   334</td><td style=\"text-align: right;\">         9249.05</td><td style=\"text-align: right;\">334000</td><td style=\"text-align: right;\"> -1.7012</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -14.54</td><td style=\"text-align: right;\">            235.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 335000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-45-17\n",
      "  done: false\n",
      "  episode_len_mean: 236.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.73\n",
      "  episode_reward_mean: -1.579899999999978\n",
      "  episode_reward_min: -14.539999999999953\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 974\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2989284101873633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7407245874404906\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007114191271779191\n",
      "          policy_loss: -0.005977618197600047\n",
      "          total_loss: 0.4185788427790006\n",
      "          vf_explained_var: 0.5591722130775452\n",
      "          vf_loss: 0.4327228812707795\n",
      "    num_agent_steps_sampled: 335000\n",
      "    num_agent_steps_trained: 335000\n",
      "    num_steps_sampled: 335000\n",
      "    num_steps_trained: 335000\n",
      "  iterations_since_restore: 335\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.465624999999996\n",
      "    ram_util_percent: 32.33203125\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668924208862388\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.11436488575929\n",
      "    mean_inference_ms: 2.1123862246544935\n",
      "    mean_raw_obs_processing_ms: 4.630457126963119\n",
      "  time_since_restore: 9338.750892400742\n",
      "  time_this_iter_s: 89.70343208312988\n",
      "  time_total_s: 9338.750892400742\n",
      "  timers:\n",
      "    learn_throughput: 1332.191\n",
      "    learn_time_ms: 750.643\n",
      "    load_throughput: 43642.614\n",
      "    load_time_ms: 22.913\n",
      "    sample_throughput: 18.54\n",
      "    sample_time_ms: 53936.717\n",
      "    update_time_ms: 3.717\n",
      "  timestamp: 1635291917\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 335000\n",
      "  training_iteration: 335\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   335</td><td style=\"text-align: right;\">         9338.75</td><td style=\"text-align: right;\">335000</td><td style=\"text-align: right;\"> -1.5799</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -14.54</td><td style=\"text-align: right;\">            236.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-46-21\n",
      "  done: false\n",
      "  episode_len_mean: 239.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.73\n",
      "  episode_reward_mean: -1.6047999999999774\n",
      "  episode_reward_min: -14.539999999999953\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 978\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2989284101873633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9512796521186828\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009840421982512934\n",
      "          policy_loss: -0.08661160990595818\n",
      "          total_loss: 0.42932170478420123\n",
      "          vf_explained_var: 0.4972379803657532\n",
      "          vf_loss: 0.5226641014218331\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_agent_steps_trained: 336000\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "  iterations_since_restore: 336\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.36703296703296\n",
      "    ram_util_percent: 32.309890109890105\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0366888718224064\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.136829547836573\n",
      "    mean_inference_ms: 2.1123982971442627\n",
      "    mean_raw_obs_processing_ms: 4.711382544609385\n",
      "  time_since_restore: 9403.052785873413\n",
      "  time_this_iter_s: 64.30189347267151\n",
      "  time_total_s: 9403.052785873413\n",
      "  timers:\n",
      "    learn_throughput: 1331.713\n",
      "    learn_time_ms: 750.913\n",
      "    load_throughput: 43618.151\n",
      "    load_time_ms: 22.926\n",
      "    sample_throughput: 17.425\n",
      "    sample_time_ms: 57388.367\n",
      "    update_time_ms: 3.623\n",
      "  timestamp: 1635291981\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 336\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   336</td><td style=\"text-align: right;\">         9403.05</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\"> -1.6048</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -14.54</td><td style=\"text-align: right;\">             239.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 337000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-47-27\n",
      "  done: false\n",
      "  episode_len_mean: 235.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.73\n",
      "  episode_reward_mean: -1.3378999999999777\n",
      "  episode_reward_min: -14.539999999999953\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 983\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2989284101873633\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.707156562142902\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.2527968056703948\n",
      "          policy_loss: 0.07263055267847246\n",
      "          total_loss: 3.889904413703415\n",
      "          vf_explained_var: 0.5316384434700012\n",
      "          vf_loss: 3.5059805197848215\n",
      "    num_agent_steps_sampled: 337000\n",
      "    num_agent_steps_trained: 337000\n",
      "    num_steps_sampled: 337000\n",
      "    num_steps_trained: 337000\n",
      "  iterations_since_restore: 337\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.626595744680834\n",
      "    ram_util_percent: 32.3468085106383\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668840789300536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.164772167645392\n",
      "    mean_inference_ms: 2.11241199315087\n",
      "    mean_raw_obs_processing_ms: 4.815230095800479\n",
      "  time_since_restore: 9468.580420017242\n",
      "  time_this_iter_s: 65.52763414382935\n",
      "  time_total_s: 9468.580420017242\n",
      "  timers:\n",
      "    learn_throughput: 1334.455\n",
      "    learn_time_ms: 749.37\n",
      "    load_throughput: 42426.275\n",
      "    load_time_ms: 23.57\n",
      "    sample_throughput: 17.579\n",
      "    sample_time_ms: 56886.899\n",
      "    update_time_ms: 3.622\n",
      "  timestamp: 1635292047\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 337000\n",
      "  training_iteration: 337\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   337</td><td style=\"text-align: right;\">         9468.58</td><td style=\"text-align: right;\">337000</td><td style=\"text-align: right;\"> -1.3379</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -14.54</td><td style=\"text-align: right;\">            235.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 338000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-48-15\n",
      "  done: false\n",
      "  episode_len_mean: 237.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.73\n",
      "  episode_reward_mean: -1.4151999999999771\n",
      "  episode_reward_min: -14.539999999999953\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 987\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9483926152810453\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1032298114564685\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005976281113837937\n",
      "          policy_loss: -0.07186758551332685\n",
      "          total_loss: 1.3119402491384082\n",
      "          vf_explained_var: 0.5207214951515198\n",
      "          vf_loss: 1.3931960145632425\n",
      "    num_agent_steps_sampled: 338000\n",
      "    num_agent_steps_trained: 338000\n",
      "    num_steps_sampled: 338000\n",
      "    num_steps_trained: 338000\n",
      "  iterations_since_restore: 338\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.55652173913044\n",
      "    ram_util_percent: 32.36666666666665\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668805024954772\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.186909882154417\n",
      "    mean_inference_ms: 2.112421795599923\n",
      "    mean_raw_obs_processing_ms: 4.891890790398153\n",
      "  time_since_restore: 9517.173927783966\n",
      "  time_this_iter_s: 48.59350776672363\n",
      "  time_total_s: 9517.173927783966\n",
      "  timers:\n",
      "    learn_throughput: 1335.586\n",
      "    learn_time_ms: 748.735\n",
      "    load_throughput: 41421.338\n",
      "    load_time_ms: 24.142\n",
      "    sample_throughput: 18.15\n",
      "    sample_time_ms: 55096.551\n",
      "    update_time_ms: 3.615\n",
      "  timestamp: 1635292095\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 338000\n",
      "  training_iteration: 338\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   338</td><td style=\"text-align: right;\">         9517.17</td><td style=\"text-align: right;\">338000</td><td style=\"text-align: right;\"> -1.4152</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -14.54</td><td style=\"text-align: right;\">            237.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 339000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-50-25\n",
      "  done: false\n",
      "  episode_len_mean: 231.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.73\n",
      "  episode_reward_mean: -1.2695999999999774\n",
      "  episode_reward_min: -14.539999999999953\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 993\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9483926152810453\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7142014728652106\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0032459680464938068\n",
      "          policy_loss: -0.0655757853968276\n",
      "          total_loss: 0.9995018366310332\n",
      "          vf_explained_var: 0.6139447093009949\n",
      "          vf_loss: 1.0758952167299058\n",
      "    num_agent_steps_sampled: 339000\n",
      "    num_agent_steps_trained: 339000\n",
      "    num_steps_sampled: 339000\n",
      "    num_steps_trained: 339000\n",
      "  iterations_since_restore: 339\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.08279569892473\n",
      "    ram_util_percent: 32.21451612903225\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668752793751225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.219513576360207\n",
      "    mean_inference_ms: 2.1124337248177567\n",
      "    mean_raw_obs_processing_ms: 5.018445739451152\n",
      "  time_since_restore: 9646.972400426865\n",
      "  time_this_iter_s: 129.79847264289856\n",
      "  time_total_s: 9646.972400426865\n",
      "  timers:\n",
      "    learn_throughput: 1334.854\n",
      "    learn_time_ms: 749.146\n",
      "    load_throughput: 41680.992\n",
      "    load_time_ms: 23.992\n",
      "    sample_throughput: 16.4\n",
      "    sample_time_ms: 60976.404\n",
      "    update_time_ms: 3.709\n",
      "  timestamp: 1635292225\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 339000\n",
      "  training_iteration: 339\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   339</td><td style=\"text-align: right;\">         9646.97</td><td style=\"text-align: right;\">339000</td><td style=\"text-align: right;\"> -1.2696</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -14.54</td><td style=\"text-align: right;\">            231.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 340000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-51-11\n",
      "  done: false\n",
      "  episode_len_mean: 231.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.73\n",
      "  episode_reward_mean: -1.2236999999999776\n",
      "  episode_reward_min: -14.539999999999953\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 998\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9741963076405227\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0866611586676704\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006402484304872639\n",
      "          policy_loss: -0.1006189270151986\n",
      "          total_loss: 0.23927283564375507\n",
      "          vf_explained_var: 0.5846540331840515\n",
      "          vf_loss: 0.35452109562853973\n",
      "    num_agent_steps_sampled: 340000\n",
      "    num_agent_steps_trained: 340000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "  iterations_since_restore: 340\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.470769230769235\n",
      "    ram_util_percent: 32.39846153846154\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036687104270899155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.24606096745731\n",
      "    mean_inference_ms: 2.11244211955064\n",
      "    mean_raw_obs_processing_ms: 5.121339071352115\n",
      "  time_since_restore: 9692.828798294067\n",
      "  time_this_iter_s: 45.85639786720276\n",
      "  time_total_s: 9692.828798294067\n",
      "  timers:\n",
      "    learn_throughput: 1333.524\n",
      "    learn_time_ms: 749.893\n",
      "    load_throughput: 41427.27\n",
      "    load_time_ms: 24.139\n",
      "    sample_throughput: 15.915\n",
      "    sample_time_ms: 62835.514\n",
      "    update_time_ms: 3.638\n",
      "  timestamp: 1635292271\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 340\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   340</td><td style=\"text-align: right;\">         9692.83</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\"> -1.2237</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -14.54</td><td style=\"text-align: right;\">            231.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 341000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-51-41\n",
      "  done: false\n",
      "  episode_len_mean: 234.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.73\n",
      "  episode_reward_mean: -1.1701999999999773\n",
      "  episode_reward_min: -14.539999999999953\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1001\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9741963076405227\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1712869577937655\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004809450614928422\n",
      "          policy_loss: -0.020479716940058602\n",
      "          total_loss: 0.40103730029529994\n",
      "          vf_explained_var: 0.36582618951797485\n",
      "          vf_loss: 0.43854453497462803\n",
      "    num_agent_steps_sampled: 341000\n",
      "    num_agent_steps_trained: 341000\n",
      "    num_steps_sampled: 341000\n",
      "    num_steps_trained: 341000\n",
      "  iterations_since_restore: 341\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.49047619047619\n",
      "    ram_util_percent: 32.40952380952381\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668684815065197\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.261973996836332\n",
      "    mean_inference_ms: 2.112446254889851\n",
      "    mean_raw_obs_processing_ms: 5.1764185103307865\n",
      "  time_since_restore: 9722.280497074127\n",
      "  time_this_iter_s: 29.451698780059814\n",
      "  time_total_s: 9722.280497074127\n",
      "  timers:\n",
      "    learn_throughput: 1335.84\n",
      "    learn_time_ms: 748.593\n",
      "    load_throughput: 41542.817\n",
      "    load_time_ms: 24.072\n",
      "    sample_throughput: 17.667\n",
      "    sample_time_ms: 56601.849\n",
      "    update_time_ms: 3.709\n",
      "  timestamp: 1635292301\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 341000\n",
      "  training_iteration: 341\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   341</td><td style=\"text-align: right;\">         9722.28</td><td style=\"text-align: right;\">341000</td><td style=\"text-align: right;\"> -1.1702</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -14.54</td><td style=\"text-align: right;\">            234.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 342000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-53-27\n",
      "  done: false\n",
      "  episode_len_mean: 235.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.73\n",
      "  episode_reward_mean: -1.1254999999999769\n",
      "  episode_reward_min: -14.539999999999953\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1006\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0527365552054513\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008283909532738153\n",
      "          policy_loss: -0.11435017660260201\n",
      "          total_loss: 0.8036831542849541\n",
      "          vf_explained_var: 0.5304070711135864\n",
      "          vf_loss: 0.9345256136523352\n",
      "    num_agent_steps_sampled: 342000\n",
      "    num_agent_steps_trained: 342000\n",
      "    num_steps_sampled: 342000\n",
      "    num_steps_trained: 342000\n",
      "  iterations_since_restore: 342\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.01842105263158\n",
      "    ram_util_percent: 32.35394736842105\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668642415220988\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.289128623258065\n",
      "    mean_inference_ms: 2.1124526765225884\n",
      "    mean_raw_obs_processing_ms: 5.2738444000009475\n",
      "  time_since_restore: 9828.593523263931\n",
      "  time_this_iter_s: 106.31302618980408\n",
      "  time_total_s: 9828.593523263931\n",
      "  timers:\n",
      "    learn_throughput: 1334.788\n",
      "    learn_time_ms: 749.183\n",
      "    load_throughput: 41774.147\n",
      "    load_time_ms: 23.938\n",
      "    sample_throughput: 15.95\n",
      "    sample_time_ms: 62696.676\n",
      "    update_time_ms: 3.708\n",
      "  timestamp: 1635292407\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 342000\n",
      "  training_iteration: 342\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   342</td><td style=\"text-align: right;\">         9828.59</td><td style=\"text-align: right;\">342000</td><td style=\"text-align: right;\"> -1.1255</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -14.54</td><td style=\"text-align: right;\">            235.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 343000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-53-57\n",
      "  done: false\n",
      "  episode_len_mean: 236.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.73\n",
      "  episode_reward_mean: -1.286099999999977\n",
      "  episode_reward_min: -16.38999999999998\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1010\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.244655179977417\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01479466013037761\n",
      "          policy_loss: 0.13789881906575627\n",
      "          total_loss: 0.7785399584306611\n",
      "          vf_explained_var: 0.40157973766326904\n",
      "          vf_loss: 0.655881238480409\n",
      "    num_agent_steps_sampled: 343000\n",
      "    num_agent_steps_trained: 343000\n",
      "    num_steps_sampled: 343000\n",
      "    num_steps_trained: 343000\n",
      "  iterations_since_restore: 343\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.61162790697675\n",
      "    ram_util_percent: 32.41162790697674\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668610163080729\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.31033667769308\n",
      "    mean_inference_ms: 2.1124570948966985\n",
      "    mean_raw_obs_processing_ms: 5.347132427008269\n",
      "  time_since_restore: 9858.990664482117\n",
      "  time_this_iter_s: 30.397141218185425\n",
      "  time_total_s: 9858.990664482117\n",
      "  timers:\n",
      "    learn_throughput: 1332.09\n",
      "    learn_time_ms: 750.7\n",
      "    load_throughput: 41607.723\n",
      "    load_time_ms: 24.034\n",
      "    sample_throughput: 15.947\n",
      "    sample_time_ms: 62707.772\n",
      "    update_time_ms: 3.711\n",
      "  timestamp: 1635292437\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 343000\n",
      "  training_iteration: 343\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   343</td><td style=\"text-align: right;\">         9858.99</td><td style=\"text-align: right;\">343000</td><td style=\"text-align: right;\"> -1.2861</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -16.39</td><td style=\"text-align: right;\">            236.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-55-07\n",
      "  done: false\n",
      "  episode_len_mean: 239.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.73\n",
      "  episode_reward_mean: -1.466499999999976\n",
      "  episode_reward_min: -16.38999999999998\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1014\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.013688189453549\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010249406247323624\n",
      "          policy_loss: 0.1509212685541974\n",
      "          total_loss: 0.691890052623219\n",
      "          vf_explained_var: 0.46875661611557007\n",
      "          vf_loss: 0.55611319343249\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_agent_steps_trained: 344000\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "  iterations_since_restore: 344\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.28099999999999\n",
      "    ram_util_percent: 32.361\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036685789143738316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.331988242267652\n",
      "    mean_inference_ms: 2.112462061556244\n",
      "    mean_raw_obs_processing_ms: 5.4199219684799536\n",
      "  time_since_restore: 9928.822988510132\n",
      "  time_this_iter_s: 69.83232402801514\n",
      "  time_total_s: 9928.822988510132\n",
      "  timers:\n",
      "    learn_throughput: 1331.736\n",
      "    learn_time_ms: 750.899\n",
      "    load_throughput: 41841.491\n",
      "    load_time_ms: 23.9\n",
      "    sample_throughput: 14.882\n",
      "    sample_time_ms: 67194.825\n",
      "    update_time_ms: 3.716\n",
      "  timestamp: 1635292507\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 344\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   344</td><td style=\"text-align: right;\">         9928.82</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\"> -1.4665</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -16.39</td><td style=\"text-align: right;\">            239.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 345000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-56-30\n",
      "  done: false\n",
      "  episode_len_mean: 237.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.73\n",
      "  episode_reward_mean: -1.3359999999999765\n",
      "  episode_reward_min: -16.38999999999998\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1019\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7713928818702698\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007309952685708983\n",
      "          policy_loss: 0.06304162591695786\n",
      "          total_loss: 0.7506109452909894\n",
      "          vf_explained_var: 0.267826110124588\n",
      "          vf_loss: 0.7017225825124317\n",
      "    num_agent_steps_sampled: 345000\n",
      "    num_agent_steps_trained: 345000\n",
      "    num_steps_sampled: 345000\n",
      "    num_steps_trained: 345000\n",
      "  iterations_since_restore: 345\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.74537815126051\n",
      "    ram_util_percent: 32.38823529411765\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668544094256383\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.35854546882039\n",
      "    mean_inference_ms: 2.1124692135641028\n",
      "    mean_raw_obs_processing_ms: 5.51601176298104\n",
      "  time_since_restore: 10012.034838438034\n",
      "  time_this_iter_s: 83.21184992790222\n",
      "  time_total_s: 10012.034838438034\n",
      "  timers:\n",
      "    learn_throughput: 1331.019\n",
      "    learn_time_ms: 751.304\n",
      "    load_throughput: 41900.302\n",
      "    load_time_ms: 23.866\n",
      "    sample_throughput: 15.027\n",
      "    sample_time_ms: 66545.259\n",
      "    update_time_ms: 3.719\n",
      "  timestamp: 1635292590\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 345000\n",
      "  training_iteration: 345\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   345</td><td style=\"text-align: right;\">           10012</td><td style=\"text-align: right;\">345000</td><td style=\"text-align: right;\">  -1.336</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -16.39</td><td style=\"text-align: right;\">            237.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 346000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-57-44\n",
      "  done: false\n",
      "  episode_len_mean: 237.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.73\n",
      "  episode_reward_mean: -1.353199999999976\n",
      "  episode_reward_min: -16.38999999999998\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1023\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.035777265495724\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009221800843921418\n",
      "          policy_loss: 0.04864893613590134\n",
      "          total_loss: 0.45488103582627243\n",
      "          vf_explained_var: 0.5580583214759827\n",
      "          vf_loss: 0.4220979475312763\n",
      "    num_agent_steps_sampled: 346000\n",
      "    num_agent_steps_trained: 346000\n",
      "    num_steps_sampled: 346000\n",
      "    num_steps_trained: 346000\n",
      "  iterations_since_restore: 346\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.325961538461534\n",
      "    ram_util_percent: 32.364423076923075\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036685174346016834\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.379931653191726\n",
      "    mean_inference_ms: 2.112474599696969\n",
      "    mean_raw_obs_processing_ms: 5.588419522100757\n",
      "  time_since_restore: 10085.306759595871\n",
      "  time_this_iter_s: 73.27192115783691\n",
      "  time_total_s: 10085.306759595871\n",
      "  timers:\n",
      "    learn_throughput: 1329.915\n",
      "    learn_time_ms: 751.928\n",
      "    load_throughput: 41737.524\n",
      "    load_time_ms: 23.959\n",
      "    sample_throughput: 14.828\n",
      "    sample_time_ms: 67441.526\n",
      "    update_time_ms: 3.732\n",
      "  timestamp: 1635292664\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 346000\n",
      "  training_iteration: 346\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   346</td><td style=\"text-align: right;\">         10085.3</td><td style=\"text-align: right;\">346000</td><td style=\"text-align: right;\"> -1.3532</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -16.39</td><td style=\"text-align: right;\">            237.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 347000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-58-34\n",
      "  done: false\n",
      "  episode_len_mean: 239.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.73\n",
      "  episode_reward_mean: -1.6569999999999754\n",
      "  episode_reward_min: -16.38999999999998\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1027\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8785767369800144\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008939603680215472\n",
      "          policy_loss: 0.10731325894594193\n",
      "          total_loss: 0.8297263655397628\n",
      "          vf_explained_var: 0.4362512230873108\n",
      "          vf_loss: 0.7368444141414431\n",
      "    num_agent_steps_sampled: 347000\n",
      "    num_agent_steps_trained: 347000\n",
      "    num_steps_sampled: 347000\n",
      "    num_steps_trained: 347000\n",
      "  iterations_since_restore: 347\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.51666666666668\n",
      "    ram_util_percent: 32.33611111111111\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036684877751123564\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.401730652396523\n",
      "    mean_inference_ms: 2.112478559202849\n",
      "    mean_raw_obs_processing_ms: 5.655827997034196\n",
      "  time_since_restore: 10135.351788759232\n",
      "  time_this_iter_s: 50.045029163360596\n",
      "  time_total_s: 10135.351788759232\n",
      "  timers:\n",
      "    learn_throughput: 1327.627\n",
      "    learn_time_ms: 753.223\n",
      "    load_throughput: 41811.168\n",
      "    load_time_ms: 23.917\n",
      "    sample_throughput: 15.176\n",
      "    sample_time_ms: 65892.01\n",
      "    update_time_ms: 3.727\n",
      "  timestamp: 1635292714\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 347000\n",
      "  training_iteration: 347\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   347</td><td style=\"text-align: right;\">         10135.4</td><td style=\"text-align: right;\">347000</td><td style=\"text-align: right;\">  -1.657</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -16.39</td><td style=\"text-align: right;\">            239.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 348000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-26_23-59-22\n",
      "  done: false\n",
      "  episode_len_mean: 239.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.73\n",
      "  episode_reward_mean: -1.3697999999999757\n",
      "  episode_reward_min: -16.38999999999998\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1031\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3068468146853975\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008920075256476749\n",
      "          policy_loss: -0.029156540003087785\n",
      "          total_loss: 1.352534790833791\n",
      "          vf_explained_var: 0.4187495708465576\n",
      "          vf_loss: 1.4004148456785415\n",
      "    num_agent_steps_sampled: 348000\n",
      "    num_agent_steps_trained: 348000\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "  iterations_since_restore: 348\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.04927536231884\n",
      "    ram_util_percent: 32.41014492753623\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668454713315209\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.423305615171486\n",
      "    mean_inference_ms: 2.112481706871977\n",
      "    mean_raw_obs_processing_ms: 5.724646874799339\n",
      "  time_since_restore: 10183.752965688705\n",
      "  time_this_iter_s: 48.40117692947388\n",
      "  time_total_s: 10183.752965688705\n",
      "  timers:\n",
      "    learn_throughput: 1328.846\n",
      "    learn_time_ms: 752.533\n",
      "    load_throughput: 41698.23\n",
      "    load_time_ms: 23.982\n",
      "    sample_throughput: 15.181\n",
      "    sample_time_ms: 65873.333\n",
      "    update_time_ms: 3.798\n",
      "  timestamp: 1635292762\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 348\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   348</td><td style=\"text-align: right;\">         10183.8</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\"> -1.3698</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -16.39</td><td style=\"text-align: right;\">            239.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 349000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-01-26\n",
      "  done: false\n",
      "  episode_len_mean: 232.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.73\n",
      "  episode_reward_mean: -1.0091999999999772\n",
      "  episode_reward_min: -16.38999999999998\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1037\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9736855586369833\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00852935121266797\n",
      "          policy_loss: 0.06806655600667\n",
      "          total_loss: 0.918837759229872\n",
      "          vf_explained_var: 0.7705672979354858\n",
      "          vf_loss: 0.8663534339931276\n",
      "    num_agent_steps_sampled: 349000\n",
      "    num_agent_steps_trained: 349000\n",
      "    num_steps_sampled: 349000\n",
      "    num_steps_trained: 349000\n",
      "  iterations_since_restore: 349\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.56306818181818\n",
      "    ram_util_percent: 32.37784090909091\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036684076665249\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.45527025410861\n",
      "    mean_inference_ms: 2.112486367890057\n",
      "    mean_raw_obs_processing_ms: 5.841498896916839\n",
      "  time_since_restore: 10307.221312761307\n",
      "  time_this_iter_s: 123.46834707260132\n",
      "  time_total_s: 10307.221312761307\n",
      "  timers:\n",
      "    learn_throughput: 1328.64\n",
      "    learn_time_ms: 752.649\n",
      "    load_throughput: 41594.066\n",
      "    load_time_ms: 24.042\n",
      "    sample_throughput: 15.328\n",
      "    sample_time_ms: 65240.144\n",
      "    update_time_ms: 3.791\n",
      "  timestamp: 1635292886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 349000\n",
      "  training_iteration: 349\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   349</td><td style=\"text-align: right;\">         10307.2</td><td style=\"text-align: right;\">349000</td><td style=\"text-align: right;\"> -1.0092</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -16.39</td><td style=\"text-align: right;\">            232.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 350000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-03-44\n",
      "  done: false\n",
      "  episode_len_mean: 228.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.73\n",
      "  episode_reward_mean: -0.9764999999999776\n",
      "  episode_reward_min: -16.38999999999998\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1044\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8879480679829916\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008701938161318385\n",
      "          policy_loss: 0.002626002248790529\n",
      "          total_loss: 0.7121127016014523\n",
      "          vf_explained_var: 0.7987840175628662\n",
      "          vf_loss: 0.7241274827056461\n",
      "    num_agent_steps_sampled: 350000\n",
      "    num_agent_steps_trained: 350000\n",
      "    num_steps_sampled: 350000\n",
      "    num_steps_trained: 350000\n",
      "  iterations_since_restore: 350\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.563959390862934\n",
      "    ram_util_percent: 32.384771573604056\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668354178265462\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.491893504133074\n",
      "    mean_inference_ms: 2.112494394048559\n",
      "    mean_raw_obs_processing_ms: 5.989528035424017\n",
      "  time_since_restore: 10445.307814121246\n",
      "  time_this_iter_s: 138.08650135993958\n",
      "  time_total_s: 10445.307814121246\n",
      "  timers:\n",
      "    learn_throughput: 1328.699\n",
      "    learn_time_ms: 752.616\n",
      "    load_throughput: 41586.436\n",
      "    load_time_ms: 24.046\n",
      "    sample_throughput: 13.429\n",
      "    sample_time_ms: 74463.196\n",
      "    update_time_ms: 3.783\n",
      "  timestamp: 1635293024\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 350000\n",
      "  training_iteration: 350\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   350</td><td style=\"text-align: right;\">         10445.3</td><td style=\"text-align: right;\">350000</td><td style=\"text-align: right;\"> -0.9765</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -16.39</td><td style=\"text-align: right;\">            228.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 351000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-06-21\n",
      "  done: false\n",
      "  episode_len_mean: 221.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.73\n",
      "  episode_reward_mean: -0.5887999999999797\n",
      "  episode_reward_min: -16.38999999999998\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1051\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5082326862547133\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01160682262877574\n",
      "          policy_loss: 0.17219979365666707\n",
      "          total_loss: 1.2473273525635402\n",
      "          vf_explained_var: 0.6753358244895935\n",
      "          vf_loss: 1.0845562120278676\n",
      "    num_agent_steps_sampled: 351000\n",
      "    num_agent_steps_trained: 351000\n",
      "    num_steps_sampled: 351000\n",
      "    num_steps_trained: 351000\n",
      "  iterations_since_restore: 351\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.684374999999996\n",
      "    ram_util_percent: 32.33973214285714\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668300160821202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.528177422420505\n",
      "    mean_inference_ms: 2.1125028868735685\n",
      "    mean_raw_obs_processing_ms: 6.150577297439137\n",
      "  time_since_restore: 10602.095404624939\n",
      "  time_this_iter_s: 156.78759050369263\n",
      "  time_total_s: 10602.095404624939\n",
      "  timers:\n",
      "    learn_throughput: 1328.61\n",
      "    learn_time_ms: 752.667\n",
      "    load_throughput: 41537.55\n",
      "    load_time_ms: 24.075\n",
      "    sample_throughput: 11.468\n",
      "    sample_time_ms: 87196.715\n",
      "    update_time_ms: 3.791\n",
      "  timestamp: 1635293181\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 351000\n",
      "  training_iteration: 351\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   351</td><td style=\"text-align: right;\">         10602.1</td><td style=\"text-align: right;\">351000</td><td style=\"text-align: right;\"> -0.5888</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -16.39</td><td style=\"text-align: right;\">            221.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 352000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-07-20\n",
      "  done: false\n",
      "  episode_len_mean: 220.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.73\n",
      "  episode_reward_mean: -0.31759999999997973\n",
      "  episode_reward_min: -16.38999999999998\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1054\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2171294702423943\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008158923901371098\n",
      "          policy_loss: -0.018609880738788182\n",
      "          total_loss: 0.8534522914224201\n",
      "          vf_explained_var: 0.6966750025749207\n",
      "          vf_loss: 0.8902592758337656\n",
      "    num_agent_steps_sampled: 352000\n",
      "    num_agent_steps_trained: 352000\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "  iterations_since_restore: 352\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.983333333333334\n",
      "    ram_util_percent: 32.390476190476186\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668276931450297\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.543527250097235\n",
      "    mean_inference_ms: 2.1125069653088984\n",
      "    mean_raw_obs_processing_ms: 6.220568299603194\n",
      "  time_since_restore: 10661.201409339905\n",
      "  time_this_iter_s: 59.10600471496582\n",
      "  time_total_s: 10661.201409339905\n",
      "  timers:\n",
      "    learn_throughput: 1326.58\n",
      "    learn_time_ms: 753.818\n",
      "    load_throughput: 41434.924\n",
      "    load_time_ms: 24.134\n",
      "    sample_throughput: 12.125\n",
      "    sample_time_ms: 82474.801\n",
      "    update_time_ms: 3.784\n",
      "  timestamp: 1635293240\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 352\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   352</td><td style=\"text-align: right;\">         10661.2</td><td style=\"text-align: right;\">352000</td><td style=\"text-align: right;\"> -0.3176</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -16.39</td><td style=\"text-align: right;\">            220.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 353000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-09-38\n",
      "  done: false\n",
      "  episode_len_mean: 213.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.73\n",
      "  episode_reward_mean: 0.08330000000001996\n",
      "  episode_reward_min: -16.38999999999998\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1061\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.552895008193122\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007886562779589844\n",
      "          policy_loss: 0.11250063363048765\n",
      "          total_loss: 0.806514526075787\n",
      "          vf_explained_var: 0.7123358249664307\n",
      "          vf_loss: 0.7057013144095738\n",
      "    num_agent_steps_sampled: 353000\n",
      "    num_agent_steps_trained: 353000\n",
      "    num_steps_sampled: 353000\n",
      "    num_steps_trained: 353000\n",
      "  iterations_since_restore: 353\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.41827411167513\n",
      "    ram_util_percent: 32.38223350253808\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668222777808912\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.57990388356511\n",
      "    mean_inference_ms: 2.1125178537046887\n",
      "    mean_raw_obs_processing_ms: 6.3919115246615545\n",
      "  time_since_restore: 10799.038326978683\n",
      "  time_this_iter_s: 137.8369176387787\n",
      "  time_total_s: 10799.038326978683\n",
      "  timers:\n",
      "    learn_throughput: 1327.299\n",
      "    learn_time_ms: 753.41\n",
      "    load_throughput: 41691.805\n",
      "    load_time_ms: 23.986\n",
      "    sample_throughput: 10.727\n",
      "    sample_time_ms: 93219.343\n",
      "    update_time_ms: 3.792\n",
      "  timestamp: 1635293378\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 353000\n",
      "  training_iteration: 353\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   353</td><td style=\"text-align: right;\">           10799</td><td style=\"text-align: right;\">353000</td><td style=\"text-align: right;\">  0.0833</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -16.39</td><td style=\"text-align: right;\">            213.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 354000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-10-48\n",
      "  done: false\n",
      "  episode_len_mean: 211.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.560000000000006\n",
      "  episode_reward_mean: -0.04409999999998004\n",
      "  episode_reward_min: -16.38999999999998\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1066\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6305907726287843\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00786967824930527\n",
      "          policy_loss: -0.0598733032329215\n",
      "          total_loss: 0.7187519624829293\n",
      "          vf_explained_var: 0.645844578742981\n",
      "          vf_loss: 0.79109787940979\n",
      "    num_agent_steps_sampled: 354000\n",
      "    num_agent_steps_trained: 354000\n",
      "    num_steps_sampled: 354000\n",
      "    num_steps_trained: 354000\n",
      "  iterations_since_restore: 354\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.45346534653466\n",
      "    ram_util_percent: 32.37623762376238\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036681864778320084\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.60590600906981\n",
      "    mean_inference_ms: 2.112523571774744\n",
      "    mean_raw_obs_processing_ms: 6.516410824890003\n",
      "  time_since_restore: 10869.708832502365\n",
      "  time_this_iter_s: 70.67050552368164\n",
      "  time_total_s: 10869.708832502365\n",
      "  timers:\n",
      "    learn_throughput: 1324.934\n",
      "    learn_time_ms: 754.754\n",
      "    load_throughput: 41639.612\n",
      "    load_time_ms: 24.016\n",
      "    sample_throughput: 10.718\n",
      "    sample_time_ms: 93301.781\n",
      "    update_time_ms: 3.786\n",
      "  timestamp: 1635293448\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 354000\n",
      "  training_iteration: 354\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   354</td><td style=\"text-align: right;\">         10869.7</td><td style=\"text-align: right;\">354000</td><td style=\"text-align: right;\"> -0.0441</td><td style=\"text-align: right;\">                9.56</td><td style=\"text-align: right;\">              -16.39</td><td style=\"text-align: right;\">            211.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 355000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-12-47\n",
      "  done: false\n",
      "  episode_len_mean: 205.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.560000000000006\n",
      "  episode_reward_mean: 0.15670000000001874\n",
      "  episode_reward_min: -18.760000000000044\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1072\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.883077159192827\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011157092502352603\n",
      "          policy_loss: -0.13733291344510185\n",
      "          total_loss: 0.9656878021028307\n",
      "          vf_explained_var: 0.6382338404655457\n",
      "          vf_loss: 1.1164168887668187\n",
      "    num_agent_steps_sampled: 355000\n",
      "    num_agent_steps_trained: 355000\n",
      "    num_steps_sampled: 355000\n",
      "    num_steps_trained: 355000\n",
      "  iterations_since_restore: 355\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.99529411764706\n",
      "    ram_util_percent: 32.39588235294117\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668144831754703\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.63682600684957\n",
      "    mean_inference_ms: 2.112531457452584\n",
      "    mean_raw_obs_processing_ms: 6.6757421187538215\n",
      "  time_since_restore: 10988.58662724495\n",
      "  time_this_iter_s: 118.87779474258423\n",
      "  time_total_s: 10988.58662724495\n",
      "  timers:\n",
      "    learn_throughput: 1327.167\n",
      "    learn_time_ms: 753.485\n",
      "    load_throughput: 41577.243\n",
      "    load_time_ms: 24.052\n",
      "    sample_throughput: 10.323\n",
      "    sample_time_ms: 96869.554\n",
      "    update_time_ms: 3.874\n",
      "  timestamp: 1635293567\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 355000\n",
      "  training_iteration: 355\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   355</td><td style=\"text-align: right;\">         10988.6</td><td style=\"text-align: right;\">355000</td><td style=\"text-align: right;\">  0.1567</td><td style=\"text-align: right;\">                9.56</td><td style=\"text-align: right;\">              -18.76</td><td style=\"text-align: right;\">            205.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 356000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-13-16\n",
      "  done: false\n",
      "  episode_len_mean: 205.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.560000000000006\n",
      "  episode_reward_mean: -0.004099999999981225\n",
      "  episode_reward_min: -18.760000000000044\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1075\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.130474030971527\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010387345173263639\n",
      "          policy_loss: -0.05827204262216886\n",
      "          total_loss: 0.4890740155345864\n",
      "          vf_explained_var: 0.6024348139762878\n",
      "          vf_loss: 0.5635911342170503\n",
      "    num_agent_steps_sampled: 356000\n",
      "    num_agent_steps_trained: 356000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "  iterations_since_restore: 356\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.5775\n",
      "    ram_util_percent: 32.4125\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668124277572737\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.652204350189677\n",
      "    mean_inference_ms: 2.112535004645855\n",
      "    mean_raw_obs_processing_ms: 6.751432326269521\n",
      "  time_since_restore: 11017.199358701706\n",
      "  time_this_iter_s: 28.612731456756592\n",
      "  time_total_s: 11017.199358701706\n",
      "  timers:\n",
      "    learn_throughput: 1328.534\n",
      "    learn_time_ms: 752.709\n",
      "    load_throughput: 42027.894\n",
      "    load_time_ms: 23.794\n",
      "    sample_throughput: 10.822\n",
      "    sample_time_ms: 92404.675\n",
      "    update_time_ms: 3.869\n",
      "  timestamp: 1635293596\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 356\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   356</td><td style=\"text-align: right;\">         11017.2</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\"> -0.0041</td><td style=\"text-align: right;\">                9.56</td><td style=\"text-align: right;\">              -18.76</td><td style=\"text-align: right;\">            205.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 357000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-14-03\n",
      "  done: false\n",
      "  episode_len_mean: 205.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.560000000000006\n",
      "  episode_reward_mean: 0.023700000000018803\n",
      "  episode_reward_min: -18.760000000000044\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1080\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9331369890107049\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0073957822012925825\n",
      "          policy_loss: -0.0371540749238597\n",
      "          total_loss: 0.6233169436454773\n",
      "          vf_explained_var: 0.7533852458000183\n",
      "          vf_loss: 0.6761999216344622\n",
      "    num_agent_steps_sampled: 357000\n",
      "    num_agent_steps_trained: 357000\n",
      "    num_steps_sampled: 357000\n",
      "    num_steps_trained: 357000\n",
      "  iterations_since_restore: 357\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.23880597014926\n",
      "    ram_util_percent: 32.41044776119403\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036680916596973444\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.677505865902482\n",
      "    mean_inference_ms: 2.1125411682327253\n",
      "    mean_raw_obs_processing_ms: 6.8742988904392375\n",
      "  time_since_restore: 11064.064681529999\n",
      "  time_this_iter_s: 46.86532282829285\n",
      "  time_total_s: 11064.064681529999\n",
      "  timers:\n",
      "    learn_throughput: 1330.623\n",
      "    learn_time_ms: 751.528\n",
      "    load_throughput: 42039.9\n",
      "    load_time_ms: 23.787\n",
      "    sample_throughput: 10.859\n",
      "    sample_time_ms: 92087.905\n",
      "    update_time_ms: 3.861\n",
      "  timestamp: 1635293643\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 357000\n",
      "  training_iteration: 357\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   357</td><td style=\"text-align: right;\">         11064.1</td><td style=\"text-align: right;\">357000</td><td style=\"text-align: right;\">  0.0237</td><td style=\"text-align: right;\">                9.56</td><td style=\"text-align: right;\">              -18.76</td><td style=\"text-align: right;\">            205.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 358000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-15-07\n",
      "  done: false\n",
      "  episode_len_mean: 206.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.560000000000006\n",
      "  episode_reward_mean: -0.06299999999998171\n",
      "  episode_reward_min: -18.760000000000044\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1085\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8865238388379415\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0059301625651461895\n",
      "          policy_loss: -0.013266207029422125\n",
      "          total_loss: 0.27983966320753095\n",
      "          vf_explained_var: 0.9045630693435669\n",
      "          vf_loss: 0.3090825361510118\n",
      "    num_agent_steps_sampled: 358000\n",
      "    num_agent_steps_trained: 358000\n",
      "    num_steps_sampled: 358000\n",
      "    num_steps_trained: 358000\n",
      "  iterations_since_restore: 358\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.55054945054945\n",
      "    ram_util_percent: 32.46923076923076\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036680587713254414\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.702390262547034\n",
      "    mean_inference_ms: 2.1125472885572028\n",
      "    mean_raw_obs_processing_ms: 6.997605851089195\n",
      "  time_since_restore: 11127.852730512619\n",
      "  time_this_iter_s: 63.78804898262024\n",
      "  time_total_s: 11127.852730512619\n",
      "  timers:\n",
      "    learn_throughput: 1328.19\n",
      "    learn_time_ms: 752.905\n",
      "    load_throughput: 41848.462\n",
      "    load_time_ms: 23.896\n",
      "    sample_throughput: 10.681\n",
      "    sample_time_ms: 93625.186\n",
      "    update_time_ms: 3.794\n",
      "  timestamp: 1635293707\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 358000\n",
      "  training_iteration: 358\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   358</td><td style=\"text-align: right;\">         11127.9</td><td style=\"text-align: right;\">358000</td><td style=\"text-align: right;\">  -0.063</td><td style=\"text-align: right;\">                9.56</td><td style=\"text-align: right;\">              -18.76</td><td style=\"text-align: right;\">            206.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 359000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-16-00\n",
      "  done: false\n",
      "  episode_len_mean: 205.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.560000000000006\n",
      "  episode_reward_mean: 0.04100000000001804\n",
      "  episode_reward_min: -18.760000000000044\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1089\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9155871735678778\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0063766103037360975\n",
      "          policy_loss: 0.025566605064604016\n",
      "          total_loss: 0.45766020702819027\n",
      "          vf_explained_var: 0.786939263343811\n",
      "          vf_loss: 0.44814343402783074\n",
      "    num_agent_steps_sampled: 359000\n",
      "    num_agent_steps_trained: 359000\n",
      "    num_steps_sampled: 359000\n",
      "    num_steps_trained: 359000\n",
      "  iterations_since_restore: 359\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.52597402597402\n",
      "    ram_util_percent: 32.42857142857142\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036680327338350735\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.721811275154863\n",
      "    mean_inference_ms: 2.1125525503294345\n",
      "    mean_raw_obs_processing_ms: 7.091763481284245\n",
      "  time_since_restore: 11181.182803153992\n",
      "  time_this_iter_s: 53.33007264137268\n",
      "  time_total_s: 11181.182803153992\n",
      "  timers:\n",
      "    learn_throughput: 1328.415\n",
      "    learn_time_ms: 752.777\n",
      "    load_throughput: 42047.275\n",
      "    load_time_ms: 23.783\n",
      "    sample_throughput: 11.546\n",
      "    sample_time_ms: 86611.604\n",
      "    update_time_ms: 3.794\n",
      "  timestamp: 1635293760\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 359000\n",
      "  training_iteration: 359\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   359</td><td style=\"text-align: right;\">         11181.2</td><td style=\"text-align: right;\">359000</td><td style=\"text-align: right;\">   0.041</td><td style=\"text-align: right;\">                9.56</td><td style=\"text-align: right;\">              -18.76</td><td style=\"text-align: right;\">            205.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-18-08\n",
      "  done: false\n",
      "  episode_len_mean: 206.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.560000000000006\n",
      "  episode_reward_mean: 0.056400000000018886\n",
      "  episode_reward_min: -18.760000000000044\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1095\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9168499204847547\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012195524385387478\n",
      "          policy_loss: -0.07202029960850874\n",
      "          total_loss: 0.5670050617721346\n",
      "          vf_explained_var: 0.8275784850120544\n",
      "          vf_loss: 0.6522534449895223\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "  iterations_since_restore: 360\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.47472527472527\n",
      "    ram_util_percent: 32.449999999999996\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667994436145702\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.751673681963023\n",
      "    mean_inference_ms: 2.112561608507046\n",
      "    mean_raw_obs_processing_ms: 7.2383871343721395\n",
      "  time_since_restore: 11309.15188908577\n",
      "  time_this_iter_s: 127.96908593177795\n",
      "  time_total_s: 11309.15188908577\n",
      "  timers:\n",
      "    learn_throughput: 1328.289\n",
      "    learn_time_ms: 752.848\n",
      "    load_throughput: 42016.274\n",
      "    load_time_ms: 23.8\n",
      "    sample_throughput: 11.682\n",
      "    sample_time_ms: 85599.753\n",
      "    update_time_ms: 3.792\n",
      "  timestamp: 1635293888\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 360\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   360</td><td style=\"text-align: right;\">         11309.2</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\">  0.0564</td><td style=\"text-align: right;\">                9.56</td><td style=\"text-align: right;\">              -18.76</td><td style=\"text-align: right;\">            206.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 361000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-19-32\n",
      "  done: false\n",
      "  episode_len_mean: 204.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.560000000000006\n",
      "  episode_reward_mean: 0.2849000000000188\n",
      "  episode_reward_min: -18.760000000000044\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1100\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0333522809876334\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010288722676999552\n",
      "          policy_loss: -0.15757371307247214\n",
      "          total_loss: 0.15425540047387282\n",
      "          vf_explained_var: 0.9347923994064331\n",
      "          vf_loss: 0.3271510172221396\n",
      "    num_agent_steps_sampled: 361000\n",
      "    num_agent_steps_trained: 361000\n",
      "    num_steps_sampled: 361000\n",
      "    num_steps_trained: 361000\n",
      "  iterations_since_restore: 361\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.62083333333333\n",
      "    ram_util_percent: 32.39666666666666\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0366796250217697\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.776562882881294\n",
      "    mean_inference_ms: 2.1125690669469437\n",
      "    mean_raw_obs_processing_ms: 7.366085585261177\n",
      "  time_since_restore: 11393.121272087097\n",
      "  time_this_iter_s: 83.96938300132751\n",
      "  time_total_s: 11393.121272087097\n",
      "  timers:\n",
      "    learn_throughput: 1326.451\n",
      "    learn_time_ms: 753.891\n",
      "    load_throughput: 41826.845\n",
      "    load_time_ms: 23.908\n",
      "    sample_throughput: 12.769\n",
      "    sample_time_ms: 78316.793\n",
      "    update_time_ms: 3.776\n",
      "  timestamp: 1635293972\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 361000\n",
      "  training_iteration: 361\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   361</td><td style=\"text-align: right;\">         11393.1</td><td style=\"text-align: right;\">361000</td><td style=\"text-align: right;\">  0.2849</td><td style=\"text-align: right;\">                9.56</td><td style=\"text-align: right;\">              -18.76</td><td style=\"text-align: right;\">            204.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 362000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-21-33\n",
      "  done: false\n",
      "  episode_len_mean: 198.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.730000000000002\n",
      "  episode_reward_mean: 0.7993000000000179\n",
      "  episode_reward_min: -18.760000000000044\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1107\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7651997221840752\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008406074343985262\n",
      "          policy_loss: -0.03764676849047343\n",
      "          total_loss: 0.714748902618885\n",
      "          vf_explained_var: 0.7388554811477661\n",
      "          vf_loss: 0.7659530849092536\n",
      "    num_agent_steps_sampled: 362000\n",
      "    num_agent_steps_trained: 362000\n",
      "    num_steps_sampled: 362000\n",
      "    num_steps_trained: 362000\n",
      "  iterations_since_restore: 362\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.61213872832371\n",
      "    ram_util_percent: 32.394219653179185\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036679172071354044\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.810376799016936\n",
      "    mean_inference_ms: 2.1125794992057627\n",
      "    mean_raw_obs_processing_ms: 7.549405123479101\n",
      "  time_since_restore: 11514.30111837387\n",
      "  time_this_iter_s: 121.17984628677368\n",
      "  time_total_s: 11514.30111837387\n",
      "  timers:\n",
      "    learn_throughput: 1327.534\n",
      "    learn_time_ms: 753.276\n",
      "    load_throughput: 41820.339\n",
      "    load_time_ms: 23.912\n",
      "    sample_throughput: 11.831\n",
      "    sample_time_ms: 84524.778\n",
      "    update_time_ms: 3.775\n",
      "  timestamp: 1635294093\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 362000\n",
      "  training_iteration: 362\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   362</td><td style=\"text-align: right;\">         11514.3</td><td style=\"text-align: right;\">362000</td><td style=\"text-align: right;\">  0.7993</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -18.76</td><td style=\"text-align: right;\">            198.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 363000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-22-20\n",
      "  done: false\n",
      "  episode_len_mean: 197.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.730000000000002\n",
      "  episode_reward_mean: 1.1087000000000176\n",
      "  episode_reward_min: -18.760000000000044\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1111\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.020669662952423\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008057743210778125\n",
      "          policy_loss: -0.008646600445111592\n",
      "          total_loss: 0.252763316863113\n",
      "          vf_explained_var: 0.7694981098175049\n",
      "          vf_loss: 0.2776917027102576\n",
      "    num_agent_steps_sampled: 363000\n",
      "    num_agent_steps_trained: 363000\n",
      "    num_steps_sampled: 363000\n",
      "    num_steps_trained: 363000\n",
      "  iterations_since_restore: 363\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.198507462686564\n",
      "    ram_util_percent: 32.40746268656716\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667889636607699\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.829388176413694\n",
      "    mean_inference_ms: 2.112585020129142\n",
      "    mean_raw_obs_processing_ms: 7.653469311060799\n",
      "  time_since_restore: 11561.17037987709\n",
      "  time_this_iter_s: 46.869261503219604\n",
      "  time_total_s: 11561.17037987709\n",
      "  timers:\n",
      "    learn_throughput: 1327.426\n",
      "    learn_time_ms: 753.338\n",
      "    load_throughput: 41751.567\n",
      "    load_time_ms: 23.951\n",
      "    sample_throughput: 13.258\n",
      "    sample_time_ms: 75427.913\n",
      "    update_time_ms: 3.778\n",
      "  timestamp: 1635294140\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 363000\n",
      "  training_iteration: 363\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   363</td><td style=\"text-align: right;\">         11561.2</td><td style=\"text-align: right;\">363000</td><td style=\"text-align: right;\">  1.1087</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -18.76</td><td style=\"text-align: right;\">            197.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 364000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-23-54\n",
      "  done: false\n",
      "  episode_len_mean: 195.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.730000000000002\n",
      "  episode_reward_mean: 1.262200000000017\n",
      "  episode_reward_min: -18.760000000000044\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1115\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5907949149608611\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.14748315985273877\n",
      "          policy_loss: 0.08996671628620889\n",
      "          total_loss: 1.6884722711311446\n",
      "          vf_explained_var: 0.32677987217903137\n",
      "          vf_loss: 1.5425747129652234\n",
      "    num_agent_steps_sampled: 364000\n",
      "    num_agent_steps_trained: 364000\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "  iterations_since_restore: 364\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.54074074074074\n",
      "    ram_util_percent: 32.45185185185185\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667860549985928\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.848454706289303\n",
      "    mean_inference_ms: 2.112590209318438\n",
      "    mean_raw_obs_processing_ms: 7.759191940059047\n",
      "  time_since_restore: 11655.563615083694\n",
      "  time_this_iter_s: 94.393235206604\n",
      "  time_total_s: 11655.563615083694\n",
      "  timers:\n",
      "    learn_throughput: 1328.608\n",
      "    learn_time_ms: 752.667\n",
      "    load_throughput: 41580.087\n",
      "    load_time_ms: 24.05\n",
      "    sample_throughput: 12.853\n",
      "    sample_time_ms: 77800.762\n",
      "    update_time_ms: 3.788\n",
      "  timestamp: 1635294234\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 364\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   364</td><td style=\"text-align: right;\">         11655.6</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\">  1.2622</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -18.76</td><td style=\"text-align: right;\">            195.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 365000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-25-12\n",
      "  done: false\n",
      "  episode_len_mean: 195.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.730000000000002\n",
      "  episode_reward_mean: 1.2967000000000177\n",
      "  episode_reward_min: -18.760000000000044\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1121\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7306472307303924\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9998515393998888\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006852155155977815\n",
      "          policy_loss: 0.059519175026151866\n",
      "          total_loss: 0.41579549958308537\n",
      "          vf_explained_var: 0.022624628618359566\n",
      "          vf_loss: 0.37126833079382776\n",
      "    num_agent_steps_sampled: 365000\n",
      "    num_agent_steps_trained: 365000\n",
      "    num_steps_sampled: 365000\n",
      "    num_steps_trained: 365000\n",
      "  iterations_since_restore: 365\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.99090909090908\n",
      "    ram_util_percent: 32.35636363636363\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667816861567758\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.875484779854613\n",
      "    mean_inference_ms: 2.112597761692567\n",
      "    mean_raw_obs_processing_ms: 7.917122826516049\n",
      "  time_since_restore: 11732.736497879028\n",
      "  time_this_iter_s: 77.17288279533386\n",
      "  time_total_s: 11732.736497879028\n",
      "  timers:\n",
      "    learn_throughput: 1328.604\n",
      "    learn_time_ms: 752.67\n",
      "    load_throughput: 41636.347\n",
      "    load_time_ms: 24.017\n",
      "    sample_throughput: 13.581\n",
      "    sample_time_ms: 73630.375\n",
      "    update_time_ms: 3.698\n",
      "  timestamp: 1635294312\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 365000\n",
      "  training_iteration: 365\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   365</td><td style=\"text-align: right;\">         11732.7</td><td style=\"text-align: right;\">365000</td><td style=\"text-align: right;\">  1.2967</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -18.76</td><td style=\"text-align: right;\">            195.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 366000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-26-17\n",
      "  done: false\n",
      "  episode_len_mean: 196.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.730000000000002\n",
      "  episode_reward_mean: 1.1874000000000178\n",
      "  episode_reward_min: -18.760000000000044\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1124\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7306472307303924\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9258229931195576\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007801424970945314\n",
      "          policy_loss: -0.04984976293312179\n",
      "          total_loss: 0.4994262651436859\n",
      "          vf_explained_var: 0.7772963643074036\n",
      "          vf_loss: 0.5628341691361534\n",
      "    num_agent_steps_sampled: 366000\n",
      "    num_agent_steps_trained: 366000\n",
      "    num_steps_sampled: 366000\n",
      "    num_steps_trained: 366000\n",
      "  iterations_since_restore: 366\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.60957446808512\n",
      "    ram_util_percent: 32.39787234042553\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667796532822193\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.88866213709325\n",
      "    mean_inference_ms: 2.112602097016955\n",
      "    mean_raw_obs_processing_ms: 7.996010855366875\n",
      "  time_since_restore: 11798.369777202606\n",
      "  time_this_iter_s: 65.63327932357788\n",
      "  time_total_s: 11798.369777202606\n",
      "  timers:\n",
      "    learn_throughput: 1328.559\n",
      "    learn_time_ms: 752.695\n",
      "    load_throughput: 41403.143\n",
      "    load_time_ms: 24.153\n",
      "    sample_throughput: 12.931\n",
      "    sample_time_ms: 77332.257\n",
      "    update_time_ms: 3.712\n",
      "  timestamp: 1635294377\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 366000\n",
      "  training_iteration: 366\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   366</td><td style=\"text-align: right;\">         11798.4</td><td style=\"text-align: right;\">366000</td><td style=\"text-align: right;\">  1.1874</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -18.76</td><td style=\"text-align: right;\">            196.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 367000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-27-42\n",
      "  done: false\n",
      "  episode_len_mean: 194.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.730000000000002\n",
      "  episode_reward_mean: 1.6535000000000164\n",
      "  episode_reward_min: -18.760000000000044\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1129\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7306472307303924\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9502778344684177\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00609173311520394\n",
      "          policy_loss: 0.021336968160337873\n",
      "          total_loss: 0.41124137971136304\n",
      "          vf_explained_var: 0.698144257068634\n",
      "          vf_loss: 0.40495627754264407\n",
      "    num_agent_steps_sampled: 367000\n",
      "    num_agent_steps_trained: 367000\n",
      "    num_steps_sampled: 367000\n",
      "    num_steps_trained: 367000\n",
      "  iterations_since_restore: 367\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.8225\n",
      "    ram_util_percent: 32.446666666666665\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036677676439600064\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.910341590304494\n",
      "    mean_inference_ms: 2.112610832715048\n",
      "    mean_raw_obs_processing_ms: 8.131755217707724\n",
      "  time_since_restore: 11882.673647880554\n",
      "  time_this_iter_s: 84.303870677948\n",
      "  time_total_s: 11882.673647880554\n",
      "  timers:\n",
      "    learn_throughput: 1327.761\n",
      "    learn_time_ms: 753.147\n",
      "    load_throughput: 41372.227\n",
      "    load_time_ms: 24.171\n",
      "    sample_throughput: 12.334\n",
      "    sample_time_ms: 81075.624\n",
      "    update_time_ms: 3.729\n",
      "  timestamp: 1635294462\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 367000\n",
      "  training_iteration: 367\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   367</td><td style=\"text-align: right;\">         11882.7</td><td style=\"text-align: right;\">367000</td><td style=\"text-align: right;\">  1.6535</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -18.76</td><td style=\"text-align: right;\">            194.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 368000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-28-03\n",
      "  done: false\n",
      "  episode_len_mean: 197.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.730000000000002\n",
      "  episode_reward_mean: 1.4291000000000167\n",
      "  episode_reward_min: -18.760000000000044\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1131\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7306472307303924\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1697054915957983\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006138069921719315\n",
      "          policy_loss: -0.11613123897049162\n",
      "          total_loss: -0.016948493321736653\n",
      "          vf_explained_var: 0.26829203963279724\n",
      "          vf_loss: 0.11639503840770986\n",
      "    num_agent_steps_sampled: 368000\n",
      "    num_agent_steps_trained: 368000\n",
      "    num_steps_sampled: 368000\n",
      "    num_steps_trained: 368000\n",
      "  iterations_since_restore: 368\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.71612903225805\n",
      "    ram_util_percent: 32.40322580645161\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036677571421065605\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.91864353084925\n",
      "    mean_inference_ms: 2.112613898062319\n",
      "    mean_raw_obs_processing_ms: 8.185193164401499\n",
      "  time_since_restore: 11904.593652963638\n",
      "  time_this_iter_s: 21.920005083084106\n",
      "  time_total_s: 11904.593652963638\n",
      "  timers:\n",
      "    learn_throughput: 1328.616\n",
      "    learn_time_ms: 752.663\n",
      "    load_throughput: 41606.279\n",
      "    load_time_ms: 24.035\n",
      "    sample_throughput: 13.006\n",
      "    sample_time_ms: 76889.335\n",
      "    update_time_ms: 3.81\n",
      "  timestamp: 1635294483\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 368000\n",
      "  training_iteration: 368\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   368</td><td style=\"text-align: right;\">         11904.6</td><td style=\"text-align: right;\">368000</td><td style=\"text-align: right;\">  1.4291</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -18.76</td><td style=\"text-align: right;\">            197.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 369000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-29-29\n",
      "  done: false\n",
      "  episode_len_mean: 201.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.730000000000002\n",
      "  episode_reward_mean: 1.1987000000000172\n",
      "  episode_reward_min: -18.760000000000044\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1136\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7306472307303924\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9581111166212293\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.050166494493445964\n",
      "          policy_loss: 0.02582622567812602\n",
      "          total_loss: 1.4907393997328149\n",
      "          vf_explained_var: 0.5641844868659973\n",
      "          vf_loss: 1.44784027867847\n",
      "    num_agent_steps_sampled: 369000\n",
      "    num_agent_steps_trained: 369000\n",
      "    num_steps_sampled: 369000\n",
      "    num_steps_trained: 369000\n",
      "  iterations_since_restore: 369\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.70081300813009\n",
      "    ram_util_percent: 32.40650406504065\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036677308985091035\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.939315858185097\n",
      "    mean_inference_ms: 2.112621564834814\n",
      "    mean_raw_obs_processing_ms: 8.312623881069618\n",
      "  time_since_restore: 11990.439201593399\n",
      "  time_this_iter_s: 85.84554862976074\n",
      "  time_total_s: 11990.439201593399\n",
      "  timers:\n",
      "    learn_throughput: 1330.078\n",
      "    learn_time_ms: 751.836\n",
      "    load_throughput: 41554.3\n",
      "    load_time_ms: 24.065\n",
      "    sample_throughput: 12.478\n",
      "    sample_time_ms: 80141.729\n",
      "    update_time_ms: 3.761\n",
      "  timestamp: 1635294569\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 369000\n",
      "  training_iteration: 369\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   369</td><td style=\"text-align: right;\">         11990.4</td><td style=\"text-align: right;\">369000</td><td style=\"text-align: right;\">  1.1987</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -18.76</td><td style=\"text-align: right;\">               201</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 370000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-30-06\n",
      "  done: false\n",
      "  episode_len_mean: 203.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.730000000000002\n",
      "  episode_reward_mean: 1.2189000000000174\n",
      "  episode_reward_min: -18.760000000000044\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1138\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0959708460955877\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0636241051885817\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003949121268386099\n",
      "          policy_loss: -0.13319630862938034\n",
      "          total_loss: 0.04171207509934902\n",
      "          vf_explained_var: 0.8473771214485168\n",
      "          vf_loss: 0.19121650137628118\n",
      "    num_agent_steps_sampled: 370000\n",
      "    num_agent_steps_trained: 370000\n",
      "    num_steps_sampled: 370000\n",
      "    num_steps_trained: 370000\n",
      "  iterations_since_restore: 370\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.89999999999999\n",
      "    ram_util_percent: 32.475\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667720175592625\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.94707789749753\n",
      "    mean_inference_ms: 2.112623710649751\n",
      "    mean_raw_obs_processing_ms: 8.36122394584801\n",
      "  time_since_restore: 12027.028142929077\n",
      "  time_this_iter_s: 36.5889413356781\n",
      "  time_total_s: 12027.028142929077\n",
      "  timers:\n",
      "    learn_throughput: 1329.028\n",
      "    learn_time_ms: 752.43\n",
      "    load_throughput: 41371.738\n",
      "    load_time_ms: 24.171\n",
      "    sample_throughput: 14.084\n",
      "    sample_time_ms: 71002.926\n",
      "    update_time_ms: 3.86\n",
      "  timestamp: 1635294606\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 370000\n",
      "  training_iteration: 370\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   370</td><td style=\"text-align: right;\">           12027</td><td style=\"text-align: right;\">370000</td><td style=\"text-align: right;\">  1.2189</td><td style=\"text-align: right;\">                9.73</td><td style=\"text-align: right;\">              -18.76</td><td style=\"text-align: right;\">            203.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 371000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-32-41\n",
      "  done: false\n",
      "  episode_len_mean: 206.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.830000000000002\n",
      "  episode_reward_mean: 1.363700000000018\n",
      "  episode_reward_min: -18.760000000000044\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1146\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5479854230477939\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9455869886610242\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021018849167887942\n",
      "          policy_loss: -0.003312432434823778\n",
      "          total_loss: 1.1259029908312692\n",
      "          vf_explained_var: 0.6482546329498291\n",
      "          vf_loss: 1.1371532764699723\n",
      "    num_agent_steps_sampled: 371000\n",
      "    num_agent_steps_trained: 371000\n",
      "    num_steps_sampled: 371000\n",
      "    num_steps_trained: 371000\n",
      "  iterations_since_restore: 371\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.29457013574661\n",
      "    ram_util_percent: 32.40588235294117\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036676756586992675\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.977072379513036\n",
      "    mean_inference_ms: 2.112630219553104\n",
      "    mean_raw_obs_processing_ms: 8.563397488706435\n",
      "  time_since_restore: 12181.774518013\n",
      "  time_this_iter_s: 154.74637508392334\n",
      "  time_total_s: 12181.774518013\n",
      "  timers:\n",
      "    learn_throughput: 1327.927\n",
      "    learn_time_ms: 753.053\n",
      "    load_throughput: 41606.485\n",
      "    load_time_ms: 24.035\n",
      "    sample_throughput: 12.807\n",
      "    sample_time_ms: 78080.2\n",
      "    update_time_ms: 3.795\n",
      "  timestamp: 1635294761\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 371000\n",
      "  training_iteration: 371\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   371</td><td style=\"text-align: right;\">         12181.8</td><td style=\"text-align: right;\">371000</td><td style=\"text-align: right;\">  1.3637</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">              -18.76</td><td style=\"text-align: right;\">            206.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 372000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-33-03\n",
      "  done: false\n",
      "  episode_len_mean: 211.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.830000000000002\n",
      "  episode_reward_mean: 1.3126000000000193\n",
      "  episode_reward_min: -18.760000000000044\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1149\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.821978134571691\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1951043592558968\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013851890977163469\n",
      "          policy_loss: 0.12290278209580316\n",
      "          total_loss: 0.49388930607173176\n",
      "          vf_explained_var: 0.5390939712524414\n",
      "          vf_loss: 0.381551605131891\n",
      "    num_agent_steps_sampled: 372000\n",
      "    num_agent_steps_trained: 372000\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "  iterations_since_restore: 372\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.11250000000001\n",
      "    ram_util_percent: 32.403125\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667659951133345\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.987536494402303\n",
      "    mean_inference_ms: 2.1126320784331436\n",
      "    mean_raw_obs_processing_ms: 8.631021791217512\n",
      "  time_since_restore: 12204.228288173676\n",
      "  time_this_iter_s: 22.45377016067505\n",
      "  time_total_s: 12204.228288173676\n",
      "  timers:\n",
      "    learn_throughput: 1327.355\n",
      "    learn_time_ms: 753.378\n",
      "    load_throughput: 41618.251\n",
      "    load_time_ms: 24.028\n",
      "    sample_throughput: 14.661\n",
      "    sample_time_ms: 68207.185\n",
      "    update_time_ms: 3.885\n",
      "  timestamp: 1635294783\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 372\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   372</td><td style=\"text-align: right;\">         12204.2</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\">  1.3126</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">              -18.76</td><td style=\"text-align: right;\">            211.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 373000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-34-18\n",
      "  done: false\n",
      "  episode_len_mean: 214.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.830000000000002\n",
      "  episode_reward_mean: 1.2908000000000197\n",
      "  episode_reward_min: -18.760000000000044\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1153\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.821978134571691\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1350719703568353\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0068793828860912575\n",
      "          policy_loss: -0.10784191936254502\n",
      "          total_loss: 0.1267677483873235\n",
      "          vf_explained_var: 0.7667474746704102\n",
      "          vf_loss: 0.2503056818826331\n",
      "    num_agent_steps_sampled: 373000\n",
      "    num_agent_steps_trained: 373000\n",
      "    num_steps_sampled: 373000\n",
      "    num_steps_trained: 373000\n",
      "  iterations_since_restore: 373\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.358878504672894\n",
      "    ram_util_percent: 32.43364485981308\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667640074356804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.000988432312862\n",
      "    mean_inference_ms: 2.112634001383157\n",
      "    mean_raw_obs_processing_ms: 8.724569325594542\n",
      "  time_since_restore: 12279.194473981857\n",
      "  time_this_iter_s: 74.96618580818176\n",
      "  time_total_s: 12279.194473981857\n",
      "  timers:\n",
      "    learn_throughput: 1328.485\n",
      "    learn_time_ms: 752.737\n",
      "    load_throughput: 41583.756\n",
      "    load_time_ms: 24.048\n",
      "    sample_throughput: 14.081\n",
      "    sample_time_ms: 71017.504\n",
      "    update_time_ms: 3.873\n",
      "  timestamp: 1635294858\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 373000\n",
      "  training_iteration: 373\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   373</td><td style=\"text-align: right;\">         12279.2</td><td style=\"text-align: right;\">373000</td><td style=\"text-align: right;\">  1.2908</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">              -18.76</td><td style=\"text-align: right;\">            214.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 374000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-36-40\n",
      "  done: false\n",
      "  episode_len_mean: 210.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.830000000000002\n",
      "  episode_reward_mean: 1.232800000000019\n",
      "  episode_reward_min: -18.760000000000044\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1161\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.821978134571691\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6769669082429675\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006080325573590371\n",
      "          policy_loss: 0.033785982347197\n",
      "          total_loss: 0.490731755644083\n",
      "          vf_explained_var: 0.8257990479469299\n",
      "          vf_loss: 0.46871754883064165\n",
      "    num_agent_steps_sampled: 374000\n",
      "    num_agent_steps_trained: 374000\n",
      "    num_steps_sampled: 374000\n",
      "    num_steps_trained: 374000\n",
      "  iterations_since_restore: 374\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.69851485148514\n",
      "    ram_util_percent: 32.45346534653466\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667609763058963\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.027032921584656\n",
      "    mean_inference_ms: 2.112638961328286\n",
      "    mean_raw_obs_processing_ms: 8.911211074959796\n",
      "  time_since_restore: 12421.04712677002\n",
      "  time_this_iter_s: 141.85265278816223\n",
      "  time_total_s: 12421.04712677002\n",
      "  timers:\n",
      "    learn_throughput: 1329.539\n",
      "    learn_time_ms: 752.141\n",
      "    load_throughput: 41477.415\n",
      "    load_time_ms: 24.11\n",
      "    sample_throughput: 13.199\n",
      "    sample_time_ms: 75763.83\n",
      "    update_time_ms: 4.018\n",
      "  timestamp: 1635295000\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 374000\n",
      "  training_iteration: 374\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   374</td><td style=\"text-align: right;\">           12421</td><td style=\"text-align: right;\">374000</td><td style=\"text-align: right;\">  1.2328</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">              -18.76</td><td style=\"text-align: right;\">            210.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 375000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-38-54\n",
      "  done: false\n",
      "  episode_len_mean: 207.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.830000000000002\n",
      "  episode_reward_mean: 1.361300000000019\n",
      "  episode_reward_min: -18.760000000000044\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1168\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.821978134571691\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.971689420276218\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0070850354010282696\n",
      "          policy_loss: -0.1477844557000531\n",
      "          total_loss: 0.1103654888872471\n",
      "          vf_explained_var: 0.9345457553863525\n",
      "          vf_loss: 0.27204309296276835\n",
      "    num_agent_steps_sampled: 375000\n",
      "    num_agent_steps_trained: 375000\n",
      "    num_steps_sampled: 375000\n",
      "    num_steps_trained: 375000\n",
      "  iterations_since_restore: 375\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.56020942408377\n",
      "    ram_util_percent: 32.415706806282714\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036675867650093424\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.04794020590872\n",
      "    mean_inference_ms: 2.1126442409838675\n",
      "    mean_raw_obs_processing_ms: 9.079570595314154\n",
      "  time_since_restore: 12554.574892282486\n",
      "  time_this_iter_s: 133.52776551246643\n",
      "  time_total_s: 12554.574892282486\n",
      "  timers:\n",
      "    learn_throughput: 1328.407\n",
      "    learn_time_ms: 752.782\n",
      "    load_throughput: 41450.443\n",
      "    load_time_ms: 24.125\n",
      "    sample_throughput: 12.285\n",
      "    sample_time_ms: 81398.679\n",
      "    update_time_ms: 4.019\n",
      "  timestamp: 1635295134\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 375000\n",
      "  training_iteration: 375\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   375</td><td style=\"text-align: right;\">         12554.6</td><td style=\"text-align: right;\">375000</td><td style=\"text-align: right;\">  1.3613</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">              -18.76</td><td style=\"text-align: right;\">            207.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-40-10\n",
      "  done: false\n",
      "  episode_len_mean: 205.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.830000000000002\n",
      "  episode_reward_mean: 1.5515000000000196\n",
      "  episode_reward_min: -14.969999999999965\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1173\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.821978134571691\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.013730952474806\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007401829182838782\n",
      "          policy_loss: -0.03932809399233924\n",
      "          total_loss: 0.15650649062461322\n",
      "          vf_explained_var: 0.8372304439544678\n",
      "          vf_loss: 0.20988775913914046\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_agent_steps_trained: 376000\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 376000\n",
      "  iterations_since_restore: 376\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.717431192660534\n",
      "    ram_util_percent: 32.472477064220186\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667570243762336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.061976735819048\n",
      "    mean_inference_ms: 2.1126479594069347\n",
      "    mean_raw_obs_processing_ms: 9.197637091813895\n",
      "  time_since_restore: 12631.051357746124\n",
      "  time_this_iter_s: 76.4764654636383\n",
      "  time_total_s: 12631.051357746124\n",
      "  timers:\n",
      "    learn_throughput: 1328.808\n",
      "    learn_time_ms: 752.554\n",
      "    load_throughput: 41123.248\n",
      "    load_time_ms: 24.317\n",
      "    sample_throughput: 12.124\n",
      "    sample_time_ms: 82482.976\n",
      "    update_time_ms: 4.091\n",
      "  timestamp: 1635295210\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 376\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   376</td><td style=\"text-align: right;\">         12631.1</td><td style=\"text-align: right;\">376000</td><td style=\"text-align: right;\">  1.5515</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">              -14.97</td><td style=\"text-align: right;\">            205.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 377000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-41-10\n",
      "  done: false\n",
      "  episode_len_mean: 204.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.830000000000002\n",
      "  episode_reward_mean: 1.7442000000000197\n",
      "  episode_reward_min: -9.79999999999994\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1178\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.821978134571691\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9565551903512743\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01020618780657152\n",
      "          policy_loss: 0.07677355984018909\n",
      "          total_loss: 0.6506828299827045\n",
      "          vf_explained_var: 0.797304630279541\n",
      "          vf_loss: 0.5850855501161681\n",
      "    num_agent_steps_sampled: 377000\n",
      "    num_agent_steps_trained: 377000\n",
      "    num_steps_sampled: 377000\n",
      "    num_steps_trained: 377000\n",
      "  iterations_since_restore: 377\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.54\n",
      "    ram_util_percent: 32.65294117647058\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667556594455619\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.074988215489675\n",
      "    mean_inference_ms: 2.1126541456210544\n",
      "    mean_raw_obs_processing_ms: 9.319104140581636\n",
      "  time_since_restore: 12690.700677156448\n",
      "  time_this_iter_s: 59.6493194103241\n",
      "  time_total_s: 12690.700677156448\n",
      "  timers:\n",
      "    learn_throughput: 1328.505\n",
      "    learn_time_ms: 752.726\n",
      "    load_throughput: 41178.358\n",
      "    load_time_ms: 24.285\n",
      "    sample_throughput: 12.497\n",
      "    sample_time_ms: 80017.318\n",
      "    update_time_ms: 4.162\n",
      "  timestamp: 1635295270\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 377000\n",
      "  training_iteration: 377\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   377</td><td style=\"text-align: right;\">         12690.7</td><td style=\"text-align: right;\">377000</td><td style=\"text-align: right;\">  1.7442</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">                -9.8</td><td style=\"text-align: right;\">            204.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 378000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-45-01\n",
      "  done: false\n",
      "  episode_len_mean: 187.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 2.3144000000000178\n",
      "  episode_reward_min: -9.79999999999994\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1190\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.821978134571691\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6587570587793985\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018573777972585265\n",
      "          policy_loss: -0.022021969159444172\n",
      "          total_loss: 0.8222092540727721\n",
      "          vf_explained_var: 0.7283947467803955\n",
      "          vf_loss: 0.8455515540670604\n",
      "    num_agent_steps_sampled: 378000\n",
      "    num_agent_steps_trained: 378000\n",
      "    num_steps_sampled: 378000\n",
      "    num_steps_trained: 378000\n",
      "  iterations_since_restore: 378\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.736555891238666\n",
      "    ram_util_percent: 32.53141993957704\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667524940244937\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.102870603405\n",
      "    mean_inference_ms: 2.112667260239918\n",
      "    mean_raw_obs_processing_ms: 9.659706199523484\n",
      "  time_since_restore: 12922.284852266312\n",
      "  time_this_iter_s: 231.58417510986328\n",
      "  time_total_s: 12922.284852266312\n",
      "  timers:\n",
      "    learn_throughput: 1327.258\n",
      "    learn_time_ms: 753.433\n",
      "    load_throughput: 41246.591\n",
      "    load_time_ms: 24.244\n",
      "    sample_throughput: 9.903\n",
      "    sample_time_ms: 100983.152\n",
      "    update_time_ms: 4.081\n",
      "  timestamp: 1635295501\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 378000\n",
      "  training_iteration: 378\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   378</td><td style=\"text-align: right;\">         12922.3</td><td style=\"text-align: right;\">378000</td><td style=\"text-align: right;\">  2.3144</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">                -9.8</td><td style=\"text-align: right;\">            187.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 379000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-45-23\n",
      "  done: false\n",
      "  episode_len_mean: 192.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 2.083700000000019\n",
      "  episode_reward_min: -9.79999999999994\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1192\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.821978134571691\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.996991397274865\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014457419309241118\n",
      "          policy_loss: 0.022914933496051365\n",
      "          total_loss: 0.3962065984081063\n",
      "          vf_explained_var: 0.35959485173225403\n",
      "          vf_loss: 0.38137789748660805\n",
      "    num_agent_steps_sampled: 379000\n",
      "    num_agent_steps_trained: 379000\n",
      "    num_steps_sampled: 379000\n",
      "    num_steps_trained: 379000\n",
      "  iterations_since_restore: 379\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.13225806451612\n",
      "    ram_util_percent: 32.5774193548387\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667520697152814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.106592611799925\n",
      "    mean_inference_ms: 2.1126685048700016\n",
      "    mean_raw_obs_processing_ms: 9.710775912027472\n",
      "  time_since_restore: 12943.914455652237\n",
      "  time_this_iter_s: 21.629603385925293\n",
      "  time_total_s: 12943.914455652237\n",
      "  timers:\n",
      "    learn_throughput: 1323.93\n",
      "    learn_time_ms: 755.327\n",
      "    load_throughput: 41125.99\n",
      "    load_time_ms: 24.316\n",
      "    sample_throughput: 10.575\n",
      "    sample_time_ms: 94559.636\n",
      "    update_time_ms: 4.043\n",
      "  timestamp: 1635295523\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 379000\n",
      "  training_iteration: 379\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   379</td><td style=\"text-align: right;\">         12943.9</td><td style=\"text-align: right;\">379000</td><td style=\"text-align: right;\">  2.0837</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">                -9.8</td><td style=\"text-align: right;\">            192.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 380000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-45-43\n",
      "  done: false\n",
      "  episode_len_mean: 198.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 1.9192000000000191\n",
      "  episode_reward_min: -9.79999999999994\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1194\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.821978134571691\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9653082225057814\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0009192907155766545\n",
      "          policy_loss: -0.11004253447883659\n",
      "          total_loss: -0.1138974939679934\n",
      "          vf_explained_var: 0.7748944759368896\n",
      "          vf_loss: 0.01504248474828071\n",
      "    num_agent_steps_sampled: 380000\n",
      "    num_agent_steps_trained: 380000\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "  iterations_since_restore: 380\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.45517241379311\n",
      "    ram_util_percent: 32.54482758620689\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667515977790373\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.11015535993556\n",
      "    mean_inference_ms: 2.112668977873056\n",
      "    mean_raw_obs_processing_ms: 9.761278761548489\n",
      "  time_since_restore: 12964.226973056793\n",
      "  time_this_iter_s: 20.312517404556274\n",
      "  time_total_s: 12964.226973056793\n",
      "  timers:\n",
      "    learn_throughput: 1325.778\n",
      "    learn_time_ms: 754.274\n",
      "    load_throughput: 41286.746\n",
      "    load_time_ms: 24.221\n",
      "    sample_throughput: 10.76\n",
      "    sample_time_ms: 92933.259\n",
      "    update_time_ms: 3.936\n",
      "  timestamp: 1635295543\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 380\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   380</td><td style=\"text-align: right;\">         12964.2</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\">  1.9192</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">                -9.8</td><td style=\"text-align: right;\">            198.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 381000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-47-55\n",
      "  done: false\n",
      "  episode_len_mean: 194.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 2.091600000000019\n",
      "  episode_reward_min: -9.79999999999994\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1201\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4109890672858455\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7596417162153455\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010619277629648583\n",
      "          policy_loss: -0.049928342468208736\n",
      "          total_loss: 0.7447840458816952\n",
      "          vf_explained_var: 0.816511869430542\n",
      "          vf_loss: 0.807944396800465\n",
      "    num_agent_steps_sampled: 381000\n",
      "    num_agent_steps_trained: 381000\n",
      "    num_steps_sampled: 381000\n",
      "    num_steps_trained: 381000\n",
      "  iterations_since_restore: 381\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.626063829787235\n",
      "    ram_util_percent: 32.555851063829785\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036674993331652236\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.12170027894984\n",
      "    mean_inference_ms: 2.1126701792319365\n",
      "    mean_raw_obs_processing_ms: 9.945747157460815\n",
      "  time_since_restore: 13096.282581806183\n",
      "  time_this_iter_s: 132.05560874938965\n",
      "  time_total_s: 13096.282581806183\n",
      "  timers:\n",
      "    learn_throughput: 1327.051\n",
      "    learn_time_ms: 753.551\n",
      "    load_throughput: 41142.853\n",
      "    load_time_ms: 24.306\n",
      "    sample_throughput: 11.03\n",
      "    sample_time_ms: 90664.826\n",
      "    update_time_ms: 3.935\n",
      "  timestamp: 1635295675\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 381000\n",
      "  training_iteration: 381\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   381</td><td style=\"text-align: right;\">         13096.3</td><td style=\"text-align: right;\">381000</td><td style=\"text-align: right;\">  2.0916</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">                -9.8</td><td style=\"text-align: right;\">            194.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 382000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-49-14\n",
      "  done: false\n",
      "  episode_len_mean: 200.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 1.7341000000000202\n",
      "  episode_reward_min: -12.85999999999994\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1207\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4109890672858455\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0098904993798996\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01352334095623533\n",
      "          policy_loss: 0.07218827944662835\n",
      "          total_loss: 0.5199244957831171\n",
      "          vf_explained_var: 0.6836577653884888\n",
      "          vf_loss: 0.46227717879745694\n",
      "    num_agent_steps_sampled: 382000\n",
      "    num_agent_steps_trained: 382000\n",
      "    num_steps_sampled: 382000\n",
      "    num_steps_trained: 382000\n",
      "  iterations_since_restore: 382\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.85178571428571\n",
      "    ram_util_percent: 32.60089285714286\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036674883254289775\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.130387263563282\n",
      "    mean_inference_ms: 2.1126710298657123\n",
      "    mean_raw_obs_processing_ms: 10.098027314025485\n",
      "  time_since_restore: 13174.819858551025\n",
      "  time_this_iter_s: 78.53727674484253\n",
      "  time_total_s: 13174.819858551025\n",
      "  timers:\n",
      "    learn_throughput: 1326.714\n",
      "    learn_time_ms: 753.742\n",
      "    load_throughput: 41119.216\n",
      "    load_time_ms: 24.32\n",
      "    sample_throughput: 10.387\n",
      "    sample_time_ms: 96272.972\n",
      "    update_time_ms: 3.938\n",
      "  timestamp: 1635295754\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 382000\n",
      "  training_iteration: 382\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   382</td><td style=\"text-align: right;\">         13174.8</td><td style=\"text-align: right;\">382000</td><td style=\"text-align: right;\">  1.7341</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">              -12.86</td><td style=\"text-align: right;\">            200.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 383000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-50-19\n",
      "  done: false\n",
      "  episode_len_mean: 198.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 1.8447000000000195\n",
      "  episode_reward_min: -12.85999999999994\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1211\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4109890672858455\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.861130776670244\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008692950044939775\n",
      "          policy_loss: -0.15029064350657992\n",
      "          total_loss: 0.15162170343101025\n",
      "          vf_explained_var: 0.788425624370575\n",
      "          vf_loss: 0.31695094514224265\n",
      "    num_agent_steps_sampled: 383000\n",
      "    num_agent_steps_trained: 383000\n",
      "    num_steps_sampled: 383000\n",
      "    num_steps_trained: 383000\n",
      "  iterations_since_restore: 383\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.62065217391305\n",
      "    ram_util_percent: 32.60760869565218\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0366748240824857\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.13570883953153\n",
      "    mean_inference_ms: 2.1126709955481413\n",
      "    mean_raw_obs_processing_ms: 10.201493082745655\n",
      "  time_since_restore: 13239.328672647476\n",
      "  time_this_iter_s: 64.5088140964508\n",
      "  time_total_s: 13239.328672647476\n",
      "  timers:\n",
      "    learn_throughput: 1326.068\n",
      "    learn_time_ms: 754.109\n",
      "    load_throughput: 40865.18\n",
      "    load_time_ms: 24.471\n",
      "    sample_throughput: 10.501\n",
      "    sample_time_ms: 95226.729\n",
      "    update_time_ms: 3.936\n",
      "  timestamp: 1635295819\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 383000\n",
      "  training_iteration: 383\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   383</td><td style=\"text-align: right;\">         13239.3</td><td style=\"text-align: right;\">383000</td><td style=\"text-align: right;\">  1.8447</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">              -12.86</td><td style=\"text-align: right;\">            198.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 384000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-51-33\n",
      "  done: false\n",
      "  episode_len_mean: 199.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 1.7497000000000202\n",
      "  episode_reward_min: -12.85999999999994\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1216\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4109890672858455\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.921075259314643\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004623701581842522\n",
      "          policy_loss: 0.07013576709561878\n",
      "          total_loss: 0.07666538713706864\n",
      "          vf_explained_var: 0.8793509602546692\n",
      "          vf_loss: 0.02384008173313406\n",
      "    num_agent_steps_sampled: 384000\n",
      "    num_agent_steps_trained: 384000\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "  iterations_since_restore: 384\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.01792452830188\n",
      "    ram_util_percent: 32.61320754716982\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036674768572507226\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.140490044379455\n",
      "    mean_inference_ms: 2.112668768455976\n",
      "    mean_raw_obs_processing_ms: 10.328023411457345\n",
      "  time_since_restore: 13313.470504045486\n",
      "  time_this_iter_s: 74.14183139801025\n",
      "  time_total_s: 13313.470504045486\n",
      "  timers:\n",
      "    learn_throughput: 1327.017\n",
      "    learn_time_ms: 753.57\n",
      "    load_throughput: 41124.861\n",
      "    load_time_ms: 24.316\n",
      "    sample_throughput: 11.305\n",
      "    sample_time_ms: 88456.489\n",
      "    update_time_ms: 3.786\n",
      "  timestamp: 1635295893\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 384\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   384</td><td style=\"text-align: right;\">         13313.5</td><td style=\"text-align: right;\">384000</td><td style=\"text-align: right;\">  1.7497</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">              -12.86</td><td style=\"text-align: right;\">            199.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 385000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-52-50\n",
      "  done: false\n",
      "  episode_len_mean: 200.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 1.8209000000000204\n",
      "  episode_reward_min: -12.85999999999994\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1221\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20549453364292275\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.023834150367313\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012407590451873713\n",
      "          policy_loss: 0.022918506132231817\n",
      "          total_loss: 0.2887178053458532\n",
      "          vf_explained_var: 0.7494115233421326\n",
      "          vf_loss: 0.2834879520866606\n",
      "    num_agent_steps_sampled: 385000\n",
      "    num_agent_steps_trained: 385000\n",
      "    num_steps_sampled: 385000\n",
      "    num_steps_trained: 385000\n",
      "  iterations_since_restore: 385\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.098181818181814\n",
      "    ram_util_percent: 32.619090909090914\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036674686604531936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.145007788201045\n",
      "    mean_inference_ms: 2.1126665977384476\n",
      "    mean_raw_obs_processing_ms: 10.455375840586989\n",
      "  time_since_restore: 13390.549129486084\n",
      "  time_this_iter_s: 77.07862544059753\n",
      "  time_total_s: 13390.549129486084\n",
      "  timers:\n",
      "    learn_throughput: 1326.48\n",
      "    learn_time_ms: 753.875\n",
      "    load_throughput: 41189.519\n",
      "    load_time_ms: 24.278\n",
      "    sample_throughput: 12.076\n",
      "    sample_time_ms: 82811.239\n",
      "    update_time_ms: 3.853\n",
      "  timestamp: 1635295970\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 385000\n",
      "  training_iteration: 385\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   385</td><td style=\"text-align: right;\">         13390.5</td><td style=\"text-align: right;\">385000</td><td style=\"text-align: right;\">  1.8209</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">              -12.86</td><td style=\"text-align: right;\">            200.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 386000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-54-24\n",
      "  done: false\n",
      "  episode_len_mean: 194.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 2.0128000000000204\n",
      "  episode_reward_min: -12.85999999999994\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1226\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20549453364292275\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9458958122465346\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012569870424596404\n",
      "          policy_loss: -0.08711696738998095\n",
      "          total_loss: 0.0860191529409753\n",
      "          vf_explained_var: 0.6864401698112488\n",
      "          vf_loss: 0.190012039616704\n",
      "    num_agent_steps_sampled: 386000\n",
      "    num_agent_steps_trained: 386000\n",
      "    num_steps_sampled: 386000\n",
      "    num_steps_trained: 386000\n",
      "  iterations_since_restore: 386\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.79253731343283\n",
      "    ram_util_percent: 32.55820895522388\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667458603145766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.147717000232134\n",
      "    mean_inference_ms: 2.1126614987195595\n",
      "    mean_raw_obs_processing_ms: 10.58478202988479\n",
      "  time_since_restore: 13484.443253040314\n",
      "  time_this_iter_s: 93.89412355422974\n",
      "  time_total_s: 13484.443253040314\n",
      "  timers:\n",
      "    learn_throughput: 1325.237\n",
      "    learn_time_ms: 754.582\n",
      "    load_throughput: 41521.555\n",
      "    load_time_ms: 24.084\n",
      "    sample_throughput: 11.827\n",
      "    sample_time_ms: 84552.565\n",
      "    update_time_ms: 3.766\n",
      "  timestamp: 1635296064\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 386000\n",
      "  training_iteration: 386\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   386</td><td style=\"text-align: right;\">         13484.4</td><td style=\"text-align: right;\">386000</td><td style=\"text-align: right;\">  2.0128</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">              -12.86</td><td style=\"text-align: right;\">            194.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 387000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-55-25\n",
      "  done: false\n",
      "  episode_len_mean: 192.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 1.9831000000000198\n",
      "  episode_reward_min: -12.85999999999994\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1231\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20549453364292275\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0735574298434787\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004293940626434198\n",
      "          policy_loss: -0.10091994860106045\n",
      "          total_loss: -0.0344233451411128\n",
      "          vf_explained_var: 0.650871217250824\n",
      "          vf_loss: 0.08634979770415359\n",
      "    num_agent_steps_sampled: 387000\n",
      "    num_agent_steps_trained: 387000\n",
      "    num_steps_sampled: 387000\n",
      "    num_steps_trained: 387000\n",
      "  iterations_since_restore: 387\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.252272727272725\n",
      "    ram_util_percent: 32.601136363636364\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667446254142542\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.150273368617704\n",
      "    mean_inference_ms: 2.1126557076105708\n",
      "    mean_raw_obs_processing_ms: 10.714093334363966\n",
      "  time_since_restore: 13545.995838165283\n",
      "  time_this_iter_s: 61.55258512496948\n",
      "  time_total_s: 13545.995838165283\n",
      "  timers:\n",
      "    learn_throughput: 1326.11\n",
      "    learn_time_ms: 754.085\n",
      "    load_throughput: 41500.52\n",
      "    load_time_ms: 24.096\n",
      "    sample_throughput: 11.8\n",
      "    sample_time_ms: 84743.433\n",
      "    update_time_ms: 3.697\n",
      "  timestamp: 1635296125\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 387000\n",
      "  training_iteration: 387\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   387</td><td style=\"text-align: right;\">           13546</td><td style=\"text-align: right;\">387000</td><td style=\"text-align: right;\">  1.9831</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">              -12.86</td><td style=\"text-align: right;\">            192.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 388000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-55-47\n",
      "  done: false\n",
      "  episode_len_mean: 194.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 1.9563000000000208\n",
      "  episode_reward_min: -12.85999999999994\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1233\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10274726682146137\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.176789395014445\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020340510935972313\n",
      "          policy_loss: -0.13432316614521875\n",
      "          total_loss: -0.08188497399290402\n",
      "          vf_explained_var: 0.5255212783813477\n",
      "          vf_loss: 0.0721161533664498\n",
      "    num_agent_steps_sampled: 388000\n",
      "    num_agent_steps_trained: 388000\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "  iterations_since_restore: 388\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.50967741935484\n",
      "    ram_util_percent: 32.68064516129033\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0366743952761836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.15084980903809\n",
      "    mean_inference_ms: 2.1126528270979725\n",
      "    mean_raw_obs_processing_ms: 10.763003874000884\n",
      "  time_since_restore: 13567.394762992859\n",
      "  time_this_iter_s: 21.398924827575684\n",
      "  time_total_s: 13567.394762992859\n",
      "  timers:\n",
      "    learn_throughput: 1328.318\n",
      "    learn_time_ms: 752.832\n",
      "    load_throughput: 41435.742\n",
      "    load_time_ms: 24.134\n",
      "    sample_throughput: 15.692\n",
      "    sample_time_ms: 63726.126\n",
      "    update_time_ms: 3.704\n",
      "  timestamp: 1635296147\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 388\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   388</td><td style=\"text-align: right;\">         13567.4</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\">  1.9563</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">              -12.86</td><td style=\"text-align: right;\">            194.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 389000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-57-39\n",
      "  done: false\n",
      "  episode_len_mean: 188.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 1.948600000000021\n",
      "  episode_reward_min: -12.85999999999994\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1240\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1541209002321921\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2635911093817818\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015047031510844737\n",
      "          policy_loss: 0.0647764036224948\n",
      "          total_loss: 0.30231782477349045\n",
      "          vf_explained_var: 0.7756863236427307\n",
      "          vf_loss: 0.25785827504263986\n",
      "    num_agent_steps_sampled: 389000\n",
      "    num_agent_steps_trained: 389000\n",
      "    num_steps_sampled: 389000\n",
      "    num_steps_trained: 389000\n",
      "  iterations_since_restore: 389\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.499375\n",
      "    ram_util_percent: 32.596875\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667415250002808\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.153301147662432\n",
      "    mean_inference_ms: 2.1126428777796504\n",
      "    mean_raw_obs_processing_ms: 10.940676643620506\n",
      "  time_since_restore: 13679.888472795486\n",
      "  time_this_iter_s: 112.49370980262756\n",
      "  time_total_s: 13679.888472795486\n",
      "  timers:\n",
      "    learn_throughput: 1330.241\n",
      "    learn_time_ms: 751.743\n",
      "    load_throughput: 41328.037\n",
      "    load_time_ms: 24.197\n",
      "    sample_throughput: 13.734\n",
      "    sample_time_ms: 72813.56\n",
      "    update_time_ms: 3.701\n",
      "  timestamp: 1635296259\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 389000\n",
      "  training_iteration: 389\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   389</td><td style=\"text-align: right;\">         13679.9</td><td style=\"text-align: right;\">389000</td><td style=\"text-align: right;\">  1.9486</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">              -12.86</td><td style=\"text-align: right;\">             188.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 390000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-58-58\n",
      "  done: false\n",
      "  episode_len_mean: 195.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 1.5893000000000217\n",
      "  episode_reward_min: -12.85999999999994\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1245\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1541209002321921\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0981694724824695\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010358994279239456\n",
      "          policy_loss: -0.08338242015904851\n",
      "          total_loss: 0.08321990430768993\n",
      "          vf_explained_var: 0.8344036340713501\n",
      "          vf_loss: 0.1859874843309323\n",
      "    num_agent_steps_sampled: 390000\n",
      "    num_agent_steps_trained: 390000\n",
      "    num_steps_sampled: 390000\n",
      "    num_steps_trained: 390000\n",
      "  iterations_since_restore: 390\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.175000000000004\n",
      "    ram_util_percent: 32.58392857142858\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667397124665107\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.154792516131646\n",
      "    mean_inference_ms: 2.112634709842331\n",
      "    mean_raw_obs_processing_ms: 11.06137152269005\n",
      "  time_since_restore: 13758.181024074554\n",
      "  time_this_iter_s: 78.292551279068\n",
      "  time_total_s: 13758.181024074554\n",
      "  timers:\n",
      "    learn_throughput: 1329.16\n",
      "    learn_time_ms: 752.355\n",
      "    load_throughput: 41206.029\n",
      "    load_time_ms: 24.268\n",
      "    sample_throughput: 12.721\n",
      "    sample_time_ms: 78610.859\n",
      "    update_time_ms: 3.713\n",
      "  timestamp: 1635296338\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 390000\n",
      "  training_iteration: 390\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   390</td><td style=\"text-align: right;\">         13758.2</td><td style=\"text-align: right;\">390000</td><td style=\"text-align: right;\">  1.5893</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">              -12.86</td><td style=\"text-align: right;\">            195.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 391000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_00-59-19\n",
      "  done: false\n",
      "  episode_len_mean: 194.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 1.650300000000021\n",
      "  episode_reward_min: -12.85999999999994\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1247\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1541209002321921\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.013722186618381\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02176590602222561\n",
      "          policy_loss: 0.07863594690958658\n",
      "          total_loss: 0.16228707664542727\n",
      "          vf_explained_var: 0.8194979429244995\n",
      "          vf_loss: 0.1004337639366794\n",
      "    num_agent_steps_sampled: 391000\n",
      "    num_agent_steps_trained: 391000\n",
      "    num_steps_sampled: 391000\n",
      "    num_steps_trained: 391000\n",
      "  iterations_since_restore: 391\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.08709677419354\n",
      "    ram_util_percent: 32.67741935483872\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667390064885812\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.155313313121013\n",
      "    mean_inference_ms: 2.1126341768414147\n",
      "    mean_raw_obs_processing_ms: 11.109293672408818\n",
      "  time_since_restore: 13779.607507705688\n",
      "  time_this_iter_s: 21.426483631134033\n",
      "  time_total_s: 13779.607507705688\n",
      "  timers:\n",
      "    learn_throughput: 1329.512\n",
      "    learn_time_ms: 752.155\n",
      "    load_throughput: 41303.944\n",
      "    load_time_ms: 24.211\n",
      "    sample_throughput: 14.804\n",
      "    sample_time_ms: 67548.209\n",
      "    update_time_ms: 3.714\n",
      "  timestamp: 1635296359\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 391000\n",
      "  training_iteration: 391\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   391</td><td style=\"text-align: right;\">         13779.6</td><td style=\"text-align: right;\">391000</td><td style=\"text-align: right;\">  1.6503</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">              -12.86</td><td style=\"text-align: right;\">            194.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-00-16\n",
      "  done: false\n",
      "  episode_len_mean: 195.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 1.5856000000000217\n",
      "  episode_reward_min: -12.85999999999994\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1251\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23118135034828802\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.117801793416341\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009681639927610498\n",
      "          policy_loss: -0.07807080621520678\n",
      "          total_loss: -0.02006881580584579\n",
      "          vf_explained_var: 0.8988699913024902\n",
      "          vf_loss: 0.07694179440538089\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "  iterations_since_restore: 392\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.40999999999999\n",
      "    ram_util_percent: 32.64\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036673754319376355\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.156354731755062\n",
      "    mean_inference_ms: 2.1126325691237313\n",
      "    mean_raw_obs_processing_ms: 11.205625654396737\n",
      "  time_since_restore: 13836.304772138596\n",
      "  time_this_iter_s: 56.697264432907104\n",
      "  time_total_s: 13836.304772138596\n",
      "  timers:\n",
      "    learn_throughput: 1331.048\n",
      "    learn_time_ms: 751.287\n",
      "    load_throughput: 41318.348\n",
      "    load_time_ms: 24.202\n",
      "    sample_throughput: 15.299\n",
      "    sample_time_ms: 65365.157\n",
      "    update_time_ms: 3.628\n",
      "  timestamp: 1635296416\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 392\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   392</td><td style=\"text-align: right;\">         13836.3</td><td style=\"text-align: right;\">392000</td><td style=\"text-align: right;\">  1.5856</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">              -12.86</td><td style=\"text-align: right;\">            195.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 393000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-01-32\n",
      "  done: false\n",
      "  episode_len_mean: 195.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 1.4793000000000216\n",
      "  episode_reward_min: -12.85999999999994\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1256\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23118135034828802\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.119832396507263\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008469051258617159\n",
      "          policy_loss: 0.11046589588125547\n",
      "          total_loss: 0.332504906753699\n",
      "          vf_explained_var: 0.8841779232025146\n",
      "          vf_loss: 0.24127944965536396\n",
      "    num_agent_steps_sampled: 393000\n",
      "    num_agent_steps_trained: 393000\n",
      "    num_steps_sampled: 393000\n",
      "    num_steps_trained: 393000\n",
      "  iterations_since_restore: 393\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.18363636363635\n",
      "    ram_util_percent: 32.66181818181817\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667355807784021\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.15756633060718\n",
      "    mean_inference_ms: 2.1126302669898447\n",
      "    mean_raw_obs_processing_ms: 11.319811912064925\n",
      "  time_since_restore: 13912.826569795609\n",
      "  time_this_iter_s: 76.52179765701294\n",
      "  time_total_s: 13912.826569795609\n",
      "  timers:\n",
      "    learn_throughput: 1331.811\n",
      "    learn_time_ms: 750.857\n",
      "    load_throughput: 41429.562\n",
      "    load_time_ms: 24.137\n",
      "    sample_throughput: 15.022\n",
      "    sample_time_ms: 66566.844\n",
      "    update_time_ms: 3.72\n",
      "  timestamp: 1635296492\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 393000\n",
      "  training_iteration: 393\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   393</td><td style=\"text-align: right;\">         13912.8</td><td style=\"text-align: right;\">393000</td><td style=\"text-align: right;\">  1.4793</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">              -12.86</td><td style=\"text-align: right;\">            195.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 394000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-03-29\n",
      "  done: false\n",
      "  episode_len_mean: 196.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 1.464900000000022\n",
      "  episode_reward_min: -12.85999999999994\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1263\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23118135034828802\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.079264461994171\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021897620734747075\n",
      "          policy_loss: 0.00025123018357488844\n",
      "          total_loss: 0.31596063102285066\n",
      "          vf_explained_var: 0.8338954448699951\n",
      "          vf_loss: 0.3314397215274059\n",
      "    num_agent_steps_sampled: 394000\n",
      "    num_agent_steps_trained: 394000\n",
      "    num_steps_sampled: 394000\n",
      "    num_steps_trained: 394000\n",
      "  iterations_since_restore: 394\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.84096385542169\n",
      "    ram_util_percent: 32.599999999999994\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036673280243250436\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.158672291865958\n",
      "    mean_inference_ms: 2.1126266604744206\n",
      "    mean_raw_obs_processing_ms: 11.481152786557567\n",
      "  time_since_restore: 14029.250637292862\n",
      "  time_this_iter_s: 116.42406749725342\n",
      "  time_total_s: 14029.250637292862\n",
      "  timers:\n",
      "    learn_throughput: 1330.045\n",
      "    learn_time_ms: 751.854\n",
      "    load_throughput: 41253.447\n",
      "    load_time_ms: 24.24\n",
      "    sample_throughput: 14.125\n",
      "    sample_time_ms: 70793.963\n",
      "    update_time_ms: 3.719\n",
      "  timestamp: 1635296609\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 394000\n",
      "  training_iteration: 394\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   394</td><td style=\"text-align: right;\">         14029.3</td><td style=\"text-align: right;\">394000</td><td style=\"text-align: right;\">  1.4649</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">              -12.86</td><td style=\"text-align: right;\">            196.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 395000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-04-28\n",
      "  done: false\n",
      "  episode_len_mean: 201.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 1.3332000000000235\n",
      "  episode_reward_min: -12.85999999999994\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1267\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.34677202552243225\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1405125167634753\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010176094826133072\n",
      "          policy_loss: 0.062175212303797404\n",
      "          total_loss: 0.17814002446830274\n",
      "          vf_explained_var: 0.2674022912979126\n",
      "          vf_loss: 0.1338411473597969\n",
      "    num_agent_steps_sampled: 395000\n",
      "    num_agent_steps_trained: 395000\n",
      "    num_steps_sampled: 395000\n",
      "    num_steps_trained: 395000\n",
      "  iterations_since_restore: 395\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.70705882352941\n",
      "    ram_util_percent: 32.65529411764705\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667313869236359\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.158928632418586\n",
      "    mean_inference_ms: 2.112624577019206\n",
      "    mean_raw_obs_processing_ms: 11.568501181547976\n",
      "  time_since_restore: 14088.77338886261\n",
      "  time_this_iter_s: 59.522751569747925\n",
      "  time_total_s: 14088.77338886261\n",
      "  timers:\n",
      "    learn_throughput: 1328.241\n",
      "    learn_time_ms: 752.875\n",
      "    load_throughput: 41210.361\n",
      "    load_time_ms: 24.266\n",
      "    sample_throughput: 14.485\n",
      "    sample_time_ms: 69037.302\n",
      "    update_time_ms: 3.736\n",
      "  timestamp: 1635296668\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 395000\n",
      "  training_iteration: 395\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   395</td><td style=\"text-align: right;\">         14088.8</td><td style=\"text-align: right;\">395000</td><td style=\"text-align: right;\">  1.3332</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">              -12.86</td><td style=\"text-align: right;\">            201.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 396000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-06-16\n",
      "  done: false\n",
      "  episode_len_mean: 198.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 1.391100000000023\n",
      "  episode_reward_min: -12.85999999999994\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1274\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.34677202552243225\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2088621881273056\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010905067295462473\n",
      "          policy_loss: 0.032387107610702515\n",
      "          total_loss: 0.5361668863437242\n",
      "          vf_explained_var: 0.7248468399047852\n",
      "          vf_loss: 0.5220868383844693\n",
      "    num_agent_steps_sampled: 396000\n",
      "    num_agent_steps_trained: 396000\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "  iterations_since_restore: 396\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.53529411764706\n",
      "    ram_util_percent: 32.65424836601307\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036672884077393435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.159674281557372\n",
      "    mean_inference_ms: 2.1126210837665864\n",
      "    mean_raw_obs_processing_ms: 11.726173943252691\n",
      "  time_since_restore: 14196.095279932022\n",
      "  time_this_iter_s: 107.32189106941223\n",
      "  time_total_s: 14196.095279932022\n",
      "  timers:\n",
      "    learn_throughput: 1326.1\n",
      "    learn_time_ms: 754.091\n",
      "    load_throughput: 41141.44\n",
      "    load_time_ms: 24.306\n",
      "    sample_throughput: 14.209\n",
      "    sample_time_ms: 70378.813\n",
      "    update_time_ms: 3.735\n",
      "  timestamp: 1635296776\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 396\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   396</td><td style=\"text-align: right;\">         14196.1</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  1.3911</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">              -12.86</td><td style=\"text-align: right;\">            198.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 397000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-07-33\n",
      "  done: false\n",
      "  episode_len_mean: 195.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 1.4018000000000228\n",
      "  episode_reward_min: -12.85999999999994\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1279\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.34677202552243225\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1338550157017178\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007653149996684544\n",
      "          policy_loss: -0.039298338194688164\n",
      "          total_loss: -0.015339550541506873\n",
      "          vf_explained_var: 0.6061784625053406\n",
      "          vf_loss: 0.042643438398631085\n",
      "    num_agent_steps_sampled: 397000\n",
      "    num_agent_steps_trained: 397000\n",
      "    num_steps_sampled: 397000\n",
      "    num_steps_trained: 397000\n",
      "  iterations_since_restore: 397\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.209009009009016\n",
      "    ram_util_percent: 32.66216216216215\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667268456766337\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.15992862749413\n",
      "    mean_inference_ms: 2.1126162737234737\n",
      "    mean_raw_obs_processing_ms: 11.835297531249719\n",
      "  time_since_restore: 14273.506835222244\n",
      "  time_this_iter_s: 77.41155529022217\n",
      "  time_total_s: 14273.506835222244\n",
      "  timers:\n",
      "    learn_throughput: 1325.827\n",
      "    learn_time_ms: 754.246\n",
      "    load_throughput: 41228.509\n",
      "    load_time_ms: 24.255\n",
      "    sample_throughput: 13.896\n",
      "    sample_time_ms: 71964.628\n",
      "    update_time_ms: 3.731\n",
      "  timestamp: 1635296853\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 397000\n",
      "  training_iteration: 397\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   397</td><td style=\"text-align: right;\">         14273.5</td><td style=\"text-align: right;\">397000</td><td style=\"text-align: right;\">  1.4018</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">              -12.86</td><td style=\"text-align: right;\">             195.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 398000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-08-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 1.3498000000000234\n",
      "  episode_reward_min: -12.85999999999994\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1281\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.34677202552243225\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1565292252434625\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013651272598091149\n",
      "          policy_loss: -0.14522840281327565\n",
      "          total_loss: -0.12132901350657145\n",
      "          vf_explained_var: 0.9378144145011902\n",
      "          vf_loss: 0.04073079861700535\n",
      "    num_agent_steps_sampled: 398000\n",
      "    num_agent_steps_trained: 398000\n",
      "    num_steps_sampled: 398000\n",
      "    num_steps_trained: 398000\n",
      "  iterations_since_restore: 398\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.24905660377358\n",
      "    ram_util_percent: 32.64528301886793\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036672605414633824\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.159781102724615\n",
      "    mean_inference_ms: 2.112614065629148\n",
      "    mean_raw_obs_processing_ms: 11.870882677193388\n",
      "  time_since_restore: 14310.722967147827\n",
      "  time_this_iter_s: 37.216131925582886\n",
      "  time_total_s: 14310.722967147827\n",
      "  timers:\n",
      "    learn_throughput: 1324.624\n",
      "    learn_time_ms: 754.931\n",
      "    load_throughput: 41122.805\n",
      "    load_time_ms: 24.317\n",
      "    sample_throughput: 13.597\n",
      "    sample_time_ms: 73545.508\n",
      "    update_time_ms: 3.816\n",
      "  timestamp: 1635296890\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 398000\n",
      "  training_iteration: 398\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   398</td><td style=\"text-align: right;\">         14310.7</td><td style=\"text-align: right;\">398000</td><td style=\"text-align: right;\">  1.3498</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">              -12.86</td><td style=\"text-align: right;\">            200.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 399000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-10-28\n",
      "  done: false\n",
      "  episode_len_mean: 210.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 0.9777000000000249\n",
      "  episode_reward_min: -12.85999999999994\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1290\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.34677202552243225\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9233068333731758\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012586879792825004\n",
      "          policy_loss: -0.1827236059639189\n",
      "          total_loss: -0.10108117270800802\n",
      "          vf_explained_var: 0.9647679924964905\n",
      "          vf_loss: 0.09651072043925524\n",
      "    num_agent_steps_sampled: 399000\n",
      "    num_agent_steps_trained: 399000\n",
      "    num_steps_sampled: 399000\n",
      "    num_steps_trained: 399000\n",
      "  iterations_since_restore: 399\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.55357142857143\n",
      "    ram_util_percent: 32.611224489795916\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036672319942935426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.159945260649835\n",
      "    mean_inference_ms: 2.1126050326987356\n",
      "    mean_raw_obs_processing_ms: 12.053044908270792\n",
      "  time_since_restore: 14447.99267411232\n",
      "  time_this_iter_s: 137.2697069644928\n",
      "  time_total_s: 14447.99267411232\n",
      "  timers:\n",
      "    learn_throughput: 1325.294\n",
      "    learn_time_ms: 754.55\n",
      "    load_throughput: 41271.714\n",
      "    load_time_ms: 24.23\n",
      "    sample_throughput: 13.154\n",
      "    sample_time_ms: 76023.565\n",
      "    update_time_ms: 3.818\n",
      "  timestamp: 1635297028\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 399000\n",
      "  training_iteration: 399\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">           14448</td><td style=\"text-align: right;\">399000</td><td style=\"text-align: right;\">  0.9777</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">              -12.86</td><td style=\"text-align: right;\">            210.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-11-05\n",
      "  done: false\n",
      "  episode_len_mean: 208.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 1.105800000000024\n",
      "  episode_reward_min: -12.85999999999994\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1292\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.34677202552243225\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.146201929781172\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011996042292092805\n",
      "          policy_loss: -0.11547306329011917\n",
      "          total_loss: -0.009525191121631199\n",
      "          vf_explained_var: 0.2245955765247345\n",
      "          vf_loss: 0.1232499952091732\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_agent_steps_trained: 400000\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "  iterations_since_restore: 400\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.14528301886792\n",
      "    ram_util_percent: 32.72641509433962\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667224695681633\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.159932474049317\n",
      "    mean_inference_ms: 2.1126031222400052\n",
      "    mean_raw_obs_processing_ms: 12.094314174991059\n",
      "  time_since_restore: 14485.735692024231\n",
      "  time_this_iter_s: 37.74301791191101\n",
      "  time_total_s: 14485.735692024231\n",
      "  timers:\n",
      "    learn_throughput: 1324.905\n",
      "    learn_time_ms: 754.771\n",
      "    load_throughput: 41431.608\n",
      "    load_time_ms: 24.136\n",
      "    sample_throughput: 13.895\n",
      "    sample_time_ms: 71968.472\n",
      "    update_time_ms: 3.82\n",
      "  timestamp: 1635297065\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 400\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   400</td><td style=\"text-align: right;\">         14485.7</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\">  1.1058</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">              -12.86</td><td style=\"text-align: right;\">            208.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 401000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-11-58\n",
      "  done: false\n",
      "  episode_len_mean: 205.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 1.266200000000024\n",
      "  episode_reward_min: -12.85999999999994\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1296\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.34677202552243225\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.19951974550883\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011329129546406413\n",
      "          policy_loss: -0.09213979459471172\n",
      "          total_loss: -0.07906255713767475\n",
      "          vf_explained_var: 0.30547821521759033\n",
      "          vf_loss: 0.03114381314177687\n",
      "    num_agent_steps_sampled: 401000\n",
      "    num_agent_steps_trained: 401000\n",
      "    num_steps_sampled: 401000\n",
      "    num_steps_trained: 401000\n",
      "  iterations_since_restore: 401\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.64736842105263\n",
      "    ram_util_percent: 32.69605263157895\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667210776535746\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.159428301237\n",
      "    mean_inference_ms: 2.112599328625613\n",
      "    mean_raw_obs_processing_ms: 12.175301471745888\n",
      "  time_since_restore: 14538.559795618057\n",
      "  time_this_iter_s: 52.824103593826294\n",
      "  time_total_s: 14538.559795618057\n",
      "  timers:\n",
      "    learn_throughput: 1324.167\n",
      "    learn_time_ms: 755.192\n",
      "    load_throughput: 41425.347\n",
      "    load_time_ms: 24.14\n",
      "    sample_throughput: 13.314\n",
      "    sample_time_ms: 75107.802\n",
      "    update_time_ms: 3.828\n",
      "  timestamp: 1635297118\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 401000\n",
      "  training_iteration: 401\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   401</td><td style=\"text-align: right;\">         14538.6</td><td style=\"text-align: right;\">401000</td><td style=\"text-align: right;\">  1.2662</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">              -12.86</td><td style=\"text-align: right;\">            205.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 402000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-13-24\n",
      "  done: false\n",
      "  episode_len_mean: 211.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.860000000000001\n",
      "  episode_reward_mean: 1.0298000000000243\n",
      "  episode_reward_min: -12.85999999999994\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1301\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.34677202552243225\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2212779680887857\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005349287867215501\n",
      "          policy_loss: -0.21148220383458669\n",
      "          total_loss: -0.2028418968121211\n",
      "          vf_explained_var: 0.9910354018211365\n",
      "          vf_loss: 0.0289981026823322\n",
      "    num_agent_steps_sampled: 402000\n",
      "    num_agent_steps_trained: 402000\n",
      "    num_steps_sampled: 402000\n",
      "    num_steps_trained: 402000\n",
      "  iterations_since_restore: 402\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.12786885245902\n",
      "    ram_util_percent: 32.717213114754095\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667193096151185\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.158577642934052\n",
      "    mean_inference_ms: 2.1125942037385013\n",
      "    mean_raw_obs_processing_ms: 12.276433436125508\n",
      "  time_since_restore: 14623.883497953415\n",
      "  time_this_iter_s: 85.32370233535767\n",
      "  time_total_s: 14623.883497953415\n",
      "  timers:\n",
      "    learn_throughput: 1322.757\n",
      "    learn_time_ms: 755.997\n",
      "    load_throughput: 41427.802\n",
      "    load_time_ms: 24.138\n",
      "    sample_throughput: 12.826\n",
      "    sample_time_ms: 77969.57\n",
      "    update_time_ms: 3.916\n",
      "  timestamp: 1635297204\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 402000\n",
      "  training_iteration: 402\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   402</td><td style=\"text-align: right;\">         14623.9</td><td style=\"text-align: right;\">402000</td><td style=\"text-align: right;\">  1.0298</td><td style=\"text-align: right;\">                9.86</td><td style=\"text-align: right;\">              -12.86</td><td style=\"text-align: right;\">            211.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 403000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-15-43\n",
      "  done: false\n",
      "  episode_len_mean: 204.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 1.3063000000000236\n",
      "  episode_reward_min: -11.579999999999961\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1310\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.34677202552243225\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9227021137873332\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014652741551117721\n",
      "          policy_loss: 0.060288728773593904\n",
      "          total_loss: 0.2436336381567849\n",
      "          vf_explained_var: 0.48375728726387024\n",
      "          vf_loss: 0.19749077196336454\n",
      "    num_agent_steps_sampled: 403000\n",
      "    num_agent_steps_trained: 403000\n",
      "    num_steps_sampled: 403000\n",
      "    num_steps_trained: 403000\n",
      "  iterations_since_restore: 403\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.52562814070352\n",
      "    ram_util_percent: 32.65477386934673\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667160761507907\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.157769274785952\n",
      "    mean_inference_ms: 2.1125867611699247\n",
      "    mean_raw_obs_processing_ms: 12.468093377230554\n",
      "  time_since_restore: 14763.715008735657\n",
      "  time_this_iter_s: 139.83151078224182\n",
      "  time_total_s: 14763.715008735657\n",
      "  timers:\n",
      "    learn_throughput: 1322.042\n",
      "    learn_time_ms: 756.405\n",
      "    load_throughput: 41741.097\n",
      "    load_time_ms: 23.957\n",
      "    sample_throughput: 11.862\n",
      "    sample_time_ms: 84300.364\n",
      "    update_time_ms: 3.828\n",
      "  timestamp: 1635297343\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 403000\n",
      "  training_iteration: 403\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   403</td><td style=\"text-align: right;\">         14763.7</td><td style=\"text-align: right;\">403000</td><td style=\"text-align: right;\">  1.3063</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">              -11.58</td><td style=\"text-align: right;\">            204.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 404000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-16-36\n",
      "  done: false\n",
      "  episode_len_mean: 197.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 1.5578000000000225\n",
      "  episode_reward_min: -11.579999999999961\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1313\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.34677202552243225\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.210082949532403\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005863092748799876\n",
      "          policy_loss: -0.053049469283885424\n",
      "          total_loss: -0.004821958558426963\n",
      "          vf_explained_var: 0.8130631446838379\n",
      "          vf_loss: 0.06829518548005985\n",
      "    num_agent_steps_sampled: 404000\n",
      "    num_agent_steps_trained: 404000\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "  iterations_since_restore: 404\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.652631578947364\n",
      "    ram_util_percent: 32.68157894736842\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667151038673669\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.157020433046277\n",
      "    mean_inference_ms: 2.112584441695625\n",
      "    mean_raw_obs_processing_ms: 12.530044132877979\n",
      "  time_since_restore: 14816.711554765701\n",
      "  time_this_iter_s: 52.996546030044556\n",
      "  time_total_s: 14816.711554765701\n",
      "  timers:\n",
      "    learn_throughput: 1321.925\n",
      "    learn_time_ms: 756.473\n",
      "    load_throughput: 41770.611\n",
      "    load_time_ms: 23.94\n",
      "    sample_throughput: 12.827\n",
      "    sample_time_ms: 77957.583\n",
      "    update_time_ms: 3.824\n",
      "  timestamp: 1635297396\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 404\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   404</td><td style=\"text-align: right;\">         14816.7</td><td style=\"text-align: right;\">404000</td><td style=\"text-align: right;\">  1.5578</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">              -11.58</td><td style=\"text-align: right;\">            197.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 405000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-17-17\n",
      "  done: false\n",
      "  episode_len_mean: 205.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 1.3369000000000235\n",
      "  episode_reward_min: -11.579999999999961\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1317\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.34677202552243225\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.145328356160058\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013385326684376529\n",
      "          policy_loss: 0.03918486005730099\n",
      "          total_loss: 0.2640971188743909\n",
      "          vf_explained_var: 0.12121852487325668\n",
      "          vf_loss: 0.2417238864245721\n",
      "    num_agent_steps_sampled: 405000\n",
      "    num_agent_steps_trained: 405000\n",
      "    num_steps_sampled: 405000\n",
      "    num_steps_trained: 405000\n",
      "  iterations_since_restore: 405\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.905263157894744\n",
      "    ram_util_percent: 32.72456140350877\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667138677326073\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.156167967012426\n",
      "    mean_inference_ms: 2.112581211905341\n",
      "    mean_raw_obs_processing_ms: 12.610377275305213\n",
      "  time_since_restore: 14856.973366975784\n",
      "  time_this_iter_s: 40.26181221008301\n",
      "  time_total_s: 14856.973366975784\n",
      "  timers:\n",
      "    learn_throughput: 1321.456\n",
      "    learn_time_ms: 756.741\n",
      "    load_throughput: 41689.65\n",
      "    load_time_ms: 23.987\n",
      "    sample_throughput: 13.152\n",
      "    sample_time_ms: 76031.288\n",
      "    update_time_ms: 3.735\n",
      "  timestamp: 1635297437\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 405000\n",
      "  training_iteration: 405\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   405</td><td style=\"text-align: right;\">           14857</td><td style=\"text-align: right;\">405000</td><td style=\"text-align: right;\">  1.3369</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">              -11.58</td><td style=\"text-align: right;\">             205.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 406000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-18-52\n",
      "  done: false\n",
      "  episode_len_mean: 206.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 1.3373000000000235\n",
      "  episode_reward_min: -11.579999999999961\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1322\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.34677202552243225\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1537471810976663\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.16954025448935434\n",
      "          policy_loss: 0.02069670938783222\n",
      "          total_loss: 0.9070319446010722\n",
      "          vf_explained_var: -0.22796662151813507\n",
      "          vf_loss: 0.8490808950737119\n",
      "    num_agent_steps_sampled: 406000\n",
      "    num_agent_steps_trained: 406000\n",
      "    num_steps_sampled: 406000\n",
      "    num_steps_trained: 406000\n",
      "  iterations_since_restore: 406\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.90514705882353\n",
      "    ram_util_percent: 32.71985294117647\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036671257515104035\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.154793770767345\n",
      "    mean_inference_ms: 2.1125760720442766\n",
      "    mean_raw_obs_processing_ms: 12.712624365108617\n",
      "  time_since_restore: 14951.984249830246\n",
      "  time_this_iter_s: 95.01088285446167\n",
      "  time_total_s: 14951.984249830246\n",
      "  timers:\n",
      "    learn_throughput: 1324.039\n",
      "    learn_time_ms: 755.265\n",
      "    load_throughput: 41672.916\n",
      "    load_time_ms: 23.996\n",
      "    sample_throughput: 13.369\n",
      "    sample_time_ms: 74801.682\n",
      "    update_time_ms: 3.725\n",
      "  timestamp: 1635297532\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 406000\n",
      "  training_iteration: 406\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   406</td><td style=\"text-align: right;\">           14952</td><td style=\"text-align: right;\">406000</td><td style=\"text-align: right;\">  1.3373</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">              -11.58</td><td style=\"text-align: right;\">             206.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 407000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-20-09\n",
      "  done: false\n",
      "  episode_len_mean: 203.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 1.3239000000000225\n",
      "  episode_reward_min: -11.579999999999961\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1327\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5201580382836484\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.173159186045329\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02245752725727804\n",
      "          policy_loss: -0.0018719711237483554\n",
      "          total_loss: 0.5642087885075145\n",
      "          vf_explained_var: 0.5127092599868774\n",
      "          vf_loss: 0.576130892501937\n",
      "    num_agent_steps_sampled: 407000\n",
      "    num_agent_steps_trained: 407000\n",
      "    num_steps_sampled: 407000\n",
      "    num_steps_trained: 407000\n",
      "  iterations_since_restore: 407\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.480000000000004\n",
      "    ram_util_percent: 32.70090909090909\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667112707869771\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.153562660611623\n",
      "    mean_inference_ms: 2.1125721297776647\n",
      "    mean_raw_obs_processing_ms: 12.812828701922932\n",
      "  time_since_restore: 15028.832913637161\n",
      "  time_this_iter_s: 76.84866380691528\n",
      "  time_total_s: 15028.832913637161\n",
      "  timers:\n",
      "    learn_throughput: 1321.76\n",
      "    learn_time_ms: 756.567\n",
      "    load_throughput: 41736.735\n",
      "    load_time_ms: 23.96\n",
      "    sample_throughput: 13.379\n",
      "    sample_time_ms: 74744.055\n",
      "    update_time_ms: 3.717\n",
      "  timestamp: 1635297609\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 407000\n",
      "  training_iteration: 407\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   407</td><td style=\"text-align: right;\">         15028.8</td><td style=\"text-align: right;\">407000</td><td style=\"text-align: right;\">  1.3239</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">              -11.58</td><td style=\"text-align: right;\">            203.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-23-57\n",
      "  done: false\n",
      "  episode_len_mean: 190.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 1.6776000000000215\n",
      "  episode_reward_min: -12.81999999999995\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1340\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7802370574254722\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.006038044558631\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01107176255326094\n",
      "          policy_loss: 0.10772333997819158\n",
      "          total_loss: 0.4431628698276149\n",
      "          vf_explained_var: 0.8279083371162415\n",
      "          vf_loss: 0.346861313117875\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_agent_steps_trained: 408000\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "  iterations_since_restore: 408\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.50306748466258\n",
      "    ram_util_percent: 32.68588957055214\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667092040487956\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.151514402611323\n",
      "    mean_inference_ms: 2.112567576621642\n",
      "    mean_raw_obs_processing_ms: 13.115341148986547\n",
      "  time_since_restore: 15257.520686388016\n",
      "  time_this_iter_s: 228.6877727508545\n",
      "  time_total_s: 15257.520686388016\n",
      "  timers:\n",
      "    learn_throughput: 1321.4\n",
      "    learn_time_ms: 756.773\n",
      "    load_throughput: 42250.029\n",
      "    load_time_ms: 23.669\n",
      "    sample_throughput: 10.651\n",
      "    sample_time_ms: 93891.403\n",
      "    update_time_ms: 3.628\n",
      "  timestamp: 1635297837\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 408\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   408</td><td style=\"text-align: right;\">         15257.5</td><td style=\"text-align: right;\">408000</td><td style=\"text-align: right;\">  1.6776</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">              -12.82</td><td style=\"text-align: right;\">            190.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 409000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-24-58\n",
      "  done: false\n",
      "  episode_len_mean: 192.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 1.5745000000000218\n",
      "  episode_reward_min: -12.81999999999995\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1344\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7802370574254722\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2781425926420424\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009925334721999787\n",
      "          policy_loss: -0.04341025814000103\n",
      "          total_loss: 0.03024854047430886\n",
      "          vf_explained_var: 0.6421990990638733\n",
      "          vf_loss: 0.08869611099362373\n",
      "    num_agent_steps_sampled: 409000\n",
      "    num_agent_steps_trained: 409000\n",
      "    num_steps_sampled: 409000\n",
      "    num_steps_trained: 409000\n",
      "  iterations_since_restore: 409\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.78139534883721\n",
      "    ram_util_percent: 32.74651162790696\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667089032725678\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.151016611027302\n",
      "    mean_inference_ms: 2.1125676235807433\n",
      "    mean_raw_obs_processing_ms: 13.202879646903781\n",
      "  time_since_restore: 15318.070232152939\n",
      "  time_this_iter_s: 60.549545764923096\n",
      "  time_total_s: 15318.070232152939\n",
      "  timers:\n",
      "    learn_throughput: 1321.385\n",
      "    learn_time_ms: 756.782\n",
      "    load_throughput: 42333.738\n",
      "    load_time_ms: 23.622\n",
      "    sample_throughput: 11.598\n",
      "    sample_time_ms: 86219.345\n",
      "    update_time_ms: 3.714\n",
      "  timestamp: 1635297898\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 409000\n",
      "  training_iteration: 409\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   409</td><td style=\"text-align: right;\">         15318.1</td><td style=\"text-align: right;\">409000</td><td style=\"text-align: right;\">  1.5745</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">              -12.82</td><td style=\"text-align: right;\">            192.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 410000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-25-37\n",
      "  done: false\n",
      "  episode_len_mean: 189.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 1.7227000000000212\n",
      "  episode_reward_min: -12.81999999999995\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1347\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7802370574254722\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.271378254890442\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008236734370729516\n",
      "          policy_loss: -0.05400952216651705\n",
      "          total_loss: -0.04530945867300033\n",
      "          vf_explained_var: 0.954103410243988\n",
      "          vf_loss: 0.024987241253256798\n",
      "    num_agent_steps_sampled: 410000\n",
      "    num_agent_steps_trained: 410000\n",
      "    num_steps_sampled: 410000\n",
      "    num_steps_trained: 410000\n",
      "  iterations_since_restore: 410\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.32678571428572\n",
      "    ram_util_percent: 32.644642857142856\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0366708665668681\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.15064004632717\n",
      "    mean_inference_ms: 2.1125647984946916\n",
      "    mean_raw_obs_processing_ms: 13.269361143776141\n",
      "  time_since_restore: 15357.066404819489\n",
      "  time_this_iter_s: 38.99617266654968\n",
      "  time_total_s: 15357.066404819489\n",
      "  timers:\n",
      "    learn_throughput: 1320.797\n",
      "    learn_time_ms: 757.118\n",
      "    load_throughput: 42372.913\n",
      "    load_time_ms: 23.6\n",
      "    sample_throughput: 11.582\n",
      "    sample_time_ms: 86344.35\n",
      "    update_time_ms: 3.71\n",
      "  timestamp: 1635297937\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 410000\n",
      "  training_iteration: 410\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   410</td><td style=\"text-align: right;\">         15357.1</td><td style=\"text-align: right;\">410000</td><td style=\"text-align: right;\">  1.7227</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">              -12.82</td><td style=\"text-align: right;\">             189.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 411000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-26-49\n",
      "  done: false\n",
      "  episode_len_mean: 185.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 1.819100000000021\n",
      "  episode_reward_min: -12.81999999999995\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1352\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7802370574254722\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2162338164117603\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0019244564728696783\n",
      "          policy_loss: -0.10925478554434247\n",
      "          total_loss: -0.0911489893992742\n",
      "          vf_explained_var: 0.7720865607261658\n",
      "          vf_loss: 0.03876659975697597\n",
      "    num_agent_steps_sampled: 411000\n",
      "    num_agent_steps_trained: 411000\n",
      "    num_steps_sampled: 411000\n",
      "    num_steps_trained: 411000\n",
      "  iterations_since_restore: 411\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.51067961165048\n",
      "    ram_util_percent: 32.75242718446603\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036670829006893835\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.149627455847437\n",
      "    mean_inference_ms: 2.1125580896149634\n",
      "    mean_raw_obs_processing_ms: 13.381567225734571\n",
      "  time_since_restore: 15429.158688545227\n",
      "  time_this_iter_s: 72.09228372573853\n",
      "  time_total_s: 15429.158688545227\n",
      "  timers:\n",
      "    learn_throughput: 1321.904\n",
      "    learn_time_ms: 756.484\n",
      "    load_throughput: 42467.643\n",
      "    load_time_ms: 23.547\n",
      "    sample_throughput: 11.329\n",
      "    sample_time_ms: 88271.857\n",
      "    update_time_ms: 3.695\n",
      "  timestamp: 1635298009\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 411000\n",
      "  training_iteration: 411\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   411</td><td style=\"text-align: right;\">         15429.2</td><td style=\"text-align: right;\">411000</td><td style=\"text-align: right;\">  1.8191</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">              -12.82</td><td style=\"text-align: right;\">            185.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 412000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-27-50\n",
      "  done: false\n",
      "  episode_len_mean: 186.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 1.8499000000000207\n",
      "  episode_reward_min: -12.81999999999995\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1357\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3901185287127361\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1193555858400135\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012342602939949012\n",
      "          policy_loss: 0.08139039344257779\n",
      "          total_loss: 0.2800765381091171\n",
      "          vf_explained_var: 0.42448797821998596\n",
      "          vf_loss: 0.21506462684935992\n",
      "    num_agent_steps_sampled: 412000\n",
      "    num_agent_steps_trained: 412000\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "  iterations_since_restore: 412\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.845977011494256\n",
      "    ram_util_percent: 32.80229885057472\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036670799573190395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.14839715098282\n",
      "    mean_inference_ms: 2.1125517628631076\n",
      "    mean_raw_obs_processing_ms: 13.490536017148026\n",
      "  time_since_restore: 15489.74322605133\n",
      "  time_this_iter_s: 60.584537506103516\n",
      "  time_total_s: 15489.74322605133\n",
      "  timers:\n",
      "    learn_throughput: 1325.076\n",
      "    learn_time_ms: 754.674\n",
      "    load_throughput: 42545.398\n",
      "    load_time_ms: 23.504\n",
      "    sample_throughput: 11.655\n",
      "    sample_time_ms: 85799.893\n",
      "    update_time_ms: 3.607\n",
      "  timestamp: 1635298070\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 412\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   412</td><td style=\"text-align: right;\">         15489.7</td><td style=\"text-align: right;\">412000</td><td style=\"text-align: right;\">  1.8499</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">              -12.82</td><td style=\"text-align: right;\">            186.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 413000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-31-39\n",
      "  done: false\n",
      "  episode_len_mean: 174.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 2.4296000000000193\n",
      "  episode_reward_min: -12.81999999999995\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1369\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3901185287127361\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0154703603850472\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012472770200184148\n",
      "          policy_loss: -0.16070012441939777\n",
      "          total_loss: -0.06881548621588283\n",
      "          vf_explained_var: 0.9791932702064514\n",
      "          vf_loss: 0.10717348218378094\n",
      "    num_agent_steps_sampled: 413000\n",
      "    num_agent_steps_trained: 413000\n",
      "    num_steps_sampled: 413000\n",
      "    num_steps_trained: 413000\n",
      "  iterations_since_restore: 413\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.68899082568807\n",
      "    ram_util_percent: 32.723241590214066\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036670638911600516\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.145681657244662\n",
      "    mean_inference_ms: 2.1125365395819755\n",
      "    mean_raw_obs_processing_ms: 13.78060280635329\n",
      "  time_since_restore: 15719.30243730545\n",
      "  time_this_iter_s: 229.55921125411987\n",
      "  time_total_s: 15719.30243730545\n",
      "  timers:\n",
      "    learn_throughput: 1323.609\n",
      "    learn_time_ms: 755.51\n",
      "    load_throughput: 42316.483\n",
      "    load_time_ms: 23.631\n",
      "    sample_throughput: 10.552\n",
      "    sample_time_ms: 94771.663\n",
      "    update_time_ms: 3.685\n",
      "  timestamp: 1635298299\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 413000\n",
      "  training_iteration: 413\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   413</td><td style=\"text-align: right;\">         15719.3</td><td style=\"text-align: right;\">413000</td><td style=\"text-align: right;\">  2.4296</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">              -12.82</td><td style=\"text-align: right;\">            174.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 414000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-32-55\n",
      "  done: false\n",
      "  episode_len_mean: 176.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 2.404700000000019\n",
      "  episode_reward_min: -12.81999999999995\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1374\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3901185287127361\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.192472963862949\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006627470364554622\n",
      "          policy_loss: -0.12215766691499286\n",
      "          total_loss: -0.09394721852408515\n",
      "          vf_explained_var: 0.6128861904144287\n",
      "          vf_loss: 0.047549678726742664\n",
      "    num_agent_steps_sampled: 414000\n",
      "    num_agent_steps_trained: 414000\n",
      "    num_steps_sampled: 414000\n",
      "    num_steps_trained: 414000\n",
      "  iterations_since_restore: 414\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.1037037037037\n",
      "    ram_util_percent: 32.72870370370371\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667056687943192\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.1440156883728\n",
      "    mean_inference_ms: 2.1125313188546966\n",
      "    mean_raw_obs_processing_ms: 13.897484479086748\n",
      "  time_since_restore: 15794.946008682251\n",
      "  time_this_iter_s: 75.64357137680054\n",
      "  time_total_s: 15794.946008682251\n",
      "  timers:\n",
      "    learn_throughput: 1325.259\n",
      "    learn_time_ms: 754.569\n",
      "    load_throughput: 42249.901\n",
      "    load_time_ms: 23.669\n",
      "    sample_throughput: 10.305\n",
      "    sample_time_ms: 97037.169\n",
      "    update_time_ms: 3.757\n",
      "  timestamp: 1635298375\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 414000\n",
      "  training_iteration: 414\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   414</td><td style=\"text-align: right;\">         15794.9</td><td style=\"text-align: right;\">414000</td><td style=\"text-align: right;\">  2.4047</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">              -12.82</td><td style=\"text-align: right;\">            176.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 415000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-33-32\n",
      "  done: false\n",
      "  episode_len_mean: 180.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 2.30590000000002\n",
      "  episode_reward_min: -12.81999999999995\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1378\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3901185287127361\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2809280051125422\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007389523728443642\n",
      "          policy_loss: 0.20422777679438392\n",
      "          total_loss: 0.21258537537521785\n",
      "          vf_explained_var: 0.7151815891265869\n",
      "          vf_loss: 0.02828408487710274\n",
      "    num_agent_steps_sampled: 415000\n",
      "    num_agent_steps_trained: 415000\n",
      "    num_steps_sampled: 415000\n",
      "    num_steps_trained: 415000\n",
      "  iterations_since_restore: 415\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.04528301886793\n",
      "    ram_util_percent: 32.81698113207547\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667052722221015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.142545990782324\n",
      "    mean_inference_ms: 2.112527447851662\n",
      "    mean_raw_obs_processing_ms: 13.986857435733917\n",
      "  time_since_restore: 15832.274449586868\n",
      "  time_this_iter_s: 37.32844090461731\n",
      "  time_total_s: 15832.274449586868\n",
      "  timers:\n",
      "    learn_throughput: 1325.836\n",
      "    learn_time_ms: 754.241\n",
      "    load_throughput: 42009.709\n",
      "    load_time_ms: 23.804\n",
      "    sample_throughput: 10.337\n",
      "    sample_time_ms: 96743.934\n",
      "    update_time_ms: 3.831\n",
      "  timestamp: 1635298412\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 415000\n",
      "  training_iteration: 415\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   415</td><td style=\"text-align: right;\">         15832.3</td><td style=\"text-align: right;\">415000</td><td style=\"text-align: right;\">  2.3059</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">              -12.82</td><td style=\"text-align: right;\">            180.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 416000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-35-07\n",
      "  done: false\n",
      "  episode_len_mean: 174.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 2.419800000000019\n",
      "  episode_reward_min: -12.81999999999995\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1383\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3901185287127361\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.27608777946896\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.14759789637115264\n",
      "          policy_loss: -0.007818671315908432\n",
      "          total_loss: 0.9740049798455503\n",
      "          vf_explained_var: 0.2102695256471634\n",
      "          vf_loss: 0.9470038540454374\n",
      "    num_agent_steps_sampled: 416000\n",
      "    num_agent_steps_trained: 416000\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "  iterations_since_restore: 416\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.26323529411764\n",
      "    ram_util_percent: 32.81838235294118\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667045829671839\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.14075339043102\n",
      "    mean_inference_ms: 2.112522830187818\n",
      "    mean_raw_obs_processing_ms: 14.100173204682612\n",
      "  time_since_restore: 15927.422379016876\n",
      "  time_this_iter_s: 95.14792943000793\n",
      "  time_total_s: 15927.422379016876\n",
      "  timers:\n",
      "    learn_throughput: 1324.683\n",
      "    learn_time_ms: 754.898\n",
      "    load_throughput: 41929.497\n",
      "    load_time_ms: 23.85\n",
      "    sample_throughput: 10.335\n",
      "    sample_time_ms: 96756.914\n",
      "    update_time_ms: 3.83\n",
      "  timestamp: 1635298507\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 416\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   416</td><td style=\"text-align: right;\">         15927.4</td><td style=\"text-align: right;\">416000</td><td style=\"text-align: right;\">  2.4198</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">              -12.82</td><td style=\"text-align: right;\">            174.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 417000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-35-50\n",
      "  done: false\n",
      "  episode_len_mean: 182.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 2.26340000000002\n",
      "  episode_reward_min: -12.81999999999995\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1387\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5851777930691044\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2867787228690255\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0060734522940135505\n",
      "          policy_loss: -0.06168895663900508\n",
      "          total_loss: -0.04122614974362983\n",
      "          vf_explained_var: 0.6620432734489441\n",
      "          vf_loss: 0.039776544293595686\n",
      "    num_agent_steps_sampled: 417000\n",
      "    num_agent_steps_trained: 417000\n",
      "    num_steps_sampled: 417000\n",
      "    num_steps_trained: 417000\n",
      "  iterations_since_restore: 417\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.11475409836064\n",
      "    ram_util_percent: 32.73770491803279\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036670396092641847\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.139348421794438\n",
      "    mean_inference_ms: 2.112519401685248\n",
      "    mean_raw_obs_processing_ms: 14.185086698469233\n",
      "  time_since_restore: 15969.960703134537\n",
      "  time_this_iter_s: 42.53832411766052\n",
      "  time_total_s: 15969.960703134537\n",
      "  timers:\n",
      "    learn_throughput: 1328.586\n",
      "    learn_time_ms: 752.68\n",
      "    load_throughput: 42233.607\n",
      "    load_time_ms: 23.678\n",
      "    sample_throughput: 10.715\n",
      "    sample_time_ms: 93328.35\n",
      "    update_time_ms: 3.828\n",
      "  timestamp: 1635298550\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 417000\n",
      "  training_iteration: 417\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   417</td><td style=\"text-align: right;\">           15970</td><td style=\"text-align: right;\">417000</td><td style=\"text-align: right;\">  2.2634</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">              -12.82</td><td style=\"text-align: right;\">            182.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 418000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-36-49\n",
      "  done: false\n",
      "  episode_len_mean: 185.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 2.3024000000000204\n",
      "  episode_reward_min: -12.81999999999995\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1391\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5851777930691044\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2998632219102646\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004656855268610806\n",
      "          policy_loss: -0.09051550726095835\n",
      "          total_loss: -0.08347375591595968\n",
      "          vf_explained_var: 0.8625399470329285\n",
      "          vf_loss: 0.027315292714370623\n",
      "    num_agent_steps_sampled: 418000\n",
      "    num_agent_steps_trained: 418000\n",
      "    num_steps_sampled: 418000\n",
      "    num_steps_trained: 418000\n",
      "  iterations_since_restore: 418\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.14047619047619\n",
      "    ram_util_percent: 32.77261904761906\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667033292262244\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.138046418081082\n",
      "    mean_inference_ms: 2.1125173909098836\n",
      "    mean_raw_obs_processing_ms: 14.27183378543631\n",
      "  time_since_restore: 16028.905574321747\n",
      "  time_this_iter_s: 58.94487118721008\n",
      "  time_total_s: 16028.905574321747\n",
      "  timers:\n",
      "    learn_throughput: 1327.328\n",
      "    learn_time_ms: 753.394\n",
      "    load_throughput: 41824.801\n",
      "    load_time_ms: 23.909\n",
      "    sample_throughput: 13.097\n",
      "    sample_time_ms: 76353.072\n",
      "    update_time_ms: 3.876\n",
      "  timestamp: 1635298609\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 418000\n",
      "  training_iteration: 418\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   418</td><td style=\"text-align: right;\">         16028.9</td><td style=\"text-align: right;\">418000</td><td style=\"text-align: right;\">  2.3024</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">              -12.82</td><td style=\"text-align: right;\">            185.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 419000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-38-05\n",
      "  done: false\n",
      "  episode_len_mean: 177.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 2.4724000000000186\n",
      "  episode_reward_min: -12.81999999999995\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1397\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2925888965345522\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.31797686152988\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006543317595398151\n",
      "          policy_loss: -0.03450890059272448\n",
      "          total_loss: -0.023611592170264987\n",
      "          vf_explained_var: 0.8596566319465637\n",
      "          vf_loss: 0.03216257628777789\n",
      "    num_agent_steps_sampled: 419000\n",
      "    num_agent_steps_trained: 419000\n",
      "    num_steps_sampled: 419000\n",
      "    num_steps_trained: 419000\n",
      "  iterations_since_restore: 419\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.414678899082574\n",
      "    ram_util_percent: 32.77522935779817\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0366702669981251\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.137094781342853\n",
      "    mean_inference_ms: 2.1125172882188377\n",
      "    mean_raw_obs_processing_ms: 14.40288063657284\n",
      "  time_since_restore: 16104.926569461823\n",
      "  time_this_iter_s: 76.02099514007568\n",
      "  time_total_s: 16104.926569461823\n",
      "  timers:\n",
      "    learn_throughput: 1328.314\n",
      "    learn_time_ms: 752.834\n",
      "    load_throughput: 41835.564\n",
      "    load_time_ms: 23.903\n",
      "    sample_throughput: 12.837\n",
      "    sample_time_ms: 77900.788\n",
      "    update_time_ms: 3.874\n",
      "  timestamp: 1635298685\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 419000\n",
      "  training_iteration: 419\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   419</td><td style=\"text-align: right;\">         16104.9</td><td style=\"text-align: right;\">419000</td><td style=\"text-align: right;\">  2.4724</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">              -12.82</td><td style=\"text-align: right;\">            177.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 420000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-40-42\n",
      "  done: false\n",
      "  episode_len_mean: 173.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 2.681500000000018\n",
      "  episode_reward_min: -12.81999999999995\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1406\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2925888965345522\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.128583331902822\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0183516757668431\n",
      "          policy_loss: -0.0633687083919843\n",
      "          total_loss: 0.6236849822931819\n",
      "          vf_explained_var: 0.786277711391449\n",
      "          vf_loss: 0.7029700252744887\n",
      "    num_agent_steps_sampled: 420000\n",
      "    num_agent_steps_trained: 420000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 420\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.027232142857144\n",
      "    ram_util_percent: 32.78482142857143\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667020834851902\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.13653759786015\n",
      "    mean_inference_ms: 2.1125190013828483\n",
      "    mean_raw_obs_processing_ms: 14.60137063407913\n",
      "  time_since_restore: 16261.906998872757\n",
      "  time_this_iter_s: 156.98042941093445\n",
      "  time_total_s: 16261.906998872757\n",
      "  timers:\n",
      "    learn_throughput: 1331.269\n",
      "    learn_time_ms: 751.163\n",
      "    load_throughput: 41761.544\n",
      "    load_time_ms: 23.945\n",
      "    sample_throughput: 11.148\n",
      "    sample_time_ms: 89700.764\n",
      "    update_time_ms: 3.953\n",
      "  timestamp: 1635298842\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 420\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   420</td><td style=\"text-align: right;\">         16261.9</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\">  2.6815</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">              -12.82</td><td style=\"text-align: right;\">            173.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 421000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-42-41\n",
      "  done: false\n",
      "  episode_len_mean: 167.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.9\n",
      "  episode_reward_mean: 2.701900000000017\n",
      "  episode_reward_min: -12.81999999999995\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1414\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2925888965345522\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.097726457648807\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012590677704129187\n",
      "          policy_loss: -0.07973045988215341\n",
      "          total_loss: 0.02264450180033843\n",
      "          vf_explained_var: 0.7107157111167908\n",
      "          vf_loss: 0.11966833203203148\n",
      "    num_agent_steps_sampled: 421000\n",
      "    num_agent_steps_trained: 421000\n",
      "    num_steps_sampled: 421000\n",
      "    num_steps_trained: 421000\n",
      "  iterations_since_restore: 421\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.282941176470594\n",
      "    ram_util_percent: 32.83470588235294\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667020887152358\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.13695591082557\n",
      "    mean_inference_ms: 2.1125223746590525\n",
      "    mean_raw_obs_processing_ms: 14.781382047912762\n",
      "  time_since_restore: 16381.284034967422\n",
      "  time_this_iter_s: 119.37703609466553\n",
      "  time_total_s: 16381.284034967422\n",
      "  timers:\n",
      "    learn_throughput: 1332.01\n",
      "    learn_time_ms: 750.745\n",
      "    load_throughput: 41458.309\n",
      "    load_time_ms: 24.121\n",
      "    sample_throughput: 10.59\n",
      "    sample_time_ms: 94429.451\n",
      "    update_time_ms: 3.972\n",
      "  timestamp: 1635298961\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 421000\n",
      "  training_iteration: 421\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   421</td><td style=\"text-align: right;\">         16381.3</td><td style=\"text-align: right;\">421000</td><td style=\"text-align: right;\">  2.7019</td><td style=\"text-align: right;\">                 9.9</td><td style=\"text-align: right;\">              -12.82</td><td style=\"text-align: right;\">            167.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 422000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-46-07\n",
      "  done: false\n",
      "  episode_len_mean: 157.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.9\n",
      "  episode_reward_mean: 2.939300000000016\n",
      "  episode_reward_min: -12.81999999999995\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1426\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2925888965345522\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.058180562655131\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02172037465336968\n",
      "          policy_loss: -0.0622211223675145\n",
      "          total_loss: 0.29839444864127374\n",
      "          vf_explained_var: 0.9392365217208862\n",
      "          vf_loss: 0.3748422456232624\n",
      "    num_agent_steps_sampled: 422000\n",
      "    num_agent_steps_trained: 422000\n",
      "    num_steps_sampled: 422000\n",
      "    num_steps_trained: 422000\n",
      "  iterations_since_restore: 422\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.61224489795919\n",
      "    ram_util_percent: 32.82959183673469\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667020121058127\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.139447678233534\n",
      "    mean_inference_ms: 2.112531023674484\n",
      "    mean_raw_obs_processing_ms: 15.076733197223039\n",
      "  time_since_restore: 16587.00543832779\n",
      "  time_this_iter_s: 205.72140336036682\n",
      "  time_total_s: 16587.00543832779\n",
      "  timers:\n",
      "    learn_throughput: 1328.33\n",
      "    learn_time_ms: 752.825\n",
      "    load_throughput: 41230.9\n",
      "    load_time_ms: 24.254\n",
      "    sample_throughput: 9.179\n",
      "    sample_time_ms: 108940.925\n",
      "    update_time_ms: 3.965\n",
      "  timestamp: 1635299167\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 422000\n",
      "  training_iteration: 422\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   422</td><td style=\"text-align: right;\">           16587</td><td style=\"text-align: right;\">422000</td><td style=\"text-align: right;\">  2.9393</td><td style=\"text-align: right;\">                 9.9</td><td style=\"text-align: right;\">              -12.82</td><td style=\"text-align: right;\">            157.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 423000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-46-32\n",
      "  done: false\n",
      "  episode_len_mean: 156.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.9\n",
      "  episode_reward_mean: 2.9919000000000153\n",
      "  episode_reward_min: -12.81999999999995\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1428\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4388833448018283\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3674715095096164\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01574001651385372\n",
      "          policy_loss: 0.05145184422532718\n",
      "          total_loss: 0.16409076232877043\n",
      "          vf_explained_var: 0.5568590760231018\n",
      "          vf_loss: 0.12940559834241866\n",
      "    num_agent_steps_sampled: 423000\n",
      "    num_agent_steps_trained: 423000\n",
      "    num_steps_sampled: 423000\n",
      "    num_steps_trained: 423000\n",
      "  iterations_since_restore: 423\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.89714285714285\n",
      "    ram_util_percent: 32.94285714285715\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667021110719697\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.13983291174738\n",
      "    mean_inference_ms: 2.1125326927197405\n",
      "    mean_raw_obs_processing_ms: 15.118569476597507\n",
      "  time_since_restore: 16611.732699394226\n",
      "  time_this_iter_s: 24.727261066436768\n",
      "  time_total_s: 16611.732699394226\n",
      "  timers:\n",
      "    learn_throughput: 1330.389\n",
      "    learn_time_ms: 751.66\n",
      "    load_throughput: 41678.01\n",
      "    load_time_ms: 23.993\n",
      "    sample_throughput: 11.305\n",
      "    sample_time_ms: 88459.164\n",
      "    update_time_ms: 3.958\n",
      "  timestamp: 1635299192\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 423000\n",
      "  training_iteration: 423\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   423</td><td style=\"text-align: right;\">         16611.7</td><td style=\"text-align: right;\">423000</td><td style=\"text-align: right;\">  2.9919</td><td style=\"text-align: right;\">                 9.9</td><td style=\"text-align: right;\">              -12.82</td><td style=\"text-align: right;\">            156.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-48-16\n",
      "  done: false\n",
      "  episode_len_mean: 160.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 3.121800000000016\n",
      "  episode_reward_min: -8.45999999999995\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1435\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4388833448018283\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0867893603112964\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008735983453057086\n",
      "          policy_loss: 0.07995273802015516\n",
      "          total_loss: 0.2587375890877512\n",
      "          vf_explained_var: 0.8347175717353821\n",
      "          vf_loss: 0.19581866822360705\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_agent_steps_trained: 424000\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "  iterations_since_restore: 424\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.88389261744966\n",
      "    ram_util_percent: 32.92416107382551\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667028974493225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.140853384654115\n",
      "    mean_inference_ms: 2.1125428608698384\n",
      "    mean_raw_obs_processing_ms: 15.259610082798764\n",
      "  time_since_restore: 16716.04695534706\n",
      "  time_this_iter_s: 104.31425595283508\n",
      "  time_total_s: 16716.04695534706\n",
      "  timers:\n",
      "    learn_throughput: 1329.842\n",
      "    learn_time_ms: 751.969\n",
      "    load_throughput: 41723.906\n",
      "    load_time_ms: 23.967\n",
      "    sample_throughput: 10.95\n",
      "    sample_time_ms: 91326.043\n",
      "    update_time_ms: 3.878\n",
      "  timestamp: 1635299296\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 424\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   424</td><td style=\"text-align: right;\">           16716</td><td style=\"text-align: right;\">424000</td><td style=\"text-align: right;\">  3.1218</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -8.46</td><td style=\"text-align: right;\">             160.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 425000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-50-15\n",
      "  done: false\n",
      "  episode_len_mean: 165.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 3.0173000000000165\n",
      "  episode_reward_min: -8.45999999999995\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1442\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4388833448018283\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.128383329179552\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011047367780591181\n",
      "          policy_loss: -0.11801805843909581\n",
      "          total_loss: 0.05336614896853765\n",
      "          vf_explained_var: 0.9086204767227173\n",
      "          vf_loss: 0.1878195310321947\n",
      "    num_agent_steps_sampled: 425000\n",
      "    num_agent_steps_trained: 425000\n",
      "    num_steps_sampled: 425000\n",
      "    num_steps_trained: 425000\n",
      "  iterations_since_restore: 425\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.8229411764706\n",
      "    ram_util_percent: 33.01294117647059\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036670363644564365\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.142432501531104\n",
      "    mean_inference_ms: 2.1125520307578487\n",
      "    mean_raw_obs_processing_ms: 15.41211293700052\n",
      "  time_since_restore: 16835.30172228813\n",
      "  time_this_iter_s: 119.25476694107056\n",
      "  time_total_s: 16835.30172228813\n",
      "  timers:\n",
      "    learn_throughput: 1332.63\n",
      "    learn_time_ms: 750.396\n",
      "    load_throughput: 41936.079\n",
      "    load_time_ms: 23.846\n",
      "    sample_throughput: 10.048\n",
      "    sample_time_ms: 99520.437\n",
      "    update_time_ms: 3.805\n",
      "  timestamp: 1635299415\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 425000\n",
      "  training_iteration: 425\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   425</td><td style=\"text-align: right;\">         16835.3</td><td style=\"text-align: right;\">425000</td><td style=\"text-align: right;\">  3.0173</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -8.46</td><td style=\"text-align: right;\">            165.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 426000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-50-55\n",
      "  done: false\n",
      "  episode_len_mean: 165.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.9770000000000163\n",
      "  episode_reward_min: -8.45999999999995\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1445\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4388833448018283\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.368697905540466\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008685667403337335\n",
      "          policy_loss: 0.060054471467932066\n",
      "          total_loss: 0.35845625292923716\n",
      "          vf_explained_var: 0.4698949158191681\n",
      "          vf_loss: 0.3182767681673997\n",
      "    num_agent_steps_sampled: 426000\n",
      "    num_agent_steps_trained: 426000\n",
      "    num_steps_sampled: 426000\n",
      "    num_steps_trained: 426000\n",
      "  iterations_since_restore: 426\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.77368421052631\n",
      "    ram_util_percent: 33.0438596491228\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036670393527844285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.143058446079912\n",
      "    mean_inference_ms: 2.112556170671718\n",
      "    mean_raw_obs_processing_ms: 15.476266651891635\n",
      "  time_since_restore: 16874.85350871086\n",
      "  time_this_iter_s: 39.55178642272949\n",
      "  time_total_s: 16874.85350871086\n",
      "  timers:\n",
      "    learn_throughput: 1332.898\n",
      "    learn_time_ms: 750.245\n",
      "    load_throughput: 42110.387\n",
      "    load_time_ms: 23.747\n",
      "    sample_throughput: 10.643\n",
      "    sample_time_ms: 93961.004\n",
      "    update_time_ms: 3.898\n",
      "  timestamp: 1635299455\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 426000\n",
      "  training_iteration: 426\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   426</td><td style=\"text-align: right;\">         16874.9</td><td style=\"text-align: right;\">426000</td><td style=\"text-align: right;\">   2.977</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -8.46</td><td style=\"text-align: right;\">            165.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 427000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-51-17\n",
      "  done: false\n",
      "  episode_len_mean: 170.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.8276000000000168\n",
      "  episode_reward_min: -8.45999999999995\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1447\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4388833448018283\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3216725163989596\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023707695710948447\n",
      "          policy_loss: 0.0714919336967998\n",
      "          total_loss: 0.06574209742248058\n",
      "          vf_explained_var: 0.3260897099971771\n",
      "          vf_loss: 0.007061979811017712\n",
      "    num_agent_steps_sampled: 427000\n",
      "    num_agent_steps_trained: 427000\n",
      "    num_steps_sampled: 427000\n",
      "    num_steps_trained: 427000\n",
      "  iterations_since_restore: 427\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.12258064516129\n",
      "    ram_util_percent: 33.12903225806453\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667042409338906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.14344788352129\n",
      "    mean_inference_ms: 2.112558973357265\n",
      "    mean_raw_obs_processing_ms: 15.518177044816012\n",
      "  time_since_restore: 16896.596551418304\n",
      "  time_this_iter_s: 21.743042707443237\n",
      "  time_total_s: 16896.596551418304\n",
      "  timers:\n",
      "    learn_throughput: 1330.736\n",
      "    learn_time_ms: 751.464\n",
      "    load_throughput: 41597.036\n",
      "    load_time_ms: 24.04\n",
      "    sample_throughput: 10.884\n",
      "    sample_time_ms: 91879.961\n",
      "    update_time_ms: 3.899\n",
      "  timestamp: 1635299477\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 427000\n",
      "  training_iteration: 427\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   427</td><td style=\"text-align: right;\">         16896.6</td><td style=\"text-align: right;\">427000</td><td style=\"text-align: right;\">  2.8276</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -8.46</td><td style=\"text-align: right;\">            170.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 428000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-54-25\n",
      "  done: false\n",
      "  episode_len_mean: 155.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 3.298500000000016\n",
      "  episode_reward_min: -8.429999999999936\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1458\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6583250172027423\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8515131142404344\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006272761617154031\n",
      "          policy_loss: -0.19877119983235994\n",
      "          total_loss: -0.16767888474795553\n",
      "          vf_explained_var: 0.35050761699676514\n",
      "          vf_loss: 0.045477929794126085\n",
      "    num_agent_steps_sampled: 428000\n",
      "    num_agent_steps_trained: 428000\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "  iterations_since_restore: 428\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.96007462686567\n",
      "    ram_util_percent: 33.02201492537314\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667065999631194\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.146498998332678\n",
      "    mean_inference_ms: 2.112576364641088\n",
      "    mean_raw_obs_processing_ms: 15.768606536105144\n",
      "  time_since_restore: 17084.476046085358\n",
      "  time_this_iter_s: 187.87949466705322\n",
      "  time_total_s: 17084.476046085358\n",
      "  timers:\n",
      "    learn_throughput: 1330.665\n",
      "    learn_time_ms: 751.504\n",
      "    load_throughput: 41556.358\n",
      "    load_time_ms: 24.064\n",
      "    sample_throughput: 9.544\n",
      "    sample_time_ms: 104773.272\n",
      "    update_time_ms: 3.949\n",
      "  timestamp: 1635299665\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 428\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   428</td><td style=\"text-align: right;\">         17084.5</td><td style=\"text-align: right;\">428000</td><td style=\"text-align: right;\">  3.2985</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -8.43</td><td style=\"text-align: right;\">            155.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 429000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-56-59\n",
      "  done: false\n",
      "  episode_len_mean: 162.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 3.1383000000000165\n",
      "  episode_reward_min: -8.429999999999936\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1468\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6583250172027423\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8224741677443186\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0051506862315873255\n",
      "          policy_loss: 0.12654623687267302\n",
      "          total_loss: 0.1895653520193365\n",
      "          vf_explained_var: 0.7480764389038086\n",
      "          vf_loss: 0.07785303428665631\n",
      "    num_agent_steps_sampled: 429000\n",
      "    num_agent_steps_trained: 429000\n",
      "    num_steps_sampled: 429000\n",
      "    num_steps_trained: 429000\n",
      "  iterations_since_restore: 429\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.06\n",
      "    ram_util_percent: 33.04863636363636\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667096246935643\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.14889779935373\n",
      "    mean_inference_ms: 2.112593783136164\n",
      "    mean_raw_obs_processing_ms: 15.978475847027061\n",
      "  time_since_restore: 17238.642248630524\n",
      "  time_this_iter_s: 154.16620254516602\n",
      "  time_total_s: 17238.642248630524\n",
      "  timers:\n",
      "    learn_throughput: 1329.643\n",
      "    learn_time_ms: 752.082\n",
      "    load_throughput: 41542.611\n",
      "    load_time_ms: 24.072\n",
      "    sample_throughput: 8.882\n",
      "    sample_time_ms: 112587.031\n",
      "    update_time_ms: 4.093\n",
      "  timestamp: 1635299819\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 429000\n",
      "  training_iteration: 429\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   429</td><td style=\"text-align: right;\">         17238.6</td><td style=\"text-align: right;\">429000</td><td style=\"text-align: right;\">  3.1383</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -8.43</td><td style=\"text-align: right;\">            162.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 430000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_01-59-10\n",
      "  done: false\n",
      "  episode_len_mean: 154.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 3.313400000000016\n",
      "  episode_reward_min: -8.429999999999936\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1476\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6583250172027423\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1872860087288752\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011012125102793075\n",
      "          policy_loss: -0.14619440096947883\n",
      "          total_loss: 0.024551109969615938\n",
      "          vf_explained_var: 0.2358894944190979\n",
      "          vf_loss: 0.18536881668907074\n",
      "    num_agent_steps_sampled: 430000\n",
      "    num_agent_steps_trained: 430000\n",
      "    num_steps_sampled: 430000\n",
      "    num_steps_trained: 430000\n",
      "  iterations_since_restore: 430\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.7957219251337\n",
      "    ram_util_percent: 33.028877005347596\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667122275199669\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.151009538649376\n",
      "    mean_inference_ms: 2.11260666700034\n",
      "    mean_raw_obs_processing_ms: 16.156754073671262\n",
      "  time_since_restore: 17369.616854429245\n",
      "  time_this_iter_s: 130.9746057987213\n",
      "  time_total_s: 17369.616854429245\n",
      "  timers:\n",
      "    learn_throughput: 1326.887\n",
      "    learn_time_ms: 753.644\n",
      "    load_throughput: 41580.458\n",
      "    load_time_ms: 24.05\n",
      "    sample_throughput: 9.092\n",
      "    sample_time_ms: 109984.954\n",
      "    update_time_ms: 4.06\n",
      "  timestamp: 1635299950\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 430000\n",
      "  training_iteration: 430\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   430</td><td style=\"text-align: right;\">         17369.6</td><td style=\"text-align: right;\">430000</td><td style=\"text-align: right;\">  3.3134</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -8.43</td><td style=\"text-align: right;\">            154.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 431000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-01-42\n",
      "  done: false\n",
      "  episode_len_mean: 149.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.415500000000016\n",
      "  episode_reward_min: -10.799999999999923\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1485\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6583250172027423\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3081818342208864\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008309865861796092\n",
      "          policy_loss: -0.031706676052676304\n",
      "          total_loss: 0.08312199094539716\n",
      "          vf_explained_var: 0.8433505296707153\n",
      "          vf_loss: 0.13243988814453284\n",
      "    num_agent_steps_sampled: 431000\n",
      "    num_agent_steps_trained: 431000\n",
      "    num_steps_sampled: 431000\n",
      "    num_steps_trained: 431000\n",
      "  iterations_since_restore: 431\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.642857142857146\n",
      "    ram_util_percent: 33.087096774193554\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667155225534422\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.153991796875076\n",
      "    mean_inference_ms: 2.1126240792373716\n",
      "    mean_raw_obs_processing_ms: 16.36942774686275\n",
      "  time_since_restore: 17522.09965491295\n",
      "  time_this_iter_s: 152.4828004837036\n",
      "  time_total_s: 17522.09965491295\n",
      "  timers:\n",
      "    learn_throughput: 1325.664\n",
      "    learn_time_ms: 754.339\n",
      "    load_throughput: 41801.042\n",
      "    load_time_ms: 23.923\n",
      "    sample_throughput: 8.827\n",
      "    sample_time_ms: 113294.911\n",
      "    update_time_ms: 4.109\n",
      "  timestamp: 1635300102\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 431000\n",
      "  training_iteration: 431\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   431</td><td style=\"text-align: right;\">         17522.1</td><td style=\"text-align: right;\">431000</td><td style=\"text-align: right;\">  3.4155</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            149.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 432000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-02-22\n",
      "  done: false\n",
      "  episode_len_mean: 147.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.541500000000015\n",
      "  episode_reward_min: -10.799999999999923\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1488\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6583250172027423\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.371953927146064\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009932166865106742\n",
      "          policy_loss: 0.08698215786781575\n",
      "          total_loss: 0.08034248567289776\n",
      "          vf_explained_var: 0.6161197423934937\n",
      "          vf_loss: 0.01054127214786907\n",
      "    num_agent_steps_sampled: 432000\n",
      "    num_agent_steps_trained: 432000\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "  iterations_since_restore: 432\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.573684210526324\n",
      "    ram_util_percent: 33.098245614035086\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667167582842622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.15483084000398\n",
      "    mean_inference_ms: 2.112630008836266\n",
      "    mean_raw_obs_processing_ms: 16.438741053808165\n",
      "  time_since_restore: 17561.520437002182\n",
      "  time_this_iter_s: 39.4207820892334\n",
      "  time_total_s: 17561.520437002182\n",
      "  timers:\n",
      "    learn_throughput: 1325.817\n",
      "    learn_time_ms: 754.252\n",
      "    load_throughput: 41966.919\n",
      "    load_time_ms: 23.828\n",
      "    sample_throughput: 10.345\n",
      "    sample_time_ms: 96665.02\n",
      "    update_time_ms: 4.114\n",
      "  timestamp: 1635300142\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 432\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   432</td><td style=\"text-align: right;\">         17561.5</td><td style=\"text-align: right;\">432000</td><td style=\"text-align: right;\">  3.5415</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            147.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 433000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-03-16\n",
      "  done: false\n",
      "  episode_len_mean: 149.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.501300000000015\n",
      "  episode_reward_min: -10.799999999999923\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1492\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6583250172027423\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3195699705017936\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006047615774425235\n",
      "          policy_loss: 0.07340347270170848\n",
      "          total_loss: 0.06959106292989518\n",
      "          vf_explained_var: 0.3293336033821106\n",
      "          vf_loss: 0.01540199750258277\n",
      "    num_agent_steps_sampled: 433000\n",
      "    num_agent_steps_trained: 433000\n",
      "    num_steps_sampled: 433000\n",
      "    num_steps_trained: 433000\n",
      "  iterations_since_restore: 433\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.76883116883117\n",
      "    ram_util_percent: 33.114285714285714\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036671881143029425\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.15549770325103\n",
      "    mean_inference_ms: 2.112637625603272\n",
      "    mean_raw_obs_processing_ms: 16.530637382811747\n",
      "  time_since_restore: 17615.638709783554\n",
      "  time_this_iter_s: 54.11827278137207\n",
      "  time_total_s: 17615.638709783554\n",
      "  timers:\n",
      "    learn_throughput: 1324.287\n",
      "    learn_time_ms: 755.123\n",
      "    load_throughput: 41576.254\n",
      "    load_time_ms: 24.052\n",
      "    sample_throughput: 10.04\n",
      "    sample_time_ms: 99603.106\n",
      "    update_time_ms: 4.044\n",
      "  timestamp: 1635300196\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 433000\n",
      "  training_iteration: 433\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   433</td><td style=\"text-align: right;\">         17615.6</td><td style=\"text-align: right;\">433000</td><td style=\"text-align: right;\">  3.5013</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">             149.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 434000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-03-33\n",
      "  done: false\n",
      "  episode_len_mean: 154.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.407700000000016\n",
      "  episode_reward_min: -10.799999999999923\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1494\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6583250172027423\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.379159683651394\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007356828617477598\n",
      "          policy_loss: 0.07454404648807314\n",
      "          total_loss: 0.0598634594016605\n",
      "          vf_explained_var: 0.3330402970314026\n",
      "          vf_loss: 0.004267826614280542\n",
      "    num_agent_steps_sampled: 434000\n",
      "    num_agent_steps_trained: 434000\n",
      "    num_steps_sampled: 434000\n",
      "    num_steps_trained: 434000\n",
      "  iterations_since_restore: 434\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.62916666666666\n",
      "    ram_util_percent: 33.12083333333334\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667198802150504\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.155566841640237\n",
      "    mean_inference_ms: 2.1126412365856257\n",
      "    mean_raw_obs_processing_ms: 16.57439974177329\n",
      "  time_since_restore: 17632.41473555565\n",
      "  time_this_iter_s: 16.776025772094727\n",
      "  time_total_s: 17632.41473555565\n",
      "  timers:\n",
      "    learn_throughput: 1323.712\n",
      "    learn_time_ms: 755.451\n",
      "    load_throughput: 42914.583\n",
      "    load_time_ms: 23.302\n",
      "    sample_throughput: 11.007\n",
      "    sample_time_ms: 90849.641\n",
      "    update_time_ms: 4.057\n",
      "  timestamp: 1635300213\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 434000\n",
      "  training_iteration: 434\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   434</td><td style=\"text-align: right;\">         17632.4</td><td style=\"text-align: right;\">434000</td><td style=\"text-align: right;\">  3.4077</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            154.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 435000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-04-34\n",
      "  done: false\n",
      "  episode_len_mean: 153.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.5121000000000167\n",
      "  episode_reward_min: -10.799999999999923\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1498\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6583250172027423\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7612279958195156\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.025076383559163803\n",
      "          policy_loss: 0.05177161759800381\n",
      "          total_loss: 0.1146443138519923\n",
      "          vf_explained_var: 0.9306673407554626\n",
      "          vf_loss: 0.06397656387545997\n",
      "    num_agent_steps_sampled: 435000\n",
      "    num_agent_steps_trained: 435000\n",
      "    num_steps_sampled: 435000\n",
      "    num_steps_trained: 435000\n",
      "  iterations_since_restore: 435\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.541379310344844\n",
      "    ram_util_percent: 33.07241379310344\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667222481695716\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.155700468451126\n",
      "    mean_inference_ms: 2.1126488464962323\n",
      "    mean_raw_obs_processing_ms: 16.660972757667583\n",
      "  time_since_restore: 17693.336483955383\n",
      "  time_this_iter_s: 60.9217483997345\n",
      "  time_total_s: 17693.336483955383\n",
      "  timers:\n",
      "    learn_throughput: 1321.144\n",
      "    learn_time_ms: 756.92\n",
      "    load_throughput: 42994.955\n",
      "    load_time_ms: 23.259\n",
      "    sample_throughput: 11.763\n",
      "    sample_time_ms: 85014.748\n",
      "    update_time_ms: 4.221\n",
      "  timestamp: 1635300274\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 435000\n",
      "  training_iteration: 435\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   435</td><td style=\"text-align: right;\">         17693.3</td><td style=\"text-align: right;\">435000</td><td style=\"text-align: right;\">  3.5121</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            153.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 436000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-05-46\n",
      "  done: false\n",
      "  episode_len_mean: 160.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.3717000000000166\n",
      "  episode_reward_min: -10.799999999999923\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1503\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9874875258041136\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1733842425876193\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0031947595247285954\n",
      "          policy_loss: -0.20555949782331784\n",
      "          total_loss: -0.11848791564504306\n",
      "          vf_explained_var: 0.6165948510169983\n",
      "          vf_loss: 0.10565063814736075\n",
      "    num_agent_steps_sampled: 436000\n",
      "    num_agent_steps_trained: 436000\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "  iterations_since_restore: 436\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.738834951456305\n",
      "    ram_util_percent: 33.105825242718446\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036672517526186964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.15469221444328\n",
      "    mean_inference_ms: 2.1126581424194453\n",
      "    mean_raw_obs_processing_ms: 16.76341691141536\n",
      "  time_since_restore: 17765.71587562561\n",
      "  time_this_iter_s: 72.37939167022705\n",
      "  time_total_s: 17765.71587562561\n",
      "  timers:\n",
      "    learn_throughput: 1323.963\n",
      "    learn_time_ms: 755.308\n",
      "    load_throughput: 42498.323\n",
      "    load_time_ms: 23.53\n",
      "    sample_throughput: 11.325\n",
      "    sample_time_ms: 88298.902\n",
      "    update_time_ms: 4.142\n",
      "  timestamp: 1635300346\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 436\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   436</td><td style=\"text-align: right;\">         17765.7</td><td style=\"text-align: right;\">436000</td><td style=\"text-align: right;\">  3.3717</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            160.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 437000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-06-54\n",
      "  done: false\n",
      "  episode_len_mean: 167.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.341200000000019\n",
      "  episode_reward_min: -10.799999999999923\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1507\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4937437629020568\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8855384051799775\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005787916713567773\n",
      "          policy_loss: 0.1015578826268514\n",
      "          total_loss: 0.21659765425655578\n",
      "          vf_explained_var: 0.36721593141555786\n",
      "          vf_loss: 0.13103740658197138\n",
      "    num_agent_steps_sampled: 437000\n",
      "    num_agent_steps_trained: 437000\n",
      "    num_steps_sampled: 437000\n",
      "    num_steps_trained: 437000\n",
      "  iterations_since_restore: 437\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.89072164948454\n",
      "    ram_util_percent: 33.053608247422694\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036672816393440937\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.153515803002144\n",
      "    mean_inference_ms: 2.1126671141875017\n",
      "    mean_raw_obs_processing_ms: 16.84629720645577\n",
      "  time_since_restore: 17833.474035978317\n",
      "  time_this_iter_s: 67.75816035270691\n",
      "  time_total_s: 17833.474035978317\n",
      "  timers:\n",
      "    learn_throughput: 1321.209\n",
      "    learn_time_ms: 756.882\n",
      "    load_throughput: 42624.564\n",
      "    load_time_ms: 23.461\n",
      "    sample_throughput: 10.764\n",
      "    sample_time_ms: 92898.903\n",
      "    update_time_ms: 4.149\n",
      "  timestamp: 1635300414\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 437000\n",
      "  training_iteration: 437\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   437</td><td style=\"text-align: right;\">         17833.5</td><td style=\"text-align: right;\">437000</td><td style=\"text-align: right;\">  3.3412</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            167.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 438000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-07-47\n",
      "  done: false\n",
      "  episode_len_mean: 175.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.11250000000002\n",
      "  episode_reward_min: -10.799999999999923\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1511\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4937437629020568\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.327386424276564\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003180077226983771\n",
      "          policy_loss: 0.07757712486717436\n",
      "          total_loss: 0.06341987550258636\n",
      "          vf_explained_var: 0.2841532528400421\n",
      "          vf_loss: 0.0075464705833130415\n",
      "    num_agent_steps_sampled: 438000\n",
      "    num_agent_steps_trained: 438000\n",
      "    num_steps_sampled: 438000\n",
      "    num_steps_trained: 438000\n",
      "  iterations_since_restore: 438\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.64933333333333\n",
      "    ram_util_percent: 33.12133333333334\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667310319884916\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.151615509685982\n",
      "    mean_inference_ms: 2.11267510709402\n",
      "    mean_raw_obs_processing_ms: 16.925265228549062\n",
      "  time_since_restore: 17886.435500621796\n",
      "  time_this_iter_s: 52.961464643478394\n",
      "  time_total_s: 17886.435500621796\n",
      "  timers:\n",
      "    learn_throughput: 1323.855\n",
      "    learn_time_ms: 755.37\n",
      "    load_throughput: 42698.07\n",
      "    load_time_ms: 23.42\n",
      "    sample_throughput: 12.593\n",
      "    sample_time_ms: 79408.796\n",
      "    update_time_ms: 4.049\n",
      "  timestamp: 1635300467\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 438000\n",
      "  training_iteration: 438\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   438</td><td style=\"text-align: right;\">         17886.4</td><td style=\"text-align: right;\">438000</td><td style=\"text-align: right;\">  3.1125</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            175.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 439000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-11-05\n",
      "  done: false\n",
      "  episode_len_mean: 172.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.1692000000000196\n",
      "  episode_reward_min: -10.799999999999923\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1521\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2468718814510284\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.242815265390608\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010134422463741742\n",
      "          policy_loss: 0.014152139094140795\n",
      "          total_loss: 0.20516733212603463\n",
      "          vf_explained_var: 0.7098106145858765\n",
      "          vf_loss: 0.20094144112533993\n",
      "    num_agent_steps_sampled: 439000\n",
      "    num_agent_steps_trained: 439000\n",
      "    num_steps_sampled: 439000\n",
      "    num_steps_trained: 439000\n",
      "  iterations_since_restore: 439\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.477031802120145\n",
      "    ram_util_percent: 33.04911660777385\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667380342518183\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.14662048474703\n",
      "    mean_inference_ms: 2.112694521680154\n",
      "    mean_raw_obs_processing_ms: 17.13110727196051\n",
      "  time_since_restore: 18084.62477207184\n",
      "  time_this_iter_s: 198.18927145004272\n",
      "  time_total_s: 18084.62477207184\n",
      "  timers:\n",
      "    learn_throughput: 1323.658\n",
      "    learn_time_ms: 755.482\n",
      "    load_throughput: 42684.904\n",
      "    load_time_ms: 23.427\n",
      "    sample_throughput: 11.932\n",
      "    sample_time_ms: 83811.251\n",
      "    update_time_ms: 3.813\n",
      "  timestamp: 1635300665\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 439000\n",
      "  training_iteration: 439\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   439</td><td style=\"text-align: right;\">         18084.6</td><td style=\"text-align: right;\">439000</td><td style=\"text-align: right;\">  3.1692</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            172.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-11-23\n",
      "  done: false\n",
      "  episode_len_mean: 182.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 2.9686000000000217\n",
      "  episode_reward_min: -10.799999999999923\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1523\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2468718814510284\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2276484065585667\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01344118073909846\n",
      "          policy_loss: -0.12649673389063942\n",
      "          total_loss: -0.13272925284173753\n",
      "          vf_explained_var: 0.21357637643814087\n",
      "          vf_loss: 0.01272571325664305\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_agent_steps_trained: 440000\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "  iterations_since_restore: 440\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.28461538461539\n",
      "    ram_util_percent: 33.111538461538466\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667393792650714\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.14539013812936\n",
      "    mean_inference_ms: 2.1126986705325863\n",
      "    mean_raw_obs_processing_ms: 17.16909764197572\n",
      "  time_since_restore: 18102.980045080185\n",
      "  time_this_iter_s: 18.355273008346558\n",
      "  time_total_s: 18102.980045080185\n",
      "  timers:\n",
      "    learn_throughput: 1324.206\n",
      "    learn_time_ms: 755.17\n",
      "    load_throughput: 42801.728\n",
      "    load_time_ms: 23.364\n",
      "    sample_throughput: 13.784\n",
      "    sample_time_ms: 72549.727\n",
      "    update_time_ms: 3.774\n",
      "  timestamp: 1635300683\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 440\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   440</td><td style=\"text-align: right;\">           18103</td><td style=\"text-align: right;\">440000</td><td style=\"text-align: right;\">  2.9686</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            182.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 441000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-12-35\n",
      "  done: false\n",
      "  episode_len_mean: 181.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.118300000000021\n",
      "  episode_reward_min: -10.799999999999923\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1528\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2468718814510284\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2461704625023735\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005195865957056937\n",
      "          policy_loss: -0.07052758791380459\n",
      "          total_loss: -0.07932798282967674\n",
      "          vf_explained_var: 0.21456006169319153\n",
      "          vf_loss: 0.012378599787431692\n",
      "    num_agent_steps_sampled: 441000\n",
      "    num_agent_steps_trained: 441000\n",
      "    num_steps_sampled: 441000\n",
      "    num_steps_trained: 441000\n",
      "  iterations_since_restore: 441\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.173786407767\n",
      "    ram_util_percent: 33.109708737864075\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667425531693449\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.141751243513113\n",
      "    mean_inference_ms: 2.1127072914459006\n",
      "    mean_raw_obs_processing_ms: 17.26879288130554\n",
      "  time_since_restore: 18174.676281929016\n",
      "  time_this_iter_s: 71.69623684883118\n",
      "  time_total_s: 18174.676281929016\n",
      "  timers:\n",
      "    learn_throughput: 1324.87\n",
      "    learn_time_ms: 754.791\n",
      "    load_throughput: 42832.937\n",
      "    load_time_ms: 23.347\n",
      "    sample_throughput: 15.511\n",
      "    sample_time_ms: 64471.503\n",
      "    update_time_ms: 3.71\n",
      "  timestamp: 1635300755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 441000\n",
      "  training_iteration: 441\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   441</td><td style=\"text-align: right;\">         18174.7</td><td style=\"text-align: right;\">441000</td><td style=\"text-align: right;\">  3.1183</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            181.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 442000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-14-47\n",
      "  done: false\n",
      "  episode_len_mean: 181.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.159200000000021\n",
      "  episode_reward_min: -10.799999999999923\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1535\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2468718814510284\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2254898481898837\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008023548428185596\n",
      "          policy_loss: 0.07178510634435548\n",
      "          total_loss: 0.05856211533149083\n",
      "          vf_explained_var: 0.13089759647846222\n",
      "          vf_loss: 0.007051118288008082\n",
      "    num_agent_steps_sampled: 442000\n",
      "    num_agent_steps_trained: 442000\n",
      "    num_steps_sampled: 442000\n",
      "    num_steps_trained: 442000\n",
      "  iterations_since_restore: 442\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.54867724867725\n",
      "    ram_util_percent: 33.095767195767195\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667462468964345\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.135880988319286\n",
      "    mean_inference_ms: 2.1127128398947295\n",
      "    mean_raw_obs_processing_ms: 17.41426611542038\n",
      "  time_since_restore: 18306.894993066788\n",
      "  time_this_iter_s: 132.2187111377716\n",
      "  time_total_s: 18306.894993066788\n",
      "  timers:\n",
      "    learn_throughput: 1324.915\n",
      "    learn_time_ms: 754.765\n",
      "    load_throughput: 42764.721\n",
      "    load_time_ms: 23.384\n",
      "    sample_throughput: 13.559\n",
      "    sample_time_ms: 73751.193\n",
      "    update_time_ms: 3.798\n",
      "  timestamp: 1635300887\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 442000\n",
      "  training_iteration: 442\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   442</td><td style=\"text-align: right;\">         18306.9</td><td style=\"text-align: right;\">442000</td><td style=\"text-align: right;\">  3.1592</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            181.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 443000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-17-20\n",
      "  done: false\n",
      "  episode_len_mean: 174.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.2714000000000207\n",
      "  episode_reward_min: -10.799999999999923\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1543\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2468718814510284\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9435804857148065\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.059976534025718446\n",
      "          policy_loss: 0.03268338640530904\n",
      "          total_loss: 0.638958180281851\n",
      "          vf_explained_var: -0.11536329239606857\n",
      "          vf_loss: 0.6109040914375025\n",
      "    num_agent_steps_sampled: 443000\n",
      "    num_agent_steps_trained: 443000\n",
      "    num_steps_sampled: 443000\n",
      "    num_steps_trained: 443000\n",
      "  iterations_since_restore: 443\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.552293577981644\n",
      "    ram_util_percent: 33.09587155963303\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036675058307013036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.12812008000576\n",
      "    mean_inference_ms: 2.112720028157643\n",
      "    mean_raw_obs_processing_ms: 17.586654123347298\n",
      "  time_since_restore: 18459.674871444702\n",
      "  time_this_iter_s: 152.77987837791443\n",
      "  time_total_s: 18459.674871444702\n",
      "  timers:\n",
      "    learn_throughput: 1324.443\n",
      "    learn_time_ms: 755.034\n",
      "    load_throughput: 43008.445\n",
      "    load_time_ms: 23.251\n",
      "    sample_throughput: 11.959\n",
      "    sample_time_ms: 83617.203\n",
      "    update_time_ms: 3.797\n",
      "  timestamp: 1635301040\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 443000\n",
      "  training_iteration: 443\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   443</td><td style=\"text-align: right;\">         18459.7</td><td style=\"text-align: right;\">443000</td><td style=\"text-align: right;\">  3.2714</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            174.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 444000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-17-56\n",
      "  done: false\n",
      "  episode_len_mean: 175.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.2949000000000206\n",
      "  episode_reward_min: -10.799999999999923\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1546\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3703078221765425\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.282734309302436\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007070866555953851\n",
      "          policy_loss: 0.026086137500695057\n",
      "          total_loss: 0.015456536619199646\n",
      "          vf_explained_var: 0.13088074326515198\n",
      "          vf_loss: 0.009579345486256191\n",
      "    num_agent_steps_sampled: 444000\n",
      "    num_agent_steps_trained: 444000\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "  iterations_since_restore: 444\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.09019607843138\n",
      "    ram_util_percent: 33.14901960784314\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036675219166294765\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.12496227581277\n",
      "    mean_inference_ms: 2.1127222114051123\n",
      "    mean_raw_obs_processing_ms: 17.651572037011466\n",
      "  time_since_restore: 18495.44858813286\n",
      "  time_this_iter_s: 35.77371668815613\n",
      "  time_total_s: 18495.44858813286\n",
      "  timers:\n",
      "    learn_throughput: 1326.86\n",
      "    learn_time_ms: 753.659\n",
      "    load_throughput: 41899.13\n",
      "    load_time_ms: 23.867\n",
      "    sample_throughput: 11.693\n",
      "    sample_time_ms: 85517.795\n",
      "    update_time_ms: 3.794\n",
      "  timestamp: 1635301076\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 444\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   444</td><td style=\"text-align: right;\">         18495.4</td><td style=\"text-align: right;\">444000</td><td style=\"text-align: right;\">  3.2949</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            175.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 445000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-19-09\n",
      "  done: false\n",
      "  episode_len_mean: 177.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.2251000000000216\n",
      "  episode_reward_min: -10.799999999999923\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1551\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3703078221765425\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.349720679389106\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007338924491471567\n",
      "          policy_loss: 0.14784325435757636\n",
      "          total_loss: 0.14298796359863547\n",
      "          vf_explained_var: -0.06202199310064316\n",
      "          vf_loss: 0.015924255046734795\n",
      "    num_agent_steps_sampled: 445000\n",
      "    num_agent_steps_trained: 445000\n",
      "    num_steps_sampled: 445000\n",
      "    num_steps_trained: 445000\n",
      "  iterations_since_restore: 445\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.28446601941748\n",
      "    ram_util_percent: 33.195145631067945\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036675444858001775\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.119162133681957\n",
      "    mean_inference_ms: 2.1127248326226042\n",
      "    mean_raw_obs_processing_ms: 17.7513086717884\n",
      "  time_since_restore: 18567.967139720917\n",
      "  time_this_iter_s: 72.51855158805847\n",
      "  time_total_s: 18567.967139720917\n",
      "  timers:\n",
      "    learn_throughput: 1328.439\n",
      "    learn_time_ms: 752.763\n",
      "    load_throughput: 41991.162\n",
      "    load_time_ms: 23.815\n",
      "    sample_throughput: 11.537\n",
      "    sample_time_ms: 86678.502\n",
      "    update_time_ms: 3.72\n",
      "  timestamp: 1635301149\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 445000\n",
      "  training_iteration: 445\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   445</td><td style=\"text-align: right;\">           18568</td><td style=\"text-align: right;\">445000</td><td style=\"text-align: right;\">  3.2251</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            177.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 446000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-20-21\n",
      "  done: false\n",
      "  episode_len_mean: 180.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.0989000000000226\n",
      "  episode_reward_min: -10.799999999999923\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1555\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3703078221765425\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1594691912333173\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007755523520375974\n",
      "          policy_loss: -0.06210206366247601\n",
      "          total_loss: -0.05511445270644294\n",
      "          vf_explained_var: 0.3849777579307556\n",
      "          vf_loss: 0.025710366958648794\n",
      "    num_agent_steps_sampled: 446000\n",
      "    num_agent_steps_trained: 446000\n",
      "    num_steps_sampled: 446000\n",
      "    num_steps_trained: 446000\n",
      "  iterations_since_restore: 446\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.47596153846154\n",
      "    ram_util_percent: 33.104807692307695\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667560875068197\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.114037562879613\n",
      "    mean_inference_ms: 2.1127261513310676\n",
      "    mean_raw_obs_processing_ms: 17.831612980540786\n",
      "  time_since_restore: 18640.917785167694\n",
      "  time_this_iter_s: 72.95064544677734\n",
      "  time_total_s: 18640.917785167694\n",
      "  timers:\n",
      "    learn_throughput: 1325.339\n",
      "    learn_time_ms: 754.524\n",
      "    load_throughput: 42437.136\n",
      "    load_time_ms: 23.564\n",
      "    sample_throughput: 11.529\n",
      "    sample_time_ms: 86734.138\n",
      "    update_time_ms: 3.712\n",
      "  timestamp: 1635301221\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 446000\n",
      "  training_iteration: 446\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   446</td><td style=\"text-align: right;\">         18640.9</td><td style=\"text-align: right;\">446000</td><td style=\"text-align: right;\">  3.0989</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            180.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 447000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-22-15\n",
      "  done: false\n",
      "  episode_len_mean: 178.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.0921000000000225\n",
      "  episode_reward_min: -10.799999999999923\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1562\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3703078221765425\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.139110839366913\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017743483729543985\n",
      "          policy_loss: 0.052757944415013\n",
      "          total_loss: 0.3728249801529778\n",
      "          vf_explained_var: 0.7596341371536255\n",
      "          vf_loss: 0.33488758967982396\n",
      "    num_agent_steps_sampled: 447000\n",
      "    num_agent_steps_trained: 447000\n",
      "    num_steps_sampled: 447000\n",
      "    num_steps_trained: 447000\n",
      "  iterations_since_restore: 447\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.590123456790124\n",
      "    ram_util_percent: 33.07283950617285\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036675885950686875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.1048006659489\n",
      "    mean_inference_ms: 2.1127284505482686\n",
      "    mean_raw_obs_processing_ms: 17.97298811301498\n",
      "  time_since_restore: 18754.391903162003\n",
      "  time_this_iter_s: 113.47411799430847\n",
      "  time_total_s: 18754.391903162003\n",
      "  timers:\n",
      "    learn_throughput: 1327.939\n",
      "    learn_time_ms: 753.047\n",
      "    load_throughput: 42337.413\n",
      "    load_time_ms: 23.62\n",
      "    sample_throughput: 10.952\n",
      "    sample_time_ms: 91307.132\n",
      "    update_time_ms: 3.709\n",
      "  timestamp: 1635301335\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 447000\n",
      "  training_iteration: 447\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   447</td><td style=\"text-align: right;\">         18754.4</td><td style=\"text-align: right;\">447000</td><td style=\"text-align: right;\">  3.0921</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            178.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 448000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-23-12\n",
      "  done: false\n",
      "  episode_len_mean: 191.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 2.622400000000024\n",
      "  episode_reward_min: -10.799999999999923\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1567\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3703078221765425\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3710160599814523\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018275695711382545\n",
      "          policy_loss: 0.05938928789562649\n",
      "          total_loss: 0.3548307473253873\n",
      "          vf_explained_var: 0.6474508047103882\n",
      "          vf_loss: 0.3123839818769031\n",
      "    num_agent_steps_sampled: 448000\n",
      "    num_agent_steps_trained: 448000\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "  iterations_since_restore: 448\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.715853658536574\n",
      "    ram_util_percent: 33.08536585365854\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667609637140584\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.098022102685878\n",
      "    mean_inference_ms: 2.1127314006248783\n",
      "    mean_raw_obs_processing_ms: 18.070178875915573\n",
      "  time_since_restore: 18811.901616096497\n",
      "  time_this_iter_s: 57.50971293449402\n",
      "  time_total_s: 18811.901616096497\n",
      "  timers:\n",
      "    learn_throughput: 1324.911\n",
      "    learn_time_ms: 754.768\n",
      "    load_throughput: 42501.424\n",
      "    load_time_ms: 23.529\n",
      "    sample_throughput: 10.898\n",
      "    sample_time_ms: 91760.297\n",
      "    update_time_ms: 3.713\n",
      "  timestamp: 1635301392\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 448\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   448</td><td style=\"text-align: right;\">         18811.9</td><td style=\"text-align: right;\">448000</td><td style=\"text-align: right;\">  2.6224</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">             191.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 449000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-24-28\n",
      "  done: false\n",
      "  episode_len_mean: 195.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 2.5526000000000253\n",
      "  episode_reward_min: -10.799999999999923\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1572\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3703078221765425\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3365169525146485\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013519628757385214\n",
      "          policy_loss: -0.13796610310673713\n",
      "          total_loss: 0.10961469275255999\n",
      "          vf_explained_var: 0.6815803050994873\n",
      "          vf_loss: 0.2659395413266288\n",
      "    num_agent_steps_sampled: 449000\n",
      "    num_agent_steps_trained: 449000\n",
      "    num_steps_sampled: 449000\n",
      "    num_steps_trained: 449000\n",
      "  iterations_since_restore: 449\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.25833333333333\n",
      "    ram_util_percent: 33.14629629629629\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667632258310707\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.091144185312025\n",
      "    mean_inference_ms: 2.112734699883543\n",
      "    mean_raw_obs_processing_ms: 18.162768107172795\n",
      "  time_since_restore: 18887.514392137527\n",
      "  time_this_iter_s: 75.61277604103088\n",
      "  time_total_s: 18887.514392137527\n",
      "  timers:\n",
      "    learn_throughput: 1324.753\n",
      "    learn_time_ms: 754.858\n",
      "    load_throughput: 42527.668\n",
      "    load_time_ms: 23.514\n",
      "    sample_throughput: 12.578\n",
      "    sample_time_ms: 79502.543\n",
      "    update_time_ms: 3.723\n",
      "  timestamp: 1635301468\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 449000\n",
      "  training_iteration: 449\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   449</td><td style=\"text-align: right;\">         18887.5</td><td style=\"text-align: right;\">449000</td><td style=\"text-align: right;\">  2.5526</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            195.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 450000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-25-06\n",
      "  done: false\n",
      "  episode_len_mean: 204.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 2.345500000000026\n",
      "  episode_reward_min: -10.799999999999923\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1575\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3703078221765425\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.051079041428036\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015122511374333828\n",
      "          policy_loss: -0.05009239522947206\n",
      "          total_loss: 0.05625306045015653\n",
      "          vf_explained_var: 0.1963443160057068\n",
      "          vf_loss: 0.12125625784715845\n",
      "    num_agent_steps_sampled: 450000\n",
      "    num_agent_steps_trained: 450000\n",
      "    num_steps_sampled: 450000\n",
      "    num_steps_trained: 450000\n",
      "  iterations_since_restore: 450\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.4037037037037\n",
      "    ram_util_percent: 33.15555555555556\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667647875529706\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.08685099697248\n",
      "    mean_inference_ms: 2.112736971539771\n",
      "    mean_raw_obs_processing_ms: 18.21691691301861\n",
      "  time_since_restore: 18924.993705034256\n",
      "  time_this_iter_s: 37.479312896728516\n",
      "  time_total_s: 18924.993705034256\n",
      "  timers:\n",
      "    learn_throughput: 1327.272\n",
      "    learn_time_ms: 753.425\n",
      "    load_throughput: 42135.092\n",
      "    load_time_ms: 23.733\n",
      "    sample_throughput: 12.283\n",
      "    sample_time_ms: 81416.086\n",
      "    update_time_ms: 3.806\n",
      "  timestamp: 1635301506\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 450000\n",
      "  training_iteration: 450\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   450</td><td style=\"text-align: right;\">           18925</td><td style=\"text-align: right;\">450000</td><td style=\"text-align: right;\">  2.3455</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            204.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 451000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-25-24\n",
      "  done: false\n",
      "  episode_len_mean: 209.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 2.1984000000000266\n",
      "  episode_reward_min: -10.799999999999923\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1577\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3703078221765425\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2214003801345825\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012219615764299648\n",
      "          policy_loss: -0.13801066502928733\n",
      "          total_loss: -0.14117671350638072\n",
      "          vf_explained_var: 0.7978315353393555\n",
      "          vf_loss: 0.01452293750933475\n",
      "    num_agent_steps_sampled: 451000\n",
      "    num_agent_steps_trained: 451000\n",
      "    num_steps_sampled: 451000\n",
      "    num_steps_trained: 451000\n",
      "  iterations_since_restore: 451\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.18461538461538\n",
      "    ram_util_percent: 33.20384615384616\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667658682710426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.0837879746623\n",
      "    mean_inference_ms: 2.112738587216987\n",
      "    mean_raw_obs_processing_ms: 18.24956905986836\n",
      "  time_since_restore: 18943.42194366455\n",
      "  time_this_iter_s: 18.4282386302948\n",
      "  time_total_s: 18943.42194366455\n",
      "  timers:\n",
      "    learn_throughput: 1328.241\n",
      "    learn_time_ms: 752.875\n",
      "    load_throughput: 41288.209\n",
      "    load_time_ms: 24.22\n",
      "    sample_throughput: 13.142\n",
      "    sample_time_ms: 76089.389\n",
      "    update_time_ms: 3.816\n",
      "  timestamp: 1635301524\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 451000\n",
      "  training_iteration: 451\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   451</td><td style=\"text-align: right;\">         18943.4</td><td style=\"text-align: right;\">451000</td><td style=\"text-align: right;\">  2.1984</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            209.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 452000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-28-03\n",
      "  done: false\n",
      "  episode_len_mean: 205.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 2.5111000000000256\n",
      "  episode_reward_min: -8.88999999999992\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1584\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3703078221765425\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.04681822028425\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.04796555637610361\n",
      "          policy_loss: -0.06351417342407836\n",
      "          total_loss: 1.6599911873125368\n",
      "          vf_explained_var: 0.8128210306167603\n",
      "          vf_loss: 1.7162115005569325\n",
      "    num_agent_steps_sampled: 452000\n",
      "    num_agent_steps_trained: 452000\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "  iterations_since_restore: 452\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.49513274336283\n",
      "    ram_util_percent: 33.20309734513274\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667697968069595\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.074221734713184\n",
      "    mean_inference_ms: 2.112745302598852\n",
      "    mean_raw_obs_processing_ms: 18.37165465537354\n",
      "  time_since_restore: 19102.024402856827\n",
      "  time_this_iter_s: 158.602459192276\n",
      "  time_total_s: 19102.024402856827\n",
      "  timers:\n",
      "    learn_throughput: 1331.861\n",
      "    learn_time_ms: 750.829\n",
      "    load_throughput: 41189.074\n",
      "    load_time_ms: 24.278\n",
      "    sample_throughput: 12.702\n",
      "    sample_time_ms: 78729.839\n",
      "    update_time_ms: 3.734\n",
      "  timestamp: 1635301683\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 452\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   452</td><td style=\"text-align: right;\">           19102</td><td style=\"text-align: right;\">452000</td><td style=\"text-align: right;\">  2.5111</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -8.89</td><td style=\"text-align: right;\">            205.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 453000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-31-33\n",
      "  done: false\n",
      "  episode_len_mean: 185.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 3.083900000000022\n",
      "  episode_reward_min: -8.88999999999992\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1596\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5554617332648137\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.812482804722256\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0059030257116620055\n",
      "          policy_loss: 0.06984921395778657\n",
      "          total_loss: 0.36924681067466736\n",
      "          vf_explained_var: 0.39440277218818665\n",
      "          vf_loss: 0.31424352372220404\n",
      "    num_agent_steps_sampled: 453000\n",
      "    num_agent_steps_trained: 453000\n",
      "    num_steps_sampled: 453000\n",
      "    num_steps_trained: 453000\n",
      "  iterations_since_restore: 453\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.11162790697674\n",
      "    ram_util_percent: 33.125913621262455\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667753315932551\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.059927286839187\n",
      "    mean_inference_ms: 2.1127577406326528\n",
      "    mean_raw_obs_processing_ms: 18.622569385916073\n",
      "  time_since_restore: 19312.78787612915\n",
      "  time_this_iter_s: 210.7634732723236\n",
      "  time_total_s: 19312.78787612915\n",
      "  timers:\n",
      "    learn_throughput: 1332.874\n",
      "    learn_time_ms: 750.258\n",
      "    load_throughput: 40998.997\n",
      "    load_time_ms: 24.391\n",
      "    sample_throughput: 11.83\n",
      "    sample_time_ms: 84528.654\n",
      "    update_time_ms: 3.74\n",
      "  timestamp: 1635301893\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 453000\n",
      "  training_iteration: 453\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   453</td><td style=\"text-align: right;\">         19312.8</td><td style=\"text-align: right;\">453000</td><td style=\"text-align: right;\">  3.0839</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -8.89</td><td style=\"text-align: right;\">            185.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 454000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-32-49\n",
      "  done: false\n",
      "  episode_len_mean: 183.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 3.0273000000000225\n",
      "  episode_reward_min: -8.88999999999992\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1601\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5554617332648137\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.079849991533491\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00959423360561718\n",
      "          policy_loss: -0.02292454954650667\n",
      "          total_loss: 0.10108068734407424\n",
      "          vf_explained_var: 0.7953027486801147\n",
      "          vf_loss: 0.13947451198990973\n",
      "    num_agent_steps_sampled: 454000\n",
      "    num_agent_steps_trained: 454000\n",
      "    num_steps_sampled: 454000\n",
      "    num_steps_trained: 454000\n",
      "  iterations_since_restore: 454\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.45321100917431\n",
      "    ram_util_percent: 33.16605504587155\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667771474288854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.05496371528998\n",
      "    mean_inference_ms: 2.112762727240273\n",
      "    mean_raw_obs_processing_ms: 18.726989201945898\n",
      "  time_since_restore: 19388.68655514717\n",
      "  time_this_iter_s: 75.89867901802063\n",
      "  time_total_s: 19388.68655514717\n",
      "  timers:\n",
      "    learn_throughput: 1330.222\n",
      "    learn_time_ms: 751.754\n",
      "    load_throughput: 40748.617\n",
      "    load_time_ms: 24.541\n",
      "    sample_throughput: 11.294\n",
      "    sample_time_ms: 88539.506\n",
      "    update_time_ms: 3.737\n",
      "  timestamp: 1635301969\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 454000\n",
      "  training_iteration: 454\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   454</td><td style=\"text-align: right;\">         19388.7</td><td style=\"text-align: right;\">454000</td><td style=\"text-align: right;\">  3.0273</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -8.89</td><td style=\"text-align: right;\">            183.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 455000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-33-49\n",
      "  done: false\n",
      "  episode_len_mean: 181.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.8850000000000215\n",
      "  episode_reward_min: -8.88999999999992\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1605\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5554617332648137\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1558874050776162\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005763329842189262\n",
      "          policy_loss: 0.008811322185728285\n",
      "          total_loss: 0.17966914230750666\n",
      "          vf_explained_var: 0.657089352607727\n",
      "          vf_loss: 0.1892153852722711\n",
      "    num_agent_steps_sampled: 455000\n",
      "    num_agent_steps_trained: 455000\n",
      "    num_steps_sampled: 455000\n",
      "    num_steps_trained: 455000\n",
      "  iterations_since_restore: 455\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.205882352941174\n",
      "    ram_util_percent: 33.2\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667788221637799\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.051446156975384\n",
      "    mean_inference_ms: 2.1127672044208223\n",
      "    mean_raw_obs_processing_ms: 18.809173244125592\n",
      "  time_since_restore: 19448.47742652893\n",
      "  time_this_iter_s: 59.790871381759644\n",
      "  time_total_s: 19448.47742652893\n",
      "  timers:\n",
      "    learn_throughput: 1328.84\n",
      "    learn_time_ms: 752.536\n",
      "    load_throughput: 40660.999\n",
      "    load_time_ms: 24.594\n",
      "    sample_throughput: 11.459\n",
      "    sample_time_ms: 87265.992\n",
      "    update_time_ms: 3.655\n",
      "  timestamp: 1635302029\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 455000\n",
      "  training_iteration: 455\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   455</td><td style=\"text-align: right;\">         19448.5</td><td style=\"text-align: right;\">455000</td><td style=\"text-align: right;\">   2.885</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -8.89</td><td style=\"text-align: right;\">            181.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 456000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-35-08\n",
      "  done: false\n",
      "  episode_len_mean: 180.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.9131000000000222\n",
      "  episode_reward_min: -8.88999999999992\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1610\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5554617332648137\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2372551414701674\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00728877395847718\n",
      "          policy_loss: 0.11954323757025931\n",
      "          total_loss: 0.19985521452294455\n",
      "          vf_explained_var: 0.13448046147823334\n",
      "          vf_loss: 0.09863589155591196\n",
      "    num_agent_steps_sampled: 456000\n",
      "    num_agent_steps_trained: 456000\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "  iterations_since_restore: 456\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.68839285714286\n",
      "    ram_util_percent: 33.261607142857144\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667804160752201\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.047520975785304\n",
      "    mean_inference_ms: 2.1127722510557083\n",
      "    mean_raw_obs_processing_ms: 18.913009279688122\n",
      "  time_since_restore: 19527.28688120842\n",
      "  time_this_iter_s: 78.80945467948914\n",
      "  time_total_s: 19527.28688120842\n",
      "  timers:\n",
      "    learn_throughput: 1330.123\n",
      "    learn_time_ms: 751.81\n",
      "    load_throughput: 40573.011\n",
      "    load_time_ms: 24.647\n",
      "    sample_throughput: 11.383\n",
      "    sample_time_ms: 87852.476\n",
      "    update_time_ms: 3.735\n",
      "  timestamp: 1635302108\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 456\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   456</td><td style=\"text-align: right;\">         19527.3</td><td style=\"text-align: right;\">456000</td><td style=\"text-align: right;\">  2.9131</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -8.89</td><td style=\"text-align: right;\">            180.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 457000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-35-44\n",
      "  done: false\n",
      "  episode_len_mean: 185.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.6989000000000214\n",
      "  episode_reward_min: -8.88999999999992\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1613\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5554617332648137\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2952095058229234\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01027260902640506\n",
      "          policy_loss: 0.12783500519063737\n",
      "          total_loss: 0.12236703948842155\n",
      "          vf_explained_var: 0.3749372661113739\n",
      "          vf_loss: 0.01177809130296939\n",
      "    num_agent_steps_sampled: 457000\n",
      "    num_agent_steps_trained: 457000\n",
      "    num_steps_sampled: 457000\n",
      "    num_steps_trained: 457000\n",
      "  iterations_since_restore: 457\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.82884615384616\n",
      "    ram_util_percent: 33.24038461538461\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0366781507871176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.045040894817806\n",
      "    mean_inference_ms: 2.1127758094683635\n",
      "    mean_raw_obs_processing_ms: 18.96747060495643\n",
      "  time_since_restore: 19563.06906414032\n",
      "  time_this_iter_s: 35.782182931900024\n",
      "  time_total_s: 19563.06906414032\n",
      "  timers:\n",
      "    learn_throughput: 1328.345\n",
      "    learn_time_ms: 752.817\n",
      "    load_throughput: 40641.339\n",
      "    load_time_ms: 24.605\n",
      "    sample_throughput: 12.487\n",
      "    sample_time_ms: 80082.35\n",
      "    update_time_ms: 3.733\n",
      "  timestamp: 1635302144\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 457000\n",
      "  training_iteration: 457\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   457</td><td style=\"text-align: right;\">         19563.1</td><td style=\"text-align: right;\">457000</td><td style=\"text-align: right;\">  2.6989</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -8.89</td><td style=\"text-align: right;\">            185.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 458000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-37-39\n",
      "  done: false\n",
      "  episode_len_mean: 190.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.7319000000000226\n",
      "  episode_reward_min: -8.88999999999992\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1619\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5554617332648137\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.021596191989051\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007504553333608064\n",
      "          policy_loss: -0.09979653126663632\n",
      "          total_loss: 0.029459385325511296\n",
      "          vf_explained_var: 0.7495729923248291\n",
      "          vf_loss: 0.14530338716641483\n",
      "    num_agent_steps_sampled: 458000\n",
      "    num_agent_steps_trained: 458000\n",
      "    num_steps_sampled: 458000\n",
      "    num_steps_trained: 458000\n",
      "  iterations_since_restore: 458\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.37134146341462\n",
      "    ram_util_percent: 33.203658536585365\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667839495408643\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.03998730065081\n",
      "    mean_inference_ms: 2.1127826058979333\n",
      "    mean_raw_obs_processing_ms: 19.078774712323707\n",
      "  time_since_restore: 19678.050002336502\n",
      "  time_this_iter_s: 114.98093819618225\n",
      "  time_total_s: 19678.050002336502\n",
      "  timers:\n",
      "    learn_throughput: 1330.416\n",
      "    learn_time_ms: 751.645\n",
      "    load_throughput: 40630.945\n",
      "    load_time_ms: 24.612\n",
      "    sample_throughput: 11.651\n",
      "    sample_time_ms: 85830.641\n",
      "    update_time_ms: 3.729\n",
      "  timestamp: 1635302259\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 458000\n",
      "  training_iteration: 458\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   458</td><td style=\"text-align: right;\">         19678.1</td><td style=\"text-align: right;\">458000</td><td style=\"text-align: right;\">  2.7319</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -8.89</td><td style=\"text-align: right;\">            190.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 459000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-39-50\n",
      "  done: false\n",
      "  episode_len_mean: 180.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.9819000000000218\n",
      "  episode_reward_min: -8.88999999999992\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1627\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5554617332648137\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9774468885527716\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0031757795538509105\n",
      "          policy_loss: -0.07900830027129915\n",
      "          total_loss: -0.039632601849734786\n",
      "          vf_explained_var: 0.3263607323169708\n",
      "          vf_loss: 0.05738614196371701\n",
      "    num_agent_steps_sampled: 459000\n",
      "    num_agent_steps_trained: 459000\n",
      "    num_steps_sampled: 459000\n",
      "    num_steps_trained: 459000\n",
      "  iterations_since_restore: 459\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.544919786096266\n",
      "    ram_util_percent: 33.2187165775401\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667879887743632\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.03402447949471\n",
      "    mean_inference_ms: 2.1127923855172828\n",
      "    mean_raw_obs_processing_ms: 19.242189206406863\n",
      "  time_since_restore: 19809.62315416336\n",
      "  time_this_iter_s: 131.57315182685852\n",
      "  time_total_s: 19809.62315416336\n",
      "  timers:\n",
      "    learn_throughput: 1329.753\n",
      "    learn_time_ms: 752.02\n",
      "    load_throughput: 40527.182\n",
      "    load_time_ms: 24.675\n",
      "    sample_throughput: 10.938\n",
      "    sample_time_ms: 91426.258\n",
      "    update_time_ms: 3.73\n",
      "  timestamp: 1635302390\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 459000\n",
      "  training_iteration: 459\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   459</td><td style=\"text-align: right;\">         19809.6</td><td style=\"text-align: right;\">459000</td><td style=\"text-align: right;\">  2.9819</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -8.89</td><td style=\"text-align: right;\">            180.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 460000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-41-26\n",
      "  done: false\n",
      "  episode_len_mean: 179.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.8309000000000215\n",
      "  episode_reward_min: -8.88999999999992\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1633\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.27773086663240687\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1610175172487893\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008647557008657753\n",
      "          policy_loss: -0.05241694665617413\n",
      "          total_loss: -0.008912036278181606\n",
      "          vf_explained_var: 0.47685641050338745\n",
      "          vf_loss: 0.0627133920426584\n",
      "    num_agent_steps_sampled: 460000\n",
      "    num_agent_steps_trained: 460000\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "  iterations_since_restore: 460\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.0985294117647\n",
      "    ram_util_percent: 33.239705882352936\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667914349694921\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.030162204668837\n",
      "    mean_inference_ms: 2.1128014875868946\n",
      "    mean_raw_obs_processing_ms: 19.359320603588813\n",
      "  time_since_restore: 19904.739993333817\n",
      "  time_this_iter_s: 95.11683917045593\n",
      "  time_total_s: 19904.739993333817\n",
      "  timers:\n",
      "    learn_throughput: 1328.027\n",
      "    learn_time_ms: 752.997\n",
      "    load_throughput: 40779.638\n",
      "    load_time_ms: 24.522\n",
      "    sample_throughput: 10.289\n",
      "    sample_time_ms: 97189.254\n",
      "    update_time_ms: 3.659\n",
      "  timestamp: 1635302486\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 460\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   460</td><td style=\"text-align: right;\">         19904.7</td><td style=\"text-align: right;\">460000</td><td style=\"text-align: right;\">  2.8309</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -8.89</td><td style=\"text-align: right;\">            179.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 461000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-43-04\n",
      "  done: false\n",
      "  episode_len_mean: 182.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.864300000000022\n",
      "  episode_reward_min: -8.88999999999992\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1639\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.27773086663240687\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8756831407546997\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.025933199543494368\n",
      "          policy_loss: 0.09120459796653854\n",
      "          total_loss: 0.5701335498442253\n",
      "          vf_explained_var: 0.5406161546707153\n",
      "          vf_loss: 0.49048332849310505\n",
      "    num_agent_steps_sampled: 461000\n",
      "    num_agent_steps_trained: 461000\n",
      "    num_steps_sampled: 461000\n",
      "    num_steps_trained: 461000\n",
      "  iterations_since_restore: 461\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.690780141843966\n",
      "    ram_util_percent: 33.244680851063826\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0366795287410427\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.026627921224428\n",
      "    mean_inference_ms: 2.112812891032464\n",
      "    mean_raw_obs_processing_ms: 19.47121671179458\n",
      "  time_since_restore: 20003.40599179268\n",
      "  time_this_iter_s: 98.6659984588623\n",
      "  time_total_s: 20003.40599179268\n",
      "  timers:\n",
      "    learn_throughput: 1324.815\n",
      "    learn_time_ms: 754.822\n",
      "    load_throughput: 41664.637\n",
      "    load_time_ms: 24.001\n",
      "    sample_throughput: 9.505\n",
      "    sample_time_ms: 105211.726\n",
      "    update_time_ms: 3.649\n",
      "  timestamp: 1635302584\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 461000\n",
      "  training_iteration: 461\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   461</td><td style=\"text-align: right;\">         20003.4</td><td style=\"text-align: right;\">461000</td><td style=\"text-align: right;\">  2.8643</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -8.89</td><td style=\"text-align: right;\">            182.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 462000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-43-40\n",
      "  done: false\n",
      "  episode_len_mean: 191.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.6881000000000226\n",
      "  episode_reward_min: -8.88999999999992\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1642\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4165962999486102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2738029268052844\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007444177087427365\n",
      "          policy_loss: -0.006080290757947498\n",
      "          total_loss: 0.019508670642971994\n",
      "          vf_explained_var: 0.17048342525959015\n",
      "          vf_loss: 0.04522577248668919\n",
      "    num_agent_steps_sampled: 462000\n",
      "    num_agent_steps_trained: 462000\n",
      "    num_steps_sampled: 462000\n",
      "    num_steps_trained: 462000\n",
      "  iterations_since_restore: 462\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.73333333333333\n",
      "    ram_util_percent: 33.29803921568627\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667974620048861\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.024614043408025\n",
      "    mean_inference_ms: 2.1128190896802233\n",
      "    mean_raw_obs_processing_ms: 19.524380581619752\n",
      "  time_since_restore: 20038.839684009552\n",
      "  time_this_iter_s: 35.43369221687317\n",
      "  time_total_s: 20038.839684009552\n",
      "  timers:\n",
      "    learn_throughput: 1320.767\n",
      "    learn_time_ms: 757.136\n",
      "    load_throughput: 41658.843\n",
      "    load_time_ms: 24.005\n",
      "    sample_throughput: 10.765\n",
      "    sample_time_ms: 92892.466\n",
      "    update_time_ms: 3.732\n",
      "  timestamp: 1635302620\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 462000\n",
      "  training_iteration: 462\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   462</td><td style=\"text-align: right;\">         20038.8</td><td style=\"text-align: right;\">462000</td><td style=\"text-align: right;\">  2.6881</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -8.89</td><td style=\"text-align: right;\">            191.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 463000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-44-14\n",
      "  done: false\n",
      "  episode_len_mean: 191.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.6895000000000233\n",
      "  episode_reward_min: -8.88999999999992\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1645\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4165962999486102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.299839798609416\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006546454785647171\n",
      "          policy_loss: -0.022868880753715834\n",
      "          total_loss: -0.02813036007185777\n",
      "          vf_explained_var: 0.16051267087459564\n",
      "          vf_loss: 0.015009692105619858\n",
      "    num_agent_steps_sampled: 463000\n",
      "    num_agent_steps_trained: 463000\n",
      "    num_steps_sampled: 463000\n",
      "    num_steps_trained: 463000\n",
      "  iterations_since_restore: 463\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.930612244897965\n",
      "    ram_util_percent: 33.265306122448976\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667998643605812\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.022403342756615\n",
      "    mean_inference_ms: 2.112826427401792\n",
      "    mean_raw_obs_processing_ms: 19.577420723999072\n",
      "  time_since_restore: 20073.186708927155\n",
      "  time_this_iter_s: 34.34702491760254\n",
      "  time_total_s: 20073.186708927155\n",
      "  timers:\n",
      "    learn_throughput: 1322.13\n",
      "    learn_time_ms: 756.355\n",
      "    load_throughput: 41632.379\n",
      "    load_time_ms: 24.02\n",
      "    sample_throughput: 13.289\n",
      "    sample_time_ms: 75251.354\n",
      "    update_time_ms: 3.968\n",
      "  timestamp: 1635302654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 463000\n",
      "  training_iteration: 463\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   463</td><td style=\"text-align: right;\">         20073.2</td><td style=\"text-align: right;\">463000</td><td style=\"text-align: right;\">  2.6895</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -8.89</td><td style=\"text-align: right;\">            191.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 464000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-45-48\n",
      "  done: false\n",
      "  episode_len_mean: 186.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.782600000000022\n",
      "  episode_reward_min: -8.88999999999992\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1650\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4165962999486102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2459756043222217\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009231999456086982\n",
      "          policy_loss: -0.03962440289970901\n",
      "          total_loss: 0.011877963774734074\n",
      "          vf_explained_var: 0.5591550469398499\n",
      "          vf_loss: 0.07011610775565108\n",
      "    num_agent_steps_sampled: 464000\n",
      "    num_agent_steps_trained: 464000\n",
      "    num_steps_sampled: 464000\n",
      "    num_steps_trained: 464000\n",
      "  iterations_since_restore: 464\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.70000000000001\n",
      "    ram_util_percent: 33.27164179104478\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668042212598295\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.019237971743884\n",
      "    mean_inference_ms: 2.1128403554036304\n",
      "    mean_raw_obs_processing_ms: 19.66819752690725\n",
      "  time_since_restore: 20167.12404704094\n",
      "  time_this_iter_s: 93.93733811378479\n",
      "  time_total_s: 20167.12404704094\n",
      "  timers:\n",
      "    learn_throughput: 1324.182\n",
      "    learn_time_ms: 755.183\n",
      "    load_throughput: 41426.329\n",
      "    load_time_ms: 24.139\n",
      "    sample_throughput: 12.978\n",
      "    sample_time_ms: 77056.126\n",
      "    update_time_ms: 4.109\n",
      "  timestamp: 1635302748\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 464000\n",
      "  training_iteration: 464\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   464</td><td style=\"text-align: right;\">         20167.1</td><td style=\"text-align: right;\">464000</td><td style=\"text-align: right;\">  2.7826</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -8.89</td><td style=\"text-align: right;\">            186.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 465000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-50-33\n",
      "  done: false\n",
      "  episode_len_mean: 169.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 3.337600000000019\n",
      "  episode_reward_min: -7.48999999999992\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 1666\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4165962999486102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6922426210509407\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011908337727185492\n",
      "          policy_loss: -0.0259988395911124\n",
      "          total_loss: 0.11637390334573057\n",
      "          vf_explained_var: 0.5792280435562134\n",
      "          vf_loss: 0.15433420462326872\n",
      "    num_agent_steps_sampled: 465000\n",
      "    num_agent_steps_trained: 465000\n",
      "    num_steps_sampled: 465000\n",
      "    num_steps_trained: 465000\n",
      "  iterations_since_restore: 465\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.2384236453202\n",
      "    ram_util_percent: 33.235960591133\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668191270093701\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.01158436181324\n",
      "    mean_inference_ms: 2.112888632927488\n",
      "    mean_raw_obs_processing_ms: 20.00736397623919\n",
      "  time_since_restore: 20451.567621946335\n",
      "  time_this_iter_s: 284.4435749053955\n",
      "  time_total_s: 20451.567621946335\n",
      "  timers:\n",
      "    learn_throughput: 1323.977\n",
      "    learn_time_ms: 755.3\n",
      "    load_throughput: 41444.75\n",
      "    load_time_ms: 24.129\n",
      "    sample_throughput: 10.048\n",
      "    sample_time_ms: 99521.218\n",
      "    update_time_ms: 4.108\n",
      "  timestamp: 1635303033\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 465000\n",
      "  training_iteration: 465\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   465</td><td style=\"text-align: right;\">         20451.6</td><td style=\"text-align: right;\">465000</td><td style=\"text-align: right;\">  3.3376</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -7.49</td><td style=\"text-align: right;\">            169.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 466000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-50-48\n",
      "  done: false\n",
      "  episode_len_mean: 175.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 3.1781000000000206\n",
      "  episode_reward_min: -7.48999999999992\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1667\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4165962999486102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1460189210044014\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006204514965048618\n",
      "          policy_loss: -0.1943187742597527\n",
      "          total_loss: -0.20066946008139186\n",
      "          vf_explained_var: 0.3528487980365753\n",
      "          vf_loss: 0.01252472306498223\n",
      "    num_agent_steps_sampled: 466000\n",
      "    num_agent_steps_trained: 466000\n",
      "    num_steps_sampled: 466000\n",
      "    num_steps_trained: 466000\n",
      "  iterations_since_restore: 466\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.43636363636364\n",
      "    ram_util_percent: 33.40454545454545\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668200749869412\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.01097828677162\n",
      "    mean_inference_ms: 2.1128914117117663\n",
      "    mean_raw_obs_processing_ms: 20.027281665468998\n",
      "  time_since_restore: 20467.429597854614\n",
      "  time_this_iter_s: 15.861975908279419\n",
      "  time_total_s: 20467.429597854614\n",
      "  timers:\n",
      "    learn_throughput: 1323.917\n",
      "    learn_time_ms: 755.334\n",
      "    load_throughput: 44032.931\n",
      "    load_time_ms: 22.71\n",
      "    sample_throughput: 10.726\n",
      "    sample_time_ms: 93227.908\n",
      "    update_time_ms: 4.025\n",
      "  timestamp: 1635303048\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 466000\n",
      "  training_iteration: 466\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   466</td><td style=\"text-align: right;\">         20467.4</td><td style=\"text-align: right;\">466000</td><td style=\"text-align: right;\">  3.1781</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -7.49</td><td style=\"text-align: right;\">            175.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 467000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-52-21\n",
      "  done: false\n",
      "  episode_len_mean: 174.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.920000000000002\n",
      "  episode_reward_mean: 3.3419000000000203\n",
      "  episode_reward_min: -7.019999999999933\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1673\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4165962999486102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9908324016465082\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009245529378814411\n",
      "          policy_loss: 0.0495823949161503\n",
      "          total_loss: 0.19075216568178602\n",
      "          vf_explained_var: 0.35711538791656494\n",
      "          vf_loss: 0.15722644040361047\n",
      "    num_agent_steps_sampled: 467000\n",
      "    num_agent_steps_trained: 467000\n",
      "    num_steps_sampled: 467000\n",
      "    num_steps_trained: 467000\n",
      "  iterations_since_restore: 467\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.73106060606061\n",
      "    ram_util_percent: 33.32121212121213\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036682587125104395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.00704975481917\n",
      "    mean_inference_ms: 2.1129078340252407\n",
      "    mean_raw_obs_processing_ms: 20.148873143218967\n",
      "  time_since_restore: 20559.497691869736\n",
      "  time_this_iter_s: 92.06809401512146\n",
      "  time_total_s: 20559.497691869736\n",
      "  timers:\n",
      "    learn_throughput: 1324.144\n",
      "    learn_time_ms: 755.205\n",
      "    load_throughput: 44080.272\n",
      "    load_time_ms: 22.686\n",
      "    sample_throughput: 10.116\n",
      "    sample_time_ms: 98856.647\n",
      "    update_time_ms: 4.02\n",
      "  timestamp: 1635303141\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 467000\n",
      "  training_iteration: 467\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   467</td><td style=\"text-align: right;\">         20559.5</td><td style=\"text-align: right;\">467000</td><td style=\"text-align: right;\">  3.3419</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -7.02</td><td style=\"text-align: right;\">             174.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 468000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-53-32\n",
      "  done: false\n",
      "  episode_len_mean: 167.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.920000000000002\n",
      "  episode_reward_mean: 3.4210000000000194\n",
      "  episode_reward_min: -7.289999999999898\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1678\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4165962999486102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.04823929866155\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007890739229262264\n",
      "          policy_loss: 0.021451398564709557\n",
      "          total_loss: 0.08082829399241341\n",
      "          vf_explained_var: 0.2040158212184906\n",
      "          vf_loss: 0.07657203628008978\n",
      "    num_agent_steps_sampled: 468000\n",
      "    num_agent_steps_trained: 468000\n",
      "    num_steps_sampled: 468000\n",
      "    num_steps_trained: 468000\n",
      "  iterations_since_restore: 468\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.68155339805825\n",
      "    ram_util_percent: 33.3631067961165\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668305340064481\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.0036390778312\n",
      "    mean_inference_ms: 2.112921464608742\n",
      "    mean_raw_obs_processing_ms: 20.252735445071316\n",
      "  time_since_restore: 20631.464122772217\n",
      "  time_this_iter_s: 71.96643090248108\n",
      "  time_total_s: 20631.464122772217\n",
      "  timers:\n",
      "    learn_throughput: 1324.4\n",
      "    learn_time_ms: 755.059\n",
      "    load_throughput: 44101.778\n",
      "    load_time_ms: 22.675\n",
      "    sample_throughput: 10.576\n",
      "    sample_time_ms: 94555.284\n",
      "    update_time_ms: 4.096\n",
      "  timestamp: 1635303212\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 468000\n",
      "  training_iteration: 468\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   468</td><td style=\"text-align: right;\">         20631.5</td><td style=\"text-align: right;\">468000</td><td style=\"text-align: right;\">   3.421</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -7.29</td><td style=\"text-align: right;\">            167.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 469000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-55-22\n",
      "  done: false\n",
      "  episode_len_mean: 169.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.29850000000002\n",
      "  episode_reward_min: -7.289999999999898\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1685\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4165962999486102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0066826343536377\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006673063047949116\n",
      "          policy_loss: 0.026495965984132556\n",
      "          total_loss: 0.04836033375726806\n",
      "          vf_explained_var: 0.5622775554656982\n",
      "          vf_loss: 0.03915121938205428\n",
      "    num_agent_steps_sampled: 469000\n",
      "    num_agent_steps_trained: 469000\n",
      "    num_steps_sampled: 469000\n",
      "    num_steps_trained: 469000\n",
      "  iterations_since_restore: 469\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.7775641025641\n",
      "    ram_util_percent: 33.40769230769231\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668372566640775\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.99761506504704\n",
      "    mean_inference_ms: 2.1129400284929307\n",
      "    mean_raw_obs_processing_ms: 20.392503218515586\n",
      "  time_since_restore: 20741.302590847015\n",
      "  time_this_iter_s: 109.83846807479858\n",
      "  time_total_s: 20741.302590847015\n",
      "  timers:\n",
      "    learn_throughput: 1322.943\n",
      "    learn_time_ms: 755.89\n",
      "    load_throughput: 44160.795\n",
      "    load_time_ms: 22.645\n",
      "    sample_throughput: 10.825\n",
      "    sample_time_ms: 92380.996\n",
      "    update_time_ms: 4.095\n",
      "  timestamp: 1635303322\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 469000\n",
      "  training_iteration: 469\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   469</td><td style=\"text-align: right;\">         20741.3</td><td style=\"text-align: right;\">469000</td><td style=\"text-align: right;\">  3.2985</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -7.29</td><td style=\"text-align: right;\">            169.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 470000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-58-12\n",
      "  done: false\n",
      "  episode_len_mean: 169.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.1834000000000207\n",
      "  episode_reward_min: -7.289999999999898\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1695\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4165962999486102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7218967888090346\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009183054663386928\n",
      "          policy_loss: -0.12459669725762473\n",
      "          total_loss: 0.07772890105843544\n",
      "          vf_explained_var: 0.7313281297683716\n",
      "          vf_loss: 0.2157189415146907\n",
      "    num_agent_steps_sampled: 470000\n",
      "    num_agent_steps_trained: 470000\n",
      "    num_steps_sampled: 470000\n",
      "    num_steps_trained: 470000\n",
      "  iterations_since_restore: 470\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.42551440329219\n",
      "    ram_util_percent: 33.37037037037036\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668471206341009\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.98888672153374\n",
      "    mean_inference_ms: 2.1129646130051127\n",
      "    mean_raw_obs_processing_ms: 20.58713552778884\n",
      "  time_since_restore: 20911.131284952164\n",
      "  time_this_iter_s: 169.82869410514832\n",
      "  time_total_s: 20911.131284952164\n",
      "  timers:\n",
      "    learn_throughput: 1320.486\n",
      "    learn_time_ms: 757.297\n",
      "    load_throughput: 44196.719\n",
      "    load_time_ms: 22.626\n",
      "    sample_throughput: 10.015\n",
      "    sample_time_ms: 99850.812\n",
      "    update_time_ms: 4.078\n",
      "  timestamp: 1635303492\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 470000\n",
      "  training_iteration: 470\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   470</td><td style=\"text-align: right;\">         20911.1</td><td style=\"text-align: right;\">470000</td><td style=\"text-align: right;\">  3.1834</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -7.29</td><td style=\"text-align: right;\">            169.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 471000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_02-58-29\n",
      "  done: false\n",
      "  episode_len_mean: 176.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.019200000000021\n",
      "  episode_reward_min: -7.289999999999898\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1696\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4165962999486102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.963492246468862\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008147081996546662\n",
      "          policy_loss: -0.09482406835175222\n",
      "          total_loss: -0.09779865226397912\n",
      "          vf_explained_var: 0.45030370354652405\n",
      "          vf_loss: 0.013266292977560726\n",
      "    num_agent_steps_sampled: 471000\n",
      "    num_agent_steps_trained: 471000\n",
      "    num_steps_sampled: 471000\n",
      "    num_steps_trained: 471000\n",
      "  iterations_since_restore: 471\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.18333333333334\n",
      "    ram_util_percent: 33.44583333333333\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668481934380139\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.987879051865647\n",
      "    mean_inference_ms: 2.1129670819389275\n",
      "    mean_raw_obs_processing_ms: 20.60614457208676\n",
      "  time_since_restore: 20927.84631061554\n",
      "  time_this_iter_s: 16.715025663375854\n",
      "  time_total_s: 20927.84631061554\n",
      "  timers:\n",
      "    learn_throughput: 1323.673\n",
      "    learn_time_ms: 755.474\n",
      "    load_throughput: 46097.078\n",
      "    load_time_ms: 21.693\n",
      "    sample_throughput: 10.91\n",
      "    sample_time_ms: 91658.481\n",
      "    update_time_ms: 4.077\n",
      "  timestamp: 1635303509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 471000\n",
      "  training_iteration: 471\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   471</td><td style=\"text-align: right;\">         20927.8</td><td style=\"text-align: right;\">471000</td><td style=\"text-align: right;\">  3.0192</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -7.29</td><td style=\"text-align: right;\">            176.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 472000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-00-58\n",
      "  done: false\n",
      "  episode_len_mean: 168.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.344400000000021\n",
      "  episode_reward_min: -7.289999999999898\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1705\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4165962999486102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8843279745843675\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006786061030916388\n",
      "          policy_loss: -0.01962637934419844\n",
      "          total_loss: -0.015058150225215488\n",
      "          vf_explained_var: 0.2776910066604614\n",
      "          vf_loss: 0.020584461038621764\n",
      "    num_agent_steps_sampled: 472000\n",
      "    num_agent_steps_trained: 472000\n",
      "    num_steps_sampled: 472000\n",
      "    num_steps_trained: 472000\n",
      "  iterations_since_restore: 472\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.13584905660377\n",
      "    ram_util_percent: 33.46037735849056\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668576896588745\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.977856784356828\n",
      "    mean_inference_ms: 2.1129886258243094\n",
      "    mean_raw_obs_processing_ms: 20.79001034941598\n",
      "  time_since_restore: 21076.79095363617\n",
      "  time_this_iter_s: 148.94464302062988\n",
      "  time_total_s: 21076.79095363617\n",
      "  timers:\n",
      "    learn_throughput: 1325.355\n",
      "    learn_time_ms: 754.515\n",
      "    load_throughput: 46086.138\n",
      "    load_time_ms: 21.698\n",
      "    sample_throughput: 9.708\n",
      "    sample_time_ms: 103010.603\n",
      "    update_time_ms: 3.987\n",
      "  timestamp: 1635303658\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 472000\n",
      "  training_iteration: 472\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   472</td><td style=\"text-align: right;\">         21076.8</td><td style=\"text-align: right;\">472000</td><td style=\"text-align: right;\">  3.3444</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -7.29</td><td style=\"text-align: right;\">            168.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 473000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-02-11\n",
      "  done: false\n",
      "  episode_len_mean: 169.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.2797000000000214\n",
      "  episode_reward_min: -7.289999999999898\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1710\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4165962999486102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.995599631468455\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005830707206477253\n",
      "          policy_loss: 0.023592269834544925\n",
      "          total_loss: 0.015100335329771042\n",
      "          vf_explained_var: 0.31924375891685486\n",
      "          vf_loss: 0.009035010591873692\n",
      "    num_agent_steps_sampled: 473000\n",
      "    num_agent_steps_trained: 473000\n",
      "    num_steps_sampled: 473000\n",
      "    num_steps_trained: 473000\n",
      "  iterations_since_restore: 473\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.698076923076925\n",
      "    ram_util_percent: 33.42884615384615\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668629567455979\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.97176654895821\n",
      "    mean_inference_ms: 2.113000695468992\n",
      "    mean_raw_obs_processing_ms: 20.890648709235\n",
      "  time_since_restore: 21149.5171854496\n",
      "  time_this_iter_s: 72.72623181343079\n",
      "  time_total_s: 21149.5171854496\n",
      "  timers:\n",
      "    learn_throughput: 1324.41\n",
      "    learn_time_ms: 755.053\n",
      "    load_throughput: 45936.281\n",
      "    load_time_ms: 21.769\n",
      "    sample_throughput: 9.359\n",
      "    sample_time_ms: 106848.15\n",
      "    update_time_ms: 3.749\n",
      "  timestamp: 1635303731\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 473000\n",
      "  training_iteration: 473\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   473</td><td style=\"text-align: right;\">         21149.5</td><td style=\"text-align: right;\">473000</td><td style=\"text-align: right;\">  3.2797</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -7.29</td><td style=\"text-align: right;\">            169.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 474000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-03-42\n",
      "  done: false\n",
      "  episode_len_mean: 163.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.465100000000021\n",
      "  episode_reward_min: -7.289999999999898\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1716\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4165962999486102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.220471265580919\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008261865346741497\n",
      "          policy_loss: 0.028373317130737836\n",
      "          total_loss: 0.09453373476862907\n",
      "          vf_explained_var: 0.4011736214160919\n",
      "          vf_loss: 0.08492326804747184\n",
      "    num_agent_steps_sampled: 474000\n",
      "    num_agent_steps_trained: 474000\n",
      "    num_steps_sampled: 474000\n",
      "    num_steps_trained: 474000\n",
      "  iterations_since_restore: 474\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.52595419847328\n",
      "    ram_util_percent: 33.45343511450382\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036686956239533386\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.96460126686955\n",
      "    mean_inference_ms: 2.1130159355830602\n",
      "    mean_raw_obs_processing_ms: 21.013384116812126\n",
      "  time_since_restore: 21240.977239608765\n",
      "  time_this_iter_s: 91.46005415916443\n",
      "  time_total_s: 21240.977239608765\n",
      "  timers:\n",
      "    learn_throughput: 1322.147\n",
      "    learn_time_ms: 756.346\n",
      "    load_throughput: 46200.967\n",
      "    load_time_ms: 21.645\n",
      "    sample_throughput: 9.381\n",
      "    sample_time_ms: 106599.404\n",
      "    update_time_ms: 3.607\n",
      "  timestamp: 1635303822\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 474000\n",
      "  training_iteration: 474\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   474</td><td style=\"text-align: right;\">           21241</td><td style=\"text-align: right;\">474000</td><td style=\"text-align: right;\">  3.4651</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -7.29</td><td style=\"text-align: right;\">            163.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 475000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-05-26\n",
      "  done: false\n",
      "  episode_len_mean: 167.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.2600000000000207\n",
      "  episode_reward_min: -7.289999999999898\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1722\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4165962999486102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1308074341879952\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01083107919460752\n",
      "          policy_loss: 0.020165752412544355\n",
      "          total_loss: 0.013603176217940119\n",
      "          vf_explained_var: 0.7255685329437256\n",
      "          vf_loss: 0.010233311897738732\n",
      "    num_agent_steps_sampled: 475000\n",
      "    num_agent_steps_trained: 475000\n",
      "    num_steps_sampled: 475000\n",
      "    num_steps_trained: 475000\n",
      "  iterations_since_restore: 475\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.00476190476191\n",
      "    ram_util_percent: 33.42585034013606\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668761306611943\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.95748961479388\n",
      "    mean_inference_ms: 2.113032431047762\n",
      "    mean_raw_obs_processing_ms: 21.132948117241057\n",
      "  time_since_restore: 21344.38126730919\n",
      "  time_this_iter_s: 103.4040277004242\n",
      "  time_total_s: 21344.38126730919\n",
      "  timers:\n",
      "    learn_throughput: 1325.05\n",
      "    learn_time_ms: 754.688\n",
      "    load_throughput: 45956.665\n",
      "    load_time_ms: 21.76\n",
      "    sample_throughput: 11.3\n",
      "    sample_time_ms: 88497.063\n",
      "    update_time_ms: 3.602\n",
      "  timestamp: 1635303926\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 475000\n",
      "  training_iteration: 475\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   475</td><td style=\"text-align: right;\">         21344.4</td><td style=\"text-align: right;\">475000</td><td style=\"text-align: right;\">    3.26</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -7.29</td><td style=\"text-align: right;\">             167.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 476000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-06-40\n",
      "  done: false\n",
      "  episode_len_mean: 167.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.2880000000000207\n",
      "  episode_reward_min: -7.289999999999898\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1726\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4165962999486102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9587669875886706\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00941781705177587\n",
      "          policy_loss: -0.005365922157135275\n",
      "          total_loss: 0.42731706059227387\n",
      "          vf_explained_var: 0.7103200554847717\n",
      "          vf_loss: 0.4483472234905801\n",
      "    num_agent_steps_sampled: 476000\n",
      "    num_agent_steps_trained: 476000\n",
      "    num_steps_sampled: 476000\n",
      "    num_steps_trained: 476000\n",
      "  iterations_since_restore: 476\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.95887850467289\n",
      "    ram_util_percent: 33.500934579439246\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0366880754761318\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.9526649313254\n",
      "    mean_inference_ms: 2.113044426857167\n",
      "    mean_raw_obs_processing_ms: 21.211393274450593\n",
      "  time_since_restore: 21419.225391626358\n",
      "  time_this_iter_s: 74.84412431716919\n",
      "  time_total_s: 21419.225391626358\n",
      "  timers:\n",
      "    learn_throughput: 1324.303\n",
      "    learn_time_ms: 755.114\n",
      "    load_throughput: 43160.734\n",
      "    load_time_ms: 23.169\n",
      "    sample_throughput: 10.594\n",
      "    sample_time_ms: 94393.452\n",
      "    update_time_ms: 3.619\n",
      "  timestamp: 1635304000\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 476000\n",
      "  training_iteration: 476\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   476</td><td style=\"text-align: right;\">         21419.2</td><td style=\"text-align: right;\">476000</td><td style=\"text-align: right;\">   3.288</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -7.29</td><td style=\"text-align: right;\">            167.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 477000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-07-19\n",
      "  done: false\n",
      "  episode_len_mean: 175.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.0985000000000222\n",
      "  episode_reward_min: -7.289999999999898\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1730\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4165962999486102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2611417134602863\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007413609137027964\n",
      "          policy_loss: -0.17789707344232333\n",
      "          total_loss: -0.14895011726766824\n",
      "          vf_explained_var: 0.6309677958488464\n",
      "          vf_loss: 0.048469891419841184\n",
      "    num_agent_steps_sampled: 477000\n",
      "    num_agent_steps_trained: 477000\n",
      "    num_steps_sampled: 477000\n",
      "    num_steps_trained: 477000\n",
      "  iterations_since_restore: 477\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.405454545454546\n",
      "    ram_util_percent: 33.44545454545455\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668856731658845\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.94774481160855\n",
      "    mean_inference_ms: 2.1130569834917017\n",
      "    mean_raw_obs_processing_ms: 21.28599517820179\n",
      "  time_since_restore: 21457.673186540604\n",
      "  time_this_iter_s: 38.447794914245605\n",
      "  time_total_s: 21457.673186540604\n",
      "  timers:\n",
      "    learn_throughput: 1324.807\n",
      "    learn_time_ms: 754.827\n",
      "    load_throughput: 43201.188\n",
      "    load_time_ms: 23.148\n",
      "    sample_throughput: 11.232\n",
      "    sample_time_ms: 89031.734\n",
      "    update_time_ms: 3.632\n",
      "  timestamp: 1635304039\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 477000\n",
      "  training_iteration: 477\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   477</td><td style=\"text-align: right;\">         21457.7</td><td style=\"text-align: right;\">477000</td><td style=\"text-align: right;\">  3.0985</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -7.29</td><td style=\"text-align: right;\">            175.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 478000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-08-15\n",
      "  done: false\n",
      "  episode_len_mean: 175.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.216600000000022\n",
      "  episode_reward_min: -7.289999999999898\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1733\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4165962999486102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1063074853685166\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016476559942691496\n",
      "          policy_loss: -0.028172478328148523\n",
      "          total_loss: -0.03444847174816661\n",
      "          vf_explained_var: 0.498823881149292\n",
      "          vf_loss: 0.00792300302742256\n",
      "    num_agent_steps_sampled: 478000\n",
      "    num_agent_steps_trained: 478000\n",
      "    num_steps_sampled: 478000\n",
      "    num_steps_trained: 478000\n",
      "  iterations_since_restore: 478\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.57974683544303\n",
      "    ram_util_percent: 33.47341772151899\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668895357155656\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.943870068618864\n",
      "    mean_inference_ms: 2.113066646973955\n",
      "    mean_raw_obs_processing_ms: 21.341954818430334\n",
      "  time_since_restore: 21513.34165596962\n",
      "  time_this_iter_s: 55.66846942901611\n",
      "  time_total_s: 21513.34165596962\n",
      "  timers:\n",
      "    learn_throughput: 1323.883\n",
      "    learn_time_ms: 755.354\n",
      "    load_throughput: 42834.468\n",
      "    load_time_ms: 23.346\n",
      "    sample_throughput: 11.441\n",
      "    sample_time_ms: 87401.249\n",
      "    update_time_ms: 3.557\n",
      "  timestamp: 1635304095\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 478000\n",
      "  training_iteration: 478\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   478</td><td style=\"text-align: right;\">         21513.3</td><td style=\"text-align: right;\">478000</td><td style=\"text-align: right;\">  3.2166</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -7.29</td><td style=\"text-align: right;\">            175.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 479000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-09-50\n",
      "  done: false\n",
      "  episode_len_mean: 177.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.2239000000000213\n",
      "  episode_reward_min: -7.289999999999898\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1739\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4165962999486102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0334161109394495\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00934707601058542\n",
      "          policy_loss: -0.12083051626880964\n",
      "          total_loss: -0.1262454212539726\n",
      "          vf_explained_var: 0.92462557554245\n",
      "          vf_loss: 0.011025300777206818\n",
      "    num_agent_steps_sampled: 479000\n",
      "    num_agent_steps_trained: 479000\n",
      "    num_steps_sampled: 479000\n",
      "    num_steps_trained: 479000\n",
      "  iterations_since_restore: 479\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.116176470588236\n",
      "    ram_util_percent: 33.487500000000004\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036689750672770825\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.935745018275902\n",
      "    mean_inference_ms: 2.1130858628990086\n",
      "    mean_raw_obs_processing_ms: 21.453353822593307\n",
      "  time_since_restore: 21608.739443540573\n",
      "  time_this_iter_s: 95.39778757095337\n",
      "  time_total_s: 21608.739443540573\n",
      "  timers:\n",
      "    learn_throughput: 1326.864\n",
      "    learn_time_ms: 753.657\n",
      "    load_throughput: 42578.957\n",
      "    load_time_ms: 23.486\n",
      "    sample_throughput: 11.633\n",
      "    sample_time_ms: 85958.666\n",
      "    update_time_ms: 3.563\n",
      "  timestamp: 1635304190\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 479000\n",
      "  training_iteration: 479\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   479</td><td style=\"text-align: right;\">         21608.7</td><td style=\"text-align: right;\">479000</td><td style=\"text-align: right;\">  3.2239</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -7.29</td><td style=\"text-align: right;\">             177.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 480000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-11-07\n",
      "  done: false\n",
      "  episode_len_mean: 169.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.340700000000021\n",
      "  episode_reward_min: -7.289999999999898\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1745\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4165962999486102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8858338899082607\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011393362164163425\n",
      "          policy_loss: 0.01842140249080128\n",
      "          total_loss: 0.14225820990072358\n",
      "          vf_explained_var: 0.9455795884132385\n",
      "          vf_loss: 0.13794871610071924\n",
      "    num_agent_steps_sampled: 480000\n",
      "    num_agent_steps_trained: 480000\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "  iterations_since_restore: 480\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.39369369369369\n",
      "    ram_util_percent: 33.538738738738736\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669054093678223\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.92878903059015\n",
      "    mean_inference_ms: 2.113105222711137\n",
      "    mean_raw_obs_processing_ms: 21.56923708015936\n",
      "  time_since_restore: 21686.142815113068\n",
      "  time_this_iter_s: 77.4033715724945\n",
      "  time_total_s: 21686.142815113068\n",
      "  timers:\n",
      "    learn_throughput: 1327.608\n",
      "    learn_time_ms: 753.234\n",
      "    load_throughput: 42520.296\n",
      "    load_time_ms: 23.518\n",
      "    sample_throughput: 13.035\n",
      "    sample_time_ms: 76716.419\n",
      "    update_time_ms: 3.654\n",
      "  timestamp: 1635304267\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 480\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   480</td><td style=\"text-align: right;\">         21686.1</td><td style=\"text-align: right;\">480000</td><td style=\"text-align: right;\">  3.3407</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -7.29</td><td style=\"text-align: right;\">            169.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 481000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-11-26\n",
      "  done: false\n",
      "  episode_len_mean: 174.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.243600000000021\n",
      "  episode_reward_min: -7.289999999999898\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1747\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4165962999486102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0823143985536365\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014677927321314302\n",
      "          policy_loss: -0.14817518418033918\n",
      "          total_loss: -0.15838756466077433\n",
      "          vf_explained_var: 0.8806220889091492\n",
      "          vf_loss: 0.004495992471412238\n",
      "    num_agent_steps_sampled: 481000\n",
      "    num_agent_steps_trained: 481000\n",
      "    num_steps_sampled: 481000\n",
      "    num_steps_trained: 481000\n",
      "  iterations_since_restore: 481\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.06923076923077\n",
      "    ram_util_percent: 33.63076923076923\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669079971630662\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.926473564941812\n",
      "    mean_inference_ms: 2.1131115178146724\n",
      "    mean_raw_obs_processing_ms: 21.604754122550684\n",
      "  time_since_restore: 21704.358293294907\n",
      "  time_this_iter_s: 18.21547818183899\n",
      "  time_total_s: 21704.358293294907\n",
      "  timers:\n",
      "    learn_throughput: 1324.754\n",
      "    learn_time_ms: 754.857\n",
      "    load_throughput: 40931.979\n",
      "    load_time_ms: 24.431\n",
      "    sample_throughput: 13.01\n",
      "    sample_time_ms: 76863.94\n",
      "    update_time_ms: 3.654\n",
      "  timestamp: 1635304286\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 481000\n",
      "  training_iteration: 481\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   481</td><td style=\"text-align: right;\">         21704.4</td><td style=\"text-align: right;\">481000</td><td style=\"text-align: right;\">  3.2436</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -7.29</td><td style=\"text-align: right;\">            174.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 482000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-13-53\n",
      "  done: false\n",
      "  episode_len_mean: 176.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.263700000000021\n",
      "  episode_reward_min: -7.289999999999898\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1756\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4165962999486102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.800100611315833\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005663012979263549\n",
      "          policy_loss: -0.008562825868527095\n",
      "          total_loss: 0.01693244915869501\n",
      "          vf_explained_var: 0.5283498167991638\n",
      "          vf_loss: 0.04113708978273078\n",
      "    num_agent_steps_sampled: 482000\n",
      "    num_agent_steps_trained: 482000\n",
      "    num_steps_sampled: 482000\n",
      "    num_steps_trained: 482000\n",
      "  iterations_since_restore: 482\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.27333333333334\n",
      "    ram_util_percent: 33.60142857142858\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669201308679055\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.915677979030747\n",
      "    mean_inference_ms: 2.1131399777850293\n",
      "    mean_raw_obs_processing_ms: 21.75287665427949\n",
      "  time_since_restore: 21851.308789491653\n",
      "  time_this_iter_s: 146.95049619674683\n",
      "  time_total_s: 21851.308789491653\n",
      "  timers:\n",
      "    learn_throughput: 1324.031\n",
      "    learn_time_ms: 755.269\n",
      "    load_throughput: 41462.49\n",
      "    load_time_ms: 24.118\n",
      "    sample_throughput: 13.044\n",
      "    sample_time_ms: 76664.336\n",
      "    update_time_ms: 3.733\n",
      "  timestamp: 1635304433\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 482000\n",
      "  training_iteration: 482\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   482</td><td style=\"text-align: right;\">         21851.3</td><td style=\"text-align: right;\">482000</td><td style=\"text-align: right;\">  3.2637</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -7.29</td><td style=\"text-align: right;\">            176.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 483000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-15-45\n",
      "  done: false\n",
      "  episode_len_mean: 182.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.0492000000000217\n",
      "  episode_reward_min: -7.289999999999898\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1762\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4165962999486102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9471359305911593\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009544752979402521\n",
      "          policy_loss: 0.0038329145974583096\n",
      "          total_loss: 0.022628837327162424\n",
      "          vf_explained_var: 0.8161336183547974\n",
      "          vf_loss: 0.034290966474347644\n",
      "    num_agent_steps_sampled: 483000\n",
      "    num_agent_steps_trained: 483000\n",
      "    num_steps_sampled: 483000\n",
      "    num_steps_trained: 483000\n",
      "  iterations_since_restore: 483\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.0527950310559\n",
      "    ram_util_percent: 33.60496894409938\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036692844168046926\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.908330205641228\n",
      "    mean_inference_ms: 2.1131590167274124\n",
      "    mean_raw_obs_processing_ms: 21.849791705968816\n",
      "  time_since_restore: 21964.10072994232\n",
      "  time_this_iter_s: 112.79194045066833\n",
      "  time_total_s: 21964.10072994232\n",
      "  timers:\n",
      "    learn_throughput: 1323.085\n",
      "    learn_time_ms: 755.809\n",
      "    load_throughput: 41546.726\n",
      "    load_time_ms: 24.069\n",
      "    sample_throughput: 12.396\n",
      "    sample_time_ms: 80670.397\n",
      "    update_time_ms: 3.733\n",
      "  timestamp: 1635304545\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 483000\n",
      "  training_iteration: 483\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   483</td><td style=\"text-align: right;\">         21964.1</td><td style=\"text-align: right;\">483000</td><td style=\"text-align: right;\">  3.0492</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -7.29</td><td style=\"text-align: right;\">            182.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 484000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-17-57\n",
      "  done: false\n",
      "  episode_len_mean: 176.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.1771000000000216\n",
      "  episode_reward_min: -7.289999999999898\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1770\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4165962999486102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.881375081009335\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013984692419545672\n",
      "          policy_loss: 0.026970361669858296\n",
      "          total_loss: 0.2560645264055994\n",
      "          vf_explained_var: 0.870242714881897\n",
      "          vf_loss: 0.24208195022462556\n",
      "    num_agent_steps_sampled: 484000\n",
      "    num_agent_steps_trained: 484000\n",
      "    num_steps_sampled: 484000\n",
      "    num_steps_trained: 484000\n",
      "  iterations_since_restore: 484\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.45935828877006\n",
      "    ram_util_percent: 33.564705882352946\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036693953201074374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.89902447884922\n",
      "    mean_inference_ms: 2.1131850814170243\n",
      "    mean_raw_obs_processing_ms: 21.992065014145183\n",
      "  time_since_restore: 22095.64559531212\n",
      "  time_this_iter_s: 131.54486536979675\n",
      "  time_total_s: 22095.64559531212\n",
      "  timers:\n",
      "    learn_throughput: 1322.518\n",
      "    learn_time_ms: 756.133\n",
      "    load_throughput: 41608.508\n",
      "    load_time_ms: 24.034\n",
      "    sample_throughput: 11.809\n",
      "    sample_time_ms: 84678.579\n",
      "    update_time_ms: 3.734\n",
      "  timestamp: 1635304677\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 484000\n",
      "  training_iteration: 484\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   484</td><td style=\"text-align: right;\">         22095.6</td><td style=\"text-align: right;\">484000</td><td style=\"text-align: right;\">  3.1771</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -7.29</td><td style=\"text-align: right;\">            176.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 485000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-20-07\n",
      "  done: false\n",
      "  episode_len_mean: 170.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.40540000000002\n",
      "  episode_reward_min: -6.639999999999933\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1778\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4165962999486102\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8584838933414882\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.032953665054888205\n",
      "          policy_loss: -0.010336771607398987\n",
      "          total_loss: 0.10069065772824817\n",
      "          vf_explained_var: 0.45390501618385315\n",
      "          vf_loss: 0.1158838922649415\n",
      "    num_agent_steps_sampled: 485000\n",
      "    num_agent_steps_trained: 485000\n",
      "    num_steps_sampled: 485000\n",
      "    num_steps_trained: 485000\n",
      "  iterations_since_restore: 485\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.7681081081081\n",
      "    ram_util_percent: 33.58162162162162\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669505177716928\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.890792303131835\n",
      "    mean_inference_ms: 2.11321156707298\n",
      "    mean_raw_obs_processing_ms: 22.141368715937297\n",
      "  time_since_restore: 22225.16218829155\n",
      "  time_this_iter_s: 129.51659297943115\n",
      "  time_total_s: 22225.16218829155\n",
      "  timers:\n",
      "    learn_throughput: 1321.774\n",
      "    learn_time_ms: 756.559\n",
      "    load_throughput: 41855.52\n",
      "    load_time_ms: 23.892\n",
      "    sample_throughput: 11.456\n",
      "    sample_time_ms: 87289.553\n",
      "    update_time_ms: 3.734\n",
      "  timestamp: 1635304807\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 485000\n",
      "  training_iteration: 485\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   485</td><td style=\"text-align: right;\">         22225.2</td><td style=\"text-align: right;\">485000</td><td style=\"text-align: right;\">  3.4054</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -6.64</td><td style=\"text-align: right;\">            170.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 486000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-21-02\n",
      "  done: false\n",
      "  episode_len_mean: 170.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.35880000000002\n",
      "  episode_reward_min: -6.639999999999933\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1782\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6248944499229154\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.061407526334127\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006462619566007863\n",
      "          policy_loss: 0.04389541405770514\n",
      "          total_loss: 0.03632490730120076\n",
      "          vf_explained_var: 0.5351939797401428\n",
      "          vf_loss: 0.009005111572332681\n",
      "    num_agent_steps_sampled: 486000\n",
      "    num_agent_steps_trained: 486000\n",
      "    num_steps_sampled: 486000\n",
      "    num_steps_trained: 486000\n",
      "  iterations_since_restore: 486\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.32911392405063\n",
      "    ram_util_percent: 33.56708860759492\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669559452800862\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.88687654027189\n",
      "    mean_inference_ms: 2.113224499806748\n",
      "    mean_raw_obs_processing_ms: 22.210089072452792\n",
      "  time_since_restore: 22280.370500564575\n",
      "  time_this_iter_s: 55.20831227302551\n",
      "  time_total_s: 22280.370500564575\n",
      "  timers:\n",
      "    learn_throughput: 1322.207\n",
      "    learn_time_ms: 756.311\n",
      "    load_throughput: 42016.864\n",
      "    load_time_ms: 23.8\n",
      "    sample_throughput: 11.72\n",
      "    sample_time_ms: 85325.687\n",
      "    update_time_ms: 4.352\n",
      "  timestamp: 1635304862\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 486000\n",
      "  training_iteration: 486\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   486</td><td style=\"text-align: right;\">         22280.4</td><td style=\"text-align: right;\">486000</td><td style=\"text-align: right;\">  3.3588</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -6.64</td><td style=\"text-align: right;\">            170.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 487000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-22-13\n",
      "  done: false\n",
      "  episode_len_mean: 176.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.237600000000021\n",
      "  episode_reward_min: -6.639999999999933\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1786\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6248944499229154\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9333956638971965\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01330964279836433\n",
      "          policy_loss: -0.07467512442833847\n",
      "          total_loss: -0.006995566230681208\n",
      "          vf_explained_var: 0.33710402250289917\n",
      "          vf_loss: 0.07869638903761775\n",
      "    num_agent_steps_sampled: 487000\n",
      "    num_agent_steps_trained: 487000\n",
      "    num_steps_sampled: 487000\n",
      "    num_steps_trained: 487000\n",
      "  iterations_since_restore: 487\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.35346534653466\n",
      "    ram_util_percent: 33.61782178217821\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036696168238719656\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.88241900430924\n",
      "    mean_inference_ms: 2.1132374979949367\n",
      "    mean_raw_obs_processing_ms: 22.278729413539324\n",
      "  time_since_restore: 22351.175761699677\n",
      "  time_this_iter_s: 70.80526113510132\n",
      "  time_total_s: 22351.175761699677\n",
      "  timers:\n",
      "    learn_throughput: 1321.214\n",
      "    learn_time_ms: 756.879\n",
      "    load_throughput: 41874.032\n",
      "    load_time_ms: 23.881\n",
      "    sample_throughput: 11.292\n",
      "    sample_time_ms: 88560.756\n",
      "    update_time_ms: 4.352\n",
      "  timestamp: 1635304933\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 487000\n",
      "  training_iteration: 487\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   487</td><td style=\"text-align: right;\">         22351.2</td><td style=\"text-align: right;\">487000</td><td style=\"text-align: right;\">  3.2376</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -6.64</td><td style=\"text-align: right;\">             176.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 488000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-24-47\n",
      "  done: false\n",
      "  episode_len_mean: 177.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.1664000000000208\n",
      "  episode_reward_min: -6.639999999999933\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1795\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6248944499229154\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9191936042573716\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007132703388079046\n",
      "          policy_loss: -0.057470847583479354\n",
      "          total_loss: -0.044431611233287385\n",
      "          vf_explained_var: 0.017184315249323845\n",
      "          vf_loss: 0.027773990109562875\n",
      "    num_agent_steps_sampled: 488000\n",
      "    num_agent_steps_trained: 488000\n",
      "    num_steps_sampled: 488000\n",
      "    num_steps_trained: 488000\n",
      "  iterations_since_restore: 488\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.50950226244343\n",
      "    ram_util_percent: 33.629411764705885\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0366974895874064\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.872235887774483\n",
      "    mean_inference_ms: 2.1132679508748566\n",
      "    mean_raw_obs_processing_ms: 22.435335327332062\n",
      "  time_since_restore: 22505.79069018364\n",
      "  time_this_iter_s: 154.614928483963\n",
      "  time_total_s: 22505.79069018364\n",
      "  timers:\n",
      "    learn_throughput: 1320.875\n",
      "    learn_time_ms: 757.074\n",
      "    load_throughput: 42044.788\n",
      "    load_time_ms: 23.784\n",
      "    sample_throughput: 10.157\n",
      "    sample_time_ms: 98455.35\n",
      "    update_time_ms: 4.354\n",
      "  timestamp: 1635305087\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 488000\n",
      "  training_iteration: 488\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   488</td><td style=\"text-align: right;\">         22505.8</td><td style=\"text-align: right;\">488000</td><td style=\"text-align: right;\">  3.1664</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -6.64</td><td style=\"text-align: right;\">            177.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 489000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-25-41\n",
      "  done: false\n",
      "  episode_len_mean: 175.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.182100000000021\n",
      "  episode_reward_min: -6.639999999999933\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1799\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6248944499229154\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0141228397687274\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008082689071299281\n",
      "          policy_loss: -0.05493536964058876\n",
      "          total_loss: -0.050646241505940756\n",
      "          vf_explained_var: 0.31331154704093933\n",
      "          vf_loss: 0.019379528007832252\n",
      "    num_agent_steps_sampled: 489000\n",
      "    num_agent_steps_trained: 489000\n",
      "    num_steps_sampled: 489000\n",
      "    num_steps_trained: 489000\n",
      "  iterations_since_restore: 489\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.471428571428575\n",
      "    ram_util_percent: 33.68311688311687\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036698073947641116\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.868026356735484\n",
      "    mean_inference_ms: 2.113281509409023\n",
      "    mean_raw_obs_processing_ms: 22.500958772606694\n",
      "  time_since_restore: 22559.530613660812\n",
      "  time_this_iter_s: 53.73992347717285\n",
      "  time_total_s: 22559.530613660812\n",
      "  timers:\n",
      "    learn_throughput: 1319.8\n",
      "    learn_time_ms: 757.69\n",
      "    load_throughput: 42362.043\n",
      "    load_time_ms: 23.606\n",
      "    sample_throughput: 10.606\n",
      "    sample_time_ms: 94289.222\n",
      "    update_time_ms: 4.346\n",
      "  timestamp: 1635305141\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 489000\n",
      "  training_iteration: 489\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   489</td><td style=\"text-align: right;\">         22559.5</td><td style=\"text-align: right;\">489000</td><td style=\"text-align: right;\">  3.1821</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -6.64</td><td style=\"text-align: right;\">            175.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 490000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-31-01\n",
      "  done: false\n",
      "  episode_len_mean: 156.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.689800000000018\n",
      "  episode_reward_min: -6.629999999999946\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 1817\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6248944499229154\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8857298758294847\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005643247886751664\n",
      "          policy_loss: 0.0037678316235542296\n",
      "          total_loss: 0.16226311417089567\n",
      "          vf_explained_var: 0.8439285159111023\n",
      "          vf_loss: 0.17382614455095285\n",
      "    num_agent_steps_sampled: 490000\n",
      "    num_agent_steps_trained: 490000\n",
      "    num_steps_sampled: 490000\n",
      "    num_steps_trained: 490000\n",
      "  iterations_since_restore: 490\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.83464912280702\n",
      "    ram_util_percent: 33.59385964912281\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036700644082853555\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.850859602811596\n",
      "    mean_inference_ms: 2.1133419332756587\n",
      "    mean_raw_obs_processing_ms: 22.86988021410315\n",
      "  time_since_restore: 22879.55948615074\n",
      "  time_this_iter_s: 320.0288724899292\n",
      "  time_total_s: 22879.55948615074\n",
      "  timers:\n",
      "    learn_throughput: 1322.554\n",
      "    learn_time_ms: 756.113\n",
      "    load_throughput: 42123.497\n",
      "    load_time_ms: 23.74\n",
      "    sample_throughput: 8.435\n",
      "    sample_time_ms: 118553.253\n",
      "    update_time_ms: 4.309\n",
      "  timestamp: 1635305461\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 490000\n",
      "  training_iteration: 490\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   490</td><td style=\"text-align: right;\">         22879.6</td><td style=\"text-align: right;\">490000</td><td style=\"text-align: right;\">  3.6898</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -6.63</td><td style=\"text-align: right;\">            156.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 491000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-33-13\n",
      "  done: false\n",
      "  episode_len_mean: 155.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.706600000000017\n",
      "  episode_reward_min: -6.629999999999946\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1825\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6248944499229154\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9362204750378926\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010127730604146378\n",
      "          policy_loss: -0.0019719324592087006\n",
      "          total_loss: 0.2042387314968639\n",
      "          vf_explained_var: 0.8680158853530884\n",
      "          vf_loss: 0.21924410409087108\n",
      "    num_agent_steps_sampled: 491000\n",
      "    num_agent_steps_trained: 491000\n",
      "    num_steps_sampled: 491000\n",
      "    num_steps_trained: 491000\n",
      "  iterations_since_restore: 491\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.126455026455034\n",
      "    ram_util_percent: 33.647089947089945\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670178150929262\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.843919272849075\n",
      "    mean_inference_ms: 2.113367590659396\n",
      "    mean_raw_obs_processing_ms: 23.029434848041905\n",
      "  time_since_restore: 23011.51214170456\n",
      "  time_this_iter_s: 131.95265555381775\n",
      "  time_total_s: 23011.51214170456\n",
      "  timers:\n",
      "    learn_throughput: 1325.04\n",
      "    learn_time_ms: 754.694\n",
      "    load_throughput: 41973.303\n",
      "    load_time_ms: 23.825\n",
      "    sample_throughput: 7.697\n",
      "    sample_time_ms: 129928.112\n",
      "    update_time_ms: 4.482\n",
      "  timestamp: 1635305593\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 491000\n",
      "  training_iteration: 491\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   491</td><td style=\"text-align: right;\">         23011.5</td><td style=\"text-align: right;\">491000</td><td style=\"text-align: right;\">  3.7066</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -6.63</td><td style=\"text-align: right;\">            155.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 492000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-36-46\n",
      "  done: false\n",
      "  episode_len_mean: 133.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 4.165800000000015\n",
      "  episode_reward_min: -6.199999999999912\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1837\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6248944499229154\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.630443569024404\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.06836276961224154\n",
      "          policy_loss: 0.19688164384828674\n",
      "          total_loss: 0.8333355893691381\n",
      "          vf_explained_var: 0.7007209658622742\n",
      "          vf_loss: 0.6100388644470109\n",
      "    num_agent_steps_sampled: 492000\n",
      "    num_agent_steps_trained: 492000\n",
      "    num_steps_sampled: 492000\n",
      "    num_steps_trained: 492000\n",
      "  iterations_since_restore: 492\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.204290429042906\n",
      "    ram_util_percent: 33.63432343234324\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670334767955537\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.836036089111122\n",
      "    mean_inference_ms: 2.1134051950922474\n",
      "    mean_raw_obs_processing_ms: 23.297715159788712\n",
      "  time_since_restore: 23224.252915382385\n",
      "  time_this_iter_s: 212.74077367782593\n",
      "  time_total_s: 23224.252915382385\n",
      "  timers:\n",
      "    learn_throughput: 1324.547\n",
      "    learn_time_ms: 754.975\n",
      "    load_throughput: 41650.404\n",
      "    load_time_ms: 24.009\n",
      "    sample_throughput: 7.326\n",
      "    sample_time_ms: 136506.768\n",
      "    update_time_ms: 4.398\n",
      "  timestamp: 1635305806\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 492000\n",
      "  training_iteration: 492\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   492</td><td style=\"text-align: right;\">         23224.3</td><td style=\"text-align: right;\">492000</td><td style=\"text-align: right;\">  4.1658</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">                -6.2</td><td style=\"text-align: right;\">            133.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 493000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-37-45\n",
      "  done: false\n",
      "  episode_len_mean: 137.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 4.019600000000016\n",
      "  episode_reward_min: -6.199999999999912\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1841\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9373416748843731\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.030192524856991\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0065197243018510894\n",
      "          policy_loss: -0.09686227807154259\n",
      "          total_loss: -0.06354361319293579\n",
      "          vf_explained_var: 0.800511360168457\n",
      "          vf_loss: 0.04750937817799342\n",
      "    num_agent_steps_sampled: 493000\n",
      "    num_agent_steps_trained: 493000\n",
      "    num_steps_sampled: 493000\n",
      "    num_steps_trained: 493000\n",
      "  iterations_since_restore: 493\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.95882352941177\n",
      "    ram_util_percent: 33.69058823529413\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036703858664400374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.833626974506345\n",
      "    mean_inference_ms: 2.1134180338012047\n",
      "    mean_raw_obs_processing_ms: 23.38344314356857\n",
      "  time_since_restore: 23283.784111976624\n",
      "  time_this_iter_s: 59.53119659423828\n",
      "  time_total_s: 23283.784111976624\n",
      "  timers:\n",
      "    learn_throughput: 1324.242\n",
      "    learn_time_ms: 755.149\n",
      "    load_throughput: 41694.955\n",
      "    load_time_ms: 23.984\n",
      "    sample_throughput: 7.623\n",
      "    sample_time_ms: 131180.562\n",
      "    update_time_ms: 4.396\n",
      "  timestamp: 1635305865\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 493000\n",
      "  training_iteration: 493\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   493</td><td style=\"text-align: right;\">         23283.8</td><td style=\"text-align: right;\">493000</td><td style=\"text-align: right;\">  4.0196</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">                -6.2</td><td style=\"text-align: right;\">            137.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 494000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-39-19\n",
      "  done: false\n",
      "  episode_len_mean: 128.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 4.268900000000013\n",
      "  episode_reward_min: -6.479999999999947\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1847\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9373416748843731\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8394667148590087\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011289196976057427\n",
      "          policy_loss: -0.1221737874050935\n",
      "          total_loss: -0.016588008569346533\n",
      "          vf_explained_var: 0.6871225833892822\n",
      "          vf_loss: 0.11339861432918244\n",
      "    num_agent_steps_sampled: 494000\n",
      "    num_agent_steps_trained: 494000\n",
      "    num_steps_sampled: 494000\n",
      "    num_steps_trained: 494000\n",
      "  iterations_since_restore: 494\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.20597014925374\n",
      "    ram_util_percent: 33.699999999999996\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670465402764769\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.829838572668518\n",
      "    mean_inference_ms: 2.113438619732108\n",
      "    mean_raw_obs_processing_ms: 23.51695213410557\n",
      "  time_since_restore: 23377.451828956604\n",
      "  time_this_iter_s: 93.66771697998047\n",
      "  time_total_s: 23377.451828956604\n",
      "  timers:\n",
      "    learn_throughput: 1324.102\n",
      "    learn_time_ms: 755.229\n",
      "    load_throughput: 41631.511\n",
      "    load_time_ms: 24.02\n",
      "    sample_throughput: 7.85\n",
      "    sample_time_ms: 127392.734\n",
      "    update_time_ms: 4.401\n",
      "  timestamp: 1635305959\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 494000\n",
      "  training_iteration: 494\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   494</td><td style=\"text-align: right;\">         23377.5</td><td style=\"text-align: right;\">494000</td><td style=\"text-align: right;\">  4.2689</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -6.48</td><td style=\"text-align: right;\">            128.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 495000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-41-48\n",
      "  done: false\n",
      "  episode_len_mean: 129.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 4.2442000000000135\n",
      "  episode_reward_min: -6.479999999999947\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1856\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9373416748843731\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9188488827811347\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0058347956656967645\n",
      "          policy_loss: 0.10118912388053206\n",
      "          total_loss: 0.13003848495168818\n",
      "          vf_explained_var: 0.049652792513370514\n",
      "          vf_loss: 0.04256865239650425\n",
      "    num_agent_steps_sampled: 495000\n",
      "    num_agent_steps_trained: 495000\n",
      "    num_steps_sampled: 495000\n",
      "    num_steps_trained: 495000\n",
      "  iterations_since_restore: 495\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.27405660377359\n",
      "    ram_util_percent: 33.61509433962265\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670580790019683\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.82461066267722\n",
      "    mean_inference_ms: 2.1134684361379863\n",
      "    mean_raw_obs_processing_ms: 23.719510185178034\n",
      "  time_since_restore: 23525.959149360657\n",
      "  time_this_iter_s: 148.50732040405273\n",
      "  time_total_s: 23525.959149360657\n",
      "  timers:\n",
      "    learn_throughput: 1324.65\n",
      "    learn_time_ms: 754.917\n",
      "    load_throughput: 41249.918\n",
      "    load_time_ms: 24.242\n",
      "    sample_throughput: 7.734\n",
      "    sample_time_ms: 129291.832\n",
      "    update_time_ms: 4.409\n",
      "  timestamp: 1635306108\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 495000\n",
      "  training_iteration: 495\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   495</td><td style=\"text-align: right;\">           23526</td><td style=\"text-align: right;\">495000</td><td style=\"text-align: right;\">  4.2442</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -6.48</td><td style=\"text-align: right;\">            129.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 496000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-46-31\n",
      "  done: false\n",
      "  episode_len_mean: 115.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 4.614100000000012\n",
      "  episode_reward_min: -6.479999999999947\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 1872\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9373416748843731\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8371709399753147\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004347161201308304\n",
      "          policy_loss: 0.07194399784008662\n",
      "          total_loss: 0.09943683747616079\n",
      "          vf_explained_var: 0.47070395946502686\n",
      "          vf_loss: 0.041789774084463716\n",
      "    num_agent_steps_sampled: 496000\n",
      "    num_agent_steps_trained: 496000\n",
      "    num_steps_sampled: 496000\n",
      "    num_steps_trained: 496000\n",
      "  iterations_since_restore: 496\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.287901234567904\n",
      "    ram_util_percent: 33.66345679012346\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670791065724584\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.816754389354127\n",
      "    mean_inference_ms: 2.113525625796888\n",
      "    mean_raw_obs_processing_ms: 24.11078519015705\n",
      "  time_since_restore: 23809.708546876907\n",
      "  time_this_iter_s: 283.7493975162506\n",
      "  time_total_s: 23809.708546876907\n",
      "  timers:\n",
      "    learn_throughput: 1325.773\n",
      "    learn_time_ms: 754.277\n",
      "    load_throughput: 41234.71\n",
      "    load_time_ms: 24.251\n",
      "    sample_throughput: 6.573\n",
      "    sample_time_ms: 152147.188\n",
      "    update_time_ms: 3.777\n",
      "  timestamp: 1635306391\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 496000\n",
      "  training_iteration: 496\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   496</td><td style=\"text-align: right;\">         23809.7</td><td style=\"text-align: right;\">496000</td><td style=\"text-align: right;\">  4.6141</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -6.48</td><td style=\"text-align: right;\">            115.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 497000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-47-27\n",
      "  done: false\n",
      "  episode_len_mean: 123.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 4.318700000000013\n",
      "  episode_reward_min: -6.479999999999947\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1876\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.46867083744218657\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9872773951954312\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01376284269071826\n",
      "          policy_loss: -0.028065216872427197\n",
      "          total_loss: -0.026362353728877172\n",
      "          vf_explained_var: 0.6303767561912537\n",
      "          vf_loss: 0.015125397846309676\n",
      "    num_agent_steps_sampled: 497000\n",
      "    num_agent_steps_trained: 497000\n",
      "    num_steps_sampled: 497000\n",
      "    num_steps_trained: 497000\n",
      "  iterations_since_restore: 497\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.077215189873414\n",
      "    ram_util_percent: 33.71012658227847\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670844991600028\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.81498164887726\n",
      "    mean_inference_ms: 2.113541422879782\n",
      "    mean_raw_obs_processing_ms: 24.20031574877081\n",
      "  time_since_restore: 23865.485043525696\n",
      "  time_this_iter_s: 55.77649664878845\n",
      "  time_total_s: 23865.485043525696\n",
      "  timers:\n",
      "    learn_throughput: 1326.186\n",
      "    learn_time_ms: 754.042\n",
      "    load_throughput: 41134.904\n",
      "    load_time_ms: 24.31\n",
      "    sample_throughput: 6.638\n",
      "    sample_time_ms: 150644.507\n",
      "    update_time_ms: 3.77\n",
      "  timestamp: 1635306447\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 497000\n",
      "  training_iteration: 497\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   497</td><td style=\"text-align: right;\">         23865.5</td><td style=\"text-align: right;\">497000</td><td style=\"text-align: right;\">  4.3187</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -6.48</td><td style=\"text-align: right;\">            123.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 498000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-49-02\n",
      "  done: false\n",
      "  episode_len_mean: 117.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 4.482200000000013\n",
      "  episode_reward_min: -6.479999999999947\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1882\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.46867083744218657\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8504459500312804\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004955082338299436\n",
      "          policy_loss: -0.18395198434591292\n",
      "          total_loss: -0.19005920572413337\n",
      "          vf_explained_var: 0.905030369758606\n",
      "          vf_loss: 0.010074938523272674\n",
      "    num_agent_steps_sampled: 498000\n",
      "    num_agent_steps_trained: 498000\n",
      "    num_steps_sampled: 498000\n",
      "    num_steps_trained: 498000\n",
      "  iterations_since_restore: 498\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.27132352941176\n",
      "    ram_util_percent: 33.63308823529412\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036709319365531734\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.81256516072482\n",
      "    mean_inference_ms: 2.113565820783811\n",
      "    mean_raw_obs_processing_ms: 24.33927525190474\n",
      "  time_since_restore: 23960.634618520737\n",
      "  time_this_iter_s: 95.1495749950409\n",
      "  time_total_s: 23960.634618520737\n",
      "  timers:\n",
      "    learn_throughput: 1327.1\n",
      "    learn_time_ms: 753.522\n",
      "    load_throughput: 41398.975\n",
      "    load_time_ms: 24.155\n",
      "    sample_throughput: 6.911\n",
      "    sample_time_ms: 144698.601\n",
      "    update_time_ms: 3.816\n",
      "  timestamp: 1635306542\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 498000\n",
      "  training_iteration: 498\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   498</td><td style=\"text-align: right;\">         23960.6</td><td style=\"text-align: right;\">498000</td><td style=\"text-align: right;\">  4.4822</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -6.48</td><td style=\"text-align: right;\">            117.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 499000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-49-42\n",
      "  done: false\n",
      "  episode_len_mean: 119.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 4.412800000000011\n",
      "  episode_reward_min: -6.479999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1885\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9178258763419258\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01095748477927917\n",
      "          policy_loss: -0.02079889507343372\n",
      "          total_loss: 0.03864221111353901\n",
      "          vf_explained_var: 0.4439784288406372\n",
      "          vf_loss: 0.076051640117334\n",
      "    num_agent_steps_sampled: 499000\n",
      "    num_agent_steps_trained: 499000\n",
      "    num_steps_sampled: 499000\n",
      "    num_steps_trained: 499000\n",
      "  iterations_since_restore: 499\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.828070175438604\n",
      "    ram_util_percent: 33.73157894736842\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367097486722044\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.811805147175967\n",
      "    mean_inference_ms: 2.113579029012969\n",
      "    mean_raw_obs_processing_ms: 24.406063841347088\n",
      "  time_since_restore: 24000.246376752853\n",
      "  time_this_iter_s: 39.6117582321167\n",
      "  time_total_s: 24000.246376752853\n",
      "  timers:\n",
      "    learn_throughput: 1325.647\n",
      "    learn_time_ms: 754.348\n",
      "    load_throughput: 41600.336\n",
      "    load_time_ms: 24.038\n",
      "    sample_throughput: 6.979\n",
      "    sample_time_ms: 143285.038\n",
      "    update_time_ms: 3.855\n",
      "  timestamp: 1635306582\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 499000\n",
      "  training_iteration: 499\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         24000.2</td><td style=\"text-align: right;\">499000</td><td style=\"text-align: right;\">  4.4128</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -6.48</td><td style=\"text-align: right;\">            119.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 500000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-51-37\n",
      "  done: false\n",
      "  episode_len_mean: 124.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 4.410200000000013\n",
      "  episode_reward_min: -6.479999999999947\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1892\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0903382513258193\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008206505789039416\n",
      "          policy_loss: -0.07254787741435899\n",
      "          total_loss: -0.05830367969142066\n",
      "          vf_explained_var: 0.6636908054351807\n",
      "          vf_loss: 0.033224504885988104\n",
      "    num_agent_steps_sampled: 500000\n",
      "    num_agent_steps_trained: 500000\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "  iterations_since_restore: 500\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.439393939393945\n",
      "    ram_util_percent: 33.68121212121212\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03671077944266487\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.81018947494109\n",
      "    mean_inference_ms: 2.1136111534664823\n",
      "    mean_raw_obs_processing_ms: 24.557810088488132\n",
      "  time_since_restore: 24115.700816869736\n",
      "  time_this_iter_s: 115.45444011688232\n",
      "  time_total_s: 24115.700816869736\n",
      "  timers:\n",
      "    learn_throughput: 1325.759\n",
      "    learn_time_ms: 754.285\n",
      "    load_throughput: 41632.255\n",
      "    load_time_ms: 24.02\n",
      "    sample_throughput: 8.141\n",
      "    sample_time_ms: 122827.748\n",
      "    update_time_ms: 3.798\n",
      "  timestamp: 1635306697\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 500\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         24115.7</td><td style=\"text-align: right;\">500000</td><td style=\"text-align: right;\">  4.4102</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -6.48</td><td style=\"text-align: right;\">            124.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 501000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-52-56\n",
      "  done: false\n",
      "  episode_len_mean: 126.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 4.351000000000012\n",
      "  episode_reward_min: -6.479999999999947\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1897\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0631297085020277\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013214254768244239\n",
      "          policy_loss: -0.05110249833928214\n",
      "          total_loss: 0.08482721174756686\n",
      "          vf_explained_var: 0.044721826910972595\n",
      "          vf_loss: 0.15346443735761567\n",
      "    num_agent_steps_sampled: 501000\n",
      "    num_agent_steps_trained: 501000\n",
      "    num_steps_sampled: 501000\n",
      "    num_steps_trained: 501000\n",
      "  iterations_since_restore: 501\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.37297297297298\n",
      "    ram_util_percent: 33.64324324324324\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03671155198573104\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.809234827251586\n",
      "    mean_inference_ms: 2.113635198838139\n",
      "    mean_raw_obs_processing_ms: 24.667095822962665\n",
      "  time_since_restore: 24193.971709251404\n",
      "  time_this_iter_s: 78.27089238166809\n",
      "  time_total_s: 24193.971709251404\n",
      "  timers:\n",
      "    learn_throughput: 1325.382\n",
      "    learn_time_ms: 754.5\n",
      "    load_throughput: 41736.818\n",
      "    load_time_ms: 23.96\n",
      "    sample_throughput: 8.514\n",
      "    sample_time_ms: 117459.59\n",
      "    update_time_ms: 3.625\n",
      "  timestamp: 1635306776\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 501000\n",
      "  training_iteration: 501\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   501</td><td style=\"text-align: right;\">           24194</td><td style=\"text-align: right;\">501000</td><td style=\"text-align: right;\">   4.351</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -6.48</td><td style=\"text-align: right;\">            126.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 502000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-53-55\n",
      "  done: false\n",
      "  episode_len_mean: 123.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 4.388700000000013\n",
      "  episode_reward_min: -6.479999999999947\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1901\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0386825932396784\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006158267395478189\n",
      "          policy_loss: -0.06335948237942325\n",
      "          total_loss: -0.06393844092057811\n",
      "          vf_explained_var: 0.24595235288143158\n",
      "          vf_loss: 0.018364769031500652\n",
      "    num_agent_steps_sampled: 502000\n",
      "    num_agent_steps_trained: 502000\n",
      "    num_steps_sampled: 502000\n",
      "    num_steps_trained: 502000\n",
      "  iterations_since_restore: 502\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.821176470588235\n",
      "    ram_util_percent: 33.74000000000001\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03671217801464844\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.808816023603832\n",
      "    mean_inference_ms: 2.113655099867086\n",
      "    mean_raw_obs_processing_ms: 24.743478955334194\n",
      "  time_since_restore: 24252.987679481506\n",
      "  time_this_iter_s: 59.01597023010254\n",
      "  time_total_s: 24252.987679481506\n",
      "  timers:\n",
      "    learn_throughput: 1325.745\n",
      "    learn_time_ms: 754.293\n",
      "    load_throughput: 41683.726\n",
      "    load_time_ms: 23.99\n",
      "    sample_throughput: 9.796\n",
      "    sample_time_ms: 102087.264\n",
      "    update_time_ms: 3.631\n",
      "  timestamp: 1635306835\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 502000\n",
      "  training_iteration: 502\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   502</td><td style=\"text-align: right;\">           24253</td><td style=\"text-align: right;\">502000</td><td style=\"text-align: right;\">  4.3887</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -6.48</td><td style=\"text-align: right;\">            123.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 503000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-55-51\n",
      "  done: false\n",
      "  episode_len_mean: 134.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 4.040500000000014\n",
      "  episode_reward_min: -6.479999999999947\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1909\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8308530264430576\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016129889024457632\n",
      "          policy_loss: -0.10998514158030351\n",
      "          total_loss: -0.06841217105587323\n",
      "          vf_explained_var: 0.7315248847007751\n",
      "          vf_loss: 0.056101693916651936\n",
      "    num_agent_steps_sampled: 503000\n",
      "    num_agent_steps_trained: 503000\n",
      "    num_steps_sampled: 503000\n",
      "    num_steps_trained: 503000\n",
      "  iterations_since_restore: 503\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.89333333333333\n",
      "    ram_util_percent: 33.66121212121211\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03671349967302152\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.808679923506087\n",
      "    mean_inference_ms: 2.1136976629907234\n",
      "    mean_raw_obs_processing_ms: 24.883923099857704\n",
      "  time_since_restore: 24368.820733308792\n",
      "  time_this_iter_s: 115.83305382728577\n",
      "  time_total_s: 24368.820733308792\n",
      "  timers:\n",
      "    learn_throughput: 1328.689\n",
      "    learn_time_ms: 752.622\n",
      "    load_throughput: 41584.251\n",
      "    load_time_ms: 24.048\n",
      "    sample_throughput: 9.283\n",
      "    sample_time_ms: 107719.073\n",
      "    update_time_ms: 3.627\n",
      "  timestamp: 1635306951\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 503000\n",
      "  training_iteration: 503\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   503</td><td style=\"text-align: right;\">         24368.8</td><td style=\"text-align: right;\">503000</td><td style=\"text-align: right;\">  4.0405</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -6.48</td><td style=\"text-align: right;\">            134.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 504000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-56-35\n",
      "  done: false\n",
      "  episode_len_mean: 139.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 3.8629000000000144\n",
      "  episode_reward_min: -6.479999999999947\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1913\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7226097067197164\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016148218236936154\n",
      "          policy_loss: 0.044485946744680406\n",
      "          total_loss: 0.21865740211473572\n",
      "          vf_explained_var: 0.6650314927101135\n",
      "          vf_loss: 0.18761344804531999\n",
      "    num_agent_steps_sampled: 504000\n",
      "    num_agent_steps_trained: 504000\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "  iterations_since_restore: 504\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.6079365079365\n",
      "    ram_util_percent: 33.72380952380952\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036714224800312195\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.80895719085238\n",
      "    mean_inference_ms: 2.1137210380594946\n",
      "    mean_raw_obs_processing_ms: 24.953473906237146\n",
      "  time_since_restore: 24413.17634868622\n",
      "  time_this_iter_s: 44.35561537742615\n",
      "  time_total_s: 24413.17634868622\n",
      "  timers:\n",
      "    learn_throughput: 1329.098\n",
      "    learn_time_ms: 752.39\n",
      "    load_throughput: 41438.894\n",
      "    load_time_ms: 24.132\n",
      "    sample_throughput: 9.729\n",
      "    sample_time_ms: 102787.992\n",
      "    update_time_ms: 3.63\n",
      "  timestamp: 1635306995\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 504\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   504</td><td style=\"text-align: right;\">         24413.2</td><td style=\"text-align: right;\">504000</td><td style=\"text-align: right;\">  3.8629</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -6.48</td><td style=\"text-align: right;\">            139.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 505000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-57-55\n",
      "  done: false\n",
      "  episode_len_mean: 146.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 3.7306000000000155\n",
      "  episode_reward_min: -6.479999999999947\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1918\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8532409773932563\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009304266902350219\n",
      "          policy_loss: -0.014569288078281615\n",
      "          total_loss: 0.006047972043355306\n",
      "          vf_explained_var: 0.7250693440437317\n",
      "          vf_loss: 0.036969347911265986\n",
      "    num_agent_steps_sampled: 505000\n",
      "    num_agent_steps_trained: 505000\n",
      "    num_steps_sampled: 505000\n",
      "    num_steps_trained: 505000\n",
      "  iterations_since_restore: 505\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.55175438596491\n",
      "    ram_util_percent: 33.784210526315796\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03671516703063497\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.809479314753897\n",
      "    mean_inference_ms: 2.1137525264965324\n",
      "    mean_raw_obs_processing_ms: 25.04158750115619\n",
      "  time_since_restore: 24492.79308629036\n",
      "  time_this_iter_s: 79.61673760414124\n",
      "  time_total_s: 24492.79308629036\n",
      "  timers:\n",
      "    learn_throughput: 1329.295\n",
      "    learn_time_ms: 752.279\n",
      "    load_throughput: 42767.468\n",
      "    load_time_ms: 23.382\n",
      "    sample_throughput: 10.428\n",
      "    sample_time_ms: 95899.842\n",
      "    update_time_ms: 3.623\n",
      "  timestamp: 1635307075\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 505000\n",
      "  training_iteration: 505\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   505</td><td style=\"text-align: right;\">         24492.8</td><td style=\"text-align: right;\">505000</td><td style=\"text-align: right;\">  3.7306</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -6.48</td><td style=\"text-align: right;\">            146.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 506000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_03-58-53\n",
      "  done: false\n",
      "  episode_len_mean: 154.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 3.5048000000000155\n",
      "  episode_reward_min: -6.479999999999947\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1923\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7304859823650784\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008503102749936767\n",
      "          policy_loss: -0.04455023668706417\n",
      "          total_loss: -0.0439220421637098\n",
      "          vf_explained_var: 0.8572321534156799\n",
      "          vf_loss: 0.01594047813107156\n",
      "    num_agent_steps_sampled: 506000\n",
      "    num_agent_steps_trained: 506000\n",
      "    num_steps_sampled: 506000\n",
      "    num_steps_trained: 506000\n",
      "  iterations_since_restore: 506\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.60000000000001\n",
      "    ram_util_percent: 33.76309523809524\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036716105999808846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.810043611764094\n",
      "    mean_inference_ms: 2.113784427260143\n",
      "    mean_raw_obs_processing_ms: 25.123714012707715\n",
      "  time_since_restore: 24551.421963214874\n",
      "  time_this_iter_s: 58.62887692451477\n",
      "  time_total_s: 24551.421963214874\n",
      "  timers:\n",
      "    learn_throughput: 1328.674\n",
      "    learn_time_ms: 752.63\n",
      "    load_throughput: 42572.863\n",
      "    load_time_ms: 23.489\n",
      "    sample_throughput: 13.626\n",
      "    sample_time_ms: 73387.349\n",
      "    update_time_ms: 3.63\n",
      "  timestamp: 1635307133\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 506000\n",
      "  training_iteration: 506\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   506</td><td style=\"text-align: right;\">         24551.4</td><td style=\"text-align: right;\">506000</td><td style=\"text-align: right;\">  3.5048</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -6.48</td><td style=\"text-align: right;\">            154.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 507000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-00-48\n",
      "  done: false\n",
      "  episode_len_mean: 155.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 3.623100000000017\n",
      "  episode_reward_min: -6.479999999999947\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1930\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9211722983254327\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014018040683290461\n",
      "          policy_loss: 0.07322831145591206\n",
      "          total_loss: 0.08776650329430898\n",
      "          vf_explained_var: 0.7767441868782043\n",
      "          vf_loss: 0.03046499252733257\n",
      "    num_agent_steps_sampled: 507000\n",
      "    num_agent_steps_trained: 507000\n",
      "    num_steps_sampled: 507000\n",
      "    num_steps_trained: 507000\n",
      "  iterations_since_restore: 507\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.5079268292683\n",
      "    ram_util_percent: 33.72012195121951\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367174374757991\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.81049634785351\n",
      "    mean_inference_ms: 2.113827809602404\n",
      "    mean_raw_obs_processing_ms: 25.231335739598066\n",
      "  time_since_restore: 24666.302736520767\n",
      "  time_this_iter_s: 114.88077330589294\n",
      "  time_total_s: 24666.302736520767\n",
      "  timers:\n",
      "    learn_throughput: 1329.319\n",
      "    learn_time_ms: 752.265\n",
      "    load_throughput: 42620.19\n",
      "    load_time_ms: 23.463\n",
      "    sample_throughput: 12.611\n",
      "    sample_time_ms: 79298.174\n",
      "    update_time_ms: 3.637\n",
      "  timestamp: 1635307248\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 507000\n",
      "  training_iteration: 507\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   507</td><td style=\"text-align: right;\">         24666.3</td><td style=\"text-align: right;\">507000</td><td style=\"text-align: right;\">  3.6231</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -6.48</td><td style=\"text-align: right;\">            155.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 508000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-01-50\n",
      "  done: false\n",
      "  episode_len_mean: 159.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 3.396100000000017\n",
      "  episode_reward_min: -7.41999999999992\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1934\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7961143175760905\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0115375783685269\n",
      "          policy_loss: -0.0011698911328696542\n",
      "          total_loss: 0.08558185001214345\n",
      "          vf_explained_var: 0.3013874590396881\n",
      "          vf_loss: 0.10200921764804258\n",
      "    num_agent_steps_sampled: 508000\n",
      "    num_agent_steps_trained: 508000\n",
      "    num_steps_sampled: 508000\n",
      "    num_steps_trained: 508000\n",
      "  iterations_since_restore: 508\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.40229885057471\n",
      "    ram_util_percent: 33.73448275862068\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03671822751488356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.81080582453516\n",
      "    mean_inference_ms: 2.113852834058978\n",
      "    mean_raw_obs_processing_ms: 25.289931578221548\n",
      "  time_since_restore: 24727.657172203064\n",
      "  time_this_iter_s: 61.35443568229675\n",
      "  time_total_s: 24727.657172203064\n",
      "  timers:\n",
      "    learn_throughput: 1330.487\n",
      "    learn_time_ms: 751.605\n",
      "    load_throughput: 42372.143\n",
      "    load_time_ms: 23.6\n",
      "    sample_throughput: 13.172\n",
      "    sample_time_ms: 75919.217\n",
      "    update_time_ms: 3.612\n",
      "  timestamp: 1635307310\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 508000\n",
      "  training_iteration: 508\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   508</td><td style=\"text-align: right;\">         24727.7</td><td style=\"text-align: right;\">508000</td><td style=\"text-align: right;\">  3.3961</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -7.42</td><td style=\"text-align: right;\">            159.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 509000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-03-23\n",
      "  done: false\n",
      "  episode_len_mean: 160.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 3.3864000000000174\n",
      "  episode_reward_min: -7.41999999999992\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1940\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8876665274302165\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018012005214566675\n",
      "          policy_loss: -0.029409504267904493\n",
      "          total_loss: 0.126394767810901\n",
      "          vf_explained_var: 0.8013403415679932\n",
      "          vf_loss: 0.17046008439113697\n",
      "    num_agent_steps_sampled: 509000\n",
      "    num_agent_steps_trained: 509000\n",
      "    num_steps_sampled: 509000\n",
      "    num_steps_trained: 509000\n",
      "  iterations_since_restore: 509\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.54179104477612\n",
      "    ram_util_percent: 33.7544776119403\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03671943777835104\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.811127670818713\n",
      "    mean_inference_ms: 2.113890611130253\n",
      "    mean_raw_obs_processing_ms: 25.382508129683536\n",
      "  time_since_restore: 24821.196206092834\n",
      "  time_this_iter_s: 93.53903388977051\n",
      "  time_total_s: 24821.196206092834\n",
      "  timers:\n",
      "    learn_throughput: 1332.725\n",
      "    learn_time_ms: 750.342\n",
      "    load_throughput: 42012.024\n",
      "    load_time_ms: 23.803\n",
      "    sample_throughput: 12.298\n",
      "    sample_time_ms: 81312.925\n",
      "    update_time_ms: 3.651\n",
      "  timestamp: 1635307403\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 509000\n",
      "  training_iteration: 509\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   509</td><td style=\"text-align: right;\">         24821.2</td><td style=\"text-align: right;\">509000</td><td style=\"text-align: right;\">  3.3864</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -7.42</td><td style=\"text-align: right;\">            160.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 510000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-04-59\n",
      "  done: false\n",
      "  episode_len_mean: 163.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 3.3164000000000184\n",
      "  episode_reward_min: -7.41999999999992\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1946\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.734250557422638\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01345690519011041\n",
      "          policy_loss: -0.05459748374091254\n",
      "          total_loss: -0.0005368390017085605\n",
      "          vf_explained_var: 0.6402098536491394\n",
      "          vf_loss: 0.06824971815286618\n",
      "    num_agent_steps_sampled: 510000\n",
      "    num_agent_steps_trained: 510000\n",
      "    num_steps_sampled: 510000\n",
      "    num_steps_trained: 510000\n",
      "  iterations_since_restore: 510\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.63308823529411\n",
      "    ram_util_percent: 33.73455882352941\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672061134960164\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.811786643065435\n",
      "    mean_inference_ms: 2.113926767936434\n",
      "    mean_raw_obs_processing_ms: 25.474826188898096\n",
      "  time_since_restore: 24916.926512241364\n",
      "  time_this_iter_s: 95.73030614852905\n",
      "  time_total_s: 24916.926512241364\n",
      "  timers:\n",
      "    learn_throughput: 1332.395\n",
      "    learn_time_ms: 750.528\n",
      "    load_throughput: 42282.782\n",
      "    load_time_ms: 23.65\n",
      "    sample_throughput: 12.604\n",
      "    sample_time_ms: 79340.383\n",
      "    update_time_ms: 3.661\n",
      "  timestamp: 1635307499\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 510000\n",
      "  training_iteration: 510\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   510</td><td style=\"text-align: right;\">         24916.9</td><td style=\"text-align: right;\">510000</td><td style=\"text-align: right;\">  3.3164</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -7.42</td><td style=\"text-align: right;\">            163.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 511000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-06-16\n",
      "  done: false\n",
      "  episode_len_mean: 157.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 3.475800000000017\n",
      "  episode_reward_min: -7.41999999999992\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1951\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.790405445628696\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010356828821405392\n",
      "          policy_loss: -0.041767064999375075\n",
      "          total_loss: 0.0006847299635410308\n",
      "          vf_explained_var: 0.8813321590423584\n",
      "          vf_loss: 0.05792887955904007\n",
      "    num_agent_steps_sampled: 511000\n",
      "    num_agent_steps_trained: 511000\n",
      "    num_steps_sampled: 511000\n",
      "    num_steps_trained: 511000\n",
      "  iterations_since_restore: 511\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.37090909090909\n",
      "    ram_util_percent: 33.74\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672158717485296\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.812720215887587\n",
      "    mean_inference_ms: 2.1139573127755766\n",
      "    mean_raw_obs_processing_ms: 25.545183601106004\n",
      "  time_since_restore: 24994.110469579697\n",
      "  time_this_iter_s: 77.18395733833313\n",
      "  time_total_s: 24994.110469579697\n",
      "  timers:\n",
      "    learn_throughput: 1330.346\n",
      "    learn_time_ms: 751.684\n",
      "    load_throughput: 42258.755\n",
      "    load_time_ms: 23.664\n",
      "    sample_throughput: 12.621\n",
      "    sample_time_ms: 79230.518\n",
      "    update_time_ms: 3.662\n",
      "  timestamp: 1635307576\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 511000\n",
      "  training_iteration: 511\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   511</td><td style=\"text-align: right;\">         24994.1</td><td style=\"text-align: right;\">511000</td><td style=\"text-align: right;\">  3.4758</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -7.42</td><td style=\"text-align: right;\">            157.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 512000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-07-52\n",
      "  done: false\n",
      "  episode_len_mean: 166.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.2347000000000175\n",
      "  episode_reward_min: -7.41999999999992\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1958\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6709901319609748\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009785129161394455\n",
      "          policy_loss: -0.008966950409942203\n",
      "          total_loss: -0.0065898785160647495\n",
      "          vf_explained_var: 0.7510635256767273\n",
      "          vf_loss: 0.016793970867810357\n",
      "    num_agent_steps_sampled: 512000\n",
      "    num_agent_steps_trained: 512000\n",
      "    num_steps_sampled: 512000\n",
      "    num_steps_trained: 512000\n",
      "  iterations_since_restore: 512\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.48905109489051\n",
      "    ram_util_percent: 33.72554744525547\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672298442826077\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.814106085359153\n",
      "    mean_inference_ms: 2.11400099231791\n",
      "    mean_raw_obs_processing_ms: 25.637590614047784\n",
      "  time_since_restore: 25089.641419887543\n",
      "  time_this_iter_s: 95.53095030784607\n",
      "  time_total_s: 25089.641419887543\n",
      "  timers:\n",
      "    learn_throughput: 1330.356\n",
      "    learn_time_ms: 751.679\n",
      "    load_throughput: 42527.668\n",
      "    load_time_ms: 23.514\n",
      "    sample_throughput: 12.065\n",
      "    sample_time_ms: 82882.073\n",
      "    update_time_ms: 3.776\n",
      "  timestamp: 1635307672\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 512000\n",
      "  training_iteration: 512\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   512</td><td style=\"text-align: right;\">         25089.6</td><td style=\"text-align: right;\">512000</td><td style=\"text-align: right;\">  3.2347</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -7.42</td><td style=\"text-align: right;\">            166.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 513000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-09-25\n",
      "  done: false\n",
      "  episode_len_mean: 174.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 2.9963000000000193\n",
      "  episode_reward_min: -7.41999999999992\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1964\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6558560623062981\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004651476424901683\n",
      "          policy_loss: -0.110961188789871\n",
      "          total_loss: -0.11731641391913096\n",
      "          vf_explained_var: 0.11152383685112\n",
      "          vf_loss: 0.00911332953773025\n",
      "    num_agent_steps_sampled: 513000\n",
      "    num_agent_steps_trained: 513000\n",
      "    num_steps_sampled: 513000\n",
      "    num_steps_trained: 513000\n",
      "  iterations_since_restore: 513\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.97537313432835\n",
      "    ram_util_percent: 33.79626865671642\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036724191733436336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.81488796960964\n",
      "    mean_inference_ms: 2.1140370717034984\n",
      "    mean_raw_obs_processing_ms: 25.70202252447145\n",
      "  time_since_restore: 25183.36125779152\n",
      "  time_this_iter_s: 93.71983790397644\n",
      "  time_total_s: 25183.36125779152\n",
      "  timers:\n",
      "    learn_throughput: 1327.991\n",
      "    learn_time_ms: 753.017\n",
      "    load_throughput: 42609.062\n",
      "    load_time_ms: 23.469\n",
      "    sample_throughput: 12.396\n",
      "    sample_time_ms: 80669.467\n",
      "    update_time_ms: 3.781\n",
      "  timestamp: 1635307765\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 513000\n",
      "  training_iteration: 513\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   513</td><td style=\"text-align: right;\">         25183.4</td><td style=\"text-align: right;\">513000</td><td style=\"text-align: right;\">  2.9963</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -7.42</td><td style=\"text-align: right;\">            174.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 514000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-12-18\n",
      "  done: false\n",
      "  episode_len_mean: 171.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 3.15730000000002\n",
      "  episode_reward_min: -7.41999999999992\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1973\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11716770936054664\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6552539653248257\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009830383066959058\n",
      "          policy_loss: -0.046159467101097106\n",
      "          total_loss: -0.03433456996248828\n",
      "          vf_explained_var: 0.8982795476913452\n",
      "          vf_loss: 0.027225635535756333\n",
      "    num_agent_steps_sampled: 514000\n",
      "    num_agent_steps_trained: 514000\n",
      "    num_steps_sampled: 514000\n",
      "    num_steps_trained: 514000\n",
      "  iterations_since_restore: 514\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.13170731707318\n",
      "    ram_util_percent: 33.78414634146342\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672607628301874\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.816000252288116\n",
      "    mean_inference_ms: 2.114092167236988\n",
      "    mean_raw_obs_processing_ms: 25.820184739930525\n",
      "  time_since_restore: 25355.852162122726\n",
      "  time_this_iter_s: 172.49090433120728\n",
      "  time_total_s: 25355.852162122726\n",
      "  timers:\n",
      "    learn_throughput: 1330.47\n",
      "    learn_time_ms: 751.614\n",
      "    load_throughput: 42708.939\n",
      "    load_time_ms: 23.414\n",
      "    sample_throughput: 10.697\n",
      "    sample_time_ms: 93484.475\n",
      "    update_time_ms: 3.776\n",
      "  timestamp: 1635307938\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 514000\n",
      "  training_iteration: 514\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   514</td><td style=\"text-align: right;\">         25355.9</td><td style=\"text-align: right;\">514000</td><td style=\"text-align: right;\">  3.1573</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -7.42</td><td style=\"text-align: right;\">            171.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 515000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-13-14\n",
      "  done: false\n",
      "  episode_len_mean: 176.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 3.0528000000000195\n",
      "  episode_reward_min: -7.41999999999992\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1978\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11716770936054664\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8239097250832452\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014715776629104147\n",
      "          policy_loss: 0.015002118547757466\n",
      "          total_loss: 0.015655133417911\n",
      "          vf_explained_var: 0.3191334307193756\n",
      "          vf_loss: 0.017167898752894768\n",
      "    num_agent_steps_sampled: 515000\n",
      "    num_agent_steps_trained: 515000\n",
      "    num_steps_sampled: 515000\n",
      "    num_steps_trained: 515000\n",
      "  iterations_since_restore: 515\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.94124999999999\n",
      "    ram_util_percent: 33.722500000000004\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672708362083588\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.81662940267036\n",
      "    mean_inference_ms: 2.1141220899707287\n",
      "    mean_raw_obs_processing_ms: 25.883926798675166\n",
      "  time_since_restore: 25412.33164525032\n",
      "  time_this_iter_s: 56.479483127593994\n",
      "  time_total_s: 25412.33164525032\n",
      "  timers:\n",
      "    learn_throughput: 1330.008\n",
      "    learn_time_ms: 751.875\n",
      "    load_throughput: 41811.126\n",
      "    load_time_ms: 23.917\n",
      "    sample_throughput: 10.969\n",
      "    sample_time_ms: 91170.003\n",
      "    update_time_ms: 3.774\n",
      "  timestamp: 1635307994\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 515000\n",
      "  training_iteration: 515\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   515</td><td style=\"text-align: right;\">         25412.3</td><td style=\"text-align: right;\">515000</td><td style=\"text-align: right;\">  3.0528</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -7.42</td><td style=\"text-align: right;\">             176.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 516000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-15-10\n",
      "  done: false\n",
      "  episode_len_mean: 173.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 3.1430000000000193\n",
      "  episode_reward_min: -7.41999999999992\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1985\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11716770936054664\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0074107196595934\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020457840514862685\n",
      "          policy_loss: 0.01694389631350835\n",
      "          total_loss: 0.10341583084728982\n",
      "          vf_explained_var: 0.7995386123657227\n",
      "          vf_loss: 0.10414904549510942\n",
      "    num_agent_steps_sampled: 516000\n",
      "    num_agent_steps_trained: 516000\n",
      "    num_steps_sampled: 516000\n",
      "    num_steps_trained: 516000\n",
      "  iterations_since_restore: 516\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.96727272727272\n",
      "    ram_util_percent: 33.793939393939404\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036728504547969144\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.81775714571495\n",
      "    mean_inference_ms: 2.114165761807172\n",
      "    mean_raw_obs_processing_ms: 25.978373122713815\n",
      "  time_since_restore: 25527.431573152542\n",
      "  time_this_iter_s: 115.09992790222168\n",
      "  time_total_s: 25527.431573152542\n",
      "  timers:\n",
      "    learn_throughput: 1328.752\n",
      "    learn_time_ms: 752.586\n",
      "    load_throughput: 41781.804\n",
      "    load_time_ms: 23.934\n",
      "    sample_throughput: 10.329\n",
      "    sample_time_ms: 96816.345\n",
      "    update_time_ms: 3.776\n",
      "  timestamp: 1635308110\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 516000\n",
      "  training_iteration: 516\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   516</td><td style=\"text-align: right;\">         25527.4</td><td style=\"text-align: right;\">516000</td><td style=\"text-align: right;\">   3.143</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -7.42</td><td style=\"text-align: right;\">            173.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 517000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-16-44\n",
      "  done: false\n",
      "  episode_len_mean: 173.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 3.1610000000000196\n",
      "  episode_reward_min: -7.41999999999992\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1991\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.17575156404082\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9808706707424588\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008071203643953594\n",
      "          policy_loss: 0.21811735332012178\n",
      "          total_loss: 0.2041392571396298\n",
      "          vf_explained_var: 0.8349637389183044\n",
      "          vf_loss: 0.004412082052375707\n",
      "    num_agent_steps_sampled: 517000\n",
      "    num_agent_steps_trained: 517000\n",
      "    num_steps_sampled: 517000\n",
      "    num_steps_trained: 517000\n",
      "  iterations_since_restore: 517\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.06268656716418\n",
      "    ram_util_percent: 33.816417910447775\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672968826299871\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.818373242075896\n",
      "    mean_inference_ms: 2.1142018692942712\n",
      "    mean_raw_obs_processing_ms: 26.057388912592323\n",
      "  time_since_restore: 25621.797820806503\n",
      "  time_this_iter_s: 94.36624765396118\n",
      "  time_total_s: 25621.797820806503\n",
      "  timers:\n",
      "    learn_throughput: 1329.98\n",
      "    learn_time_ms: 751.891\n",
      "    load_throughput: 41863.04\n",
      "    load_time_ms: 23.887\n",
      "    sample_throughput: 10.552\n",
      "    sample_time_ms: 94765.626\n",
      "    update_time_ms: 3.772\n",
      "  timestamp: 1635308204\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 517000\n",
      "  training_iteration: 517\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   517</td><td style=\"text-align: right;\">         25621.8</td><td style=\"text-align: right;\">517000</td><td style=\"text-align: right;\">   3.161</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -7.42</td><td style=\"text-align: right;\">            173.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 518000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-17-07\n",
      "  done: false\n",
      "  episode_len_mean: 177.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 2.9718000000000195\n",
      "  episode_reward_min: -9.929999999999936\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1993\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.17575156404082\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8524173312717014\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.030196591494613283\n",
      "          policy_loss: 0.07551500482691659\n",
      "          total_loss: 0.4584478014873134\n",
      "          vf_explained_var: 0.5816366672515869\n",
      "          vf_loss: 0.39614986230929694\n",
      "    num_agent_steps_sampled: 518000\n",
      "    num_agent_steps_trained: 518000\n",
      "    num_steps_sampled: 518000\n",
      "    num_steps_trained: 518000\n",
      "  iterations_since_restore: 518\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.39696969696969\n",
      "    ram_util_percent: 33.89393939393939\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673009379945204\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.81858124645909\n",
      "    mean_inference_ms: 2.1142142381635494\n",
      "    mean_raw_obs_processing_ms: 26.08209319287689\n",
      "  time_since_restore: 25644.61899447441\n",
      "  time_this_iter_s: 22.821173667907715\n",
      "  time_total_s: 25644.61899447441\n",
      "  timers:\n",
      "    learn_throughput: 1330.203\n",
      "    learn_time_ms: 751.765\n",
      "    load_throughput: 42087.273\n",
      "    load_time_ms: 23.76\n",
      "    sample_throughput: 11.0\n",
      "    sample_time_ms: 90912.569\n",
      "    update_time_ms: 3.752\n",
      "  timestamp: 1635308227\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 518000\n",
      "  training_iteration: 518\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   518</td><td style=\"text-align: right;\">         25644.6</td><td style=\"text-align: right;\">518000</td><td style=\"text-align: right;\">  2.9718</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -9.93</td><td style=\"text-align: right;\">            177.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 519000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-21-50\n",
      "  done: false\n",
      "  episode_len_mean: 160.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 3.508700000000017\n",
      "  episode_reward_min: -9.929999999999936\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 2009\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.26362734606122995\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7244225687450834\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.07533666259501891\n",
      "          policy_loss: 0.08105358117156558\n",
      "          total_loss: 0.6513787364794148\n",
      "          vf_explained_var: 0.7926180362701416\n",
      "          vf_loss: 0.5677085820171568\n",
      "    num_agent_steps_sampled: 519000\n",
      "    num_agent_steps_trained: 519000\n",
      "    num_steps_sampled: 519000\n",
      "    num_steps_trained: 519000\n",
      "  iterations_since_restore: 519\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.306188118811875\n",
      "    ram_util_percent: 33.87475247524752\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673314323825935\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.82017074803139\n",
      "    mean_inference_ms: 2.114309003405685\n",
      "    mean_raw_obs_processing_ms: 26.333744758371246\n",
      "  time_since_restore: 25927.806809425354\n",
      "  time_this_iter_s: 283.187814950943\n",
      "  time_total_s: 25927.806809425354\n",
      "  timers:\n",
      "    learn_throughput: 1330.91\n",
      "    learn_time_ms: 751.366\n",
      "    load_throughput: 42231.524\n",
      "    load_time_ms: 23.679\n",
      "    sample_throughput: 9.101\n",
      "    sample_time_ms: 109877.929\n",
      "    update_time_ms: 3.768\n",
      "  timestamp: 1635308510\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 519000\n",
      "  training_iteration: 519\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   519</td><td style=\"text-align: right;\">         25927.8</td><td style=\"text-align: right;\">519000</td><td style=\"text-align: right;\">  3.5087</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -9.93</td><td style=\"text-align: right;\">            160.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 520000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-23-07\n",
      "  done: false\n",
      "  episode_len_mean: 155.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 3.603500000000017\n",
      "  episode_reward_min: -9.929999999999936\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 2014\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.39544101909184504\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.011595768398709\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01219745425230118\n",
      "          policy_loss: 0.03952701220081912\n",
      "          total_loss: 0.058812432611982025\n",
      "          vf_explained_var: 0.5599509477615356\n",
      "          vf_loss: 0.03457800517272618\n",
      "    num_agent_steps_sampled: 520000\n",
      "    num_agent_steps_trained: 520000\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "  iterations_since_restore: 520\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.603603603603595\n",
      "    ram_util_percent: 33.88288288288288\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673402245013999\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.82006128776653\n",
      "    mean_inference_ms: 2.1143354971297446\n",
      "    mean_raw_obs_processing_ms: 26.411796457264714\n",
      "  time_since_restore: 26005.197422742844\n",
      "  time_this_iter_s: 77.39061331748962\n",
      "  time_total_s: 26005.197422742844\n",
      "  timers:\n",
      "    learn_throughput: 1329.33\n",
      "    learn_time_ms: 752.259\n",
      "    load_throughput: 41789.214\n",
      "    load_time_ms: 23.93\n",
      "    sample_throughput: 9.256\n",
      "    sample_time_ms: 108042.814\n",
      "    update_time_ms: 3.856\n",
      "  timestamp: 1635308587\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 520\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   520</td><td style=\"text-align: right;\">         26005.2</td><td style=\"text-align: right;\">520000</td><td style=\"text-align: right;\">  3.6035</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -9.93</td><td style=\"text-align: right;\">             155.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 521000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-25-24\n",
      "  done: false\n",
      "  episode_len_mean: 151.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 3.780000000000017\n",
      "  episode_reward_min: -9.929999999999936\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2022\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.39544101909184504\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9168968266910977\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009378479774427954\n",
      "          policy_loss: -0.12268908553653293\n",
      "          total_loss: -0.10075493794348504\n",
      "          vf_explained_var: 0.9935054183006287\n",
      "          vf_loss: 0.03739447996227278\n",
      "    num_agent_steps_sampled: 521000\n",
      "    num_agent_steps_trained: 521000\n",
      "    num_steps_sampled: 521000\n",
      "    num_steps_trained: 521000\n",
      "  iterations_since_restore: 521\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.240512820512826\n",
      "    ram_util_percent: 33.86102564102563\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673541378121033\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.82017234961376\n",
      "    mean_inference_ms: 2.114376512517824\n",
      "    mean_raw_obs_processing_ms: 26.544968568846535\n",
      "  time_since_restore: 26142.006346940994\n",
      "  time_this_iter_s: 136.80892419815063\n",
      "  time_total_s: 26142.006346940994\n",
      "  timers:\n",
      "    learn_throughput: 1331.912\n",
      "    learn_time_ms: 750.8\n",
      "    load_throughput: 41791.296\n",
      "    load_time_ms: 23.928\n",
      "    sample_throughput: 8.771\n",
      "    sample_time_ms: 114006.782\n",
      "    update_time_ms: 3.857\n",
      "  timestamp: 1635308724\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 521000\n",
      "  training_iteration: 521\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   521</td><td style=\"text-align: right;\">           26142</td><td style=\"text-align: right;\">521000</td><td style=\"text-align: right;\">    3.78</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -9.93</td><td style=\"text-align: right;\">            151.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 522000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-26-45\n",
      "  done: false\n",
      "  episode_len_mean: 150.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 3.806500000000016\n",
      "  episode_reward_min: -9.929999999999936\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 2028\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.39544101909184504\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.19668882422977\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016118189455191098\n",
      "          policy_loss: -0.08935538588298692\n",
      "          total_loss: 0.011137964824835459\n",
      "          vf_explained_var: 0.3352469503879547\n",
      "          vf_loss: 0.11608644628690348\n",
      "    num_agent_steps_sampled: 522000\n",
      "    num_agent_steps_trained: 522000\n",
      "    num_steps_sampled: 522000\n",
      "    num_steps_trained: 522000\n",
      "  iterations_since_restore: 522\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.23391304347826\n",
      "    ram_util_percent: 33.88521739130434\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036736444508800384\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.820666384811403\n",
      "    mean_inference_ms: 2.1144078862652522\n",
      "    mean_raw_obs_processing_ms: 26.640788939023473\n",
      "  time_since_restore: 26222.62784600258\n",
      "  time_this_iter_s: 80.62149906158447\n",
      "  time_total_s: 26222.62784600258\n",
      "  timers:\n",
      "    learn_throughput: 1331.424\n",
      "    learn_time_ms: 751.076\n",
      "    load_throughput: 41729.386\n",
      "    load_time_ms: 23.964\n",
      "    sample_throughput: 8.888\n",
      "    sample_time_ms: 112515.612\n",
      "    update_time_ms: 3.773\n",
      "  timestamp: 1635308805\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 522000\n",
      "  training_iteration: 522\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   522</td><td style=\"text-align: right;\">         26222.6</td><td style=\"text-align: right;\">522000</td><td style=\"text-align: right;\">  3.8065</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -9.93</td><td style=\"text-align: right;\">            150.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 523000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-27-08\n",
      "  done: false\n",
      "  episode_len_mean: 158.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 3.527400000000018\n",
      "  episode_reward_min: -9.929999999999936\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2030\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.39544101909184504\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1514730758137173\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010939596763854966\n",
      "          policy_loss: -0.08786425747805171\n",
      "          total_loss: -0.08375097894006306\n",
      "          vf_explained_var: 0.12462367862462997\n",
      "          vf_loss: 0.0213020462801473\n",
      "    num_agent_steps_sampled: 523000\n",
      "    num_agent_steps_trained: 523000\n",
      "    num_steps_sampled: 523000\n",
      "    num_steps_trained: 523000\n",
      "  iterations_since_restore: 523\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.4090909090909\n",
      "    ram_util_percent: 33.93636363636363\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673682174260501\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.820825506575485\n",
      "    mean_inference_ms: 2.114419309415555\n",
      "    mean_raw_obs_processing_ms: 26.6712567331647\n",
      "  time_since_restore: 26245.645312309265\n",
      "  time_this_iter_s: 23.0174663066864\n",
      "  time_total_s: 26245.645312309265\n",
      "  timers:\n",
      "    learn_throughput: 1330.26\n",
      "    learn_time_ms: 751.733\n",
      "    load_throughput: 42020.863\n",
      "    load_time_ms: 23.798\n",
      "    sample_throughput: 9.484\n",
      "    sample_time_ms: 105444.883\n",
      "    update_time_ms: 3.773\n",
      "  timestamp: 1635308828\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 523000\n",
      "  training_iteration: 523\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   523</td><td style=\"text-align: right;\">         26245.6</td><td style=\"text-align: right;\">523000</td><td style=\"text-align: right;\">  3.5274</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -9.93</td><td style=\"text-align: right;\">            158.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 524000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-28-26\n",
      "  done: false\n",
      "  episode_len_mean: 154.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 3.6969000000000163\n",
      "  episode_reward_min: -9.929999999999936\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 2035\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.39544101909184504\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0871734499931334\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010172789666101422\n",
      "          policy_loss: -0.04685737159517076\n",
      "          total_loss: -0.05493523681329356\n",
      "          vf_explained_var: 0.7680670022964478\n",
      "          vf_loss: 0.008771131751644942\n",
      "    num_agent_steps_sampled: 524000\n",
      "    num_agent_steps_trained: 524000\n",
      "    num_steps_sampled: 524000\n",
      "    num_steps_trained: 524000\n",
      "  iterations_since_restore: 524\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.17927927927929\n",
      "    ram_util_percent: 33.788288288288285\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036737752140582926\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.82109957979429\n",
      "    mean_inference_ms: 2.1144483428721004\n",
      "    mean_raw_obs_processing_ms: 26.748048500073004\n",
      "  time_since_restore: 26323.41073822975\n",
      "  time_this_iter_s: 77.76542592048645\n",
      "  time_total_s: 26323.41073822975\n",
      "  timers:\n",
      "    learn_throughput: 1328.396\n",
      "    learn_time_ms: 752.788\n",
      "    load_throughput: 42088.287\n",
      "    load_time_ms: 23.76\n",
      "    sample_throughput: 10.42\n",
      "    sample_time_ms: 95971.304\n",
      "    update_time_ms: 3.77\n",
      "  timestamp: 1635308906\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 524000\n",
      "  training_iteration: 524\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   524</td><td style=\"text-align: right;\">         26323.4</td><td style=\"text-align: right;\">524000</td><td style=\"text-align: right;\">  3.6969</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -9.93</td><td style=\"text-align: right;\">            154.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 525000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-28-49\n",
      "  done: false\n",
      "  episode_len_mean: 161.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 3.313100000000016\n",
      "  episode_reward_min: -15.330000000000043\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2038\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.39544101909184504\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0095635771751406\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.034963817194390394\n",
      "          policy_loss: 0.06522536642021604\n",
      "          total_loss: 0.3497944337626298\n",
      "          vf_explained_var: 0.5293348431587219\n",
      "          vf_loss: 0.290838587594529\n",
      "    num_agent_steps_sampled: 525000\n",
      "    num_agent_steps_trained: 525000\n",
      "    num_steps_sampled: 525000\n",
      "    num_steps_trained: 525000\n",
      "  iterations_since_restore: 525\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.209375\n",
      "    ram_util_percent: 33.928125\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673832229524936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.821307152635853\n",
      "    mean_inference_ms: 2.114466231566001\n",
      "    mean_raw_obs_processing_ms: 26.790398919442556\n",
      "  time_since_restore: 26346.243048667908\n",
      "  time_this_iter_s: 22.832310438156128\n",
      "  time_total_s: 26346.243048667908\n",
      "  timers:\n",
      "    learn_throughput: 1327.513\n",
      "    learn_time_ms: 753.288\n",
      "    load_throughput: 41947.067\n",
      "    load_time_ms: 23.84\n",
      "    sample_throughput: 10.798\n",
      "    sample_time_ms: 92605.973\n",
      "    update_time_ms: 3.817\n",
      "  timestamp: 1635308929\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 525000\n",
      "  training_iteration: 525\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   525</td><td style=\"text-align: right;\">         26346.2</td><td style=\"text-align: right;\">525000</td><td style=\"text-align: right;\">  3.3131</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">              -15.33</td><td style=\"text-align: right;\">            161.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 526000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-30-24\n",
      "  done: false\n",
      "  episode_len_mean: 162.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 3.246700000000016\n",
      "  episode_reward_min: -15.330000000000043\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 2044\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5931615286377675\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9681620849503412\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.022626704894352763\n",
      "          policy_loss: -0.007083997792667813\n",
      "          total_loss: 0.2450926038953993\n",
      "          vf_explained_var: 0.4891227185726166\n",
      "          vf_loss: 0.25843693152483965\n",
      "    num_agent_steps_sampled: 526000\n",
      "    num_agent_steps_trained: 526000\n",
      "    num_steps_sampled: 526000\n",
      "    num_steps_trained: 526000\n",
      "  iterations_since_restore: 526\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.188235294117646\n",
      "    ram_util_percent: 33.88823529411765\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673949006072358\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.821636476206486\n",
      "    mean_inference_ms: 2.1145043075383114\n",
      "    mean_raw_obs_processing_ms: 26.876630760516885\n",
      "  time_since_restore: 26441.399620771408\n",
      "  time_this_iter_s: 95.15657210350037\n",
      "  time_total_s: 26441.399620771408\n",
      "  timers:\n",
      "    learn_throughput: 1329.469\n",
      "    learn_time_ms: 752.18\n",
      "    load_throughput: 42136.446\n",
      "    load_time_ms: 23.732\n",
      "    sample_throughput: 11.036\n",
      "    sample_time_ms: 90612.879\n",
      "    update_time_ms: 3.81\n",
      "  timestamp: 1635309024\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 526000\n",
      "  training_iteration: 526\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   526</td><td style=\"text-align: right;\">         26441.4</td><td style=\"text-align: right;\">526000</td><td style=\"text-align: right;\">  3.2467</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">              -15.33</td><td style=\"text-align: right;\">            162.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 527000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-33-00\n",
      "  done: false\n",
      "  episode_len_mean: 159.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 3.371800000000016\n",
      "  episode_reward_min: -15.330000000000043\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2053\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8897422929566512\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9966849353578355\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008420528808466561\n",
      "          policy_loss: 0.2473854010303815\n",
      "          total_loss: 0.3641950367225541\n",
      "          vf_explained_var: 0.7944587469100952\n",
      "          vf_loss: 0.1292843824976848\n",
      "    num_agent_steps_sampled: 527000\n",
      "    num_agent_steps_trained: 527000\n",
      "    num_steps_sampled: 527000\n",
      "    num_steps_trained: 527000\n",
      "  iterations_since_restore: 527\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.11659192825111\n",
      "    ram_util_percent: 33.88206278026906\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036741247795945786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.822414412817633\n",
      "    mean_inference_ms: 2.1145626980743946\n",
      "    mean_raw_obs_processing_ms: 27.015321511953072\n",
      "  time_since_restore: 26597.607011556625\n",
      "  time_this_iter_s: 156.20739078521729\n",
      "  time_total_s: 26597.607011556625\n",
      "  timers:\n",
      "    learn_throughput: 1329.53\n",
      "    learn_time_ms: 752.146\n",
      "    load_throughput: 42082.713\n",
      "    load_time_ms: 23.763\n",
      "    sample_throughput: 10.331\n",
      "    sample_time_ms: 96796.98\n",
      "    update_time_ms: 3.811\n",
      "  timestamp: 1635309180\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 527000\n",
      "  training_iteration: 527\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   527</td><td style=\"text-align: right;\">         26597.6</td><td style=\"text-align: right;\">527000</td><td style=\"text-align: right;\">  3.3718</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">              -15.33</td><td style=\"text-align: right;\">            159.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 528000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-35-36\n",
      "  done: false\n",
      "  episode_len_mean: 150.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 3.578900000000015\n",
      "  episode_reward_min: -15.330000000000043\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2062\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8897422929566512\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.830541565683153\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01107100626843832\n",
      "          policy_loss: 0.18575446986489827\n",
      "          total_loss: 0.3436108370621999\n",
      "          vf_explained_var: 0.890595018863678\n",
      "          vf_loss: 0.16631143951995506\n",
      "    num_agent_steps_sampled: 528000\n",
      "    num_agent_steps_trained: 528000\n",
      "    num_steps_sampled: 528000\n",
      "    num_steps_trained: 528000\n",
      "  iterations_since_restore: 528\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.22387387387388\n",
      "    ram_util_percent: 33.86351351351352\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674301738425493\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.82408386035702\n",
      "    mean_inference_ms: 2.1146243161396314\n",
      "    mean_raw_obs_processing_ms: 27.16007643515705\n",
      "  time_since_restore: 26753.251347780228\n",
      "  time_this_iter_s: 155.6443362236023\n",
      "  time_total_s: 26753.251347780228\n",
      "  timers:\n",
      "    learn_throughput: 1329.004\n",
      "    learn_time_ms: 752.443\n",
      "    load_throughput: 41819.338\n",
      "    load_time_ms: 23.912\n",
      "    sample_throughput: 9.084\n",
      "    sample_time_ms: 110078.844\n",
      "    update_time_ms: 3.805\n",
      "  timestamp: 1635309336\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 528000\n",
      "  training_iteration: 528\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   528</td><td style=\"text-align: right;\">         26753.3</td><td style=\"text-align: right;\">528000</td><td style=\"text-align: right;\">  3.5789</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">              -15.33</td><td style=\"text-align: right;\">            150.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 529000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-36-19\n",
      "  done: false\n",
      "  episode_len_mean: 157.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 3.3805000000000156\n",
      "  episode_reward_min: -15.330000000000043\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2064\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8897422929566512\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0837163236406115\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014812604156621568\n",
      "          policy_loss: -0.061770896199676725\n",
      "          total_loss: 0.27785166737933953\n",
      "          vf_explained_var: 0.5733751654624939\n",
      "          vf_loss: 0.3472803216841486\n",
      "    num_agent_steps_sampled: 529000\n",
      "    num_agent_steps_trained: 529000\n",
      "    num_steps_sampled: 529000\n",
      "    num_steps_trained: 529000\n",
      "  iterations_since_restore: 529\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.43064516129033\n",
      "    ram_util_percent: 33.895161290322584\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674342089438336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.824625228819347\n",
      "    mean_inference_ms: 2.1146387625908893\n",
      "    mean_raw_obs_processing_ms: 27.190829937184763\n",
      "  time_since_restore: 26796.657476902008\n",
      "  time_this_iter_s: 43.406129121780396\n",
      "  time_total_s: 26796.657476902008\n",
      "  timers:\n",
      "    learn_throughput: 1325.78\n",
      "    learn_time_ms: 754.273\n",
      "    load_throughput: 41870.938\n",
      "    load_time_ms: 23.883\n",
      "    sample_throughput: 11.615\n",
      "    sample_time_ms: 86098.982\n",
      "    update_time_ms: 3.713\n",
      "  timestamp: 1635309379\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 529000\n",
      "  training_iteration: 529\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   529</td><td style=\"text-align: right;\">         26796.7</td><td style=\"text-align: right;\">529000</td><td style=\"text-align: right;\">  3.3805</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">              -15.33</td><td style=\"text-align: right;\">            157.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 530000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-37-19\n",
      "  done: false\n",
      "  episode_len_mean: 164.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.0872000000000157\n",
      "  episode_reward_min: -15.330000000000043\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 2069\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8897422929566512\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1333385003937617\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0057973531944269475\n",
      "          policy_loss: 0.000211193785071373\n",
      "          total_loss: 0.12549445368349552\n",
      "          vf_explained_var: 0.6065760850906372\n",
      "          vf_loss: 0.1414584918672012\n",
      "    num_agent_steps_sampled: 530000\n",
      "    num_agent_steps_trained: 530000\n",
      "    num_steps_sampled: 530000\n",
      "    num_steps_trained: 530000\n",
      "  iterations_since_restore: 530\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.77790697674419\n",
      "    ram_util_percent: 33.89186046511628\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674446756262702\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.826216231852456\n",
      "    mean_inference_ms: 2.1146764359090464\n",
      "    mean_raw_obs_processing_ms: 27.256369803436108\n",
      "  time_since_restore: 26856.844733715057\n",
      "  time_this_iter_s: 60.187256813049316\n",
      "  time_total_s: 26856.844733715057\n",
      "  timers:\n",
      "    learn_throughput: 1327.773\n",
      "    learn_time_ms: 753.141\n",
      "    load_throughput: 42044.283\n",
      "    load_time_ms: 23.784\n",
      "    sample_throughput: 11.851\n",
      "    sample_time_ms: 84379.97\n",
      "    update_time_ms: 3.624\n",
      "  timestamp: 1635309439\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 530000\n",
      "  training_iteration: 530\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   530</td><td style=\"text-align: right;\">         26856.8</td><td style=\"text-align: right;\">530000</td><td style=\"text-align: right;\">  3.0872</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">              -15.33</td><td style=\"text-align: right;\">            164.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 531000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-39-15\n",
      "  done: false\n",
      "  episode_len_mean: 162.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.1196000000000152\n",
      "  episode_reward_min: -15.330000000000043\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2076\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8897422929566512\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.012925104300181\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013758594458502277\n",
      "          policy_loss: -0.057207885715696544\n",
      "          total_loss: 0.07187291267845365\n",
      "          vf_explained_var: 0.9199143052101135\n",
      "          vf_loss: 0.13696844718522497\n",
      "    num_agent_steps_sampled: 531000\n",
      "    num_agent_steps_trained: 531000\n",
      "    num_steps_sampled: 531000\n",
      "    num_steps_trained: 531000\n",
      "  iterations_since_restore: 531\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.996385542168674\n",
      "    ram_util_percent: 33.81626506024097\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674601477651328\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.828862712173123\n",
      "    mean_inference_ms: 2.114730511620188\n",
      "    mean_raw_obs_processing_ms: 27.355920378076977\n",
      "  time_since_restore: 26972.983775377274\n",
      "  time_this_iter_s: 116.13904166221619\n",
      "  time_total_s: 26972.983775377274\n",
      "  timers:\n",
      "    learn_throughput: 1327.809\n",
      "    learn_time_ms: 753.12\n",
      "    load_throughput: 42085.457\n",
      "    load_time_ms: 23.761\n",
      "    sample_throughput: 12.149\n",
      "    sample_time_ms: 82312.989\n",
      "    update_time_ms: 3.621\n",
      "  timestamp: 1635309555\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 531000\n",
      "  training_iteration: 531\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   531</td><td style=\"text-align: right;\">           26973</td><td style=\"text-align: right;\">531000</td><td style=\"text-align: right;\">  3.1196</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">              -15.33</td><td style=\"text-align: right;\">            162.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 532000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-42-49\n",
      "  done: false\n",
      "  episode_len_mean: 150.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.350400000000014\n",
      "  episode_reward_min: -15.330000000000043\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 2088\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8897422929566512\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.924218205610911\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005806773553272058\n",
      "          policy_loss: 0.101699415097634\n",
      "          total_loss: 0.11384958202640215\n",
      "          vf_explained_var: 0.9930328130722046\n",
      "          vf_loss: 0.026225815589229266\n",
      "    num_agent_steps_sampled: 532000\n",
      "    num_agent_steps_trained: 532000\n",
      "    num_steps_sampled: 532000\n",
      "    num_steps_trained: 532000\n",
      "  iterations_since_restore: 532\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.18426229508197\n",
      "    ram_util_percent: 33.856393442622945\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036748574496774294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.834469908673764\n",
      "    mean_inference_ms: 2.1148198278598995\n",
      "    mean_raw_obs_processing_ms: 27.546021244246564\n",
      "  time_since_restore: 27186.981589078903\n",
      "  time_this_iter_s: 213.99781370162964\n",
      "  time_total_s: 27186.981589078903\n",
      "  timers:\n",
      "    learn_throughput: 1330.045\n",
      "    learn_time_ms: 751.854\n",
      "    load_throughput: 41847.46\n",
      "    load_time_ms: 23.896\n",
      "    sample_throughput: 10.455\n",
      "    sample_time_ms: 95651.773\n",
      "    update_time_ms: 3.586\n",
      "  timestamp: 1635309769\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 532000\n",
      "  training_iteration: 532\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   532</td><td style=\"text-align: right;\">           27187</td><td style=\"text-align: right;\">532000</td><td style=\"text-align: right;\">  3.3504</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">              -15.33</td><td style=\"text-align: right;\">            150.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 533000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-43-15\n",
      "  done: false\n",
      "  episode_len_mean: 157.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.097500000000015\n",
      "  episode_reward_min: -15.330000000000043\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2090\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8897422929566512\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0991914868354797\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006075018769897156\n",
      "          policy_loss: -0.14582690041926172\n",
      "          total_loss: -0.06870388719770644\n",
      "          vf_explained_var: 0.7890663146972656\n",
      "          vf_loss: 0.09270972922547824\n",
      "    num_agent_steps_sampled: 533000\n",
      "    num_agent_steps_trained: 533000\n",
      "    num_steps_sampled: 533000\n",
      "    num_steps_trained: 533000\n",
      "  iterations_since_restore: 533\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.53243243243244\n",
      "    ram_util_percent: 33.894594594594594\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674901020321228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.835553778571224\n",
      "    mean_inference_ms: 2.1148351064440756\n",
      "    mean_raw_obs_processing_ms: 27.574863626632563\n",
      "  time_since_restore: 27212.390290737152\n",
      "  time_this_iter_s: 25.4087016582489\n",
      "  time_total_s: 27212.390290737152\n",
      "  timers:\n",
      "    learn_throughput: 1333.081\n",
      "    learn_time_ms: 750.142\n",
      "    load_throughput: 41171.244\n",
      "    load_time_ms: 24.289\n",
      "    sample_throughput: 10.428\n",
      "    sample_time_ms: 95892.218\n",
      "    update_time_ms: 3.59\n",
      "  timestamp: 1635309795\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 533000\n",
      "  training_iteration: 533\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   533</td><td style=\"text-align: right;\">         27212.4</td><td style=\"text-align: right;\">533000</td><td style=\"text-align: right;\">  3.0975</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">              -15.33</td><td style=\"text-align: right;\">            157.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 534000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-46-25\n",
      "  done: false\n",
      "  episode_len_mean: 152.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.165600000000014\n",
      "  episode_reward_min: -15.330000000000043\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 2102\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8897422929566512\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8737118257416618\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007491613326009312\n",
      "          policy_loss: -0.05133714994622601\n",
      "          total_loss: 0.09231072730488248\n",
      "          vf_explained_var: 0.9786083698272705\n",
      "          vf_loss: 0.15571939177397226\n",
      "    num_agent_steps_sampled: 534000\n",
      "    num_agent_steps_trained: 534000\n",
      "    num_steps_sampled: 534000\n",
      "    num_steps_trained: 534000\n",
      "  iterations_since_restore: 534\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.56974169741698\n",
      "    ram_util_percent: 33.897785977859776\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675165996754614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.842587312194567\n",
      "    mean_inference_ms: 2.114926324015492\n",
      "    mean_raw_obs_processing_ms: 27.743754969391993\n",
      "  time_since_restore: 27402.37509894371\n",
      "  time_this_iter_s: 189.98480820655823\n",
      "  time_total_s: 27402.37509894371\n",
      "  timers:\n",
      "    learn_throughput: 1331.584\n",
      "    learn_time_ms: 750.985\n",
      "    load_throughput: 41342.988\n",
      "    load_time_ms: 24.188\n",
      "    sample_throughput: 9.336\n",
      "    sample_time_ms: 107113.408\n",
      "    update_time_ms: 3.596\n",
      "  timestamp: 1635309985\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 534000\n",
      "  training_iteration: 534\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   534</td><td style=\"text-align: right;\">         27402.4</td><td style=\"text-align: right;\">534000</td><td style=\"text-align: right;\">  3.1656</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">              -15.33</td><td style=\"text-align: right;\">            152.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 535000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-50-35\n",
      "  done: false\n",
      "  episode_len_mean: 143.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.456400000000012\n",
      "  episode_reward_min: -15.330000000000043\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 2115\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8897422929566512\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7680168218082852\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011055831648998272\n",
      "          policy_loss: -0.07585188680224948\n",
      "          total_loss: 0.009866791052950753\n",
      "          vf_explained_var: 0.9548815488815308\n",
      "          vf_loss: 0.09356200338289555\n",
      "    num_agent_steps_sampled: 535000\n",
      "    num_agent_steps_trained: 535000\n",
      "    num_steps_sampled: 535000\n",
      "    num_steps_trained: 535000\n",
      "  iterations_since_restore: 535\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.98398876404495\n",
      "    ram_util_percent: 33.9188202247191\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675457774383369\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.851020363318135\n",
      "    mean_inference_ms: 2.115029343228214\n",
      "    mean_raw_obs_processing_ms: 27.95522506934716\n",
      "  time_since_restore: 27652.178139686584\n",
      "  time_this_iter_s: 249.80304074287415\n",
      "  time_total_s: 27652.178139686584\n",
      "  timers:\n",
      "    learn_throughput: 1328.888\n",
      "    learn_time_ms: 752.509\n",
      "    load_throughput: 41379.493\n",
      "    load_time_ms: 24.167\n",
      "    sample_throughput: 7.704\n",
      "    sample_time_ms: 129808.929\n",
      "    update_time_ms: 3.622\n",
      "  timestamp: 1635310235\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 535000\n",
      "  training_iteration: 535\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   535</td><td style=\"text-align: right;\">         27652.2</td><td style=\"text-align: right;\">535000</td><td style=\"text-align: right;\">  3.4564</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">              -15.33</td><td style=\"text-align: right;\">            143.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 536000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-51-52\n",
      "  done: false\n",
      "  episode_len_mean: 152.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.1968000000000143\n",
      "  episode_reward_min: -15.330000000000043\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 2121\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8897422929566512\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.305041609870063\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0042693765976729225\n",
      "          policy_loss: 0.057140387925836776\n",
      "          total_loss: 0.043224500368038814\n",
      "          vf_explained_var: 0.7869454026222229\n",
      "          vf_loss: 0.00533587995192243\n",
      "    num_agent_steps_sampled: 536000\n",
      "    num_agent_steps_trained: 536000\n",
      "    num_steps_sampled: 536000\n",
      "    num_steps_trained: 536000\n",
      "  iterations_since_restore: 536\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.79818181818182\n",
      "    ram_util_percent: 33.94\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675590970894769\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.854656652882955\n",
      "    mean_inference_ms: 2.1150762781476837\n",
      "    mean_raw_obs_processing_ms: 28.04496658453544\n",
      "  time_since_restore: 27729.024201869965\n",
      "  time_this_iter_s: 76.84606218338013\n",
      "  time_total_s: 27729.024201869965\n",
      "  timers:\n",
      "    learn_throughput: 1328.864\n",
      "    learn_time_ms: 752.523\n",
      "    load_throughput: 41336.509\n",
      "    load_time_ms: 24.192\n",
      "    sample_throughput: 7.814\n",
      "    sample_time_ms: 127977.838\n",
      "    update_time_ms: 3.621\n",
      "  timestamp: 1635310312\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 536000\n",
      "  training_iteration: 536\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   536</td><td style=\"text-align: right;\">           27729</td><td style=\"text-align: right;\">536000</td><td style=\"text-align: right;\">  3.1968</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">              -15.33</td><td style=\"text-align: right;\">            152.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 537000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-54-51\n",
      "  done: false\n",
      "  episode_len_mean: 138.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.526600000000012\n",
      "  episode_reward_min: -15.330000000000043\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2131\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4448711464783256\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0260865887006125\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.002330278161648991\n",
      "          policy_loss: -0.19074058102236854\n",
      "          total_loss: -0.20464521365033256\n",
      "          vf_explained_var: 0.9245277643203735\n",
      "          vf_loss: 0.005319560435600579\n",
      "    num_agent_steps_sampled: 537000\n",
      "    num_agent_steps_trained: 537000\n",
      "    num_steps_sampled: 537000\n",
      "    num_steps_trained: 537000\n",
      "  iterations_since_restore: 537\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.731517509727624\n",
      "    ram_util_percent: 33.914396887159526\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675811607600371\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.86072099402764\n",
      "    mean_inference_ms: 2.1151553803380687\n",
      "    mean_raw_obs_processing_ms: 28.214180671142476\n",
      "  time_since_restore: 27908.894814252853\n",
      "  time_this_iter_s: 179.8706123828888\n",
      "  time_total_s: 27908.894814252853\n",
      "  timers:\n",
      "    learn_throughput: 1329.245\n",
      "    learn_time_ms: 752.307\n",
      "    load_throughput: 41389.17\n",
      "    load_time_ms: 24.161\n",
      "    sample_throughput: 7.672\n",
      "    sample_time_ms: 130344.374\n",
      "    update_time_ms: 3.619\n",
      "  timestamp: 1635310491\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 537000\n",
      "  training_iteration: 537\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   537</td><td style=\"text-align: right;\">         27908.9</td><td style=\"text-align: right;\">537000</td><td style=\"text-align: right;\">  3.5266</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">              -15.33</td><td style=\"text-align: right;\">            138.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 538000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-55-31\n",
      "  done: false\n",
      "  episode_len_mean: 141.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.3920000000000123\n",
      "  episode_reward_min: -15.330000000000043\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2134\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2224355732391628\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.082663271162245\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013314209984745378\n",
      "          policy_loss: -0.020052724828322727\n",
      "          total_loss: -0.024298028730683856\n",
      "          vf_explained_var: 0.6553983092308044\n",
      "          vf_loss: 0.013619778566579852\n",
      "    num_agent_steps_sampled: 538000\n",
      "    num_agent_steps_trained: 538000\n",
      "    num_steps_sampled: 538000\n",
      "    num_steps_trained: 538000\n",
      "  iterations_since_restore: 538\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.06964285714285\n",
      "    ram_util_percent: 34.01428571428572\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036758756349005606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.862471886921853\n",
      "    mean_inference_ms: 2.1151788127720312\n",
      "    mean_raw_obs_processing_ms: 28.26379609544977\n",
      "  time_since_restore: 27948.065321445465\n",
      "  time_this_iter_s: 39.170507192611694\n",
      "  time_total_s: 27948.065321445465\n",
      "  timers:\n",
      "    learn_throughput: 1328.241\n",
      "    learn_time_ms: 752.876\n",
      "    load_throughput: 41357.093\n",
      "    load_time_ms: 24.18\n",
      "    sample_throughput: 8.425\n",
      "    sample_time_ms: 118696.409\n",
      "    update_time_ms: 3.622\n",
      "  timestamp: 1635310531\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 538000\n",
      "  training_iteration: 538\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   538</td><td style=\"text-align: right;\">         27948.1</td><td style=\"text-align: right;\">538000</td><td style=\"text-align: right;\">   3.392</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">              -15.33</td><td style=\"text-align: right;\">            141.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 539000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-58-06\n",
      "  done: false\n",
      "  episode_len_mean: 128.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.807400000000012\n",
      "  episode_reward_min: -8.669999999999959\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2143\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2224355732391628\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8940320544772677\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013094855093767304\n",
      "          policy_loss: -0.08223340316779083\n",
      "          total_loss: -0.06289265238576465\n",
      "          vf_explained_var: 0.9778695702552795\n",
      "          vf_loss: 0.03536831091365052\n",
      "    num_agent_steps_sampled: 539000\n",
      "    num_agent_steps_trained: 539000\n",
      "    num_steps_sampled: 539000\n",
      "    num_steps_trained: 539000\n",
      "  iterations_since_restore: 539\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.11711711711712\n",
      "    ram_util_percent: 34.194594594594605\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036760664394306454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.868248196426652\n",
      "    mean_inference_ms: 2.1152474678493616\n",
      "    mean_raw_obs_processing_ms: 28.42934566717478\n",
      "  time_since_restore: 28103.77827692032\n",
      "  time_this_iter_s: 155.71295547485352\n",
      "  time_total_s: 28103.77827692032\n",
      "  timers:\n",
      "    learn_throughput: 1331.311\n",
      "    learn_time_ms: 751.139\n",
      "    load_throughput: 41200.281\n",
      "    load_time_ms: 24.272\n",
      "    sample_throughput: 7.697\n",
      "    sample_time_ms: 129928.65\n",
      "    update_time_ms: 3.698\n",
      "  timestamp: 1635310686\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 539000\n",
      "  training_iteration: 539\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   539</td><td style=\"text-align: right;\">         28103.8</td><td style=\"text-align: right;\">539000</td><td style=\"text-align: right;\">  3.8074</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -8.67</td><td style=\"text-align: right;\">            128.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 540000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_04-59-45\n",
      "  done: false\n",
      "  episode_len_mean: 127.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.799200000000012\n",
      "  episode_reward_min: -8.669999999999959\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 2149\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2224355732391628\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9521892680062187\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008566637790584537\n",
      "          policy_loss: -0.08413126253419452\n",
      "          total_loss: -0.08925779188672701\n",
      "          vf_explained_var: 0.9023393392562866\n",
      "          vf_loss: 0.01248983648709125\n",
      "    num_agent_steps_sampled: 540000\n",
      "    num_agent_steps_trained: 540000\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "  iterations_since_restore: 540\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.748226950354606\n",
      "    ram_util_percent: 34.21702127659574\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036761926390837335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.87222310108073\n",
      "    mean_inference_ms: 2.115292233310173\n",
      "    mean_raw_obs_processing_ms: 28.532800778470463\n",
      "  time_since_restore: 28202.687775850296\n",
      "  time_this_iter_s: 98.90949892997742\n",
      "  time_total_s: 28202.687775850296\n",
      "  timers:\n",
      "    learn_throughput: 1329.571\n",
      "    learn_time_ms: 752.122\n",
      "    load_throughput: 41490.544\n",
      "    load_time_ms: 24.102\n",
      "    sample_throughput: 7.474\n",
      "    sample_time_ms: 133800.054\n",
      "    update_time_ms: 3.698\n",
      "  timestamp: 1635310785\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 540\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   540</td><td style=\"text-align: right;\">         28202.7</td><td style=\"text-align: right;\">540000</td><td style=\"text-align: right;\">  3.7992</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -8.67</td><td style=\"text-align: right;\">            127.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 541000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-02-40\n",
      "  done: false\n",
      "  episode_len_mean: 130.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.789000000000012\n",
      "  episode_reward_min: -8.669999999999959\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2160\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2224355732391628\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.935020515653822\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0045516009227611525\n",
      "          policy_loss: -0.16876012062033016\n",
      "          total_loss: -0.15864361367291874\n",
      "          vf_explained_var: 0.3271622359752655\n",
      "          vf_loss: 0.028454274484991198\n",
      "    num_agent_steps_sampled: 541000\n",
      "    num_agent_steps_trained: 541000\n",
      "    num_steps_sampled: 541000\n",
      "    num_steps_trained: 541000\n",
      "  iterations_since_restore: 541\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.794399999999996\n",
      "    ram_util_percent: 34.144\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676425845691376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.879793094654005\n",
      "    mean_inference_ms: 2.1153755020590803\n",
      "    mean_raw_obs_processing_ms: 28.729546530968392\n",
      "  time_since_restore: 28377.816452741623\n",
      "  time_this_iter_s: 175.1286768913269\n",
      "  time_total_s: 28377.816452741623\n",
      "  timers:\n",
      "    learn_throughput: 1330.213\n",
      "    learn_time_ms: 751.76\n",
      "    load_throughput: 41331.336\n",
      "    load_time_ms: 24.195\n",
      "    sample_throughput: 7.158\n",
      "    sample_time_ms: 139699.323\n",
      "    update_time_ms: 3.7\n",
      "  timestamp: 1635310960\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 541000\n",
      "  training_iteration: 541\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   541</td><td style=\"text-align: right;\">         28377.8</td><td style=\"text-align: right;\">541000</td><td style=\"text-align: right;\">   3.789</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -8.67</td><td style=\"text-align: right;\">            130.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 542000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-03-27\n",
      "  done: false\n",
      "  episode_len_mean: 133.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.697800000000013\n",
      "  episode_reward_min: -8.669999999999959\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2163\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1112177866195814\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.110244768195682\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011732917772159122\n",
      "          policy_loss: -0.06492426449226009\n",
      "          total_loss: -0.06794272571181258\n",
      "          vf_explained_var: 0.8625754714012146\n",
      "          vf_loss: 0.016779076799543366\n",
      "    num_agent_steps_sampled: 542000\n",
      "    num_agent_steps_trained: 542000\n",
      "    num_steps_sampled: 542000\n",
      "    num_steps_trained: 542000\n",
      "  iterations_since_restore: 542\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.26268656716419\n",
      "    ram_util_percent: 34.1820895522388\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676490968356991\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.88203013107103\n",
      "    mean_inference_ms: 2.1153990868132984\n",
      "    mean_raw_obs_processing_ms: 28.78067404305877\n",
      "  time_since_restore: 28424.457407474518\n",
      "  time_this_iter_s: 46.6409547328949\n",
      "  time_total_s: 28424.457407474518\n",
      "  timers:\n",
      "    learn_throughput: 1330.017\n",
      "    learn_time_ms: 751.87\n",
      "    load_throughput: 41327.508\n",
      "    load_time_ms: 24.197\n",
      "    sample_throughput: 8.132\n",
      "    sample_time_ms: 122963.529\n",
      "    update_time_ms: 3.718\n",
      "  timestamp: 1635311007\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 542000\n",
      "  training_iteration: 542\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   542</td><td style=\"text-align: right;\">         28424.5</td><td style=\"text-align: right;\">542000</td><td style=\"text-align: right;\">  3.6978</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -8.67</td><td style=\"text-align: right;\">            133.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 543000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-04-27\n",
      "  done: false\n",
      "  episode_len_mean: 129.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.723000000000012\n",
      "  episode_reward_min: -8.669999999999959\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 2168\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1112177866195814\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.108553198973338\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012508493261690528\n",
      "          policy_loss: -0.17353933428724608\n",
      "          total_loss: -0.16234474008282027\n",
      "          vf_explained_var: 0.8650817275047302\n",
      "          vf_loss: 0.030888957913137144\n",
      "    num_agent_steps_sampled: 543000\n",
      "    num_agent_steps_trained: 543000\n",
      "    num_steps_sampled: 543000\n",
      "    num_steps_trained: 543000\n",
      "  iterations_since_restore: 543\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.22470588235292\n",
      "    ram_util_percent: 34.192941176470576\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676598130252568\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.885669162851826\n",
      "    mean_inference_ms: 2.1154380134204263\n",
      "    mean_raw_obs_processing_ms: 28.86641166540575\n",
      "  time_since_restore: 28484.488171339035\n",
      "  time_this_iter_s: 60.03076386451721\n",
      "  time_total_s: 28484.488171339035\n",
      "  timers:\n",
      "    learn_throughput: 1327.959\n",
      "    learn_time_ms: 753.035\n",
      "    load_throughput: 41681.282\n",
      "    load_time_ms: 23.992\n",
      "    sample_throughput: 7.91\n",
      "    sample_time_ms: 126424.602\n",
      "    update_time_ms: 3.738\n",
      "  timestamp: 1635311067\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 543000\n",
      "  training_iteration: 543\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   543</td><td style=\"text-align: right;\">         28484.5</td><td style=\"text-align: right;\">543000</td><td style=\"text-align: right;\">   3.723</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -8.67</td><td style=\"text-align: right;\">            129.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 544000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-05-32\n",
      "  done: false\n",
      "  episode_len_mean: 134.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.6472000000000135\n",
      "  episode_reward_min: -8.669999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2172\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1112177866195814\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.946756276819441\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01569661727529067\n",
      "          policy_loss: -0.1356973591984974\n",
      "          total_loss: -0.13028848566528822\n",
      "          vf_explained_var: 0.773313045501709\n",
      "          vf_loss: 0.023130693048652676\n",
      "    num_agent_steps_sampled: 544000\n",
      "    num_agent_steps_trained: 544000\n",
      "    num_steps_sampled: 544000\n",
      "    num_steps_trained: 544000\n",
      "  iterations_since_restore: 544\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.61739130434783\n",
      "    ram_util_percent: 34.19456521739131\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036766843174070446\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.888517108257503\n",
      "    mean_inference_ms: 2.1154699025395325\n",
      "    mean_raw_obs_processing_ms: 28.932110387689555\n",
      "  time_since_restore: 28549.032658815384\n",
      "  time_this_iter_s: 64.54448747634888\n",
      "  time_total_s: 28549.032658815384\n",
      "  timers:\n",
      "    learn_throughput: 1330.427\n",
      "    learn_time_ms: 751.639\n",
      "    load_throughput: 41534.383\n",
      "    load_time_ms: 24.076\n",
      "    sample_throughput: 8.781\n",
      "    sample_time_ms: 113881.891\n",
      "    update_time_ms: 3.731\n",
      "  timestamp: 1635311132\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 544000\n",
      "  training_iteration: 544\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   544</td><td style=\"text-align: right;\">           28549</td><td style=\"text-align: right;\">544000</td><td style=\"text-align: right;\">  3.6472</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -8.67</td><td style=\"text-align: right;\">            134.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 545000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-06-00\n",
      "  done: false\n",
      "  episode_len_mean: 140.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.4330000000000127\n",
      "  episode_reward_min: -8.669999999999959\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2175\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1112177866195814\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1604598336749605\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01653260701409384\n",
      "          policy_loss: -0.08066957741975785\n",
      "          total_loss: 0.021426494129829936\n",
      "          vf_explained_var: 0.5749815106391907\n",
      "          vf_loss: 0.12186195063922141\n",
      "    num_agent_steps_sampled: 545000\n",
      "    num_agent_steps_trained: 545000\n",
      "    num_steps_sampled: 545000\n",
      "    num_steps_trained: 545000\n",
      "  iterations_since_restore: 545\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.97\n",
      "    ram_util_percent: 34.2325\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676752088092882\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.89087486401108\n",
      "    mean_inference_ms: 2.1154955226647996\n",
      "    mean_raw_obs_processing_ms: 28.978876799352243\n",
      "  time_since_restore: 28576.81543636322\n",
      "  time_this_iter_s: 27.782777547836304\n",
      "  time_total_s: 28576.81543636322\n",
      "  timers:\n",
      "    learn_throughput: 1331.568\n",
      "    learn_time_ms: 750.994\n",
      "    load_throughput: 41550.842\n",
      "    load_time_ms: 24.067\n",
      "    sample_throughput: 10.907\n",
      "    sample_time_ms: 91680.556\n",
      "    update_time_ms: 3.658\n",
      "  timestamp: 1635311160\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 545000\n",
      "  training_iteration: 545\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   545</td><td style=\"text-align: right;\">         28576.8</td><td style=\"text-align: right;\">545000</td><td style=\"text-align: right;\">   3.433</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -8.67</td><td style=\"text-align: right;\">            140.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 546000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-06-59\n",
      "  done: false\n",
      "  episode_len_mean: 145.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 3.2395000000000125\n",
      "  episode_reward_min: -8.669999999999959\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2179\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1112177866195814\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9474928193622165\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.15829596808995391\n",
      "          policy_loss: 0.10896521359682083\n",
      "          total_loss: 0.6863110806999935\n",
      "          vf_explained_var: 0.2339855134487152\n",
      "          vf_loss: 0.5792154600429866\n",
      "    num_agent_steps_sampled: 546000\n",
      "    num_agent_steps_trained: 546000\n",
      "    num_steps_sampled: 546000\n",
      "    num_steps_trained: 546000\n",
      "  iterations_since_restore: 546\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.61666666666667\n",
      "    ram_util_percent: 34.18928571428572\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676844548357622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.893799799617337\n",
      "    mean_inference_ms: 2.115529742304215\n",
      "    mean_raw_obs_processing_ms: 29.03273102444416\n",
      "  time_since_restore: 28635.85542988777\n",
      "  time_this_iter_s: 59.03999352455139\n",
      "  time_total_s: 28635.85542988777\n",
      "  timers:\n",
      "    learn_throughput: 1328.693\n",
      "    learn_time_ms: 752.619\n",
      "    load_throughput: 41660.498\n",
      "    load_time_ms: 24.004\n",
      "    sample_throughput: 11.124\n",
      "    sample_time_ms: 89898.376\n",
      "    update_time_ms: 3.662\n",
      "  timestamp: 1635311219\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 546000\n",
      "  training_iteration: 546\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   546</td><td style=\"text-align: right;\">         28635.9</td><td style=\"text-align: right;\">546000</td><td style=\"text-align: right;\">  3.2395</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -8.67</td><td style=\"text-align: right;\">            145.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 547000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-07-25\n",
      "  done: false\n",
      "  episode_len_mean: 151.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 2.9085000000000134\n",
      "  episode_reward_min: -10.88999999999994\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2182\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16682667992937208\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5992809467845492\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.025609078516729618\n",
      "          policy_loss: -0.09681599042895768\n",
      "          total_loss: 0.24893217210968335\n",
      "          vf_explained_var: 0.3898363709449768\n",
      "          vf_loss: 0.35746869266861014\n",
      "    num_agent_steps_sampled: 547000\n",
      "    num_agent_steps_trained: 547000\n",
      "    num_steps_sampled: 547000\n",
      "    num_steps_trained: 547000\n",
      "  iterations_since_restore: 547\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.20789473684208\n",
      "    ram_util_percent: 34.265789473684215\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676917333227495\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.896131765553925\n",
      "    mean_inference_ms: 2.1155565898007493\n",
      "    mean_raw_obs_processing_ms: 29.069269023201628\n",
      "  time_since_restore: 28662.543054819107\n",
      "  time_this_iter_s: 26.68762493133545\n",
      "  time_total_s: 28662.543054819107\n",
      "  timers:\n",
      "    learn_throughput: 1326.93\n",
      "    learn_time_ms: 753.619\n",
      "    load_throughput: 41408.171\n",
      "    load_time_ms: 24.15\n",
      "    sample_throughput: 13.409\n",
      "    sample_time_ms: 74578.974\n",
      "    update_time_ms: 3.67\n",
      "  timestamp: 1635311245\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 547000\n",
      "  training_iteration: 547\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   547</td><td style=\"text-align: right;\">         28662.5</td><td style=\"text-align: right;\">547000</td><td style=\"text-align: right;\">  2.9085</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">              -10.89</td><td style=\"text-align: right;\">            151.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 548000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-08-22\n",
      "  done: false\n",
      "  episode_len_mean: 160.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 2.700300000000014\n",
      "  episode_reward_min: -10.88999999999994\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2185\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.25024001989405814\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3470121118757459\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0179385791917318\n",
      "          policy_loss: 0.07411421140034993\n",
      "          total_loss: 0.45505568914943273\n",
      "          vf_explained_var: 0.08124815672636032\n",
      "          vf_loss: 0.3899226483371523\n",
      "    num_agent_steps_sampled: 548000\n",
      "    num_agent_steps_trained: 548000\n",
      "    num_steps_sampled: 548000\n",
      "    num_steps_trained: 548000\n",
      "  iterations_since_restore: 548\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.82592592592592\n",
      "    ram_util_percent: 34.150617283950616\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676994063026067\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.898272012970775\n",
      "    mean_inference_ms: 2.115584898795681\n",
      "    mean_raw_obs_processing_ms: 29.10623017551986\n",
      "  time_since_restore: 28719.189805984497\n",
      "  time_this_iter_s: 56.646751165390015\n",
      "  time_total_s: 28719.189805984497\n",
      "  timers:\n",
      "    learn_throughput: 1325.838\n",
      "    learn_time_ms: 754.24\n",
      "    load_throughput: 41383.82\n",
      "    load_time_ms: 24.164\n",
      "    sample_throughput: 13.102\n",
      "    sample_time_ms: 76325.965\n",
      "    update_time_ms: 3.671\n",
      "  timestamp: 1635311302\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 548000\n",
      "  training_iteration: 548\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   548</td><td style=\"text-align: right;\">         28719.2</td><td style=\"text-align: right;\">548000</td><td style=\"text-align: right;\">  2.7003</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">              -10.89</td><td style=\"text-align: right;\">            160.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 549000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-08-47\n",
      "  done: false\n",
      "  episode_len_mean: 168.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 2.5603000000000145\n",
      "  episode_reward_min: -10.88999999999994\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2187\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.25024001989405814\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.957253157430225\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008828058758579536\n",
      "          policy_loss: 0.007836666661832067\n",
      "          total_loss: 0.30146666011876533\n",
      "          vf_explained_var: 0.6066108345985413\n",
      "          vf_loss: 0.3009933873597119\n",
      "    num_agent_steps_sampled: 549000\n",
      "    num_agent_steps_trained: 549000\n",
      "    num_steps_sampled: 549000\n",
      "    num_steps_trained: 549000\n",
      "  iterations_since_restore: 549\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.14285714285714\n",
      "    ram_util_percent: 34.29714285714285\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036770488137202594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.89975197853904\n",
      "    mean_inference_ms: 2.1156048641259804\n",
      "    mean_raw_obs_processing_ms: 29.129804397804573\n",
      "  time_since_restore: 28743.791901111603\n",
      "  time_this_iter_s: 24.602095127105713\n",
      "  time_total_s: 28743.791901111603\n",
      "  timers:\n",
      "    learn_throughput: 1324.802\n",
      "    learn_time_ms: 754.83\n",
      "    load_throughput: 41439.058\n",
      "    load_time_ms: 24.132\n",
      "    sample_throughput: 15.819\n",
      "    sample_time_ms: 63214.421\n",
      "    update_time_ms: 3.595\n",
      "  timestamp: 1635311327\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 549000\n",
      "  training_iteration: 549\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   549</td><td style=\"text-align: right;\">         28743.8</td><td style=\"text-align: right;\">549000</td><td style=\"text-align: right;\">  2.5603</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">              -10.89</td><td style=\"text-align: right;\">            168.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 550000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-09-06\n",
      "  done: false\n",
      "  episode_len_mean: 172.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 2.3535000000000155\n",
      "  episode_reward_min: -11.299999999999942\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2189\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.25024001989405814\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7336464140150283\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011844712609130227\n",
      "          policy_loss: 0.06021096623606152\n",
      "          total_loss: 0.2613750541375743\n",
      "          vf_explained_var: 0.5413718819618225\n",
      "          vf_loss: 0.2155365340411663\n",
      "    num_agent_steps_sampled: 550000\n",
      "    num_agent_steps_trained: 550000\n",
      "    num_steps_sampled: 550000\n",
      "    num_steps_trained: 550000\n",
      "  iterations_since_restore: 550\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.05357142857143\n",
      "    ram_util_percent: 34.3\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367710551881231\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.901060402682294\n",
      "    mean_inference_ms: 2.1156252622478364\n",
      "    mean_raw_obs_processing_ms: 29.152842498425635\n",
      "  time_since_restore: 28763.438385486603\n",
      "  time_this_iter_s: 19.646484375\n",
      "  time_total_s: 28763.438385486603\n",
      "  timers:\n",
      "    learn_throughput: 1324.874\n",
      "    learn_time_ms: 754.789\n",
      "    load_throughput: 41364.23\n",
      "    load_time_ms: 24.175\n",
      "    sample_throughput: 18.087\n",
      "    sample_time_ms: 55288.118\n",
      "    update_time_ms: 3.594\n",
      "  timestamp: 1635311346\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 550000\n",
      "  training_iteration: 550\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   550</td><td style=\"text-align: right;\">         28763.4</td><td style=\"text-align: right;\">550000</td><td style=\"text-align: right;\">  2.3535</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -11.3</td><td style=\"text-align: right;\">            172.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 551000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-09-29\n",
      "  done: false\n",
      "  episode_len_mean: 174.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 2.4112000000000156\n",
      "  episode_reward_min: -11.299999999999942\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2191\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.25024001989405814\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0685572604338327\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008315816263132063\n",
      "          policy_loss: 0.022891060676839617\n",
      "          total_loss: 0.3797274465362231\n",
      "          vf_explained_var: 0.3604948818683624\n",
      "          vf_loss: 0.3654410027795368\n",
      "    num_agent_steps_sampled: 551000\n",
      "    num_agent_steps_trained: 551000\n",
      "    num_steps_sampled: 551000\n",
      "    num_steps_trained: 551000\n",
      "  iterations_since_restore: 551\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.63125000000001\n",
      "    ram_util_percent: 34.259375\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677161550350406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.902235465915805\n",
      "    mean_inference_ms: 2.1156453800225825\n",
      "    mean_raw_obs_processing_ms: 29.172810189672575\n",
      "  time_since_restore: 28785.846480607986\n",
      "  time_this_iter_s: 22.408095121383667\n",
      "  time_total_s: 28785.846480607986\n",
      "  timers:\n",
      "    learn_throughput: 1324.81\n",
      "    learn_time_ms: 754.825\n",
      "    load_throughput: 41468.393\n",
      "    load_time_ms: 24.115\n",
      "    sample_throughput: 24.99\n",
      "    sample_time_ms: 40016.095\n",
      "    update_time_ms: 3.588\n",
      "  timestamp: 1635311369\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 551000\n",
      "  training_iteration: 551\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   551</td><td style=\"text-align: right;\">         28785.8</td><td style=\"text-align: right;\">551000</td><td style=\"text-align: right;\">  2.4112</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -11.3</td><td style=\"text-align: right;\">            174.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 552000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-09-49\n",
      "  done: false\n",
      "  episode_len_mean: 180.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 2.259300000000017\n",
      "  episode_reward_min: -11.299999999999942\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2193\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.25024001989405814\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.512510311603546\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01538661279924922\n",
      "          policy_loss: -0.04877232927829027\n",
      "          total_loss: 0.41236048407024806\n",
      "          vf_explained_var: 0.38597843050956726\n",
      "          vf_loss: 0.47240757721786697\n",
      "    num_agent_steps_sampled: 552000\n",
      "    num_agent_steps_trained: 552000\n",
      "    num_steps_sampled: 552000\n",
      "    num_steps_trained: 552000\n",
      "  iterations_since_restore: 552\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.54\n",
      "    ram_util_percent: 34.24666666666666\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677218045877063\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.903251244324455\n",
      "    mean_inference_ms: 2.1156657096483253\n",
      "    mean_raw_obs_processing_ms: 29.189185186136907\n",
      "  time_since_restore: 28806.342460870743\n",
      "  time_this_iter_s: 20.495980262756348\n",
      "  time_total_s: 28806.342460870743\n",
      "  timers:\n",
      "    learn_throughput: 1322.93\n",
      "    learn_time_ms: 755.898\n",
      "    load_throughput: 41460.195\n",
      "    load_time_ms: 24.12\n",
      "    sample_throughput: 26.738\n",
      "    sample_time_ms: 37400.536\n",
      "    update_time_ms: 3.566\n",
      "  timestamp: 1635311389\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 552000\n",
      "  training_iteration: 552\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   552</td><td style=\"text-align: right;\">         28806.3</td><td style=\"text-align: right;\">552000</td><td style=\"text-align: right;\">  2.2593</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -11.3</td><td style=\"text-align: right;\">            180.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 553000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-10-12\n",
      "  done: false\n",
      "  episode_len_mean: 194.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 1.883700000000018\n",
      "  episode_reward_min: -11.299999999999942\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2196\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.25024001989405814\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3007833030488756\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017884087586784524\n",
      "          policy_loss: -0.05335258493820826\n",
      "          total_loss: 0.2585846659209993\n",
      "          vf_explained_var: 0.31926867365837097\n",
      "          vf_loss: 0.32046976933876675\n",
      "    num_agent_steps_sampled: 553000\n",
      "    num_agent_steps_trained: 553000\n",
      "    num_steps_sampled: 553000\n",
      "    num_steps_trained: 553000\n",
      "  iterations_since_restore: 553\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.615625\n",
      "    ram_util_percent: 34.25\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036773068716849135\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.90474757660547\n",
      "    mean_inference_ms: 2.115697113751838\n",
      "    mean_raw_obs_processing_ms: 29.212173132587075\n",
      "  time_since_restore: 28829.00309896469\n",
      "  time_this_iter_s: 22.660638093948364\n",
      "  time_total_s: 28829.00309896469\n",
      "  timers:\n",
      "    learn_throughput: 1322.794\n",
      "    learn_time_ms: 755.975\n",
      "    load_throughput: 42390.0\n",
      "    load_time_ms: 23.59\n",
      "    sample_throughput: 29.705\n",
      "    sample_time_ms: 33664.003\n",
      "    update_time_ms: 3.62\n",
      "  timestamp: 1635311412\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 553000\n",
      "  training_iteration: 553\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   553</td><td style=\"text-align: right;\">           28829</td><td style=\"text-align: right;\">553000</td><td style=\"text-align: right;\">  1.8837</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -11.3</td><td style=\"text-align: right;\">            194.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 554000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-10-57\n",
      "  done: false\n",
      "  episode_len_mean: 202.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 1.6708000000000185\n",
      "  episode_reward_min: -11.299999999999942\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2199\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.25024001989405814\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2627405994468266\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01088137925731445\n",
      "          policy_loss: -0.07392122476465172\n",
      "          total_loss: 0.17148749633795685\n",
      "          vf_explained_var: 0.15776848793029785\n",
      "          vf_loss: 0.2553131716118919\n",
      "    num_agent_steps_sampled: 554000\n",
      "    num_agent_steps_trained: 554000\n",
      "    num_steps_sampled: 554000\n",
      "    num_steps_trained: 554000\n",
      "  iterations_since_restore: 554\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.4296875\n",
      "    ram_util_percent: 34.368750000000006\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677399818206318\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.90646235436788\n",
      "    mean_inference_ms: 2.1157311359021276\n",
      "    mean_raw_obs_processing_ms: 29.234546346710644\n",
      "  time_since_restore: 28873.85911130905\n",
      "  time_this_iter_s: 44.85601234436035\n",
      "  time_total_s: 28873.85911130905\n",
      "  timers:\n",
      "    learn_throughput: 1320.427\n",
      "    learn_time_ms: 757.331\n",
      "    load_throughput: 42429.151\n",
      "    load_time_ms: 23.569\n",
      "    sample_throughput: 31.552\n",
      "    sample_time_ms: 31693.826\n",
      "    update_time_ms: 3.625\n",
      "  timestamp: 1635311457\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 554000\n",
      "  training_iteration: 554\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   554</td><td style=\"text-align: right;\">         28873.9</td><td style=\"text-align: right;\">554000</td><td style=\"text-align: right;\">  1.6708</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -11.3</td><td style=\"text-align: right;\">            202.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 555000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-11-53\n",
      "  done: false\n",
      "  episode_len_mean: 209.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 1.5951000000000193\n",
      "  episode_reward_min: -11.299999999999942\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2202\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.25024001989405814\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8522265063391792\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016663041686143156\n",
      "          policy_loss: -0.13325514954825243\n",
      "          total_loss: 0.6046662751171324\n",
      "          vf_explained_var: 0.3817618191242218\n",
      "          vf_loss: 0.7522739330927531\n",
      "    num_agent_steps_sampled: 555000\n",
      "    num_agent_steps_trained: 555000\n",
      "    num_steps_sampled: 555000\n",
      "    num_steps_trained: 555000\n",
      "  iterations_since_restore: 555\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.4037037037037\n",
      "    ram_util_percent: 34.38271604938271\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677496755893424\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.908108894520034\n",
      "    mean_inference_ms: 2.1157669707863622\n",
      "    mean_raw_obs_processing_ms: 29.257233037965037\n",
      "  time_since_restore: 28930.55183529854\n",
      "  time_this_iter_s: 56.692723989486694\n",
      "  time_total_s: 28930.55183529854\n",
      "  timers:\n",
      "    learn_throughput: 1318.31\n",
      "    learn_time_ms: 758.547\n",
      "    load_throughput: 42443.921\n",
      "    load_time_ms: 23.561\n",
      "    sample_throughput: 28.915\n",
      "    sample_time_ms: 34583.653\n",
      "    update_time_ms: 3.622\n",
      "  timestamp: 1635311513\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 555000\n",
      "  training_iteration: 555\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   555</td><td style=\"text-align: right;\">         28930.6</td><td style=\"text-align: right;\">555000</td><td style=\"text-align: right;\">  1.5951</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -11.3</td><td style=\"text-align: right;\">            209.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 556000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-12-38\n",
      "  done: false\n",
      "  episode_len_mean: 217.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 1.4353000000000193\n",
      "  episode_reward_min: -11.299999999999942\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2205\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.25024001989405814\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1276059514946408\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020473874582587728\n",
      "          policy_loss: 0.09370189276006487\n",
      "          total_loss: 0.6770645532343122\n",
      "          vf_explained_var: 0.528502345085144\n",
      "          vf_loss: 0.5895153360234366\n",
      "    num_agent_steps_sampled: 556000\n",
      "    num_agent_steps_trained: 556000\n",
      "    num_steps_sampled: 556000\n",
      "    num_steps_trained: 556000\n",
      "  iterations_since_restore: 556\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.46984126984126\n",
      "    ram_util_percent: 34.330158730158736\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036775935990765646\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.90975277610464\n",
      "    mean_inference_ms: 2.1158023491745466\n",
      "    mean_raw_obs_processing_ms: 29.268371286990654\n",
      "  time_since_restore: 28974.85893893242\n",
      "  time_this_iter_s: 44.307103633880615\n",
      "  time_total_s: 28974.85893893242\n",
      "  timers:\n",
      "    learn_throughput: 1319.041\n",
      "    learn_time_ms: 758.126\n",
      "    load_throughput: 42400.885\n",
      "    load_time_ms: 23.584\n",
      "    sample_throughput: 30.202\n",
      "    sample_time_ms: 33110.769\n",
      "    update_time_ms: 3.618\n",
      "  timestamp: 1635311558\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 556000\n",
      "  training_iteration: 556\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   556</td><td style=\"text-align: right;\">         28974.9</td><td style=\"text-align: right;\">556000</td><td style=\"text-align: right;\">  1.4353</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -11.3</td><td style=\"text-align: right;\">               217</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 557000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-13-39\n",
      "  done: false\n",
      "  episode_len_mean: 223.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 1.15570000000002\n",
      "  episode_reward_min: -11.299999999999942\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2208\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3753600298410872\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8608987477090624\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016937505832339373\n",
      "          policy_loss: -0.0676473417215877\n",
      "          total_loss: 0.17247656418217552\n",
      "          vf_explained_var: 0.6033308506011963\n",
      "          vf_loss: 0.2523752324283123\n",
      "    num_agent_steps_sampled: 557000\n",
      "    num_agent_steps_trained: 557000\n",
      "    num_steps_sampled: 557000\n",
      "    num_steps_trained: 557000\n",
      "  iterations_since_restore: 557\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.50795454545455\n",
      "    ram_util_percent: 34.38295454545454\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677696957410681\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.911463869559032\n",
      "    mean_inference_ms: 2.1158394825089855\n",
      "    mean_raw_obs_processing_ms: 29.279927386405593\n",
      "  time_since_restore: 29035.99512887001\n",
      "  time_this_iter_s: 61.13618993759155\n",
      "  time_total_s: 29035.99512887001\n",
      "  timers:\n",
      "    learn_throughput: 1319.685\n",
      "    learn_time_ms: 757.757\n",
      "    load_throughput: 42683.427\n",
      "    load_time_ms: 23.428\n",
      "    sample_throughput: 27.355\n",
      "    sample_time_ms: 36556.165\n",
      "    update_time_ms: 3.613\n",
      "  timestamp: 1635311619\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 557000\n",
      "  training_iteration: 557\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   557</td><td style=\"text-align: right;\">           29036</td><td style=\"text-align: right;\">557000</td><td style=\"text-align: right;\">  1.1557</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -11.3</td><td style=\"text-align: right;\">            223.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 558000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-14-41\n",
      "  done: false\n",
      "  episode_len_mean: 232.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 0.9328000000000205\n",
      "  episode_reward_min: -11.299999999999942\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2212\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3753600298410872\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.051316973235872\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01899538220856757\n",
      "          policy_loss: 0.11442149927218755\n",
      "          total_loss: 0.6971673739453157\n",
      "          vf_explained_var: 0.6600651144981384\n",
      "          vf_loss: 0.5861289358801312\n",
      "    num_agent_steps_sampled: 558000\n",
      "    num_agent_steps_trained: 558000\n",
      "    num_steps_sampled: 558000\n",
      "    num_steps_trained: 558000\n",
      "  iterations_since_restore: 558\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.73522727272728\n",
      "    ram_util_percent: 34.34090909090909\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677840612800937\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.91393708913298\n",
      "    mean_inference_ms: 2.1158909097856817\n",
      "    mean_raw_obs_processing_ms: 29.295841321204772\n",
      "  time_since_restore: 29097.8687376976\n",
      "  time_this_iter_s: 61.87360882759094\n",
      "  time_total_s: 29097.8687376976\n",
      "  timers:\n",
      "    learn_throughput: 1321.064\n",
      "    learn_time_ms: 756.966\n",
      "    load_throughput: 42664.54\n",
      "    load_time_ms: 23.439\n",
      "    sample_throughput: 26.969\n",
      "    sample_time_ms: 37079.629\n",
      "    update_time_ms: 3.606\n",
      "  timestamp: 1635311681\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 558000\n",
      "  training_iteration: 558\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   558</td><td style=\"text-align: right;\">         29097.9</td><td style=\"text-align: right;\">558000</td><td style=\"text-align: right;\">  0.9328</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -11.3</td><td style=\"text-align: right;\">            232.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 559000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-16-00\n",
      "  done: false\n",
      "  episode_len_mean: 236.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 0.883600000000021\n",
      "  episode_reward_min: -11.299999999999942\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2216\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3753600298410872\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6692178434795804\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02159080139915503\n",
      "          policy_loss: -0.067451301879353\n",
      "          total_loss: 0.4002081340385808\n",
      "          vf_explained_var: 0.6645035743713379\n",
      "          vf_loss: 0.47624728580315906\n",
      "    num_agent_steps_sampled: 559000\n",
      "    num_agent_steps_trained: 559000\n",
      "    num_steps_sampled: 559000\n",
      "    num_steps_trained: 559000\n",
      "  iterations_since_restore: 559\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.04424778761061\n",
      "    ram_util_percent: 34.31858407079646\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677989125736429\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.916479155343147\n",
      "    mean_inference_ms: 2.1159438554617376\n",
      "    mean_raw_obs_processing_ms: 29.313168533214885\n",
      "  time_since_restore: 29177.292493104935\n",
      "  time_this_iter_s: 79.42375540733337\n",
      "  time_total_s: 29177.292493104935\n",
      "  timers:\n",
      "    learn_throughput: 1321.908\n",
      "    learn_time_ms: 756.483\n",
      "    load_throughput: 42463.085\n",
      "    load_time_ms: 23.55\n",
      "    sample_throughput: 23.495\n",
      "    sample_time_ms: 42562.17\n",
      "    update_time_ms: 3.604\n",
      "  timestamp: 1635311760\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 559000\n",
      "  training_iteration: 559\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   559</td><td style=\"text-align: right;\">         29177.3</td><td style=\"text-align: right;\">559000</td><td style=\"text-align: right;\">  0.8836</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -11.3</td><td style=\"text-align: right;\">            236.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 560000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-17-03\n",
      "  done: false\n",
      "  episode_len_mean: 238.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 0.7939000000000209\n",
      "  episode_reward_min: -11.299999999999942\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2220\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5630400447616307\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3687485893567404\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0171203431122521\n",
      "          policy_loss: 0.04552156759632958\n",
      "          total_loss: 0.4915357554952304\n",
      "          vf_explained_var: 0.6390172243118286\n",
      "          vf_loss: 0.4500622335407469\n",
      "    num_agent_steps_sampled: 560000\n",
      "    num_agent_steps_trained: 560000\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "  iterations_since_restore: 560\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.45444444444445\n",
      "    ram_util_percent: 34.28555555555556\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678138647587228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.919386007095127\n",
      "    mean_inference_ms: 2.1159975024619038\n",
      "    mean_raw_obs_processing_ms: 29.32948958244982\n",
      "  time_since_restore: 29240.242871522903\n",
      "  time_this_iter_s: 62.95037841796875\n",
      "  time_total_s: 29240.242871522903\n",
      "  timers:\n",
      "    learn_throughput: 1319.654\n",
      "    learn_time_ms: 757.774\n",
      "    load_throughput: 42406.072\n",
      "    load_time_ms: 23.582\n",
      "    sample_throughput: 21.326\n",
      "    sample_time_ms: 46891.224\n",
      "    update_time_ms: 3.594\n",
      "  timestamp: 1635311823\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 560\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   560</td><td style=\"text-align: right;\">         29240.2</td><td style=\"text-align: right;\">560000</td><td style=\"text-align: right;\">  0.7939</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -11.3</td><td style=\"text-align: right;\">            238.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 561000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-18-01\n",
      "  done: false\n",
      "  episode_len_mean: 241.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 0.6722000000000216\n",
      "  episode_reward_min: -11.299999999999942\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2224\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5630400447616307\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1011491338411967\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008954741569809688\n",
      "          policy_loss: 0.19841546333498425\n",
      "          total_loss: 0.3815595279137293\n",
      "          vf_explained_var: 0.30361202359199524\n",
      "          vf_loss: 0.19911367238188785\n",
      "    num_agent_steps_sampled: 561000\n",
      "    num_agent_steps_trained: 561000\n",
      "    num_steps_sampled: 561000\n",
      "    num_steps_trained: 561000\n",
      "  iterations_since_restore: 561\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.017073170731706\n",
      "    ram_util_percent: 34.31829268292683\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678290530982592\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.921972932918976\n",
      "    mean_inference_ms: 2.116051686052096\n",
      "    mean_raw_obs_processing_ms: 29.339337163108894\n",
      "  time_since_restore: 29297.590052604675\n",
      "  time_this_iter_s: 57.34718108177185\n",
      "  time_total_s: 29297.590052604675\n",
      "  timers:\n",
      "    learn_throughput: 1317.961\n",
      "    learn_time_ms: 758.748\n",
      "    load_throughput: 42337.627\n",
      "    load_time_ms: 23.62\n",
      "    sample_throughput: 19.848\n",
      "    sample_time_ms: 50383.996\n",
      "    update_time_ms: 3.687\n",
      "  timestamp: 1635311881\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 561000\n",
      "  training_iteration: 561\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   561</td><td style=\"text-align: right;\">         29297.6</td><td style=\"text-align: right;\">561000</td><td style=\"text-align: right;\">  0.6722</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -11.3</td><td style=\"text-align: right;\">            241.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 562000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-18-37\n",
      "  done: false\n",
      "  episode_len_mean: 247.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 0.5181000000000219\n",
      "  episode_reward_min: -11.299999999999942\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2225\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5630400447616307\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6894480347633363\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014492012192405883\n",
      "          policy_loss: 0.06751319865385691\n",
      "          total_loss: 0.1982758426003986\n",
      "          vf_explained_var: 0.3494747579097748\n",
      "          vf_loss: 0.13949753809720278\n",
      "    num_agent_steps_sampled: 562000\n",
      "    num_agent_steps_trained: 562000\n",
      "    num_steps_sampled: 562000\n",
      "    num_steps_trained: 562000\n",
      "  iterations_since_restore: 562\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.66792452830189\n",
      "    ram_util_percent: 34.29056603773584\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678329310041649\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.922532983228518\n",
      "    mean_inference_ms: 2.116065407255704\n",
      "    mean_raw_obs_processing_ms: 29.341014779385702\n",
      "  time_since_restore: 29334.401714086533\n",
      "  time_this_iter_s: 36.8116614818573\n",
      "  time_total_s: 29334.401714086533\n",
      "  timers:\n",
      "    learn_throughput: 1317.5\n",
      "    learn_time_ms: 759.013\n",
      "    load_throughput: 42394.842\n",
      "    load_time_ms: 23.588\n",
      "    sample_throughput: 19.225\n",
      "    sample_time_ms: 52015.315\n",
      "    update_time_ms: 3.706\n",
      "  timestamp: 1635311917\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 562000\n",
      "  training_iteration: 562\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   562</td><td style=\"text-align: right;\">         29334.4</td><td style=\"text-align: right;\">562000</td><td style=\"text-align: right;\">  0.5181</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -11.3</td><td style=\"text-align: right;\">            247.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 563000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-20-36\n",
      "  done: false\n",
      "  episode_len_mean: 253.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 0.48510000000002246\n",
      "  episode_reward_min: -11.299999999999942\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2232\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5630400447616307\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4455244700113932\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.025327025227685675\n",
      "          policy_loss: -0.07330302769939105\n",
      "          total_loss: 0.8058356884039111\n",
      "          vf_explained_var: 0.6155346035957336\n",
      "          vf_loss: 0.8793338331911299\n",
      "    num_agent_steps_sampled: 563000\n",
      "    num_agent_steps_trained: 563000\n",
      "    num_steps_sampled: 563000\n",
      "    num_steps_trained: 563000\n",
      "  iterations_since_restore: 563\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.974404761904765\n",
      "    ram_util_percent: 34.260714285714286\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367861004450319\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.926828003158956\n",
      "    mean_inference_ms: 2.1161643026858012\n",
      "    mean_raw_obs_processing_ms: 29.36082417173071\n",
      "  time_since_restore: 29452.58965611458\n",
      "  time_this_iter_s: 118.18794202804565\n",
      "  time_total_s: 29452.58965611458\n",
      "  timers:\n",
      "    learn_throughput: 1317.39\n",
      "    learn_time_ms: 759.077\n",
      "    load_throughput: 41443.153\n",
      "    load_time_ms: 24.129\n",
      "    sample_throughput: 16.242\n",
      "    sample_time_ms: 61567.484\n",
      "    update_time_ms: 3.72\n",
      "  timestamp: 1635312036\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 563000\n",
      "  training_iteration: 563\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   563</td><td style=\"text-align: right;\">         29452.6</td><td style=\"text-align: right;\">563000</td><td style=\"text-align: right;\">  0.4851</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -11.3</td><td style=\"text-align: right;\">             253.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 564000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-23-09\n",
      "  done: false\n",
      "  episode_len_mean: 256.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 0.48530000000002216\n",
      "  episode_reward_min: -11.299999999999942\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2240\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8445600671424461\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5658951428201464\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013555305200304938\n",
      "          policy_loss: 0.03623309797710843\n",
      "          total_loss: 0.8332809229691823\n",
      "          vf_explained_var: 0.520719587802887\n",
      "          vf_loss: 0.8012584997547998\n",
      "    num_agent_steps_sampled: 564000\n",
      "    num_agent_steps_trained: 564000\n",
      "    num_steps_sampled: 564000\n",
      "    num_steps_trained: 564000\n",
      "  iterations_since_restore: 564\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.565296803652984\n",
      "    ram_util_percent: 34.20639269406393\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678921868130228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.931421722486874\n",
      "    mean_inference_ms: 2.1162737217137026\n",
      "    mean_raw_obs_processing_ms: 29.388146876475272\n",
      "  time_since_restore: 29605.63108277321\n",
      "  time_this_iter_s: 153.04142665863037\n",
      "  time_total_s: 29605.63108277321\n",
      "  timers:\n",
      "    learn_throughput: 1317.707\n",
      "    learn_time_ms: 758.894\n",
      "    load_throughput: 41111.196\n",
      "    load_time_ms: 24.324\n",
      "    sample_throughput: 13.815\n",
      "    sample_time_ms: 72386.029\n",
      "    update_time_ms: 3.713\n",
      "  timestamp: 1635312189\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 564000\n",
      "  training_iteration: 564\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   564</td><td style=\"text-align: right;\">         29605.6</td><td style=\"text-align: right;\">564000</td><td style=\"text-align: right;\">  0.4853</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -11.3</td><td style=\"text-align: right;\">             256.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 565000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-23-31\n",
      "  done: false\n",
      "  episode_len_mean: 261.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 0.2637000000000228\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2242\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8445600671424461\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7307927846908568\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01736778693686802\n",
      "          policy_loss: 0.015662220120429993\n",
      "          total_loss: 0.5717601516180568\n",
      "          vf_explained_var: 0.29871666431427\n",
      "          vf_loss: 0.5587377241916127\n",
      "    num_agent_steps_sampled: 565000\n",
      "    num_agent_steps_trained: 565000\n",
      "    num_steps_sampled: 565000\n",
      "    num_steps_trained: 565000\n",
      "  iterations_since_restore: 565\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.225\n",
      "    ram_util_percent: 34.24375\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679002438949156\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.93251753462578\n",
      "    mean_inference_ms: 2.11630185607421\n",
      "    mean_raw_obs_processing_ms: 29.39300223944357\n",
      "  time_since_restore: 29628.052878141403\n",
      "  time_this_iter_s: 22.42179536819458\n",
      "  time_total_s: 29628.052878141403\n",
      "  timers:\n",
      "    learn_throughput: 1318.932\n",
      "    learn_time_ms: 758.189\n",
      "    load_throughput: 41127.361\n",
      "    load_time_ms: 24.315\n",
      "    sample_throughput: 14.501\n",
      "    sample_time_ms: 68959.582\n",
      "    update_time_ms: 3.802\n",
      "  timestamp: 1635312211\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 565000\n",
      "  training_iteration: 565\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   565</td><td style=\"text-align: right;\">         29628.1</td><td style=\"text-align: right;\">565000</td><td style=\"text-align: right;\">  0.2637</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            261.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 566000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-26-02\n",
      "  done: false\n",
      "  episode_len_mean: 263.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 0.29860000000002196\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2249\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8445600671424461\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6633813169267442\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012018116369619096\n",
      "          policy_loss: 0.04982167267137104\n",
      "          total_loss: 0.7022027158074908\n",
      "          vf_explained_var: 0.5828573703765869\n",
      "          vf_loss: 0.6588648325867124\n",
      "    num_agent_steps_sampled: 566000\n",
      "    num_agent_steps_trained: 566000\n",
      "    num_steps_sampled: 566000\n",
      "    num_steps_trained: 566000\n",
      "  iterations_since_restore: 566\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.46186046511628\n",
      "    ram_util_percent: 34.27302325581396\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679287713061426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.935628209551002\n",
      "    mean_inference_ms: 2.1164012967095998\n",
      "    mean_raw_obs_processing_ms: 29.417728594732694\n",
      "  time_since_restore: 29779.17645716667\n",
      "  time_this_iter_s: 151.12357902526855\n",
      "  time_total_s: 29779.17645716667\n",
      "  timers:\n",
      "    learn_throughput: 1320.31\n",
      "    learn_time_ms: 757.398\n",
      "    load_throughput: 41081.157\n",
      "    load_time_ms: 24.342\n",
      "    sample_throughput: 12.556\n",
      "    sample_time_ms: 79642.0\n",
      "    update_time_ms: 3.805\n",
      "  timestamp: 1635312362\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 566000\n",
      "  training_iteration: 566\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   566</td><td style=\"text-align: right;\">         29779.2</td><td style=\"text-align: right;\">566000</td><td style=\"text-align: right;\">  0.2986</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            263.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 567000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-26-42\n",
      "  done: false\n",
      "  episode_len_mean: 266.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 0.25240000000002244\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2251\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8445600671424461\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8640348725848728\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005094532568217403\n",
      "          policy_loss: -0.12375483906103504\n",
      "          total_loss: 0.06422825675043795\n",
      "          vf_explained_var: 0.5155735611915588\n",
      "          vf_loss: 0.2023208071788152\n",
      "    num_agent_steps_sampled: 567000\n",
      "    num_agent_steps_trained: 567000\n",
      "    num_steps_sampled: 567000\n",
      "    num_steps_trained: 567000\n",
      "  iterations_since_restore: 567\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.40526315789474\n",
      "    ram_util_percent: 34.27894736842106\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679368999791628\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.936241649682152\n",
      "    mean_inference_ms: 2.116429094235026\n",
      "    mean_raw_obs_processing_ms: 29.419830127291096\n",
      "  time_since_restore: 29819.086060523987\n",
      "  time_this_iter_s: 39.90960335731506\n",
      "  time_total_s: 29819.086060523987\n",
      "  timers:\n",
      "    learn_throughput: 1322.239\n",
      "    learn_time_ms: 756.293\n",
      "    load_throughput: 40971.003\n",
      "    load_time_ms: 24.408\n",
      "    sample_throughput: 12.9\n",
      "    sample_time_ms: 77520.378\n",
      "    update_time_ms: 3.803\n",
      "  timestamp: 1635312402\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 567000\n",
      "  training_iteration: 567\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   567</td><td style=\"text-align: right;\">         29819.1</td><td style=\"text-align: right;\">567000</td><td style=\"text-align: right;\">  0.2524</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            266.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 568000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-27-17\n",
      "  done: false\n",
      "  episode_len_mean: 273.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 0.11200000000002332\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2253\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8445600671424461\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3867952565352122\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0025393703031325325\n",
      "          policy_loss: 0.05266930133932167\n",
      "          total_loss: 0.17983219251036645\n",
      "          vf_explained_var: 0.439062237739563\n",
      "          vf_loss: 0.13888619432432783\n",
      "    num_agent_steps_sampled: 568000\n",
      "    num_agent_steps_trained: 568000\n",
      "    num_steps_sampled: 568000\n",
      "    num_steps_trained: 568000\n",
      "  iterations_since_restore: 568\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.75999999999999\n",
      "    ram_util_percent: 34.306\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679453375005025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.936663814073064\n",
      "    mean_inference_ms: 2.116457856162792\n",
      "    mean_raw_obs_processing_ms: 29.421497018240412\n",
      "  time_since_restore: 29853.90694642067\n",
      "  time_this_iter_s: 34.82088589668274\n",
      "  time_total_s: 29853.90694642067\n",
      "  timers:\n",
      "    learn_throughput: 1323.504\n",
      "    learn_time_ms: 755.57\n",
      "    load_throughput: 41023.498\n",
      "    load_time_ms: 24.376\n",
      "    sample_throughput: 13.366\n",
      "    sample_time_ms: 74815.854\n",
      "    update_time_ms: 3.808\n",
      "  timestamp: 1635312437\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 568000\n",
      "  training_iteration: 568\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   568</td><td style=\"text-align: right;\">         29853.9</td><td style=\"text-align: right;\">568000</td><td style=\"text-align: right;\">   0.112</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            273.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 569000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-27-57\n",
      "  done: false\n",
      "  episode_len_mean: 284.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: -0.2419999999999756\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2256\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.42228003357122307\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5569517374038697\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011841841423158467\n",
      "          policy_loss: 0.03004824924800131\n",
      "          total_loss: 0.25324083910220196\n",
      "          vf_explained_var: 0.6390843987464905\n",
      "          vf_loss: 0.23376153401202626\n",
      "    num_agent_steps_sampled: 569000\n",
      "    num_agent_steps_trained: 569000\n",
      "    num_steps_sampled: 569000\n",
      "    num_steps_trained: 569000\n",
      "  iterations_since_restore: 569\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.17719298245614\n",
      "    ram_util_percent: 34.36842105263158\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679583626572337\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.937220881770894\n",
      "    mean_inference_ms: 2.116502159625298\n",
      "    mean_raw_obs_processing_ms: 29.423409492538987\n",
      "  time_since_restore: 29893.887847185135\n",
      "  time_this_iter_s: 39.98090076446533\n",
      "  time_total_s: 29893.887847185135\n",
      "  timers:\n",
      "    learn_throughput: 1320.655\n",
      "    learn_time_ms: 757.2\n",
      "    load_throughput: 41288.209\n",
      "    load_time_ms: 24.22\n",
      "    sample_throughput: 14.11\n",
      "    sample_time_ms: 70870.088\n",
      "    update_time_ms: 3.812\n",
      "  timestamp: 1635312477\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 569000\n",
      "  training_iteration: 569\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   569</td><td style=\"text-align: right;\">         29893.9</td><td style=\"text-align: right;\">569000</td><td style=\"text-align: right;\">  -0.242</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            284.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 570000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-28-53\n",
      "  done: false\n",
      "  episode_len_mean: 289.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: -0.3344999999999749\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2259\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.42228003357122307\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.670816671848297\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00931542588027935\n",
      "          policy_loss: -0.08885085334380467\n",
      "          total_loss: 0.20702946436487965\n",
      "          vf_explained_var: 0.6717248558998108\n",
      "          vf_loss: 0.3086547660330931\n",
      "    num_agent_steps_sampled: 570000\n",
      "    num_agent_steps_trained: 570000\n",
      "    num_steps_sampled: 570000\n",
      "    num_steps_trained: 570000\n",
      "  iterations_since_restore: 570\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.42\n",
      "    ram_util_percent: 34.30125\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679718665725444\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.937714241558602\n",
      "    mean_inference_ms: 2.1165478552440202\n",
      "    mean_raw_obs_processing_ms: 29.425549226528542\n",
      "  time_since_restore: 29949.555263519287\n",
      "  time_this_iter_s: 55.66741633415222\n",
      "  time_total_s: 29949.555263519287\n",
      "  timers:\n",
      "    learn_throughput: 1320.955\n",
      "    learn_time_ms: 757.028\n",
      "    load_throughput: 41271.836\n",
      "    load_time_ms: 24.23\n",
      "    sample_throughput: 14.257\n",
      "    sample_time_ms: 70141.976\n",
      "    update_time_ms: 3.815\n",
      "  timestamp: 1635312533\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 570000\n",
      "  training_iteration: 570\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   570</td><td style=\"text-align: right;\">         29949.6</td><td style=\"text-align: right;\">570000</td><td style=\"text-align: right;\"> -0.3345</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            289.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 571000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-29-31\n",
      "  done: false\n",
      "  episode_len_mean: 295.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.46779999999997424\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2262\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.42228003357122307\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.566180588139428\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016033919273945912\n",
      "          policy_loss: 0.05714340549376276\n",
      "          total_loss: 0.45188258008824456\n",
      "          vf_explained_var: 0.3485754728317261\n",
      "          vf_loss: 0.40363017751110924\n",
      "    num_agent_steps_sampled: 571000\n",
      "    num_agent_steps_trained: 571000\n",
      "    num_steps_sampled: 571000\n",
      "    num_steps_trained: 571000\n",
      "  iterations_since_restore: 571\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.45925925925926\n",
      "    ram_util_percent: 34.242592592592594\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679855849285537\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.937904103705435\n",
      "    mean_inference_ms: 2.1165936007540584\n",
      "    mean_raw_obs_processing_ms: 29.427454871126447\n",
      "  time_since_restore: 29987.79213809967\n",
      "  time_this_iter_s: 38.2368745803833\n",
      "  time_total_s: 29987.79213809967\n",
      "  timers:\n",
      "    learn_throughput: 1319.292\n",
      "    learn_time_ms: 757.982\n",
      "    load_throughput: 41309.761\n",
      "    load_time_ms: 24.207\n",
      "    sample_throughput: 14.656\n",
      "    sample_time_ms: 68230.126\n",
      "    update_time_ms: 3.73\n",
      "  timestamp: 1635312571\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 571000\n",
      "  training_iteration: 571\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   571</td><td style=\"text-align: right;\">         29987.8</td><td style=\"text-align: right;\">571000</td><td style=\"text-align: right;\"> -0.4678</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            295.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 572000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-29-50\n",
      "  done: false\n",
      "  episode_len_mean: 297.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.5299999999999738\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2263\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.42228003357122307\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.559641538725959\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012463595850695085\n",
      "          policy_loss: 0.025308118305272527\n",
      "          total_loss: 0.31656204482747446\n",
      "          vf_explained_var: 0.48750126361846924\n",
      "          vf_loss: 0.3015872172183461\n",
      "    num_agent_steps_sampled: 572000\n",
      "    num_agent_steps_trained: 572000\n",
      "    num_steps_sampled: 572000\n",
      "    num_steps_trained: 572000\n",
      "  iterations_since_restore: 572\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.21428571428571\n",
      "    ram_util_percent: 34.33571428571428\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679902208416406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.93786762439737\n",
      "    mean_inference_ms: 2.116609071418547\n",
      "    mean_raw_obs_processing_ms: 29.427646878755343\n",
      "  time_since_restore: 30007.219594955444\n",
      "  time_this_iter_s: 19.427456855773926\n",
      "  time_total_s: 30007.219594955444\n",
      "  timers:\n",
      "    learn_throughput: 1319.466\n",
      "    learn_time_ms: 757.882\n",
      "    load_throughput: 41269.237\n",
      "    load_time_ms: 24.231\n",
      "    sample_throughput: 15.039\n",
      "    sample_time_ms: 66491.793\n",
      "    update_time_ms: 3.72\n",
      "  timestamp: 1635312590\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 572000\n",
      "  training_iteration: 572\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   572</td><td style=\"text-align: right;\">         30007.2</td><td style=\"text-align: right;\">572000</td><td style=\"text-align: right;\">   -0.53</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            297.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 573000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-31-45\n",
      "  done: false\n",
      "  episode_len_mean: 292.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.22599999999997486\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2270\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.42228003357122307\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2577415757709078\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013148085558293054\n",
      "          policy_loss: 0.14603231698274613\n",
      "          total_loss: 0.6323247689339849\n",
      "          vf_explained_var: 0.5231624841690063\n",
      "          vf_loss: 0.4933176875114441\n",
      "    num_agent_steps_sampled: 573000\n",
      "    num_agent_steps_trained: 573000\n",
      "    num_steps_sampled: 573000\n",
      "    num_steps_trained: 573000\n",
      "  iterations_since_restore: 573\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.467484662576695\n",
      "    ram_util_percent: 34.282208588957054\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036802196001204414\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.93727865088307\n",
      "    mean_inference_ms: 2.11671516646129\n",
      "    mean_raw_obs_processing_ms: 29.43544330906101\n",
      "  time_since_restore: 30121.4046125412\n",
      "  time_this_iter_s: 114.1850175857544\n",
      "  time_total_s: 30121.4046125412\n",
      "  timers:\n",
      "    learn_throughput: 1318.593\n",
      "    learn_time_ms: 758.384\n",
      "    load_throughput: 41126.837\n",
      "    load_time_ms: 24.315\n",
      "    sample_throughput: 15.131\n",
      "    sample_time_ms: 66090.993\n",
      "    update_time_ms: 3.631\n",
      "  timestamp: 1635312705\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 573000\n",
      "  training_iteration: 573\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   573</td><td style=\"text-align: right;\">         30121.4</td><td style=\"text-align: right;\">573000</td><td style=\"text-align: right;\">  -0.226</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            292.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 574000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-32-08\n",
      "  done: false\n",
      "  episode_len_mean: 301.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.4280999999999737\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2272\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.42228003357122307\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.469133616818322\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014222135428917405\n",
      "          policy_loss: 0.05880148344569736\n",
      "          total_loss: 0.5670811134907935\n",
      "          vf_explained_var: 0.5121554732322693\n",
      "          vf_loss: 0.5169652428891923\n",
      "    num_agent_steps_sampled: 574000\n",
      "    num_agent_steps_trained: 574000\n",
      "    num_steps_sampled: 574000\n",
      "    num_steps_trained: 574000\n",
      "  iterations_since_restore: 574\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.22121212121212\n",
      "    ram_util_percent: 34.266666666666666\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680310184967208\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.937070588529792\n",
      "    mean_inference_ms: 2.1167453515167205\n",
      "    mean_raw_obs_processing_ms: 29.436358863513824\n",
      "  time_since_restore: 30144.288452625275\n",
      "  time_this_iter_s: 22.883840084075928\n",
      "  time_total_s: 30144.288452625275\n",
      "  timers:\n",
      "    learn_throughput: 1319.137\n",
      "    learn_time_ms: 758.071\n",
      "    load_throughput: 41006.212\n",
      "    load_time_ms: 24.387\n",
      "    sample_throughput: 18.841\n",
      "    sample_time_ms: 53075.469\n",
      "    update_time_ms: 3.642\n",
      "  timestamp: 1635312728\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 574000\n",
      "  training_iteration: 574\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   574</td><td style=\"text-align: right;\">         30144.3</td><td style=\"text-align: right;\">574000</td><td style=\"text-align: right;\"> -0.4281</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            301.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 575000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-32-27\n",
      "  done: false\n",
      "  episode_len_mean: 304.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.38899999999997353\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2274\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.42228003357122307\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.900863328244951\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01129234503610961\n",
      "          policy_loss: -0.04051348807083236\n",
      "          total_loss: 0.4308162107856737\n",
      "          vf_explained_var: 0.19748352468013763\n",
      "          vf_loss: 0.48556980142990747\n",
      "    num_agent_steps_sampled: 575000\n",
      "    num_agent_steps_trained: 575000\n",
      "    num_steps_sampled: 575000\n",
      "    num_steps_trained: 575000\n",
      "  iterations_since_restore: 575\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.65714285714286\n",
      "    ram_util_percent: 34.29642857142857\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680400717699164\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.936567967793092\n",
      "    mean_inference_ms: 2.1167749199093144\n",
      "    mean_raw_obs_processing_ms: 29.437327559744507\n",
      "  time_since_restore: 30163.775072574615\n",
      "  time_this_iter_s: 19.48661994934082\n",
      "  time_total_s: 30163.775072574615\n",
      "  timers:\n",
      "    learn_throughput: 1322.0\n",
      "    learn_time_ms: 756.43\n",
      "    load_throughput: 40987.098\n",
      "    load_time_ms: 24.398\n",
      "    sample_throughput: 18.945\n",
      "    sample_time_ms: 52783.598\n",
      "    update_time_ms: 3.628\n",
      "  timestamp: 1635312747\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 575000\n",
      "  training_iteration: 575\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   575</td><td style=\"text-align: right;\">         30163.8</td><td style=\"text-align: right;\">575000</td><td style=\"text-align: right;\">  -0.389</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            304.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 576000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-32-44\n",
      "  done: false\n",
      "  episode_len_mean: 307.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.3966999999999728\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2275\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.42228003357122307\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6463931494288975\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008894736592484884\n",
      "          policy_loss: -0.003208655243118604\n",
      "          total_loss: 0.2729764289326138\n",
      "          vf_explained_var: 0.3653833568096161\n",
      "          vf_loss: 0.2888929428325759\n",
      "    num_agent_steps_sampled: 576000\n",
      "    num_agent_steps_trained: 576000\n",
      "    num_steps_sampled: 576000\n",
      "    num_steps_trained: 576000\n",
      "  iterations_since_restore: 576\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.65833333333333\n",
      "    ram_util_percent: 34.275\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680447284704949\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.936212173465268\n",
      "    mean_inference_ms: 2.1167900242486737\n",
      "    mean_raw_obs_processing_ms: 29.437301692626747\n",
      "  time_since_restore: 30180.818576574326\n",
      "  time_this_iter_s: 17.043503999710083\n",
      "  time_total_s: 30180.818576574326\n",
      "  timers:\n",
      "    learn_throughput: 1322.451\n",
      "    learn_time_ms: 756.172\n",
      "    load_throughput: 41743.257\n",
      "    load_time_ms: 23.956\n",
      "    sample_throughput: 25.396\n",
      "    sample_time_ms: 39376.299\n",
      "    update_time_ms: 3.634\n",
      "  timestamp: 1635312764\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 576000\n",
      "  training_iteration: 576\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   576</td><td style=\"text-align: right;\">         30180.8</td><td style=\"text-align: right;\">576000</td><td style=\"text-align: right;\"> -0.3967</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            307.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 577000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-33-19\n",
      "  done: false\n",
      "  episode_len_mean: 309.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.3980999999999718\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2277\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.42228003357122307\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.41804352733824\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009258126631204563\n",
      "          policy_loss: -0.026155369356274605\n",
      "          total_loss: 0.5900063355763753\n",
      "          vf_explained_var: 0.30996087193489075\n",
      "          vf_loss: 0.6264326161808438\n",
      "    num_agent_steps_sampled: 577000\n",
      "    num_agent_steps_trained: 577000\n",
      "    num_steps_sampled: 577000\n",
      "    num_steps_trained: 577000\n",
      "  iterations_since_restore: 577\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.684\n",
      "    ram_util_percent: 34.216\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036805398676402375\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.935327851723308\n",
      "    mean_inference_ms: 2.116820017784036\n",
      "    mean_raw_obs_processing_ms: 29.436579327904923\n",
      "  time_since_restore: 30215.848032951355\n",
      "  time_this_iter_s: 35.02945637702942\n",
      "  time_total_s: 30215.848032951355\n",
      "  timers:\n",
      "    learn_throughput: 1321.119\n",
      "    learn_time_ms: 756.934\n",
      "    load_throughput: 41871.774\n",
      "    load_time_ms: 23.882\n",
      "    sample_throughput: 25.715\n",
      "    sample_time_ms: 38887.607\n",
      "    update_time_ms: 3.631\n",
      "  timestamp: 1635312799\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 577000\n",
      "  training_iteration: 577\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   577</td><td style=\"text-align: right;\">         30215.8</td><td style=\"text-align: right;\">577000</td><td style=\"text-align: right;\"> -0.3981</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            309.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 578000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-33-38\n",
      "  done: false\n",
      "  episode_len_mean: 320.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.5775999999999708\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2279\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.42228003357122307\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1395279718769922\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011582665334582456\n",
      "          policy_loss: 0.002344273527463277\n",
      "          total_loss: 0.41977949548098775\n",
      "          vf_explained_var: 0.5480680465698242\n",
      "          vf_loss: 0.4239393777317471\n",
      "    num_agent_steps_sampled: 578000\n",
      "    num_agent_steps_trained: 578000\n",
      "    num_steps_sampled: 578000\n",
      "    num_steps_trained: 578000\n",
      "  iterations_since_restore: 578\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.51923076923077\n",
      "    ram_util_percent: 34.357692307692304\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036806353829625105\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.934285224058616\n",
      "    mean_inference_ms: 2.116850957535087\n",
      "    mean_raw_obs_processing_ms: 29.434844243830074\n",
      "  time_since_restore: 30234.39574098587\n",
      "  time_this_iter_s: 18.54770803451538\n",
      "  time_total_s: 30234.39574098587\n",
      "  timers:\n",
      "    learn_throughput: 1318.068\n",
      "    learn_time_ms: 758.686\n",
      "    load_throughput: 41879.007\n",
      "    load_time_ms: 23.878\n",
      "    sample_throughput: 26.839\n",
      "    sample_time_ms: 37258.537\n",
      "    update_time_ms: 3.63\n",
      "  timestamp: 1635312818\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 578000\n",
      "  training_iteration: 578\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   578</td><td style=\"text-align: right;\">         30234.4</td><td style=\"text-align: right;\">578000</td><td style=\"text-align: right;\"> -0.5776</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            320.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 579000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-34-30\n",
      "  done: false\n",
      "  episode_len_mean: 319.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.4154999999999713\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2282\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.42228003357122307\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2713834702968598\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016612134138558884\n",
      "          policy_loss: 0.10820640731188987\n",
      "          total_loss: 0.6105923033422894\n",
      "          vf_explained_var: 0.2509578764438629\n",
      "          vf_loss: 0.508084752327866\n",
      "    num_agent_steps_sampled: 579000\n",
      "    num_agent_steps_trained: 579000\n",
      "    num_steps_sampled: 579000\n",
      "    num_steps_trained: 579000\n",
      "  iterations_since_restore: 579\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.522666666666666\n",
      "    ram_util_percent: 34.34533333333334\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036807779427060636\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.932183546922282\n",
      "    mean_inference_ms: 2.1168967223779522\n",
      "    mean_raw_obs_processing_ms: 29.43420555994633\n",
      "  time_since_restore: 30286.94850754738\n",
      "  time_this_iter_s: 52.55276656150818\n",
      "  time_total_s: 30286.94850754738\n",
      "  timers:\n",
      "    learn_throughput: 1319.681\n",
      "    learn_time_ms: 757.759\n",
      "    load_throughput: 41724.57\n",
      "    load_time_ms: 23.967\n",
      "    sample_throughput: 25.963\n",
      "    sample_time_ms: 38516.554\n",
      "    update_time_ms: 3.632\n",
      "  timestamp: 1635312870\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 579000\n",
      "  training_iteration: 579\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   579</td><td style=\"text-align: right;\">         30286.9</td><td style=\"text-align: right;\">579000</td><td style=\"text-align: right;\"> -0.4155</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            319.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 580000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-34-49\n",
      "  done: false\n",
      "  episode_len_mean: 320.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.36629999999997154\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2283\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.42228003357122307\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3789465566476187\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014671842997326963\n",
      "          policy_loss: 0.028985311463475227\n",
      "          total_loss: 0.46809294985400307\n",
      "          vf_explained_var: 0.33525168895721436\n",
      "          vf_loss: 0.4467014806138145\n",
      "    num_agent_steps_sampled: 580000\n",
      "    num_agent_steps_trained: 580000\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 580000\n",
      "  iterations_since_restore: 580\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.18888888888888\n",
      "    ram_util_percent: 34.35185185185185\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036808256863200904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.931470193769698\n",
      "    mean_inference_ms: 2.116911933990926\n",
      "    mean_raw_obs_processing_ms: 29.43334688541012\n",
      "  time_since_restore: 30305.548612594604\n",
      "  time_this_iter_s: 18.600105047225952\n",
      "  time_total_s: 30305.548612594604\n",
      "  timers:\n",
      "    learn_throughput: 1320.873\n",
      "    learn_time_ms: 757.075\n",
      "    load_throughput: 41869.684\n",
      "    load_time_ms: 23.884\n",
      "    sample_throughput: 28.727\n",
      "    sample_time_ms: 34810.56\n",
      "    update_time_ms: 3.632\n",
      "  timestamp: 1635312889\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 580\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   580</td><td style=\"text-align: right;\">         30305.5</td><td style=\"text-align: right;\">580000</td><td style=\"text-align: right;\"> -0.3663</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            320.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 581000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-35-07\n",
      "  done: false\n",
      "  episode_len_mean: 327.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.5225999999999701\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2285\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.42228003357122307\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.633412981033325\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012348986488526571\n",
      "          policy_loss: 0.1313946106367641\n",
      "          total_loss: 0.3263186970518695\n",
      "          vf_explained_var: 0.637689471244812\n",
      "          vf_loss: 0.20604348302715356\n",
      "    num_agent_steps_sampled: 581000\n",
      "    num_agent_steps_trained: 581000\n",
      "    num_steps_sampled: 581000\n",
      "    num_steps_trained: 581000\n",
      "  iterations_since_restore: 581\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.832\n",
      "    ram_util_percent: 34.388\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680924740943472\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.92986254058444\n",
      "    mean_inference_ms: 2.1169435028531223\n",
      "    mean_raw_obs_processing_ms: 29.43062529871924\n",
      "  time_since_restore: 30323.382175922394\n",
      "  time_this_iter_s: 17.833563327789307\n",
      "  time_total_s: 30323.382175922394\n",
      "  timers:\n",
      "    learn_throughput: 1322.342\n",
      "    learn_time_ms: 756.234\n",
      "    load_throughput: 43666.332\n",
      "    load_time_ms: 22.901\n",
      "    sample_throughput: 30.514\n",
      "    sample_time_ms: 32771.878\n",
      "    update_time_ms: 3.689\n",
      "  timestamp: 1635312907\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 581000\n",
      "  training_iteration: 581\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   581</td><td style=\"text-align: right;\">         30323.4</td><td style=\"text-align: right;\">581000</td><td style=\"text-align: right;\"> -0.5226</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            327.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 582000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-35-23\n",
      "  done: false\n",
      "  episode_len_mean: 332.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.5918999999999691\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2287\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.42228003357122307\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.414782296286689\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013225435704642156\n",
      "          policy_loss: -0.14983304796947372\n",
      "          total_loss: 0.11633916935986943\n",
      "          vf_explained_var: 0.5488587021827698\n",
      "          vf_loss: 0.27473520172966853\n",
      "    num_agent_steps_sampled: 582000\n",
      "    num_agent_steps_trained: 582000\n",
      "    num_steps_sampled: 582000\n",
      "    num_steps_trained: 582000\n",
      "  iterations_since_restore: 582\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.73913043478261\n",
      "    ram_util_percent: 34.339130434782604\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0368102273972592\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.927949904977215\n",
      "    mean_inference_ms: 2.116974387714209\n",
      "    mean_raw_obs_processing_ms: 29.427969337404132\n",
      "  time_since_restore: 30339.089585781097\n",
      "  time_this_iter_s: 15.707409858703613\n",
      "  time_total_s: 30339.089585781097\n",
      "  timers:\n",
      "    learn_throughput: 1326.146\n",
      "    learn_time_ms: 754.065\n",
      "    load_throughput: 46453.848\n",
      "    load_time_ms: 21.527\n",
      "    sample_throughput: 30.861\n",
      "    sample_time_ms: 32403.41\n",
      "    update_time_ms: 3.688\n",
      "  timestamp: 1635312923\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 582000\n",
      "  training_iteration: 582\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   582</td><td style=\"text-align: right;\">         30339.1</td><td style=\"text-align: right;\">582000</td><td style=\"text-align: right;\"> -0.5919</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            332.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 583000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-36-17\n",
      "  done: false\n",
      "  episode_len_mean: 326.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.35289999999996996\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2290\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.42228003357122307\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5100896782345241\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03665168824514033\n",
      "          policy_loss: -0.016197632915443846\n",
      "          total_loss: 0.42362735577755506\n",
      "          vf_explained_var: 0.30663245916366577\n",
      "          vf_loss: 0.43944860729906293\n",
      "    num_agent_steps_sampled: 583000\n",
      "    num_agent_steps_trained: 583000\n",
      "    num_steps_sampled: 583000\n",
      "    num_steps_trained: 583000\n",
      "  iterations_since_restore: 583\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.489610389610384\n",
      "    ram_util_percent: 34.31428571428571\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036811678799384955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.92506247151636\n",
      "    mean_inference_ms: 2.1170200001659025\n",
      "    mean_raw_obs_processing_ms: 29.42642684669597\n",
      "  time_since_restore: 30393.08757662773\n",
      "  time_this_iter_s: 53.99799084663391\n",
      "  time_total_s: 30393.08757662773\n",
      "  timers:\n",
      "    learn_throughput: 1327.954\n",
      "    learn_time_ms: 753.038\n",
      "    load_throughput: 46799.833\n",
      "    load_time_ms: 21.368\n",
      "    sample_throughput: 37.899\n",
      "    sample_time_ms: 26385.917\n",
      "    update_time_ms: 3.68\n",
      "  timestamp: 1635312977\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 583000\n",
      "  training_iteration: 583\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   583</td><td style=\"text-align: right;\">         30393.1</td><td style=\"text-align: right;\">583000</td><td style=\"text-align: right;\"> -0.3529</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            326.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 584000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-36-51\n",
      "  done: false\n",
      "  episode_len_mean: 326.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.3092999999999702\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2291\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6334200503568346\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0657227734724681\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008565682645701086\n",
      "          policy_loss: -0.04816197488043043\n",
      "          total_loss: 0.3254573944542143\n",
      "          vf_explained_var: 0.5764315724372864\n",
      "          vf_loss: 0.37885092033280265\n",
      "    num_agent_steps_sampled: 584000\n",
      "    num_agent_steps_trained: 584000\n",
      "    num_steps_sampled: 584000\n",
      "    num_steps_trained: 584000\n",
      "  iterations_since_restore: 584\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.95510204081632\n",
      "    ram_util_percent: 34.33673469387755\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036812167346153404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.9239768873786\n",
      "    mean_inference_ms: 2.1170352907812555\n",
      "    mean_raw_obs_processing_ms: 29.426095227819477\n",
      "  time_since_restore: 30427.580951213837\n",
      "  time_this_iter_s: 34.49337458610535\n",
      "  time_total_s: 30427.580951213837\n",
      "  timers:\n",
      "    learn_throughput: 1328.613\n",
      "    learn_time_ms: 752.665\n",
      "    load_throughput: 47232.515\n",
      "    load_time_ms: 21.172\n",
      "    sample_throughput: 36.301\n",
      "    sample_time_ms: 27547.448\n",
      "    update_time_ms: 3.669\n",
      "  timestamp: 1635313011\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 584000\n",
      "  training_iteration: 584\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   584</td><td style=\"text-align: right;\">         30427.6</td><td style=\"text-align: right;\">584000</td><td style=\"text-align: right;\"> -0.3093</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            326.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 585000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-37-09\n",
      "  done: false\n",
      "  episode_len_mean: 330.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.3264999999999699\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2293\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6334200503568346\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8467044644885593\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.025260001156848903\n",
      "          policy_loss: 0.002177796016136805\n",
      "          total_loss: 0.5623498409986496\n",
      "          vf_explained_var: 0.37550175189971924\n",
      "          vf_loss: 0.5626388927300771\n",
      "    num_agent_steps_sampled: 585000\n",
      "    num_agent_steps_trained: 585000\n",
      "    num_steps_sampled: 585000\n",
      "    num_steps_trained: 585000\n",
      "  iterations_since_restore: 585\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.7076923076923\n",
      "    ram_util_percent: 34.33846153846154\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03681315052033809\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.921722004591416\n",
      "    mean_inference_ms: 2.117066182300006\n",
      "    mean_raw_obs_processing_ms: 29.425492796390554\n",
      "  time_since_restore: 30445.36560511589\n",
      "  time_this_iter_s: 17.784653902053833\n",
      "  time_total_s: 30445.36560511589\n",
      "  timers:\n",
      "    learn_throughput: 1327.308\n",
      "    learn_time_ms: 753.405\n",
      "    load_throughput: 48729.953\n",
      "    load_time_ms: 20.521\n",
      "    sample_throughput: 36.527\n",
      "    sample_time_ms: 27377.224\n",
      "    update_time_ms: 3.603\n",
      "  timestamp: 1635313029\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 585000\n",
      "  training_iteration: 585\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   585</td><td style=\"text-align: right;\">         30445.4</td><td style=\"text-align: right;\">585000</td><td style=\"text-align: right;\"> -0.3265</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            330.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 586000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-37-45\n",
      "  done: false\n",
      "  episode_len_mean: 329.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.23529999999997017\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2296\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9501300755352523\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4185053706169128\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009509567628814252\n",
      "          policy_loss: -0.04723807420167658\n",
      "          total_loss: 0.35906266503863865\n",
      "          vf_explained_var: 0.6183658838272095\n",
      "          vf_loss: 0.41145046469238067\n",
      "    num_agent_steps_sampled: 586000\n",
      "    num_agent_steps_trained: 586000\n",
      "    num_steps_sampled: 586000\n",
      "    num_steps_trained: 586000\n",
      "  iterations_since_restore: 586\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.95192307692308\n",
      "    ram_util_percent: 34.332692307692305\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03681462307795863\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.918203265508787\n",
      "    mean_inference_ms: 2.1171130424173046\n",
      "    mean_raw_obs_processing_ms: 29.425534136440675\n",
      "  time_since_restore: 30481.92096400261\n",
      "  time_this_iter_s: 36.55535888671875\n",
      "  time_total_s: 30481.92096400261\n",
      "  timers:\n",
      "    learn_throughput: 1327.164\n",
      "    learn_time_ms: 753.486\n",
      "    load_throughput: 47869.691\n",
      "    load_time_ms: 20.89\n",
      "    sample_throughput: 34.097\n",
      "    sample_time_ms: 29327.868\n",
      "    update_time_ms: 3.669\n",
      "  timestamp: 1635313065\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 586000\n",
      "  training_iteration: 586\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   586</td><td style=\"text-align: right;\">         30481.9</td><td style=\"text-align: right;\">586000</td><td style=\"text-align: right;\"> -0.2353</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            329.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 587000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-38-00\n",
      "  done: false\n",
      "  episode_len_mean: 331.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.23529999999996942\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2297\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9501300755352523\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4544795950253804\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0076173238888612915\n",
      "          policy_loss: 0.05213591274287965\n",
      "          total_loss: 0.3145394174589051\n",
      "          vf_explained_var: 0.5022387504577637\n",
      "          vf_loss: 0.2697108483976788\n",
      "    num_agent_steps_sampled: 587000\n",
      "    num_agent_steps_trained: 587000\n",
      "    num_steps_sampled: 587000\n",
      "    num_steps_trained: 587000\n",
      "  iterations_since_restore: 587\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.48095238095239\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036815116554328356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.91682077638403\n",
      "    mean_inference_ms: 2.117128214378522\n",
      "    mean_raw_obs_processing_ms: 29.42525801473502\n",
      "  time_since_restore: 30496.863621234894\n",
      "  time_this_iter_s: 14.942657232284546\n",
      "  time_total_s: 30496.863621234894\n",
      "  timers:\n",
      "    learn_throughput: 1326.113\n",
      "    learn_time_ms: 754.084\n",
      "    load_throughput: 51112.276\n",
      "    load_time_ms: 19.565\n",
      "    sample_throughput: 36.603\n",
      "    sample_time_ms: 27319.894\n",
      "    update_time_ms: 3.678\n",
      "  timestamp: 1635313080\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 587000\n",
      "  training_iteration: 587\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   587</td><td style=\"text-align: right;\">         30496.9</td><td style=\"text-align: right;\">587000</td><td style=\"text-align: right;\"> -0.2353</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            331.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 588000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-39-29\n",
      "  done: false\n",
      "  episode_len_mean: 330.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.17909999999996934\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2301\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9501300755352523\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2648780173725551\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016468080176678182\n",
      "          policy_loss: -0.05963662829664018\n",
      "          total_loss: 0.6231868921054734\n",
      "          vf_explained_var: 0.6079559922218323\n",
      "          vf_loss: 0.6798254804478752\n",
      "    num_agent_steps_sampled: 588000\n",
      "    num_agent_steps_trained: 588000\n",
      "    num_steps_sampled: 588000\n",
      "    num_steps_trained: 588000\n",
      "  iterations_since_restore: 588\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.12619047619048\n",
      "    ram_util_percent: 34.32777777777779\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03681712595358974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.910821692756077\n",
      "    mean_inference_ms: 2.117189532069272\n",
      "    mean_raw_obs_processing_ms: 29.426925650672\n",
      "  time_since_restore: 30585.076417684555\n",
      "  time_this_iter_s: 88.21279644966125\n",
      "  time_total_s: 30585.076417684555\n",
      "  timers:\n",
      "    learn_throughput: 1330.019\n",
      "    learn_time_ms: 751.869\n",
      "    load_throughput: 51157.66\n",
      "    load_time_ms: 19.547\n",
      "    sample_throughput: 29.164\n",
      "    sample_time_ms: 34288.637\n",
      "    update_time_ms: 3.683\n",
      "  timestamp: 1635313169\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 588000\n",
      "  training_iteration: 588\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   588</td><td style=\"text-align: right;\">         30585.1</td><td style=\"text-align: right;\">588000</td><td style=\"text-align: right;\"> -0.1791</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            330.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 589000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-40-08\n",
      "  done: false\n",
      "  episode_len_mean: 332.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.11259999999996896\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2304\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9501300755352523\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3398890124426948\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020537923606161593\n",
      "          policy_loss: -0.023931100592017175\n",
      "          total_loss: 0.4888970888323254\n",
      "          vf_explained_var: 0.5198366641998291\n",
      "          vf_loss: 0.5067133832308981\n",
      "    num_agent_steps_sampled: 589000\n",
      "    num_agent_steps_trained: 589000\n",
      "    num_steps_sampled: 589000\n",
      "    num_steps_trained: 589000\n",
      "  iterations_since_restore: 589\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.58596491228071\n",
      "    ram_util_percent: 34.25964912280702\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036818623850833496\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.906134660139262\n",
      "    mean_inference_ms: 2.1172355200541646\n",
      "    mean_raw_obs_processing_ms: 29.42788417588531\n",
      "  time_since_restore: 30624.567870378494\n",
      "  time_this_iter_s: 39.49145269393921\n",
      "  time_total_s: 30624.567870378494\n",
      "  timers:\n",
      "    learn_throughput: 1330.103\n",
      "    learn_time_ms: 751.822\n",
      "    load_throughput: 50967.746\n",
      "    load_time_ms: 19.62\n",
      "    sample_throughput: 30.319\n",
      "    sample_time_ms: 32982.438\n",
      "    update_time_ms: 3.73\n",
      "  timestamp: 1635313208\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 589000\n",
      "  training_iteration: 589\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   589</td><td style=\"text-align: right;\">         30624.6</td><td style=\"text-align: right;\">589000</td><td style=\"text-align: right;\"> -0.1126</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            332.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 590000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-40-25\n",
      "  done: false\n",
      "  episode_len_mean: 336.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.25299999999996825\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2305\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4251951133028784\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6463233960999384\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007072563954334833\n",
      "          policy_loss: -0.20123240053653718\n",
      "          total_loss: -0.09057083138161236\n",
      "          vf_explained_var: 0.6440590620040894\n",
      "          vf_loss: 0.11704501724905438\n",
      "    num_agent_steps_sampled: 590000\n",
      "    num_agent_steps_trained: 590000\n",
      "    num_steps_sampled: 590000\n",
      "    num_steps_trained: 590000\n",
      "  iterations_since_restore: 590\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.51739130434783\n",
      "    ram_util_percent: 34.35217391304347\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03681913145341467\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.904439349271016\n",
      "    mean_inference_ms: 2.1172510160603983\n",
      "    mean_raw_obs_processing_ms: 29.4277838767734\n",
      "  time_since_restore: 30641.158484458923\n",
      "  time_this_iter_s: 16.590614080429077\n",
      "  time_total_s: 30641.158484458923\n",
      "  timers:\n",
      "    learn_throughput: 1333.06\n",
      "    learn_time_ms: 750.154\n",
      "    load_throughput: 53462.634\n",
      "    load_time_ms: 18.705\n",
      "    sample_throughput: 30.503\n",
      "    sample_time_ms: 32784.096\n",
      "    update_time_ms: 3.736\n",
      "  timestamp: 1635313225\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 590000\n",
      "  training_iteration: 590\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   590</td><td style=\"text-align: right;\">         30641.2</td><td style=\"text-align: right;\">590000</td><td style=\"text-align: right;\">  -0.253</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">             336.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 591000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-41-15\n",
      "  done: false\n",
      "  episode_len_mean: 344.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.41259999999996694\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2308\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4251951133028784\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0035500407218934\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008925643053297868\n",
      "          policy_loss: 0.046377541290389165\n",
      "          total_loss: 0.3564421555234326\n",
      "          vf_explained_var: 0.5088234543800354\n",
      "          vf_loss: 0.30737933615843455\n",
      "    num_agent_steps_sampled: 591000\n",
      "    num_agent_steps_trained: 591000\n",
      "    num_steps_sampled: 591000\n",
      "    num_steps_trained: 591000\n",
      "  iterations_since_restore: 591\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.38472222222222\n",
      "    ram_util_percent: 34.33611111111111\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03682062520797246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.898878411406407\n",
      "    mean_inference_ms: 2.117297241653164\n",
      "    mean_raw_obs_processing_ms: 29.427403130565164\n",
      "  time_since_restore: 30691.735483646393\n",
      "  time_this_iter_s: 50.57699918746948\n",
      "  time_total_s: 30691.735483646393\n",
      "  timers:\n",
      "    learn_throughput: 1332.809\n",
      "    learn_time_ms: 750.295\n",
      "    load_throughput: 50816.026\n",
      "    load_time_ms: 19.679\n",
      "    sample_throughput: 27.733\n",
      "    sample_time_ms: 36057.499\n",
      "    update_time_ms: 3.675\n",
      "  timestamp: 1635313275\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 591000\n",
      "  training_iteration: 591\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   591</td><td style=\"text-align: right;\">         30691.7</td><td style=\"text-align: right;\">591000</td><td style=\"text-align: right;\"> -0.4126</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            344.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 592000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-41-52\n",
      "  done: false\n",
      "  episode_len_mean: 343.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.3349999999999669\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2310\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4251951133028784\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9850164572397868\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006862868047196357\n",
      "          policy_loss: -0.14189068381157185\n",
      "          total_loss: 0.08603820157133871\n",
      "          vf_explained_var: 0.24128331243991852\n",
      "          vf_loss: 0.23799812821671368\n",
      "    num_agent_steps_sampled: 592000\n",
      "    num_agent_steps_trained: 592000\n",
      "    num_steps_sampled: 592000\n",
      "    num_steps_trained: 592000\n",
      "  iterations_since_restore: 592\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.994230769230775\n",
      "    ram_util_percent: 34.407692307692315\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036821628936309796\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.894855514272276\n",
      "    mean_inference_ms: 2.11732840933282\n",
      "    mean_raw_obs_processing_ms: 29.42658855541648\n",
      "  time_since_restore: 30728.072386980057\n",
      "  time_this_iter_s: 36.33690333366394\n",
      "  time_total_s: 30728.072386980057\n",
      "  timers:\n",
      "    learn_throughput: 1331.331\n",
      "    learn_time_ms: 751.128\n",
      "    load_throughput: 47490.769\n",
      "    load_time_ms: 21.057\n",
      "    sample_throughput: 26.234\n",
      "    sample_time_ms: 38118.255\n",
      "    update_time_ms: 3.662\n",
      "  timestamp: 1635313312\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 592000\n",
      "  training_iteration: 592\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   592</td><td style=\"text-align: right;\">         30728.1</td><td style=\"text-align: right;\">592000</td><td style=\"text-align: right;\">  -0.335</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">             343.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 593000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-42-27\n",
      "  done: false\n",
      "  episode_len_mean: 349.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.4886999999999658\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2312\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4251951133028784\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.72864611281289\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00596946287344201\n",
      "          policy_loss: -0.025913631393470696\n",
      "          total_loss: 0.10397277297452093\n",
      "          vf_explained_var: 0.044285841286182404\n",
      "          vf_loss: 0.13866522222136457\n",
      "    num_agent_steps_sampled: 593000\n",
      "    num_agent_steps_trained: 593000\n",
      "    num_steps_sampled: 593000\n",
      "    num_steps_trained: 593000\n",
      "  iterations_since_restore: 593\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.692156862745094\n",
      "    ram_util_percent: 34.39019607843137\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036822653088196225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.890685289902976\n",
      "    mean_inference_ms: 2.1173605585213213\n",
      "    mean_raw_obs_processing_ms: 29.42537319807036\n",
      "  time_since_restore: 30763.73568558693\n",
      "  time_this_iter_s: 35.66329860687256\n",
      "  time_total_s: 30763.73568558693\n",
      "  timers:\n",
      "    learn_throughput: 1332.392\n",
      "    learn_time_ms: 750.53\n",
      "    load_throughput: 47144.28\n",
      "    load_time_ms: 21.211\n",
      "    sample_throughput: 27.559\n",
      "    sample_time_ms: 36285.217\n",
      "    update_time_ms: 3.665\n",
      "  timestamp: 1635313347\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 593000\n",
      "  training_iteration: 593\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   593</td><td style=\"text-align: right;\">         30763.7</td><td style=\"text-align: right;\">593000</td><td style=\"text-align: right;\"> -0.4887</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            349.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 594000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-43-18\n",
      "  done: false\n",
      "  episode_len_mean: 351.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.39809999999996576\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2315\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4251951133028784\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.281593644618988\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01062394233974093\n",
      "          policy_loss: -0.19360523575709926\n",
      "          total_loss: 0.15762551890479193\n",
      "          vf_explained_var: 0.46093180775642395\n",
      "          vf_loss: 0.34890550143188903\n",
      "    num_agent_steps_sampled: 594000\n",
      "    num_agent_steps_trained: 594000\n",
      "    num_steps_sampled: 594000\n",
      "    num_steps_trained: 594000\n",
      "  iterations_since_restore: 594\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.653424657534245\n",
      "    ram_util_percent: 34.38904109589041\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036824190304474314\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.883955747605647\n",
      "    mean_inference_ms: 2.117408829268561\n",
      "    mean_raw_obs_processing_ms: 29.422510954003357\n",
      "  time_since_restore: 30814.631409406662\n",
      "  time_this_iter_s: 50.895723819732666\n",
      "  time_total_s: 30814.631409406662\n",
      "  timers:\n",
      "    learn_throughput: 1334.265\n",
      "    learn_time_ms: 749.476\n",
      "    load_throughput: 47114.783\n",
      "    load_time_ms: 21.225\n",
      "    sample_throughput: 26.367\n",
      "    sample_time_ms: 37926.498\n",
      "    update_time_ms: 3.664\n",
      "  timestamp: 1635313398\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 594000\n",
      "  training_iteration: 594\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   594</td><td style=\"text-align: right;\">         30814.6</td><td style=\"text-align: right;\">594000</td><td style=\"text-align: right;\"> -0.3981</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">             351.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 595000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-43-37\n",
      "  done: false\n",
      "  episode_len_mean: 357.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.5291999999999646\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2317\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4251951133028784\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8477844013108147\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006181802648712035\n",
      "          policy_loss: 0.002918201560775439\n",
      "          total_loss: 0.2664471009539233\n",
      "          vf_explained_var: 0.5028960108757019\n",
      "          vf_loss: 0.2731964749180608\n",
      "    num_agent_steps_sampled: 595000\n",
      "    num_agent_steps_trained: 595000\n",
      "    num_steps_sampled: 595000\n",
      "    num_steps_trained: 595000\n",
      "  iterations_since_restore: 595\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.38846153846154\n",
      "    ram_util_percent: 34.33461538461538\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03682522354706443\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.879210285419312\n",
      "    mean_inference_ms: 2.117441545689896\n",
      "    mean_raw_obs_processing_ms: 29.41953613320894\n",
      "  time_since_restore: 30832.78659081459\n",
      "  time_this_iter_s: 18.155181407928467\n",
      "  time_total_s: 30832.78659081459\n",
      "  timers:\n",
      "    learn_throughput: 1333.916\n",
      "    learn_time_ms: 749.672\n",
      "    load_throughput: 45748.94\n",
      "    load_time_ms: 21.858\n",
      "    sample_throughput: 26.342\n",
      "    sample_time_ms: 37962.669\n",
      "    update_time_ms: 3.723\n",
      "  timestamp: 1635313417\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 595000\n",
      "  training_iteration: 595\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   595</td><td style=\"text-align: right;\">         30832.8</td><td style=\"text-align: right;\">595000</td><td style=\"text-align: right;\"> -0.5292</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            357.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 596000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-44-48\n",
      "  done: false\n",
      "  episode_len_mean: 354.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.44179999999996483\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2321\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4251951133028784\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.494267914030287\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.025942418443582443\n",
      "          policy_loss: -0.018537209348546135\n",
      "          total_loss: 0.7732614702648587\n",
      "          vf_explained_var: 0.6853718757629395\n",
      "          vf_loss: 0.7697683433691661\n",
      "    num_agent_steps_sampled: 596000\n",
      "    num_agent_steps_trained: 596000\n",
      "    num_steps_sampled: 596000\n",
      "    num_steps_trained: 596000\n",
      "  iterations_since_restore: 596\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.76435643564356\n",
      "    ram_util_percent: 34.328712871287124\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03682732089605127\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.869153236326166\n",
      "    mean_inference_ms: 2.117507326235591\n",
      "    mean_raw_obs_processing_ms: 29.414969947722785\n",
      "  time_since_restore: 30903.87609910965\n",
      "  time_this_iter_s: 71.0895082950592\n",
      "  time_total_s: 30903.87609910965\n",
      "  timers:\n",
      "    learn_throughput: 1331.568\n",
      "    learn_time_ms: 750.994\n",
      "    load_throughput: 45746.246\n",
      "    load_time_ms: 21.86\n",
      "    sample_throughput: 24.146\n",
      "    sample_time_ms: 41414.726\n",
      "    update_time_ms: 3.743\n",
      "  timestamp: 1635313488\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 596000\n",
      "  training_iteration: 596\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   596</td><td style=\"text-align: right;\">         30903.9</td><td style=\"text-align: right;\">596000</td><td style=\"text-align: right;\"> -0.4418</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            354.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 597000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-45-21\n",
      "  done: false\n",
      "  episode_len_mean: 356.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.41609999999996483\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2323\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.137792669954316\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5060768855942621\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0059313555857297105\n",
      "          policy_loss: -0.11372532095346186\n",
      "          total_loss: 0.11428670038779577\n",
      "          vf_explained_var: 0.5019403696060181\n",
      "          vf_loss: 0.23039278458389972\n",
      "    num_agent_steps_sampled: 597000\n",
      "    num_agent_steps_trained: 597000\n",
      "    num_steps_sampled: 597000\n",
      "    num_steps_trained: 597000\n",
      "  iterations_since_restore: 597\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.387499999999996\n",
      "    ram_util_percent: 34.37291666666667\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03682836922220896\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.863941511044395\n",
      "    mean_inference_ms: 2.1175404634476926\n",
      "    mean_raw_obs_processing_ms: 29.41211322873822\n",
      "  time_since_restore: 30937.230597496033\n",
      "  time_this_iter_s: 33.35449838638306\n",
      "  time_total_s: 30937.230597496033\n",
      "  timers:\n",
      "    learn_throughput: 1333.413\n",
      "    learn_time_ms: 749.955\n",
      "    load_throughput: 42669.748\n",
      "    load_time_ms: 23.436\n",
      "    sample_throughput: 23.119\n",
      "    sample_time_ms: 43255.388\n",
      "    update_time_ms: 3.734\n",
      "  timestamp: 1635313521\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 597000\n",
      "  training_iteration: 597\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   597</td><td style=\"text-align: right;\">         30937.2</td><td style=\"text-align: right;\">597000</td><td style=\"text-align: right;\"> -0.4161</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            356.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 598000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-46-15\n",
      "  done: false\n",
      "  episode_len_mean: 358.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.388399999999965\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2326\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.137792669954316\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5024325887362162\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007265660424839851\n",
      "          policy_loss: -0.09252629520164596\n",
      "          total_loss: 0.290413236969875\n",
      "          vf_explained_var: 0.6775919795036316\n",
      "          vf_loss: 0.38243138417601585\n",
      "    num_agent_steps_sampled: 598000\n",
      "    num_agent_steps_trained: 598000\n",
      "    num_steps_sampled: 598000\n",
      "    num_steps_trained: 598000\n",
      "  iterations_since_restore: 598\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.4051948051948\n",
      "    ram_util_percent: 34.41428571428572\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036829939132467474\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.856045365876284\n",
      "    mean_inference_ms: 2.1175903126027813\n",
      "    mean_raw_obs_processing_ms: 29.40738567187183\n",
      "  time_since_restore: 30991.08819937706\n",
      "  time_this_iter_s: 53.85760188102722\n",
      "  time_total_s: 30991.08819937706\n",
      "  timers:\n",
      "    learn_throughput: 1332.275\n",
      "    learn_time_ms: 750.596\n",
      "    load_throughput: 42633.099\n",
      "    load_time_ms: 23.456\n",
      "    sample_throughput: 25.114\n",
      "    sample_time_ms: 39819.179\n",
      "    update_time_ms: 3.73\n",
      "  timestamp: 1635313575\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 598000\n",
      "  training_iteration: 598\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   598</td><td style=\"text-align: right;\">         30991.1</td><td style=\"text-align: right;\">598000</td><td style=\"text-align: right;\"> -0.3884</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            358.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 599000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-46-50\n",
      "  done: false\n",
      "  episode_len_mean: 364.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.5504999999999642\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2328\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.137792669954316\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5937271661228605\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008654188052070803\n",
      "          policy_loss: -0.09283769792980617\n",
      "          total_loss: 0.2145633030268881\n",
      "          vf_explained_var: 0.5908175706863403\n",
      "          vf_loss: 0.30483741416699356\n",
      "    num_agent_steps_sampled: 599000\n",
      "    num_agent_steps_trained: 599000\n",
      "    num_steps_sampled: 599000\n",
      "    num_steps_trained: 599000\n",
      "  iterations_since_restore: 599\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.438\n",
      "    ram_util_percent: 34.348\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036830984457984245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.85059997285784\n",
      "    mean_inference_ms: 2.117623497886731\n",
      "    mean_raw_obs_processing_ms: 29.402484432039447\n",
      "  time_since_restore: 31026.40185046196\n",
      "  time_this_iter_s: 35.3136510848999\n",
      "  time_total_s: 31026.40185046196\n",
      "  timers:\n",
      "    learn_throughput: 1333.205\n",
      "    learn_time_ms: 750.072\n",
      "    load_throughput: 42814.268\n",
      "    load_time_ms: 23.357\n",
      "    sample_throughput: 25.379\n",
      "    sample_time_ms: 39402.033\n",
      "    update_time_ms: 3.688\n",
      "  timestamp: 1635313610\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 599000\n",
      "  training_iteration: 599\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         31026.4</td><td style=\"text-align: right;\">599000</td><td style=\"text-align: right;\"> -0.5505</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            364.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 600000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-47-25\n",
      "  done: false\n",
      "  episode_len_mean: 373.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.8780999999999622\n",
      "  episode_reward_min: -13.759999999999936\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2331\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.137792669954316\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5885991626315648\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008205254032990142\n",
      "          policy_loss: 0.11196058516701063\n",
      "          total_loss: 0.3201725276807944\n",
      "          vf_explained_var: 0.5549613237380981\n",
      "          vf_loss: 0.20655680058730974\n",
      "    num_agent_steps_sampled: 600000\n",
      "    num_agent_steps_trained: 600000\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "  iterations_since_restore: 600\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.053999999999995\n",
      "    ram_util_percent: 34.352\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03683260053700971\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.842165430879927\n",
      "    mean_inference_ms: 2.1176744005984656\n",
      "    mean_raw_obs_processing_ms: 29.394533895972458\n",
      "  time_since_restore: 31060.916910886765\n",
      "  time_this_iter_s: 34.51506042480469\n",
      "  time_total_s: 31060.916910886765\n",
      "  timers:\n",
      "    learn_throughput: 1329.208\n",
      "    learn_time_ms: 752.328\n",
      "    load_throughput: 41108.094\n",
      "    load_time_ms: 24.326\n",
      "    sample_throughput: 24.277\n",
      "    sample_time_ms: 41191.057\n",
      "    update_time_ms: 3.875\n",
      "  timestamp: 1635313645\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 600\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   600</td><td style=\"text-align: right;\">         31060.9</td><td style=\"text-align: right;\">600000</td><td style=\"text-align: right;\"> -0.8781</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -13.76</td><td style=\"text-align: right;\">            373.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 601000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-47-42\n",
      "  done: false\n",
      "  episode_len_mean: 379.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -1.077199999999961\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2332\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.137792669954316\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5875556959046258\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0054056073105803\n",
      "          policy_loss: -0.06587851407627264\n",
      "          total_loss: 0.10517630154887835\n",
      "          vf_explained_var: 0.5184046626091003\n",
      "          vf_loss: 0.17537430516547628\n",
      "    num_agent_steps_sampled: 601000\n",
      "    num_agent_steps_trained: 601000\n",
      "    num_steps_sampled: 601000\n",
      "    num_steps_trained: 601000\n",
      "  iterations_since_restore: 601\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.28333333333335\n",
      "    ram_util_percent: 34.42916666666667\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03683315532990089\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.83925721113601\n",
      "    mean_inference_ms: 2.1176918075319944\n",
      "    mean_raw_obs_processing_ms: 29.391402356704653\n",
      "  time_since_restore: 31077.95331788063\n",
      "  time_this_iter_s: 17.036406993865967\n",
      "  time_total_s: 31077.95331788063\n",
      "  timers:\n",
      "    learn_throughput: 1328.485\n",
      "    learn_time_ms: 752.737\n",
      "    load_throughput: 42806.927\n",
      "    load_time_ms: 23.361\n",
      "    sample_throughput: 26.429\n",
      "    sample_time_ms: 37837.496\n",
      "    update_time_ms: 3.933\n",
      "  timestamp: 1635313662\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 601000\n",
      "  training_iteration: 601\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   601</td><td style=\"text-align: right;\">           31078</td><td style=\"text-align: right;\">601000</td><td style=\"text-align: right;\"> -1.0772</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            379.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 602000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-47-57\n",
      "  done: false\n",
      "  episode_len_mean: 380.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -1.1084999999999605\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2333\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.137792669954316\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0767623055312368\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0030288133027250395\n",
      "          policy_loss: -0.037264755968418384\n",
      "          total_loss: 0.21659303520702652\n",
      "          vf_explained_var: 0.3228014409542084\n",
      "          vf_loss: 0.2581504406614436\n",
      "    num_agent_steps_sampled: 602000\n",
      "    num_agent_steps_trained: 602000\n",
      "    num_steps_sampled: 602000\n",
      "    num_steps_trained: 602000\n",
      "  iterations_since_restore: 602\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.27142857142859\n",
      "    ram_util_percent: 34.43809523809524\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03683370814772179\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.83622283772078\n",
      "    mean_inference_ms: 2.1177092043212213\n",
      "    mean_raw_obs_processing_ms: 29.385997040847336\n",
      "  time_since_restore: 31092.66864848137\n",
      "  time_this_iter_s: 14.715330600738525\n",
      "  time_total_s: 31092.66864848137\n",
      "  timers:\n",
      "    learn_throughput: 1329.261\n",
      "    learn_time_ms: 752.297\n",
      "    load_throughput: 45511.852\n",
      "    load_time_ms: 21.972\n",
      "    sample_throughput: 28.029\n",
      "    sample_time_ms: 35677.152\n",
      "    update_time_ms: 3.951\n",
      "  timestamp: 1635313677\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 602000\n",
      "  training_iteration: 602\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   602</td><td style=\"text-align: right;\">         31092.7</td><td style=\"text-align: right;\">602000</td><td style=\"text-align: right;\"> -1.1085</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            380.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 603000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-48-30\n",
      "  done: false\n",
      "  episode_len_mean: 392.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.660000000000004\n",
      "  episode_reward_mean: -1.4454999999999578\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2336\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.068896334977158\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5440449039141337\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00972691521847019\n",
      "          policy_loss: -0.04432283569541243\n",
      "          total_loss: 0.2139647088944912\n",
      "          vf_explained_var: 0.4968821704387665\n",
      "          vf_loss: 0.26333093303773136\n",
      "    num_agent_steps_sampled: 603000\n",
      "    num_agent_steps_trained: 603000\n",
      "    num_steps_sampled: 603000\n",
      "    num_steps_trained: 603000\n",
      "  iterations_since_restore: 603\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.03541666666666\n",
      "    ram_util_percent: 34.385416666666664\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03683540658730449\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.826795035054666\n",
      "    mean_inference_ms: 2.1177628046508827\n",
      "    mean_raw_obs_processing_ms: 29.36919971638904\n",
      "  time_since_restore: 31126.112958669662\n",
      "  time_this_iter_s: 33.44431018829346\n",
      "  time_total_s: 31126.112958669662\n",
      "  timers:\n",
      "    learn_throughput: 1329.619\n",
      "    learn_time_ms: 752.095\n",
      "    load_throughput: 45829.621\n",
      "    load_time_ms: 21.82\n",
      "    sample_throughput: 28.204\n",
      "    sample_time_ms: 35455.597\n",
      "    update_time_ms: 3.954\n",
      "  timestamp: 1635313710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 603000\n",
      "  training_iteration: 603\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   603</td><td style=\"text-align: right;\">         31126.1</td><td style=\"text-align: right;\">603000</td><td style=\"text-align: right;\"> -1.4455</td><td style=\"text-align: right;\">                9.66</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            392.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 604000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-48-47\n",
      "  done: false\n",
      "  episode_len_mean: 396.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.660000000000004\n",
      "  episode_reward_mean: -1.6300999999999568\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2337\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.068896334977158\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8573942793740166\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0074340156395572244\n",
      "          policy_loss: 0.06323893976708253\n",
      "          total_loss: 0.23617343393464882\n",
      "          vf_explained_var: 0.4389694333076477\n",
      "          vf_loss: 0.18356224008732372\n",
      "    num_agent_steps_sampled: 604000\n",
      "    num_agent_steps_trained: 604000\n",
      "    num_steps_sampled: 604000\n",
      "    num_steps_trained: 604000\n",
      "  iterations_since_restore: 604\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.85\n",
      "    ram_util_percent: 34.39166666666666\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036835984257898864\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.82355874579172\n",
      "    mean_inference_ms: 2.1177809768933713\n",
      "    mean_raw_obs_processing_ms: 29.363123568686934\n",
      "  time_since_restore: 31143.260875701904\n",
      "  time_this_iter_s: 17.14791703224182\n",
      "  time_total_s: 31143.260875701904\n",
      "  timers:\n",
      "    learn_throughput: 1326.741\n",
      "    learn_time_ms: 753.726\n",
      "    load_throughput: 48073.74\n",
      "    load_time_ms: 20.801\n",
      "    sample_throughput: 31.172\n",
      "    sample_time_ms: 32080.164\n",
      "    update_time_ms: 3.96\n",
      "  timestamp: 1635313727\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 604000\n",
      "  training_iteration: 604\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   604</td><td style=\"text-align: right;\">         31143.3</td><td style=\"text-align: right;\">604000</td><td style=\"text-align: right;\"> -1.6301</td><td style=\"text-align: right;\">                9.66</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            396.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 605000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-49-03\n",
      "  done: false\n",
      "  episode_len_mean: 408.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.660000000000004\n",
      "  episode_reward_mean: -1.9009999999999556\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2339\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.068896334977158\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7468896481725904\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013362155741033159\n",
      "          policy_loss: 0.012199472553200192\n",
      "          total_loss: 0.1576484702527523\n",
      "          vf_explained_var: 0.6237393617630005\n",
      "          vf_loss: 0.14863513567381434\n",
      "    num_agent_steps_sampled: 605000\n",
      "    num_agent_steps_trained: 605000\n",
      "    num_steps_sampled: 605000\n",
      "    num_steps_trained: 605000\n",
      "  iterations_since_restore: 605\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.33043478260869\n",
      "    ram_util_percent: 34.40434782608695\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036837167299984916\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.81685154785946\n",
      "    mean_inference_ms: 2.1178178786977044\n",
      "    mean_raw_obs_processing_ms: 29.35002159036928\n",
      "  time_since_restore: 31158.976715803146\n",
      "  time_this_iter_s: 15.715840101242065\n",
      "  time_total_s: 31158.976715803146\n",
      "  timers:\n",
      "    learn_throughput: 1329.257\n",
      "    learn_time_ms: 752.3\n",
      "    load_throughput: 51410.676\n",
      "    load_time_ms: 19.451\n",
      "    sample_throughput: 31.408\n",
      "    sample_time_ms: 31839.07\n",
      "    update_time_ms: 3.894\n",
      "  timestamp: 1635313743\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 605000\n",
      "  training_iteration: 605\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   605</td><td style=\"text-align: right;\">           31159</td><td style=\"text-align: right;\">605000</td><td style=\"text-align: right;\">  -1.901</td><td style=\"text-align: right;\">                9.66</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            408.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 606000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-49-21\n",
      "  done: false\n",
      "  episode_len_mean: 415.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.660000000000004\n",
      "  episode_reward_mean: -1.9995999999999554\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2341\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.068896334977158\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.702115571498871\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011051302175071599\n",
      "          policy_loss: 0.05271272568239106\n",
      "          total_loss: 0.39383805348641343\n",
      "          vf_explained_var: 0.6382012367248535\n",
      "          vf_loss: 0.3463337867986411\n",
      "    num_agent_steps_sampled: 606000\n",
      "    num_agent_steps_trained: 606000\n",
      "    num_steps_sampled: 606000\n",
      "    num_steps_trained: 606000\n",
      "  iterations_since_restore: 606\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.024\n",
      "    ram_util_percent: 34.355999999999995\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03683836131704763\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.80998650573686\n",
      "    mean_inference_ms: 2.117854884182304\n",
      "    mean_raw_obs_processing_ms: 29.336492288337258\n",
      "  time_since_restore: 31176.593257188797\n",
      "  time_this_iter_s: 17.616541385650635\n",
      "  time_total_s: 31176.593257188797\n",
      "  timers:\n",
      "    learn_throughput: 1332.301\n",
      "    learn_time_ms: 750.581\n",
      "    load_throughput: 53677.472\n",
      "    load_time_ms: 18.63\n",
      "    sample_throughput: 37.744\n",
      "    sample_time_ms: 26494.435\n",
      "    update_time_ms: 3.807\n",
      "  timestamp: 1635313761\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 606000\n",
      "  training_iteration: 606\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   606</td><td style=\"text-align: right;\">         31176.6</td><td style=\"text-align: right;\">606000</td><td style=\"text-align: right;\"> -1.9996</td><td style=\"text-align: right;\">                9.66</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            415.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 607000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-49-59\n",
      "  done: false\n",
      "  episode_len_mean: 415.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.660000000000004\n",
      "  episode_reward_mean: -1.873299999999955\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2343\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.068896334977158\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8068928122520447\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.07407995085077258\n",
      "          policy_loss: 0.03545822906825277\n",
      "          total_loss: 0.9334858742853006\n",
      "          vf_explained_var: 0.4365668296813965\n",
      "          vf_loss: 0.8369127888232469\n",
      "    num_agent_steps_sampled: 607000\n",
      "    num_agent_steps_trained: 607000\n",
      "    num_steps_sampled: 607000\n",
      "    num_steps_trained: 607000\n",
      "  iterations_since_restore: 607\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.968518518518515\n",
      "    ram_util_percent: 34.329629629629636\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03683954963003818\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.80296638226363\n",
      "    mean_inference_ms: 2.11789149710036\n",
      "    mean_raw_obs_processing_ms: 29.321462568046872\n",
      "  time_since_restore: 31214.554632663727\n",
      "  time_this_iter_s: 37.96137547492981\n",
      "  time_total_s: 31214.554632663727\n",
      "  timers:\n",
      "    learn_throughput: 1328.935\n",
      "    learn_time_ms: 752.482\n",
      "    load_throughput: 54375.018\n",
      "    load_time_ms: 18.391\n",
      "    sample_throughput: 37.101\n",
      "    sample_time_ms: 26953.462\n",
      "    update_time_ms: 3.808\n",
      "  timestamp: 1635313799\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 607000\n",
      "  training_iteration: 607\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   607</td><td style=\"text-align: right;\">         31214.6</td><td style=\"text-align: right;\">607000</td><td style=\"text-align: right;\"> -1.8733</td><td style=\"text-align: right;\">                9.66</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            415.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 608000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-50-15\n",
      "  done: false\n",
      "  episode_len_mean: 427.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.660000000000004\n",
      "  episode_reward_mean: -2.1006999999999536\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2345\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.603344502465738\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.748195430967543\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0046105797751758125\n",
      "          policy_loss: 0.03649691674444411\n",
      "          total_loss: 0.27617392672432794\n",
      "          vf_explained_var: 0.24292869865894318\n",
      "          vf_loss: 0.2497666155712472\n",
      "    num_agent_steps_sampled: 608000\n",
      "    num_agent_steps_trained: 608000\n",
      "    num_steps_sampled: 608000\n",
      "    num_steps_trained: 608000\n",
      "  iterations_since_restore: 608\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.85000000000001\n",
      "    ram_util_percent: 34.47083333333333\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03684075256220675\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.79579716836457\n",
      "    mean_inference_ms: 2.1179283867870686\n",
      "    mean_raw_obs_processing_ms: 29.303699770262032\n",
      "  time_since_restore: 31231.366176843643\n",
      "  time_this_iter_s: 16.811544179916382\n",
      "  time_total_s: 31231.366176843643\n",
      "  timers:\n",
      "    learn_throughput: 1327.046\n",
      "    learn_time_ms: 753.554\n",
      "    load_throughput: 57444.337\n",
      "    load_time_ms: 17.408\n",
      "    sample_throughput: 43.013\n",
      "    sample_time_ms: 23248.825\n",
      "    update_time_ms: 3.803\n",
      "  timestamp: 1635313815\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 608000\n",
      "  training_iteration: 608\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   608</td><td style=\"text-align: right;\">         31231.4</td><td style=\"text-align: right;\">608000</td><td style=\"text-align: right;\"> -2.1007</td><td style=\"text-align: right;\">                9.66</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">             427.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 609000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-50-31\n",
      "  done: false\n",
      "  episode_len_mean: 433.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.660000000000004\n",
      "  episode_reward_mean: -2.1779999999999533\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2346\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.801672251232869\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9572016543812223\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00784290595211724\n",
      "          policy_loss: 0.038628637376758784\n",
      "          total_loss: 0.22899287930793233\n",
      "          vf_explained_var: 0.4911979138851166\n",
      "          vf_loss: 0.20364881532473697\n",
      "    num_agent_steps_sampled: 609000\n",
      "    num_agent_steps_trained: 609000\n",
      "    num_steps_sampled: 609000\n",
      "    num_steps_trained: 609000\n",
      "  iterations_since_restore: 609\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.8826086956522\n",
      "    ram_util_percent: 34.53913043478261\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03684136903544369\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.792100256542064\n",
      "    mean_inference_ms: 2.117947050552288\n",
      "    mean_raw_obs_processing_ms: 29.294348545745358\n",
      "  time_since_restore: 31247.263649225235\n",
      "  time_this_iter_s: 15.897472381591797\n",
      "  time_total_s: 31247.263649225235\n",
      "  timers:\n",
      "    learn_throughput: 1327.053\n",
      "    learn_time_ms: 753.549\n",
      "    load_throughput: 62339.54\n",
      "    load_time_ms: 16.041\n",
      "    sample_throughput: 46.93\n",
      "    sample_time_ms: 21308.547\n",
      "    update_time_ms: 3.869\n",
      "  timestamp: 1635313831\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 609000\n",
      "  training_iteration: 609\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   609</td><td style=\"text-align: right;\">         31247.3</td><td style=\"text-align: right;\">609000</td><td style=\"text-align: right;\">  -2.178</td><td style=\"text-align: right;\">                9.66</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            433.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 610000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-50-51\n",
      "  done: false\n",
      "  episode_len_mean: 437.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.660000000000004\n",
      "  episode_reward_mean: -2.303099999999952\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2348\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.801672251232869\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.74510018825531\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011894580035640069\n",
      "          policy_loss: -0.073771039603485\n",
      "          total_loss: 0.3185903441367878\n",
      "          vf_explained_var: 0.557884931564331\n",
      "          vf_loss: 0.4002768299645848\n",
      "    num_agent_steps_sampled: 610000\n",
      "    num_agent_steps_trained: 610000\n",
      "    num_steps_sampled: 610000\n",
      "    num_steps_trained: 610000\n",
      "  iterations_since_restore: 610\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.97407407407407\n",
      "    ram_util_percent: 34.492592592592594\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036842629990469986\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.784596045913734\n",
      "    mean_inference_ms: 2.1179848552799623\n",
      "    mean_raw_obs_processing_ms: 29.274710627656983\n",
      "  time_since_restore: 31266.68186855316\n",
      "  time_this_iter_s: 19.418219327926636\n",
      "  time_total_s: 31266.68186855316\n",
      "  timers:\n",
      "    learn_throughput: 1327.498\n",
      "    learn_time_ms: 753.297\n",
      "    load_throughput: 62407.623\n",
      "    load_time_ms: 16.024\n",
      "    sample_throughput: 50.507\n",
      "    sample_time_ms: 19799.337\n",
      "    update_time_ms: 3.68\n",
      "  timestamp: 1635313851\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 610000\n",
      "  training_iteration: 610\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   610</td><td style=\"text-align: right;\">         31266.7</td><td style=\"text-align: right;\">610000</td><td style=\"text-align: right;\"> -2.3031</td><td style=\"text-align: right;\">                9.66</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            437.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 611000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-51-25\n",
      "  done: false\n",
      "  episode_len_mean: 443.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.660000000000004\n",
      "  episode_reward_mean: -2.4406999999999512\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2351\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.801672251232869\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.739174066649543\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009642718258888827\n",
      "          policy_loss: -0.008533219910330243\n",
      "          total_loss: 0.3841719619515869\n",
      "          vf_explained_var: 0.5222167372703552\n",
      "          vf_loss: 0.4023666228271193\n",
      "    num_agent_steps_sampled: 611000\n",
      "    num_agent_steps_trained: 611000\n",
      "    num_steps_sampled: 611000\n",
      "    num_steps_trained: 611000\n",
      "  iterations_since_restore: 611\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.33673469387755\n",
      "    ram_util_percent: 34.471428571428575\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036844533402729966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.77310868292319\n",
      "    mean_inference_ms: 2.1180426320664827\n",
      "    mean_raw_obs_processing_ms: 29.245075022703713\n",
      "  time_since_restore: 31300.502345085144\n",
      "  time_this_iter_s: 33.82047653198242\n",
      "  time_total_s: 31300.502345085144\n",
      "  timers:\n",
      "    learn_throughput: 1327.486\n",
      "    learn_time_ms: 753.304\n",
      "    load_throughput: 58471.704\n",
      "    load_time_ms: 17.102\n",
      "    sample_throughput: 46.562\n",
      "    sample_time_ms: 21476.653\n",
      "    update_time_ms: 3.622\n",
      "  timestamp: 1635313885\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 611000\n",
      "  training_iteration: 611\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   611</td><td style=\"text-align: right;\">         31300.5</td><td style=\"text-align: right;\">611000</td><td style=\"text-align: right;\"> -2.4407</td><td style=\"text-align: right;\">                9.66</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            443.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 612000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-51-40\n",
      "  done: false\n",
      "  episode_len_mean: 443.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.660000000000004\n",
      "  episode_reward_mean: -2.415999999999951\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2352\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.801672251232869\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9848327808909947\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005581748099246909\n",
      "          policy_loss: -0.07723879449897342\n",
      "          total_loss: -0.06299261053403218\n",
      "          vf_explained_var: 0.5775437951087952\n",
      "          vf_loss: 0.029619780578650536\n",
      "    num_agent_steps_sampled: 612000\n",
      "    num_agent_steps_trained: 612000\n",
      "    num_steps_sampled: 612000\n",
      "    num_steps_trained: 612000\n",
      "  iterations_since_restore: 612\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.98636363636363\n",
      "    ram_util_percent: 34.49545454545454\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03684516074233521\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.769271720703806\n",
      "    mean_inference_ms: 2.1180615632329047\n",
      "    mean_raw_obs_processing_ms: 29.235009198479187\n",
      "  time_since_restore: 31316.048340320587\n",
      "  time_this_iter_s: 15.545995235443115\n",
      "  time_total_s: 31316.048340320587\n",
      "  timers:\n",
      "    learn_throughput: 1327.313\n",
      "    learn_time_ms: 753.402\n",
      "    load_throughput: 58465.673\n",
      "    load_time_ms: 17.104\n",
      "    sample_throughput: 46.383\n",
      "    sample_time_ms: 21559.629\n",
      "    update_time_ms: 3.617\n",
      "  timestamp: 1635313900\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 612000\n",
      "  training_iteration: 612\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   612</td><td style=\"text-align: right;\">           31316</td><td style=\"text-align: right;\">612000</td><td style=\"text-align: right;\">  -2.416</td><td style=\"text-align: right;\">                9.66</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            443.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 613000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-51-55\n",
      "  done: false\n",
      "  episode_len_mean: 447.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.660000000000004\n",
      "  episode_reward_mean: -2.5254999999999503\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2354\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.801672251232869\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4746800985601214\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005322609195119396\n",
      "          policy_loss: 0.09323088808192147\n",
      "          total_loss: 0.18276887966526878\n",
      "          vf_explained_var: 0.4919469654560089\n",
      "          vf_loss: 0.10001781214701219\n",
      "    num_agent_steps_sampled: 613000\n",
      "    num_agent_steps_trained: 613000\n",
      "    num_steps_sampled: 613000\n",
      "    num_steps_trained: 613000\n",
      "  iterations_since_restore: 613\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.90476190476191\n",
      "    ram_util_percent: 34.542857142857144\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03684643409558943\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.76136587490346\n",
      "    mean_inference_ms: 2.1180996883617937\n",
      "    mean_raw_obs_processing_ms: 29.214145995189334\n",
      "  time_since_restore: 31330.897245168686\n",
      "  time_this_iter_s: 14.848904848098755\n",
      "  time_total_s: 31330.897245168686\n",
      "  timers:\n",
      "    learn_throughput: 1325.848\n",
      "    learn_time_ms: 754.234\n",
      "    load_throughput: 63313.689\n",
      "    load_time_ms: 15.794\n",
      "    sample_throughput: 50.76\n",
      "    sample_time_ms: 19700.581\n",
      "    update_time_ms: 3.617\n",
      "  timestamp: 1635313915\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 613000\n",
      "  training_iteration: 613\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   613</td><td style=\"text-align: right;\">         31330.9</td><td style=\"text-align: right;\">613000</td><td style=\"text-align: right;\"> -2.5255</td><td style=\"text-align: right;\">                9.66</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            447.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 614000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-52-31\n",
      "  done: false\n",
      "  episode_len_mean: 446.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.660000000000004\n",
      "  episode_reward_mean: -2.436199999999951\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2356\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.801672251232869\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.225720689031813\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008089829271609833\n",
      "          policy_loss: 0.06389586478471757\n",
      "          total_loss: 0.19208507868978714\n",
      "          vf_explained_var: 0.29072675108909607\n",
      "          vf_loss: 0.14396102930315666\n",
      "    num_agent_steps_sampled: 614000\n",
      "    num_agent_steps_trained: 614000\n",
      "    num_steps_sampled: 614000\n",
      "    num_steps_trained: 614000\n",
      "  iterations_since_restore: 614\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.63076923076923\n",
      "    ram_util_percent: 34.503846153846155\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036847719708197786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.75337254146173\n",
      "    mean_inference_ms: 2.1181380766169102\n",
      "    mean_raw_obs_processing_ms: 29.19310153845996\n",
      "  time_since_restore: 31366.975796461105\n",
      "  time_this_iter_s: 36.078551292419434\n",
      "  time_total_s: 31366.975796461105\n",
      "  timers:\n",
      "    learn_throughput: 1325.788\n",
      "    learn_time_ms: 754.268\n",
      "    load_throughput: 59511.992\n",
      "    load_time_ms: 16.803\n",
      "    sample_throughput: 46.312\n",
      "    sample_time_ms: 21592.62\n",
      "    update_time_ms: 3.618\n",
      "  timestamp: 1635313951\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 614000\n",
      "  training_iteration: 614\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   614</td><td style=\"text-align: right;\">           31367</td><td style=\"text-align: right;\">614000</td><td style=\"text-align: right;\"> -2.4362</td><td style=\"text-align: right;\">                9.66</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">               446</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 615000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-53-07\n",
      "  done: false\n",
      "  episode_len_mean: 447.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.660000000000004\n",
      "  episode_reward_mean: -2.4924999999999504\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2359\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.801672251232869\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.168194532394409\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006027051056467138\n",
      "          policy_loss: -0.2131381054305368\n",
      "          total_loss: -0.1392909180579914\n",
      "          vf_explained_var: 0.5593042373657227\n",
      "          vf_loss: 0.09069741200655698\n",
      "    num_agent_steps_sampled: 615000\n",
      "    num_agent_steps_trained: 615000\n",
      "    num_steps_sampled: 615000\n",
      "    num_steps_trained: 615000\n",
      "  iterations_since_restore: 615\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.43000000000001\n",
      "    ram_util_percent: 34.440000000000005\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036849639790366324\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.74122966543851\n",
      "    mean_inference_ms: 2.1181958185619454\n",
      "    mean_raw_obs_processing_ms: 29.160755569133567\n",
      "  time_since_restore: 31402.322239637375\n",
      "  time_this_iter_s: 35.34644317626953\n",
      "  time_total_s: 31402.322239637375\n",
      "  timers:\n",
      "    learn_throughput: 1324.735\n",
      "    learn_time_ms: 754.868\n",
      "    load_throughput: 55022.997\n",
      "    load_time_ms: 18.174\n",
      "    sample_throughput: 42.457\n",
      "    sample_time_ms: 23553.47\n",
      "    update_time_ms: 3.812\n",
      "  timestamp: 1635313987\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 615000\n",
      "  training_iteration: 615\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   615</td><td style=\"text-align: right;\">         31402.3</td><td style=\"text-align: right;\">615000</td><td style=\"text-align: right;\"> -2.4925</td><td style=\"text-align: right;\">                9.66</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            447.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 616000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-53-45\n",
      "  done: false\n",
      "  episode_len_mean: 450.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.660000000000004\n",
      "  episode_reward_mean: -2.53759999999995\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2362\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.801672251232869\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0311781578593786\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008580046667639202\n",
      "          policy_loss: 0.008659511059522628\n",
      "          total_loss: 0.28687130726046034\n",
      "          vf_explained_var: 0.369459331035614\n",
      "          vf_loss: 0.29164518867101935\n",
      "    num_agent_steps_sampled: 616000\n",
      "    num_agent_steps_trained: 616000\n",
      "    num_steps_sampled: 616000\n",
      "    num_steps_trained: 616000\n",
      "  iterations_since_restore: 616\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.34545454545455\n",
      "    ram_util_percent: 34.45636363636365\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036851538499124634\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.72909057580932\n",
      "    mean_inference_ms: 2.118253194212372\n",
      "    mean_raw_obs_processing_ms: 29.12853214244693\n",
      "  time_since_restore: 31440.531221151352\n",
      "  time_this_iter_s: 38.20898151397705\n",
      "  time_total_s: 31440.531221151352\n",
      "  timers:\n",
      "    learn_throughput: 1322.49\n",
      "    learn_time_ms: 756.15\n",
      "    load_throughput: 52168.544\n",
      "    load_time_ms: 19.169\n",
      "    sample_throughput: 39.047\n",
      "    sample_time_ms: 25610.384\n",
      "    update_time_ms: 3.803\n",
      "  timestamp: 1635314025\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 616000\n",
      "  training_iteration: 616\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   616</td><td style=\"text-align: right;\">         31440.5</td><td style=\"text-align: right;\">616000</td><td style=\"text-align: right;\"> -2.5376</td><td style=\"text-align: right;\">                9.66</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            450.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 617000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-54-05\n",
      "  done: false\n",
      "  episode_len_mean: 447.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.660000000000004\n",
      "  episode_reward_mean: -2.4307999999999508\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2364\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.801672251232869\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.964883397685157\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009653807075379502\n",
      "          policy_loss: 0.021152372078763113\n",
      "          total_loss: 0.2174762312736776\n",
      "          vf_explained_var: 0.3630954623222351\n",
      "          vf_loss: 0.20823350693616602\n",
      "    num_agent_steps_sampled: 617000\n",
      "    num_agent_steps_trained: 617000\n",
      "    num_steps_sampled: 617000\n",
      "    num_steps_trained: 617000\n",
      "  iterations_since_restore: 617\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.30344827586208\n",
      "    ram_util_percent: 34.44137931034483\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0368527937770988\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.721053076492627\n",
      "    mean_inference_ms: 2.1182907867243133\n",
      "    mean_raw_obs_processing_ms: 29.10608978366825\n",
      "  time_since_restore: 31461.17191171646\n",
      "  time_this_iter_s: 20.640690565109253\n",
      "  time_total_s: 31461.17191171646\n",
      "  timers:\n",
      "    learn_throughput: 1323.913\n",
      "    learn_time_ms: 755.336\n",
      "    load_throughput: 52277.522\n",
      "    load_time_ms: 19.129\n",
      "    sample_throughput: 41.878\n",
      "    sample_time_ms: 23879.068\n",
      "    update_time_ms: 3.898\n",
      "  timestamp: 1635314045\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 617000\n",
      "  training_iteration: 617\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   617</td><td style=\"text-align: right;\">         31461.2</td><td style=\"text-align: right;\">617000</td><td style=\"text-align: right;\"> -2.4308</td><td style=\"text-align: right;\">                9.66</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            447.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 618000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-54-22\n",
      "  done: false\n",
      "  episode_len_mean: 456.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.660000000000004\n",
      "  episode_reward_mean: -2.6520999999999497\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2366\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.801672251232869\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8823073625564575\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00841470911425242\n",
      "          policy_loss: -0.10889491008387672\n",
      "          total_loss: 0.18004769566986295\n",
      "          vf_explained_var: 0.3995082974433899\n",
      "          vf_loss: 0.30101983849373126\n",
      "    num_agent_steps_sampled: 618000\n",
      "    num_agent_steps_trained: 618000\n",
      "    num_steps_sampled: 618000\n",
      "    num_steps_trained: 618000\n",
      "  iterations_since_restore: 618\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.72\n",
      "    ram_util_percent: 34.408\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03685405529634788\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.71282575094674\n",
      "    mean_inference_ms: 2.118328292467339\n",
      "    mean_raw_obs_processing_ms: 29.08166136648219\n",
      "  time_since_restore: 31478.191132068634\n",
      "  time_this_iter_s: 17.01922035217285\n",
      "  time_total_s: 31478.191132068634\n",
      "  timers:\n",
      "    learn_throughput: 1324.462\n",
      "    learn_time_ms: 755.024\n",
      "    load_throughput: 52280.259\n",
      "    load_time_ms: 19.128\n",
      "    sample_throughput: 41.841\n",
      "    sample_time_ms: 23900.121\n",
      "    update_time_ms: 3.901\n",
      "  timestamp: 1635314062\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 618000\n",
      "  training_iteration: 618\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   618</td><td style=\"text-align: right;\">         31478.2</td><td style=\"text-align: right;\">618000</td><td style=\"text-align: right;\"> -2.6521</td><td style=\"text-align: right;\">                9.66</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            456.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 619000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-54-39\n",
      "  done: false\n",
      "  episode_len_mean: 459.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.660000000000004\n",
      "  episode_reward_mean: -2.694299999999949\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2367\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.801672251232869\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9493513955010309\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00527589645135254\n",
      "          policy_loss: 0.13158571061988672\n",
      "          total_loss: 0.13762562562608058\n",
      "          vf_explained_var: 0.558136522769928\n",
      "          vf_loss: 0.02130388851898412\n",
      "    num_agent_steps_sampled: 619000\n",
      "    num_agent_steps_trained: 619000\n",
      "    num_steps_sampled: 619000\n",
      "    num_steps_trained: 619000\n",
      "  iterations_since_restore: 619\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.8304347826087\n",
      "    ram_util_percent: 34.304347826086946\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03685470009735393\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.708612847927796\n",
      "    mean_inference_ms: 2.118347247541134\n",
      "    mean_raw_obs_processing_ms: 29.06899034773301\n",
      "  time_since_restore: 31494.68605852127\n",
      "  time_this_iter_s: 16.49492645263672\n",
      "  time_total_s: 31494.68605852127\n",
      "  timers:\n",
      "    learn_throughput: 1325.276\n",
      "    learn_time_ms: 754.56\n",
      "    load_throughput: 51278.495\n",
      "    load_time_ms: 19.501\n",
      "    sample_throughput: 41.736\n",
      "    sample_time_ms: 23960.01\n",
      "    update_time_ms: 3.836\n",
      "  timestamp: 1635314079\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 619000\n",
      "  training_iteration: 619\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   619</td><td style=\"text-align: right;\">         31494.7</td><td style=\"text-align: right;\">619000</td><td style=\"text-align: right;\"> -2.6943</td><td style=\"text-align: right;\">                9.66</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">             459.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 620000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-55-14\n",
      "  done: false\n",
      "  episode_len_mean: 468.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.660000000000004\n",
      "  episode_reward_mean: -2.8984999999999492\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2369\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.801672251232869\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7857708228958977\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008119661068884515\n",
      "          policy_loss: 0.17385137072867818\n",
      "          total_loss: 0.3484636666874091\n",
      "          vf_explained_var: 0.5943260788917542\n",
      "          vf_loss: 0.18596069949368635\n",
      "    num_agent_steps_sampled: 620000\n",
      "    num_agent_steps_trained: 620000\n",
      "    num_steps_sampled: 620000\n",
      "    num_steps_trained: 620000\n",
      "  iterations_since_restore: 620\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.61836734693878\n",
      "    ram_util_percent: 34.36326530612245\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036856012677066646\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.70001817312032\n",
      "    mean_inference_ms: 2.1183859404120513\n",
      "    mean_raw_obs_processing_ms: 29.04328970719938\n",
      "  time_since_restore: 31529.23904323578\n",
      "  time_this_iter_s: 34.55298471450806\n",
      "  time_total_s: 31529.23904323578\n",
      "  timers:\n",
      "    learn_throughput: 1325.022\n",
      "    learn_time_ms: 754.705\n",
      "    load_throughput: 51164.275\n",
      "    load_time_ms: 19.545\n",
      "    sample_throughput: 39.257\n",
      "    sample_time_ms: 25473.29\n",
      "    update_time_ms: 3.836\n",
      "  timestamp: 1635314114\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 620000\n",
      "  training_iteration: 620\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   620</td><td style=\"text-align: right;\">         31529.2</td><td style=\"text-align: right;\">620000</td><td style=\"text-align: right;\"> -2.8985</td><td style=\"text-align: right;\">                9.66</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            468.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 621000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-55-33\n",
      "  done: false\n",
      "  episode_len_mean: 473.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.960000000000018\n",
      "  episode_reward_mean: -3.003499999999948\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2371\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.801672251232869\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.024201358689202\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007511509399751482\n",
      "          policy_loss: -0.08068852341837353\n",
      "          total_loss: -0.020905437817176183\n",
      "          vf_explained_var: 0.21674686670303345\n",
      "          vf_loss: 0.07400333437447747\n",
      "    num_agent_steps_sampled: 621000\n",
      "    num_agent_steps_trained: 621000\n",
      "    num_steps_sampled: 621000\n",
      "    num_steps_trained: 621000\n",
      "  iterations_since_restore: 621\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.28620689655173\n",
      "    ram_util_percent: 34.44137931034483\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03685734086376771\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.69133041936494\n",
      "    mean_inference_ms: 2.1184256994906896\n",
      "    mean_raw_obs_processing_ms: 29.017194749696014\n",
      "  time_since_restore: 31548.928738594055\n",
      "  time_this_iter_s: 19.689695358276367\n",
      "  time_total_s: 31548.928738594055\n",
      "  timers:\n",
      "    learn_throughput: 1326.248\n",
      "    learn_time_ms: 754.007\n",
      "    load_throughput: 51289.406\n",
      "    load_time_ms: 19.497\n",
      "    sample_throughput: 41.561\n",
      "    sample_time_ms: 24061.014\n",
      "    update_time_ms: 3.845\n",
      "  timestamp: 1635314133\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 621000\n",
      "  training_iteration: 621\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   621</td><td style=\"text-align: right;\">         31548.9</td><td style=\"text-align: right;\">621000</td><td style=\"text-align: right;\"> -3.0035</td><td style=\"text-align: right;\">                7.96</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            473.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 622000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-55-50\n",
      "  done: false\n",
      "  episode_len_mean: 474.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.960000000000018\n",
      "  episode_reward_mean: -3.030399999999948\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2372\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.801672251232869\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9175964328977797\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01222624266099144\n",
      "          policy_loss: -0.06836012448701594\n",
      "          total_loss: -0.058109817860855\n",
      "          vf_explained_var: 0.6524533033370972\n",
      "          vf_loss: 0.019624829002552562\n",
      "    num_agent_steps_sampled: 622000\n",
      "    num_agent_steps_trained: 622000\n",
      "    num_steps_sampled: 622000\n",
      "    num_steps_trained: 622000\n",
      "  iterations_since_restore: 622\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.84782608695652\n",
      "    ram_util_percent: 34.50869565217391\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036858016354491416\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.68688823439138\n",
      "    mean_inference_ms: 2.1184456853587177\n",
      "    mean_raw_obs_processing_ms: 29.003950942182584\n",
      "  time_since_restore: 31565.31473517418\n",
      "  time_this_iter_s: 16.3859965801239\n",
      "  time_total_s: 31565.31473517418\n",
      "  timers:\n",
      "    learn_throughput: 1324.066\n",
      "    learn_time_ms: 755.249\n",
      "    load_throughput: 50284.783\n",
      "    load_time_ms: 19.887\n",
      "    sample_throughput: 41.419\n",
      "    sample_time_ms: 24143.382\n",
      "    update_time_ms: 3.837\n",
      "  timestamp: 1635314150\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 622000\n",
      "  training_iteration: 622\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   622</td><td style=\"text-align: right;\">         31565.3</td><td style=\"text-align: right;\">622000</td><td style=\"text-align: right;\"> -3.0304</td><td style=\"text-align: right;\">                7.96</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            474.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 623000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-56-06\n",
      "  done: false\n",
      "  episode_len_mean: 477.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.960000000000018\n",
      "  episode_reward_mean: -3.0901999999999465\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2374\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.801672251232869\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9615858448876275\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00940743650888652\n",
      "          policy_loss: 0.05866180393430922\n",
      "          total_loss: 0.18334931515985065\n",
      "          vf_explained_var: 0.4657707214355469\n",
      "          vf_loss: 0.13676168616447185\n",
      "    num_agent_steps_sampled: 623000\n",
      "    num_agent_steps_trained: 623000\n",
      "    num_steps_sampled: 623000\n",
      "    num_steps_trained: 623000\n",
      "  iterations_since_restore: 623\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.77916666666665\n",
      "    ram_util_percent: 34.479166666666664\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036859360503535936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.677938928340357\n",
      "    mean_inference_ms: 2.118485493690544\n",
      "    mean_raw_obs_processing_ms: 28.977584936596635\n",
      "  time_since_restore: 31582.026788711548\n",
      "  time_this_iter_s: 16.712053537368774\n",
      "  time_total_s: 31582.026788711548\n",
      "  timers:\n",
      "    learn_throughput: 1325.724\n",
      "    learn_time_ms: 754.305\n",
      "    load_throughput: 49337.672\n",
      "    load_time_ms: 20.268\n",
      "    sample_throughput: 41.101\n",
      "    sample_time_ms: 24330.165\n",
      "    update_time_ms: 3.922\n",
      "  timestamp: 1635314166\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 623000\n",
      "  training_iteration: 623\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   623</td><td style=\"text-align: right;\">           31582</td><td style=\"text-align: right;\">623000</td><td style=\"text-align: right;\"> -3.0902</td><td style=\"text-align: right;\">                7.96</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">             477.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 624000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-56-24\n",
      "  done: false\n",
      "  episode_len_mean: 475.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.960000000000018\n",
      "  episode_reward_mean: -3.0488999999999478\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2376\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.801672251232869\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0163243797090318\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0069908740685281755\n",
      "          policy_loss: -0.09307911569873492\n",
      "          total_loss: -0.09328518791331185\n",
      "          vf_explained_var: 0.3302997946739197\n",
      "          vf_loss: 0.014352779750091334\n",
      "    num_agent_steps_sampled: 624000\n",
      "    num_agent_steps_trained: 624000\n",
      "    num_steps_sampled: 624000\n",
      "    num_steps_trained: 624000\n",
      "  iterations_since_restore: 624\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.6923076923077\n",
      "    ram_util_percent: 34.48846153846154\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0368606932954195\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.669138869651263\n",
      "    mean_inference_ms: 2.1185249084384568\n",
      "    mean_raw_obs_processing_ms: 28.951554260528937\n",
      "  time_since_restore: 31600.088017702103\n",
      "  time_this_iter_s: 18.06122899055481\n",
      "  time_total_s: 31600.088017702103\n",
      "  timers:\n",
      "    learn_throughput: 1328.283\n",
      "    learn_time_ms: 752.852\n",
      "    load_throughput: 49365.428\n",
      "    load_time_ms: 20.257\n",
      "    sample_throughput: 44.385\n",
      "    sample_time_ms: 22529.91\n",
      "    update_time_ms: 3.909\n",
      "  timestamp: 1635314184\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 624000\n",
      "  training_iteration: 624\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   624</td><td style=\"text-align: right;\">         31600.1</td><td style=\"text-align: right;\">624000</td><td style=\"text-align: right;\"> -3.0489</td><td style=\"text-align: right;\">                7.96</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            475.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 625000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-56-39\n",
      "  done: false\n",
      "  episode_len_mean: 477.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.960000000000018\n",
      "  episode_reward_mean: -3.137799999999947\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2377\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.801672251232869\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.648399563630422\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009829316540511475\n",
      "          policy_loss: -0.08069522421186169\n",
      "          total_loss: 0.067585687742879\n",
      "          vf_explained_var: 0.26474639773368835\n",
      "          vf_loss: 0.15688501231424096\n",
      "    num_agent_steps_sampled: 625000\n",
      "    num_agent_steps_trained: 625000\n",
      "    num_steps_sampled: 625000\n",
      "    num_steps_trained: 625000\n",
      "  iterations_since_restore: 625\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.90499999999999\n",
      "    ram_util_percent: 34.505\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03686136970309101\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.664651135510173\n",
      "    mean_inference_ms: 2.1185447719194808\n",
      "    mean_raw_obs_processing_ms: 28.938197401686356\n",
      "  time_since_restore: 31614.475884199142\n",
      "  time_this_iter_s: 14.387866497039795\n",
      "  time_total_s: 31614.475884199142\n",
      "  timers:\n",
      "    learn_throughput: 1329.8\n",
      "    learn_time_ms: 751.993\n",
      "    load_throughput: 52910.951\n",
      "    load_time_ms: 18.9\n",
      "    sample_throughput: 48.932\n",
      "    sample_time_ms: 20436.498\n",
      "    update_time_ms: 3.721\n",
      "  timestamp: 1635314199\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 625000\n",
      "  training_iteration: 625\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   625</td><td style=\"text-align: right;\">         31614.5</td><td style=\"text-align: right;\">625000</td><td style=\"text-align: right;\"> -3.1378</td><td style=\"text-align: right;\">                7.96</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            477.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 626000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-56-55\n",
      "  done: false\n",
      "  episode_len_mean: 479.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.960000000000018\n",
      "  episode_reward_mean: -3.1794999999999463\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2379\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.801672251232869\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7550653047031826\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009751209807405656\n",
      "          policy_loss: 0.04793620448973444\n",
      "          total_loss: 0.09329293372316493\n",
      "          vf_explained_var: -0.3034621477127075\n",
      "          vf_loss: 0.05509010844010239\n",
      "    num_agent_steps_sampled: 626000\n",
      "    num_agent_steps_trained: 626000\n",
      "    num_steps_sampled: 626000\n",
      "    num_steps_trained: 626000\n",
      "  iterations_since_restore: 626\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.47500000000001\n",
      "    ram_util_percent: 34.50833333333333\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03686272730287272\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.65563979374212\n",
      "    mean_inference_ms: 2.1185842924078147\n",
      "    mean_raw_obs_processing_ms: 28.91160388315624\n",
      "  time_since_restore: 31631.043404579163\n",
      "  time_this_iter_s: 16.56752038002014\n",
      "  time_total_s: 31631.043404579163\n",
      "  timers:\n",
      "    learn_throughput: 1330.587\n",
      "    learn_time_ms: 751.548\n",
      "    load_throughput: 56105.161\n",
      "    load_time_ms: 17.824\n",
      "    sample_throughput: 54.723\n",
      "    sample_time_ms: 18273.937\n",
      "    update_time_ms: 3.719\n",
      "  timestamp: 1635314215\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 626000\n",
      "  training_iteration: 626\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   626</td><td style=\"text-align: right;\">           31631</td><td style=\"text-align: right;\">626000</td><td style=\"text-align: right;\"> -3.1795</td><td style=\"text-align: right;\">                7.96</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            479.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 627000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-57-11\n",
      "  done: false\n",
      "  episode_len_mean: 481.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.960000000000018\n",
      "  episode_reward_mean: -3.248399999999945\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2381\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.801672251232869\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.834433032406701\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005893816802093157\n",
      "          policy_loss: 0.06463550710015827\n",
      "          total_loss: 0.06113554421398375\n",
      "          vf_explained_var: 0.1615619957447052\n",
      "          vf_loss: 0.01011945298458967\n",
      "    num_agent_steps_sampled: 627000\n",
      "    num_agent_steps_trained: 627000\n",
      "    num_steps_sampled: 627000\n",
      "    num_steps_trained: 627000\n",
      "  iterations_since_restore: 627\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.89545454545456\n",
      "    ram_util_percent: 34.51818181818182\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03686408918100187\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.6466392067593\n",
      "    mean_inference_ms: 2.1186236811673544\n",
      "    mean_raw_obs_processing_ms: 28.883881787787896\n",
      "  time_since_restore: 31646.762272834778\n",
      "  time_this_iter_s: 15.718868255615234\n",
      "  time_total_s: 31646.762272834778\n",
      "  timers:\n",
      "    learn_throughput: 1333.649\n",
      "    learn_time_ms: 749.822\n",
      "    load_throughput: 60505.969\n",
      "    load_time_ms: 16.527\n",
      "    sample_throughput: 56.228\n",
      "    sample_time_ms: 17784.887\n",
      "    update_time_ms: 3.621\n",
      "  timestamp: 1635314231\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 627000\n",
      "  training_iteration: 627\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   627</td><td style=\"text-align: right;\">         31646.8</td><td style=\"text-align: right;\">627000</td><td style=\"text-align: right;\"> -3.2484</td><td style=\"text-align: right;\">                7.96</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">             481.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 628000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-57-26\n",
      "  done: false\n",
      "  episode_len_mean: 488.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.960000000000018\n",
      "  episode_reward_mean: -3.352199999999944\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2382\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.801672251232869\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.841899331410726\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004224142466013506\n",
      "          policy_loss: -0.03055383919013871\n",
      "          total_loss: -0.03290601782500744\n",
      "          vf_explained_var: -0.36020922660827637\n",
      "          vf_loss: 0.012680438005675872\n",
      "    num_agent_steps_sampled: 628000\n",
      "    num_agent_steps_trained: 628000\n",
      "    num_steps_sampled: 628000\n",
      "    num_steps_trained: 628000\n",
      "  iterations_since_restore: 628\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.8809523809524\n",
      "    ram_util_percent: 34.523809523809526\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03686478229355818\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.64201034533412\n",
      "    mean_inference_ms: 2.118643538402269\n",
      "    mean_raw_obs_processing_ms: 28.869576516333378\n",
      "  time_since_restore: 31661.24717617035\n",
      "  time_this_iter_s: 14.484903335571289\n",
      "  time_total_s: 31661.24717617035\n",
      "  timers:\n",
      "    learn_throughput: 1334.176\n",
      "    learn_time_ms: 749.526\n",
      "    load_throughput: 61970.013\n",
      "    load_time_ms: 16.137\n",
      "    sample_throughput: 57.038\n",
      "    sample_time_ms: 17532.149\n",
      "    update_time_ms: 3.627\n",
      "  timestamp: 1635314246\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 628000\n",
      "  training_iteration: 628\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   628</td><td style=\"text-align: right;\">         31661.2</td><td style=\"text-align: right;\">628000</td><td style=\"text-align: right;\"> -3.3522</td><td style=\"text-align: right;\">                7.96</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            488.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 629000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-57-42\n",
      "  done: false\n",
      "  episode_len_mean: 488.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.960000000000018\n",
      "  episode_reward_mean: -3.345599999999944\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2384\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4008361256164345\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6235658685366312\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009738913572514038\n",
      "          policy_loss: -0.018977928658326468\n",
      "          total_loss: -0.015406658086511824\n",
      "          vf_explained_var: 0.7243502140045166\n",
      "          vf_loss: 0.0159032218494556\n",
      "    num_agent_steps_sampled: 629000\n",
      "    num_agent_steps_trained: 629000\n",
      "    num_steps_sampled: 629000\n",
      "    num_steps_trained: 629000\n",
      "  iterations_since_restore: 629\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.80869565217391\n",
      "    ram_util_percent: 34.53913043478261\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036866145436171636\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.63278432300403\n",
      "    mean_inference_ms: 2.1186823007651685\n",
      "    mean_raw_obs_processing_ms: 28.84159360480481\n",
      "  time_since_restore: 31677.168643712997\n",
      "  time_this_iter_s: 15.921467542648315\n",
      "  time_total_s: 31677.168643712997\n",
      "  timers:\n",
      "    learn_throughput: 1334.313\n",
      "    learn_time_ms: 749.449\n",
      "    load_throughput: 63390.432\n",
      "    load_time_ms: 15.775\n",
      "    sample_throughput: 57.224\n",
      "    sample_time_ms: 17475.262\n",
      "    update_time_ms: 3.615\n",
      "  timestamp: 1635314262\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 629000\n",
      "  training_iteration: 629\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   629</td><td style=\"text-align: right;\">         31677.2</td><td style=\"text-align: right;\">629000</td><td style=\"text-align: right;\"> -3.3456</td><td style=\"text-align: right;\">                7.96</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            488.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 630000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-57-57\n",
      "  done: false\n",
      "  episode_len_mean: 489.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.960000000000018\n",
      "  episode_reward_mean: -3.346699999999944\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2385\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4008361256164345\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7624103837543064\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011402375961511094\n",
      "          policy_loss: -0.032897322790490256\n",
      "          total_loss: 0.09464740637275908\n",
      "          vf_explained_var: 0.3021707832813263\n",
      "          vf_loss: 0.1405983500337849\n",
      "    num_agent_steps_sampled: 630000\n",
      "    num_agent_steps_trained: 630000\n",
      "    num_steps_sampled: 630000\n",
      "    num_steps_trained: 630000\n",
      "  iterations_since_restore: 630\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.89545454545454\n",
      "    ram_util_percent: 34.527272727272724\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036866831050883835\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.62810186474228\n",
      "    mean_inference_ms: 2.1187016270021446\n",
      "    mean_raw_obs_processing_ms: 28.82741213973679\n",
      "  time_since_restore: 31692.507845163345\n",
      "  time_this_iter_s: 15.3392014503479\n",
      "  time_total_s: 31692.507845163345\n",
      "  timers:\n",
      "    learn_throughput: 1335.281\n",
      "    learn_time_ms: 748.906\n",
      "    load_throughput: 69492.513\n",
      "    load_time_ms: 14.39\n",
      "    sample_throughput: 64.285\n",
      "    sample_time_ms: 15555.825\n",
      "    update_time_ms: 3.618\n",
      "  timestamp: 1635314277\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 630000\n",
      "  training_iteration: 630\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   630</td><td style=\"text-align: right;\">         31692.5</td><td style=\"text-align: right;\">630000</td><td style=\"text-align: right;\"> -3.3467</td><td style=\"text-align: right;\">                7.96</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            489.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 631000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-58-12\n",
      "  done: false\n",
      "  episode_len_mean: 490.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.960000000000018\n",
      "  episode_reward_mean: -3.327799999999944\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2387\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4008361256164345\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.850740283065372\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007706431372644238\n",
      "          policy_loss: -0.0406924941473537\n",
      "          total_loss: -0.024918856554561192\n",
      "          vf_explained_var: -0.4765196144580841\n",
      "          vf_loss: 0.031192024182140208\n",
      "    num_agent_steps_sampled: 631000\n",
      "    num_agent_steps_trained: 631000\n",
      "    num_steps_sampled: 631000\n",
      "    num_steps_trained: 631000\n",
      "  iterations_since_restore: 631\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.92380952380954\n",
      "    ram_util_percent: 34.50476190476191\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03686820885665167\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.618738197828268\n",
      "    mean_inference_ms: 2.1187403155714746\n",
      "    mean_raw_obs_processing_ms: 28.799171868540924\n",
      "  time_since_restore: 31707.177637577057\n",
      "  time_this_iter_s: 14.669792413711548\n",
      "  time_total_s: 31707.177637577057\n",
      "  timers:\n",
      "    learn_throughput: 1336.683\n",
      "    learn_time_ms: 748.12\n",
      "    load_throughput: 77188.155\n",
      "    load_time_ms: 12.955\n",
      "    sample_throughput: 66.418\n",
      "    sample_time_ms: 15056.054\n",
      "    update_time_ms: 3.616\n",
      "  timestamp: 1635314292\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 631000\n",
      "  training_iteration: 631\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   631</td><td style=\"text-align: right;\">         31707.2</td><td style=\"text-align: right;\">631000</td><td style=\"text-align: right;\"> -3.3278</td><td style=\"text-align: right;\">                7.96</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            490.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 632000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-58-30\n",
      "  done: false\n",
      "  episode_len_mean: 490.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.960000000000018\n",
      "  episode_reward_mean: -3.3067999999999445\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2388\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4008361256164345\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.845888113975525\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00868531566485608\n",
      "          policy_loss: -0.03607989094323582\n",
      "          total_loss: -0.01718051474955347\n",
      "          vf_explained_var: 0.5071998834609985\n",
      "          vf_loss: 0.03387686785103546\n",
      "    num_agent_steps_sampled: 632000\n",
      "    num_agent_steps_trained: 632000\n",
      "    num_steps_sampled: 632000\n",
      "    num_steps_trained: 632000\n",
      "  iterations_since_restore: 632\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.80384615384615\n",
      "    ram_util_percent: 34.47692307692308\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036868895468173776\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.61406848952131\n",
      "    mean_inference_ms: 2.1187595815819473\n",
      "    mean_raw_obs_processing_ms: 28.78450707549527\n",
      "  time_since_restore: 31725.792288303375\n",
      "  time_this_iter_s: 18.61465072631836\n",
      "  time_total_s: 31725.792288303375\n",
      "  timers:\n",
      "    learn_throughput: 1338.025\n",
      "    learn_time_ms: 747.37\n",
      "    load_throughput: 71806.011\n",
      "    load_time_ms: 13.926\n",
      "    sample_throughput: 65.451\n",
      "    sample_time_ms: 15278.699\n",
      "    update_time_ms: 3.623\n",
      "  timestamp: 1635314310\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 632000\n",
      "  training_iteration: 632\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   632</td><td style=\"text-align: right;\">         31725.8</td><td style=\"text-align: right;\">632000</td><td style=\"text-align: right;\"> -3.3068</td><td style=\"text-align: right;\">                7.96</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">               490</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 633000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-58-48\n",
      "  done: false\n",
      "  episode_len_mean: 499.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.960000000000018\n",
      "  episode_reward_mean: -3.498999999999943\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2390\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4008361256164345\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.919203027089437\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012965437825834196\n",
      "          policy_loss: 0.07315900673468907\n",
      "          total_loss: 0.17480938111742336\n",
      "          vf_explained_var: 0.6711323857307434\n",
      "          vf_loss: 0.11564539079036977\n",
      "    num_agent_steps_sampled: 633000\n",
      "    num_agent_steps_trained: 633000\n",
      "    num_steps_sampled: 633000\n",
      "    num_steps_trained: 633000\n",
      "  iterations_since_restore: 633\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.74\n",
      "    ram_util_percent: 34.440000000000005\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03687030002218119\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.604570585116736\n",
      "    mean_inference_ms: 2.1187987291471764\n",
      "    mean_raw_obs_processing_ms: 28.754304500312514\n",
      "  time_since_restore: 31743.343797683716\n",
      "  time_this_iter_s: 17.551509380340576\n",
      "  time_total_s: 31743.343797683716\n",
      "  timers:\n",
      "    learn_throughput: 1336.19\n",
      "    learn_time_ms: 748.396\n",
      "    load_throughput: 71742.88\n",
      "    load_time_ms: 13.939\n",
      "    sample_throughput: 65.097\n",
      "    sample_time_ms: 15361.697\n",
      "    update_time_ms: 3.534\n",
      "  timestamp: 1635314328\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 633000\n",
      "  training_iteration: 633\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   633</td><td style=\"text-align: right;\">         31743.3</td><td style=\"text-align: right;\">633000</td><td style=\"text-align: right;\">  -3.499</td><td style=\"text-align: right;\">                7.96</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            499.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 634000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-59-04\n",
      "  done: false\n",
      "  episode_len_mean: 499.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.960000000000018\n",
      "  episode_reward_mean: -3.554899999999942\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2392\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4008361256164345\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.761874384350247\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011917727590388378\n",
      "          policy_loss: -0.025193016231060027\n",
      "          total_loss: 0.002676659408542845\n",
      "          vf_explained_var: 0.5676082968711853\n",
      "          vf_loss: 0.04071136269097527\n",
      "    num_agent_steps_sampled: 634000\n",
      "    num_agent_steps_trained: 634000\n",
      "    num_steps_sampled: 634000\n",
      "    num_steps_trained: 634000\n",
      "  iterations_since_restore: 634\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.4695652173913\n",
      "    ram_util_percent: 34.42173913043477\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036871683232229856\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.595220847941103\n",
      "    mean_inference_ms: 2.1188370532390524\n",
      "    mean_raw_obs_processing_ms: 28.724069845867167\n",
      "  time_since_restore: 31759.404349327087\n",
      "  time_this_iter_s: 16.060551643371582\n",
      "  time_total_s: 31759.404349327087\n",
      "  timers:\n",
      "    learn_throughput: 1334.626\n",
      "    learn_time_ms: 749.273\n",
      "    load_throughput: 77265.223\n",
      "    load_time_ms: 12.942\n",
      "    sample_throughput: 65.955\n",
      "    sample_time_ms: 15161.745\n",
      "    update_time_ms: 3.549\n",
      "  timestamp: 1635314344\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 634000\n",
      "  training_iteration: 634\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   634</td><td style=\"text-align: right;\">         31759.4</td><td style=\"text-align: right;\">634000</td><td style=\"text-align: right;\"> -3.5549</td><td style=\"text-align: right;\">                7.96</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            499.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 635000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-59-20\n",
      "  done: false\n",
      "  episode_len_mean: 500.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.960000000000018\n",
      "  episode_reward_mean: -3.5500999999999423\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2393\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4008361256164345\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.802342579099867\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013519737929747953\n",
      "          policy_loss: -0.138179195434269\n",
      "          total_loss: -0.07821996610404716\n",
      "          vf_explained_var: -0.2666940689086914\n",
      "          vf_loss: 0.07256345537413532\n",
      "    num_agent_steps_sampled: 635000\n",
      "    num_agent_steps_trained: 635000\n",
      "    num_steps_sampled: 635000\n",
      "    num_steps_trained: 635000\n",
      "  iterations_since_restore: 635\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.01739130434783\n",
      "    ram_util_percent: 34.36956521739129\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03687237861327128\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.590484531746668\n",
      "    mean_inference_ms: 2.1188562172062477\n",
      "    mean_raw_obs_processing_ms: 28.70876663209413\n",
      "  time_since_restore: 31775.139678001404\n",
      "  time_this_iter_s: 15.735328674316406\n",
      "  time_total_s: 31775.139678001404\n",
      "  timers:\n",
      "    learn_throughput: 1334.176\n",
      "    learn_time_ms: 749.526\n",
      "    load_throughput: 77329.897\n",
      "    load_time_ms: 12.932\n",
      "    sample_throughput: 65.375\n",
      "    sample_time_ms: 15296.25\n",
      "    update_time_ms: 3.542\n",
      "  timestamp: 1635314360\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 635000\n",
      "  training_iteration: 635\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   635</td><td style=\"text-align: right;\">         31775.1</td><td style=\"text-align: right;\">635000</td><td style=\"text-align: right;\"> -3.5501</td><td style=\"text-align: right;\">                7.96</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            500.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 636000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-59-35\n",
      "  done: false\n",
      "  episode_len_mean: 506.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.960000000000018\n",
      "  episode_reward_mean: -3.7170999999999412\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2395\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4008361256164345\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8577978928883871\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008058851948512588\n",
      "          policy_loss: -0.016396630224254395\n",
      "          total_loss: 0.013141981760660807\n",
      "          vf_explained_var: 0.11945796757936478\n",
      "          vf_loss: 0.044886310269228284\n",
      "    num_agent_steps_sampled: 636000\n",
      "    num_agent_steps_trained: 636000\n",
      "    num_steps_sampled: 636000\n",
      "    num_steps_trained: 636000\n",
      "  iterations_since_restore: 636\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.81818181818181\n",
      "    ram_util_percent: 34.30454545454545\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03687377051760257\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.58089392793179\n",
      "    mean_inference_ms: 2.118893993525786\n",
      "    mean_raw_obs_processing_ms: 28.677715631466896\n",
      "  time_since_restore: 31790.45607304573\n",
      "  time_this_iter_s: 15.316395044326782\n",
      "  time_total_s: 31790.45607304573\n",
      "  timers:\n",
      "    learn_throughput: 1334.218\n",
      "    learn_time_ms: 749.502\n",
      "    load_throughput: 79597.374\n",
      "    load_time_ms: 12.563\n",
      "    sample_throughput: 65.913\n",
      "    sample_time_ms: 15171.493\n",
      "    update_time_ms: 3.562\n",
      "  timestamp: 1635314375\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 636000\n",
      "  training_iteration: 636\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   636</td><td style=\"text-align: right;\">         31790.5</td><td style=\"text-align: right;\">636000</td><td style=\"text-align: right;\"> -3.7171</td><td style=\"text-align: right;\">                7.96</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            506.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 637000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_05-59-50\n",
      "  done: false\n",
      "  episode_len_mean: 506.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.960000000000018\n",
      "  episode_reward_mean: -3.735199999999941\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2396\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4008361256164345\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.02400672170851\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006243477659949459\n",
      "          policy_loss: 0.22842605171932115\n",
      "          total_loss: 0.21731395655208163\n",
      "          vf_explained_var: -0.2477981299161911\n",
      "          vf_loss: 0.006625358602226091\n",
      "    num_agent_steps_sampled: 637000\n",
      "    num_agent_steps_trained: 637000\n",
      "    num_steps_sampled: 637000\n",
      "    num_steps_trained: 637000\n",
      "  iterations_since_restore: 637\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.91428571428571\n",
      "    ram_util_percent: 34.271428571428565\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03687447866358609\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.575982805665884\n",
      "    mean_inference_ms: 2.118913132203968\n",
      "    mean_raw_obs_processing_ms: 28.661758360021853\n",
      "  time_since_restore: 31805.5414352417\n",
      "  time_this_iter_s: 15.085362195968628\n",
      "  time_total_s: 31805.5414352417\n",
      "  timers:\n",
      "    learn_throughput: 1333.116\n",
      "    learn_time_ms: 750.122\n",
      "    load_throughput: 79462.257\n",
      "    load_time_ms: 12.585\n",
      "    sample_throughput: 66.192\n",
      "    sample_time_ms: 15107.488\n",
      "    update_time_ms: 3.565\n",
      "  timestamp: 1635314390\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 637000\n",
      "  training_iteration: 637\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   637</td><td style=\"text-align: right;\">         31805.5</td><td style=\"text-align: right;\">637000</td><td style=\"text-align: right;\"> -3.7352</td><td style=\"text-align: right;\">                7.96</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            506.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 638000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-00-09\n",
      "  done: false\n",
      "  episode_len_mean: 506.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.960000000000018\n",
      "  episode_reward_mean: -3.711699999999942\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2398\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4008361256164345\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9279461397065056\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02195924388107099\n",
      "          policy_loss: -0.08429894361438023\n",
      "          total_loss: 0.05510411156962315\n",
      "          vf_explained_var: 0.526257336139679\n",
      "          vf_loss: 0.1498804591389166\n",
      "    num_agent_steps_sampled: 638000\n",
      "    num_agent_steps_trained: 638000\n",
      "    num_steps_sampled: 638000\n",
      "    num_steps_trained: 638000\n",
      "  iterations_since_restore: 638\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.67857142857142\n",
      "    ram_util_percent: 34.21071428571429\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03687586993107445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.566455717240913\n",
      "    mean_inference_ms: 2.118950812769964\n",
      "    mean_raw_obs_processing_ms: 28.629229322716498\n",
      "  time_since_restore: 31824.677532196045\n",
      "  time_this_iter_s: 19.136096954345703\n",
      "  time_total_s: 31824.677532196045\n",
      "  timers:\n",
      "    learn_throughput: 1331.896\n",
      "    learn_time_ms: 750.809\n",
      "    load_throughput: 71365.441\n",
      "    load_time_ms: 14.012\n",
      "    sample_throughput: 64.224\n",
      "    sample_time_ms: 15570.514\n",
      "    update_time_ms: 3.553\n",
      "  timestamp: 1635314409\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 638000\n",
      "  training_iteration: 638\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   638</td><td style=\"text-align: right;\">         31824.7</td><td style=\"text-align: right;\">638000</td><td style=\"text-align: right;\"> -3.7117</td><td style=\"text-align: right;\">                7.96</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            506.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 639000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-00-42\n",
      "  done: false\n",
      "  episode_len_mean: 516.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.180000000000012\n",
      "  episode_reward_mean: -3.9556999999999403\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2400\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6012541884246518\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5287062320444318\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008871253793962848\n",
      "          policy_loss: -0.027770901937037705\n",
      "          total_loss: -0.016975927187336814\n",
      "          vf_explained_var: 0.36042556166648865\n",
      "          vf_loss: 0.02074815857793308\n",
      "    num_agent_steps_sampled: 639000\n",
      "    num_agent_steps_trained: 639000\n",
      "    num_steps_sampled: 639000\n",
      "    num_steps_trained: 639000\n",
      "  iterations_since_restore: 639\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.8304347826087\n",
      "    ram_util_percent: 34.12173913043479\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036877275155923143\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.556844467389215\n",
      "    mean_inference_ms: 2.1189888081562245\n",
      "    mean_raw_obs_processing_ms: 28.595631026102836\n",
      "  time_since_restore: 31857.4150056839\n",
      "  time_this_iter_s: 32.737473487854004\n",
      "  time_total_s: 31857.4150056839\n",
      "  timers:\n",
      "    learn_throughput: 1331.409\n",
      "    learn_time_ms: 751.084\n",
      "    load_throughput: 64171.16\n",
      "    load_time_ms: 15.583\n",
      "    sample_throughput: 57.97\n",
      "    sample_time_ms: 17250.188\n",
      "    update_time_ms: 3.631\n",
      "  timestamp: 1635314442\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 639000\n",
      "  training_iteration: 639\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   639</td><td style=\"text-align: right;\">         31857.4</td><td style=\"text-align: right;\">639000</td><td style=\"text-align: right;\"> -3.9557</td><td style=\"text-align: right;\">                7.18</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            516.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 640000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-00-59\n",
      "  done: false\n",
      "  episode_len_mean: 520.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.180000000000012\n",
      "  episode_reward_mean: -4.033199999999939\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2401\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6012541884246518\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.091659853193495\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004745819656525847\n",
      "          policy_loss: -0.02436101676689254\n",
      "          total_loss: -0.036312742365731135\n",
      "          vf_explained_var: 0.11263960599899292\n",
      "          vf_loss: 0.0061114271598247194\n",
      "    num_agent_steps_sampled: 640000\n",
      "    num_agent_steps_trained: 640000\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "  iterations_since_restore: 640\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.57600000000001\n",
      "    ram_util_percent: 34.288000000000004\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03687799139077322\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.55195330835448\n",
      "    mean_inference_ms: 2.1190081267122314\n",
      "    mean_raw_obs_processing_ms: 28.578403715888314\n",
      "  time_since_restore: 31874.39938044548\n",
      "  time_this_iter_s: 16.98437476158142\n",
      "  time_total_s: 31874.39938044548\n",
      "  timers:\n",
      "    learn_throughput: 1331.656\n",
      "    learn_time_ms: 750.945\n",
      "    load_throughput: 60406.019\n",
      "    load_time_ms: 16.555\n",
      "    sample_throughput: 57.425\n",
      "    sample_time_ms: 17413.871\n",
      "    update_time_ms: 3.623\n",
      "  timestamp: 1635314459\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 640\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   640</td><td style=\"text-align: right;\">         31874.4</td><td style=\"text-align: right;\">640000</td><td style=\"text-align: right;\"> -4.0332</td><td style=\"text-align: right;\">                7.18</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            520.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 641000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-01-18\n",
      "  done: false\n",
      "  episode_len_mean: 523.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.180000000000012\n",
      "  episode_reward_mean: -4.091099999999939\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2403\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3006270942123259\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9584858708911472\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020141178697331005\n",
      "          policy_loss: -0.10705564415289295\n",
      "          total_loss: 0.010429864273303086\n",
      "          vf_explained_var: 0.5627822875976562\n",
      "          vf_loss: 0.13101538223110967\n",
      "    num_agent_steps_sampled: 641000\n",
      "    num_agent_steps_trained: 641000\n",
      "    num_steps_sampled: 641000\n",
      "    num_steps_trained: 641000\n",
      "  iterations_since_restore: 641\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.01851851851852\n",
      "    ram_util_percent: 34.3074074074074\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03687941764663324\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.54210240390778\n",
      "    mean_inference_ms: 2.11904656516627\n",
      "    mean_raw_obs_processing_ms: 28.543478114172185\n",
      "  time_since_restore: 31893.275005102158\n",
      "  time_this_iter_s: 18.875624656677246\n",
      "  time_total_s: 31893.275005102158\n",
      "  timers:\n",
      "    learn_throughput: 1332.269\n",
      "    learn_time_ms: 750.599\n",
      "    load_throughput: 55887.391\n",
      "    load_time_ms: 17.893\n",
      "    sample_throughput: 56.074\n",
      "    sample_time_ms: 17833.462\n",
      "    update_time_ms: 3.623\n",
      "  timestamp: 1635314478\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 641000\n",
      "  training_iteration: 641\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   641</td><td style=\"text-align: right;\">         31893.3</td><td style=\"text-align: right;\">641000</td><td style=\"text-align: right;\"> -4.0911</td><td style=\"text-align: right;\">                7.18</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            523.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 642000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-01-36\n",
      "  done: false\n",
      "  episode_len_mean: 522.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.180000000000012\n",
      "  episode_reward_mean: -4.06199999999994\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2405\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45094064131848877\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.026220821009742\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015366508466281579\n",
      "          policy_loss: 0.043780621969037585\n",
      "          total_loss: 0.12256619969589842\n",
      "          vf_explained_var: 0.11306548863649368\n",
      "          vf_loss: 0.09211839984378054\n",
      "    num_agent_steps_sampled: 642000\n",
      "    num_agent_steps_trained: 642000\n",
      "    num_steps_sampled: 642000\n",
      "    num_steps_trained: 642000\n",
      "  iterations_since_restore: 642\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.92399999999999\n",
      "    ram_util_percent: 34.31999999999999\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03688085200434291\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.532214951280498\n",
      "    mean_inference_ms: 2.11908525200046\n",
      "    mean_raw_obs_processing_ms: 28.508195950190547\n",
      "  time_since_restore: 31911.146023750305\n",
      "  time_this_iter_s: 17.871018648147583\n",
      "  time_total_s: 31911.146023750305\n",
      "  timers:\n",
      "    learn_throughput: 1333.9\n",
      "    learn_time_ms: 749.681\n",
      "    load_throughput: 58929.952\n",
      "    load_time_ms: 16.969\n",
      "    sample_throughput: 56.304\n",
      "    sample_time_ms: 17760.841\n",
      "    update_time_ms: 3.711\n",
      "  timestamp: 1635314496\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 642000\n",
      "  training_iteration: 642\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   642</td><td style=\"text-align: right;\">         31911.1</td><td style=\"text-align: right;\">642000</td><td style=\"text-align: right;\">  -4.062</td><td style=\"text-align: right;\">                7.18</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">             522.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 643000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-01-53\n",
      "  done: false\n",
      "  episode_len_mean: 526.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.180000000000012\n",
      "  episode_reward_mean: -4.060399999999939\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2407\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45094064131848877\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0219615472687615\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01588096244299849\n",
      "          policy_loss: -0.04422377559045951\n",
      "          total_loss: 0.035181465331051084\n",
      "          vf_explained_var: 0.35851165652275085\n",
      "          vf_loss: 0.09246348714223132\n",
      "    num_agent_steps_sampled: 643000\n",
      "    num_agent_steps_trained: 643000\n",
      "    num_steps_sampled: 643000\n",
      "    num_steps_trained: 643000\n",
      "  iterations_since_restore: 643\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.78\n",
      "    ram_util_percent: 34.38399999999999\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03688227059273466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.522547021329288\n",
      "    mean_inference_ms: 2.1191231619048887\n",
      "    mean_raw_obs_processing_ms: 28.472334504873125\n",
      "  time_since_restore: 31928.502480983734\n",
      "  time_this_iter_s: 17.356457233428955\n",
      "  time_total_s: 31928.502480983734\n",
      "  timers:\n",
      "    learn_throughput: 1334.514\n",
      "    learn_time_ms: 749.336\n",
      "    load_throughput: 58964.913\n",
      "    load_time_ms: 16.959\n",
      "    sample_throughput: 56.365\n",
      "    sample_time_ms: 17741.619\n",
      "    update_time_ms: 3.793\n",
      "  timestamp: 1635314513\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 643000\n",
      "  training_iteration: 643\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   643</td><td style=\"text-align: right;\">         31928.5</td><td style=\"text-align: right;\">643000</td><td style=\"text-align: right;\"> -4.0604</td><td style=\"text-align: right;\">                7.18</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            526.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 644000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-02-12\n",
      "  done: false\n",
      "  episode_len_mean: 527.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.180000000000012\n",
      "  episode_reward_mean: -4.10699999999994\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2409\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45094064131848877\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9994304643736944\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011950085058156507\n",
      "          policy_loss: 0.028805669107370906\n",
      "          total_loss: 0.23839770733482307\n",
      "          vf_explained_var: 0.37431368231773376\n",
      "          vf_loss: 0.2241975583963924\n",
      "    num_agent_steps_sampled: 644000\n",
      "    num_agent_steps_trained: 644000\n",
      "    num_steps_sampled: 644000\n",
      "    num_steps_trained: 644000\n",
      "  iterations_since_restore: 644\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.83333333333333\n",
      "    ram_util_percent: 34.455555555555556\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03688369392711304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.51287354134924\n",
      "    mean_inference_ms: 2.11916101250466\n",
      "    mean_raw_obs_processing_ms: 28.4357831539471\n",
      "  time_since_restore: 31947.276685476303\n",
      "  time_this_iter_s: 18.77420449256897\n",
      "  time_total_s: 31947.276685476303\n",
      "  timers:\n",
      "    learn_throughput: 1334.758\n",
      "    learn_time_ms: 749.199\n",
      "    load_throughput: 55717.601\n",
      "    load_time_ms: 17.948\n",
      "    sample_throughput: 55.518\n",
      "    sample_time_ms: 18012.123\n",
      "    update_time_ms: 3.802\n",
      "  timestamp: 1635314532\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 644000\n",
      "  training_iteration: 644\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   644</td><td style=\"text-align: right;\">         31947.3</td><td style=\"text-align: right;\">644000</td><td style=\"text-align: right;\">  -4.107</td><td style=\"text-align: right;\">                7.18</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            527.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 645000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-02-30\n",
      "  done: false\n",
      "  episode_len_mean: 527.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.180000000000012\n",
      "  episode_reward_mean: -4.08469999999994\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2410\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45094064131848877\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0427525480588278\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013006996588661125\n",
      "          policy_loss: -0.05098726683192783\n",
      "          total_loss: 0.009659947413537238\n",
      "          vf_explained_var: 0.6005343794822693\n",
      "          vf_loss: 0.0752093574239148\n",
      "    num_agent_steps_sampled: 645000\n",
      "    num_agent_steps_trained: 645000\n",
      "    num_steps_sampled: 645000\n",
      "    num_steps_trained: 645000\n",
      "  iterations_since_restore: 645\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.84400000000001\n",
      "    ram_util_percent: 34.492000000000004\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03688440518104111\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.508020082348246\n",
      "    mean_inference_ms: 2.1191797893512687\n",
      "    mean_raw_obs_processing_ms: 28.417163036940043\n",
      "  time_since_restore: 31965.076937437057\n",
      "  time_this_iter_s: 17.800251960754395\n",
      "  time_total_s: 31965.076937437057\n",
      "  timers:\n",
      "    learn_throughput: 1331.853\n",
      "    learn_time_ms: 750.834\n",
      "    load_throughput: 54571.907\n",
      "    load_time_ms: 18.324\n",
      "    sample_throughput: 54.895\n",
      "    sample_time_ms: 18216.597\n",
      "    update_time_ms: 3.804\n",
      "  timestamp: 1635314550\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 645000\n",
      "  training_iteration: 645\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   645</td><td style=\"text-align: right;\">         31965.1</td><td style=\"text-align: right;\">645000</td><td style=\"text-align: right;\"> -4.0847</td><td style=\"text-align: right;\">                7.18</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            527.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 646000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-03-03\n",
      "  done: false\n",
      "  episode_len_mean: 521.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.180000000000012\n",
      "  episode_reward_mean: -4.03749999999994\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2413\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45094064131848877\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.092796222368876\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.042119039270454635\n",
      "          policy_loss: -0.07245223960942693\n",
      "          total_loss: 0.14695337290565172\n",
      "          vf_explained_var: 0.6222816109657288\n",
      "          vf_loss: 0.22134039004643757\n",
      "    num_agent_steps_sampled: 646000\n",
      "    num_agent_steps_trained: 646000\n",
      "    num_steps_sampled: 646000\n",
      "    num_steps_trained: 646000\n",
      "  iterations_since_restore: 646\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.44042553191489\n",
      "    ram_util_percent: 34.46382978723403\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036886524541320125\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.493515396558234\n",
      "    mean_inference_ms: 2.1192350522994703\n",
      "    mean_raw_obs_processing_ms: 28.3613079316293\n",
      "  time_since_restore: 31997.899680376053\n",
      "  time_this_iter_s: 32.82274293899536\n",
      "  time_total_s: 31997.899680376053\n",
      "  timers:\n",
      "    learn_throughput: 1329.934\n",
      "    learn_time_ms: 751.917\n",
      "    load_throughput: 50888.84\n",
      "    load_time_ms: 19.651\n",
      "    sample_throughput: 50.088\n",
      "    sample_time_ms: 19964.851\n",
      "    update_time_ms: 3.786\n",
      "  timestamp: 1635314583\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 646000\n",
      "  training_iteration: 646\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   646</td><td style=\"text-align: right;\">         31997.9</td><td style=\"text-align: right;\">646000</td><td style=\"text-align: right;\"> -4.0375</td><td style=\"text-align: right;\">                7.18</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            521.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 647000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-03-21\n",
      "  done: false\n",
      "  episode_len_mean: 532.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.83\n",
      "  episode_reward_mean: -4.242499999999939\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2415\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6764109619777329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0097891012827556\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013719122074117528\n",
      "          policy_loss: 0.02385219410061836\n",
      "          total_loss: 0.12132357226477729\n",
      "          vf_explained_var: 0.41100645065307617\n",
      "          vf_loss: 0.10828950433577929\n",
      "    num_agent_steps_sampled: 647000\n",
      "    num_agent_steps_trained: 647000\n",
      "    num_steps_sampled: 647000\n",
      "    num_steps_trained: 647000\n",
      "  iterations_since_restore: 647\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.42692307692309\n",
      "    ram_util_percent: 34.61153846153846\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03688794281859915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.483888104233053\n",
      "    mean_inference_ms: 2.119271693570748\n",
      "    mean_raw_obs_processing_ms: 28.323071681642496\n",
      "  time_since_restore: 32015.762296438217\n",
      "  time_this_iter_s: 17.862616062164307\n",
      "  time_total_s: 32015.762296438217\n",
      "  timers:\n",
      "    learn_throughput: 1328.019\n",
      "    learn_time_ms: 753.001\n",
      "    load_throughput: 47421.075\n",
      "    load_time_ms: 21.088\n",
      "    sample_throughput: 49.407\n",
      "    sample_time_ms: 20239.968\n",
      "    update_time_ms: 3.876\n",
      "  timestamp: 1635314601\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 647000\n",
      "  training_iteration: 647\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   647</td><td style=\"text-align: right;\">         32015.8</td><td style=\"text-align: right;\">647000</td><td style=\"text-align: right;\"> -4.2425</td><td style=\"text-align: right;\">                5.83</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            532.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 648000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-03-38\n",
      "  done: false\n",
      "  episode_len_mean: 531.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.83\n",
      "  episode_reward_mean: -4.21319999999994\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2416\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6764109619777329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8551023973359002\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010245116355382401\n",
      "          policy_loss: 0.012269205310278468\n",
      "          total_loss: 0.1771661346571313\n",
      "          vf_explained_var: 0.12891671061515808\n",
      "          vf_loss: 0.17651804303362345\n",
      "    num_agent_steps_sampled: 648000\n",
      "    num_agent_steps_trained: 648000\n",
      "    num_steps_sampled: 648000\n",
      "    num_steps_trained: 648000\n",
      "  iterations_since_restore: 648\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.17916666666666\n",
      "    ram_util_percent: 34.60833333333334\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03688865112079335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.479076642494327\n",
      "    mean_inference_ms: 2.119289706464621\n",
      "    mean_raw_obs_processing_ms: 28.30402238540856\n",
      "  time_since_restore: 32033.08264017105\n",
      "  time_this_iter_s: 17.320343732833862\n",
      "  time_total_s: 32033.08264017105\n",
      "  timers:\n",
      "    learn_throughput: 1331.617\n",
      "    learn_time_ms: 750.967\n",
      "    load_throughput: 48349.437\n",
      "    load_time_ms: 20.683\n",
      "    sample_throughput: 49.848\n",
      "    sample_time_ms: 20060.82\n",
      "    update_time_ms: 3.881\n",
      "  timestamp: 1635314618\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 648000\n",
      "  training_iteration: 648\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   648</td><td style=\"text-align: right;\">         32033.1</td><td style=\"text-align: right;\">648000</td><td style=\"text-align: right;\"> -4.2132</td><td style=\"text-align: right;\">                5.83</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            531.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 649000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-03-55\n",
      "  done: false\n",
      "  episode_len_mean: 537.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.83\n",
      "  episode_reward_mean: -4.27339999999994\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2418\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6764109619777329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.131728182898627\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009775842229114358\n",
      "          policy_loss: -0.05799460249642531\n",
      "          total_loss: -0.008449926558468077\n",
      "          vf_explained_var: 0.1951400488615036\n",
      "          vf_loss: 0.06424946863618163\n",
      "    num_agent_steps_sampled: 649000\n",
      "    num_agent_steps_trained: 649000\n",
      "    num_steps_sampled: 649000\n",
      "    num_steps_trained: 649000\n",
      "  iterations_since_restore: 649\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.756\n",
      "    ram_util_percent: 34.604\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03689007529688582\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.469386110458515\n",
      "    mean_inference_ms: 2.1193258642831303\n",
      "    mean_raw_obs_processing_ms: 28.264668962347\n",
      "  time_since_restore: 32050.074378490448\n",
      "  time_this_iter_s: 16.991738319396973\n",
      "  time_total_s: 32050.074378490448\n",
      "  timers:\n",
      "    learn_throughput: 1331.084\n",
      "    learn_time_ms: 751.267\n",
      "    load_throughput: 51306.408\n",
      "    load_time_ms: 19.491\n",
      "    sample_throughput: 54.091\n",
      "    sample_time_ms: 18487.204\n",
      "    update_time_ms: 3.806\n",
      "  timestamp: 1635314635\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 649000\n",
      "  training_iteration: 649\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   649</td><td style=\"text-align: right;\">         32050.1</td><td style=\"text-align: right;\">649000</td><td style=\"text-align: right;\"> -4.2734</td><td style=\"text-align: right;\">                5.83</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            537.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 650000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-04-14\n",
      "  done: false\n",
      "  episode_len_mean: 540.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.83\n",
      "  episode_reward_mean: -4.34889999999994\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2420\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6764109619777329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.175370881292555\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.026399499982105685\n",
      "          policy_loss: 0.06340514909889963\n",
      "          total_loss: 0.40801332394282025\n",
      "          vf_explained_var: 0.4129008948802948\n",
      "          vf_loss: 0.34850497398939395\n",
      "    num_agent_steps_sampled: 650000\n",
      "    num_agent_steps_trained: 650000\n",
      "    num_steps_sampled: 650000\n",
      "    num_steps_trained: 650000\n",
      "  iterations_since_restore: 650\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.74074074074073\n",
      "    ram_util_percent: 34.6037037037037\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036891500080645206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.45968732400046\n",
      "    mean_inference_ms: 2.1193619263098302\n",
      "    mean_raw_obs_processing_ms: 28.22406313525815\n",
      "  time_since_restore: 32068.986053943634\n",
      "  time_this_iter_s: 18.911675453186035\n",
      "  time_total_s: 32068.986053943634\n",
      "  timers:\n",
      "    learn_throughput: 1330.572\n",
      "    learn_time_ms: 751.556\n",
      "    load_throughput: 50306.736\n",
      "    load_time_ms: 19.878\n",
      "    sample_throughput: 53.535\n",
      "    sample_time_ms: 18679.244\n",
      "    update_time_ms: 3.811\n",
      "  timestamp: 1635314654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 650000\n",
      "  training_iteration: 650\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   650</td><td style=\"text-align: right;\">           32069</td><td style=\"text-align: right;\">650000</td><td style=\"text-align: right;\"> -4.3489</td><td style=\"text-align: right;\">                5.83</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            540.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 651000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-04-32\n",
      "  done: false\n",
      "  episode_len_mean: 545.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.83\n",
      "  episode_reward_mean: -4.45309999999994\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2422\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0146164429665994\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1188643905851574\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005514662125268297\n",
      "          policy_loss: -0.0203448173072603\n",
      "          total_loss: 0.07597186350160175\n",
      "          vf_explained_var: 0.039980027824640274\n",
      "          vf_loss: 0.111910055950284\n",
      "    num_agent_steps_sampled: 651000\n",
      "    num_agent_steps_trained: 651000\n",
      "    num_steps_sampled: 651000\n",
      "    num_steps_trained: 651000\n",
      "  iterations_since_restore: 651\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.788\n",
      "    ram_util_percent: 34.592\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03689293154934637\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.44996852563639\n",
      "    mean_inference_ms: 2.119397733630816\n",
      "    mean_raw_obs_processing_ms: 28.18282619666921\n",
      "  time_since_restore: 32087.044041395187\n",
      "  time_this_iter_s: 18.057987451553345\n",
      "  time_total_s: 32087.044041395187\n",
      "  timers:\n",
      "    learn_throughput: 1327.138\n",
      "    learn_time_ms: 753.501\n",
      "    load_throughput: 50114.033\n",
      "    load_time_ms: 19.954\n",
      "    sample_throughput: 53.777\n",
      "    sample_time_ms: 18595.349\n",
      "    update_time_ms: 3.909\n",
      "  timestamp: 1635314672\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 651000\n",
      "  training_iteration: 651\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   651</td><td style=\"text-align: right;\">           32087</td><td style=\"text-align: right;\">651000</td><td style=\"text-align: right;\"> -4.4531</td><td style=\"text-align: right;\">                5.83</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            545.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 652000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-04-49\n",
      "  done: false\n",
      "  episode_len_mean: 551.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.83\n",
      "  episode_reward_mean: -4.55099999999994\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2423\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0146164429665994\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9968666579988268\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0057444415117192255\n",
      "          policy_loss: -0.058408829466336305\n",
      "          total_loss: 0.046220914440022576\n",
      "          vf_explained_var: 0.06832709908485413\n",
      "          vf_loss: 0.11877000369907667\n",
      "    num_agent_steps_sampled: 652000\n",
      "    num_agent_steps_trained: 652000\n",
      "    num_steps_sampled: 652000\n",
      "    num_steps_trained: 652000\n",
      "  iterations_since_restore: 652\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.85833333333333\n",
      "    ram_util_percent: 34.59166666666667\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036893651651851746\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.44507766674114\n",
      "    mean_inference_ms: 2.119415582455307\n",
      "    mean_raw_obs_processing_ms: 28.16189290730047\n",
      "  time_since_restore: 32103.633917570114\n",
      "  time_this_iter_s: 16.589876174926758\n",
      "  time_total_s: 32103.633917570114\n",
      "  timers:\n",
      "    learn_throughput: 1326.665\n",
      "    learn_time_ms: 753.77\n",
      "    load_throughput: 49618.883\n",
      "    load_time_ms: 20.154\n",
      "    sample_throughput: 54.151\n",
      "    sample_time_ms: 18466.841\n",
      "    update_time_ms: 3.831\n",
      "  timestamp: 1635314689\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 652000\n",
      "  training_iteration: 652\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   652</td><td style=\"text-align: right;\">         32103.6</td><td style=\"text-align: right;\">652000</td><td style=\"text-align: right;\">  -4.551</td><td style=\"text-align: right;\">                5.83</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            551.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 653000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-05-05\n",
      "  done: false\n",
      "  episode_len_mean: 553.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.83\n",
      "  episode_reward_mean: -4.622999999999939\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2425\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0146164429665994\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.20089737839169\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012599883417642212\n",
      "          policy_loss: -0.06941798836406735\n",
      "          total_loss: 0.24724427215341066\n",
      "          vf_explained_var: 0.32853376865386963\n",
      "          vf_loss: 0.32588718425896435\n",
      "    num_agent_steps_sampled: 653000\n",
      "    num_agent_steps_trained: 653000\n",
      "    num_steps_sampled: 653000\n",
      "    num_steps_trained: 653000\n",
      "  iterations_since_restore: 653\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.625\n",
      "    ram_util_percent: 34.6375\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03689509305958191\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.43522362104662\n",
      "    mean_inference_ms: 2.1194514928294423\n",
      "    mean_raw_obs_processing_ms: 28.11902583405728\n",
      "  time_since_restore: 32120.069316864014\n",
      "  time_this_iter_s: 16.435399293899536\n",
      "  time_total_s: 32120.069316864014\n",
      "  timers:\n",
      "    learn_throughput: 1327.194\n",
      "    learn_time_ms: 753.469\n",
      "    load_throughput: 49613.307\n",
      "    load_time_ms: 20.156\n",
      "    sample_throughput: 54.421\n",
      "    sample_time_ms: 18375.103\n",
      "    update_time_ms: 3.763\n",
      "  timestamp: 1635314705\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 653000\n",
      "  training_iteration: 653\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   653</td><td style=\"text-align: right;\">         32120.1</td><td style=\"text-align: right;\">653000</td><td style=\"text-align: right;\">  -4.623</td><td style=\"text-align: right;\">                5.83</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            553.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 654000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-05-40\n",
      "  done: false\n",
      "  episode_len_mean: 551.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.83\n",
      "  episode_reward_mean: -4.628799999999938\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2427\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0146164429665994\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.033888014157613\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006655988595908744\n",
      "          policy_loss: -0.12121135377221637\n",
      "          total_loss: -0.01529453806579113\n",
      "          vf_explained_var: 0.19283485412597656\n",
      "          vf_loss: 0.11950241959032913\n",
      "    num_agent_steps_sampled: 654000\n",
      "    num_agent_steps_trained: 654000\n",
      "    num_steps_sampled: 654000\n",
      "    num_steps_trained: 654000\n",
      "  iterations_since_restore: 654\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.02000000000001\n",
      "    ram_util_percent: 34.553999999999995\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036896543837160366\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.425334558304847\n",
      "    mean_inference_ms: 2.1194873578850473\n",
      "    mean_raw_obs_processing_ms: 28.076039576105387\n",
      "  time_since_restore: 32155.19878554344\n",
      "  time_this_iter_s: 35.1294686794281\n",
      "  time_total_s: 32155.19878554344\n",
      "  timers:\n",
      "    learn_throughput: 1326.971\n",
      "    learn_time_ms: 753.596\n",
      "    load_throughput: 49821.811\n",
      "    load_time_ms: 20.072\n",
      "    sample_throughput: 49.974\n",
      "    sample_time_ms: 20010.597\n",
      "    update_time_ms: 3.751\n",
      "  timestamp: 1635314740\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 654000\n",
      "  training_iteration: 654\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   654</td><td style=\"text-align: right;\">         32155.2</td><td style=\"text-align: right;\">654000</td><td style=\"text-align: right;\"> -4.6288</td><td style=\"text-align: right;\">                5.83</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            551.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 655000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-06-17\n",
      "  done: false\n",
      "  episode_len_mean: 548.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.83\n",
      "  episode_reward_mean: -4.466699999999939\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2430\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0146164429665994\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1949244764116074\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01488196235866541\n",
      "          policy_loss: -0.1118783368004693\n",
      "          total_loss: 0.19283909855617418\n",
      "          vf_explained_var: 0.7012792229652405\n",
      "          vf_loss: 0.311567193393906\n",
      "    num_agent_steps_sampled: 655000\n",
      "    num_agent_steps_trained: 655000\n",
      "    num_steps_sampled: 655000\n",
      "    num_steps_trained: 655000\n",
      "  iterations_since_restore: 655\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.54528301886793\n",
      "    ram_util_percent: 34.53584905660377\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03689869608212107\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.41064294356564\n",
      "    mean_inference_ms: 2.1195404886371283\n",
      "    mean_raw_obs_processing_ms: 28.011857960590408\n",
      "  time_since_restore: 32192.340262889862\n",
      "  time_this_iter_s: 37.14147734642029\n",
      "  time_total_s: 32192.340262889862\n",
      "  timers:\n",
      "    learn_throughput: 1329.48\n",
      "    learn_time_ms: 752.174\n",
      "    load_throughput: 47125.846\n",
      "    load_time_ms: 21.22\n",
      "    sample_throughput: 45.568\n",
      "    sample_time_ms: 21945.011\n",
      "    update_time_ms: 3.748\n",
      "  timestamp: 1635314777\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 655000\n",
      "  training_iteration: 655\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   655</td><td style=\"text-align: right;\">         32192.3</td><td style=\"text-align: right;\">655000</td><td style=\"text-align: right;\"> -4.4667</td><td style=\"text-align: right;\">                5.83</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            548.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 656000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-06-33\n",
      "  done: false\n",
      "  episode_len_mean: 554.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.640000000000002\n",
      "  episode_reward_mean: -4.595099999999938\n",
      "  episode_reward_min: -14.579999999999886\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2431\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0146164429665994\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5531656722227731\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.024315118564587835\n",
      "          policy_loss: 0.0544961160255803\n",
      "          total_loss: 0.3323194364292754\n",
      "          vf_explained_var: -0.042083267122507095\n",
      "          vf_loss: 0.26868445813371283\n",
      "    num_agent_steps_sampled: 656000\n",
      "    num_agent_steps_trained: 656000\n",
      "    num_steps_sampled: 656000\n",
      "    num_steps_trained: 656000\n",
      "  iterations_since_restore: 656\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.92727272727274\n",
      "    ram_util_percent: 34.550000000000004\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036899420637077206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.40567330644133\n",
      "    mean_inference_ms: 2.1195583033745238\n",
      "    mean_raw_obs_processing_ms: 27.99012212595495\n",
      "  time_since_restore: 32207.97474360466\n",
      "  time_this_iter_s: 15.634480714797974\n",
      "  time_total_s: 32207.97474360466\n",
      "  timers:\n",
      "    learn_throughput: 1333.468\n",
      "    learn_time_ms: 749.924\n",
      "    load_throughput: 50231.547\n",
      "    load_time_ms: 19.908\n",
      "    sample_throughput: 49.432\n",
      "    sample_time_ms: 20229.688\n",
      "    update_time_ms: 3.754\n",
      "  timestamp: 1635314793\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 656000\n",
      "  training_iteration: 656\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   656</td><td style=\"text-align: right;\">           32208</td><td style=\"text-align: right;\">656000</td><td style=\"text-align: right;\"> -4.5951</td><td style=\"text-align: right;\">                5.64</td><td style=\"text-align: right;\">              -14.58</td><td style=\"text-align: right;\">            554.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 657000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-06-50\n",
      "  done: false\n",
      "  episode_len_mean: 553.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.640000000000002\n",
      "  episode_reward_mean: -4.453599999999939\n",
      "  episode_reward_min: -13.919999999999884\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2433\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5219246644498996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1312141948276095\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017488858706813718\n",
      "          policy_loss: -0.007111678189701504\n",
      "          total_loss: 0.37465976116557914\n",
      "          vf_explained_var: 0.44699957966804504\n",
      "          vf_loss: 0.3764668574142787\n",
      "    num_agent_steps_sampled: 657000\n",
      "    num_agent_steps_trained: 657000\n",
      "    num_steps_sampled: 657000\n",
      "    num_steps_trained: 657000\n",
      "  iterations_since_restore: 657\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.84166666666667\n",
      "    ram_util_percent: 34.53333333333333\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03690085027495318\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.395889976298648\n",
      "    mean_inference_ms: 2.1195931325540593\n",
      "    mean_raw_obs_processing_ms: 27.947279965737234\n",
      "  time_since_restore: 32224.707891702652\n",
      "  time_this_iter_s: 16.733148097991943\n",
      "  time_total_s: 32224.707891702652\n",
      "  timers:\n",
      "    learn_throughput: 1334.074\n",
      "    learn_time_ms: 749.584\n",
      "    load_throughput: 53036.868\n",
      "    load_time_ms: 18.855\n",
      "    sample_throughput: 49.706\n",
      "    sample_time_ms: 20118.222\n",
      "    update_time_ms: 3.66\n",
      "  timestamp: 1635314810\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 657000\n",
      "  training_iteration: 657\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   657</td><td style=\"text-align: right;\">         32224.7</td><td style=\"text-align: right;\">657000</td><td style=\"text-align: right;\"> -4.4536</td><td style=\"text-align: right;\">                5.64</td><td style=\"text-align: right;\">              -13.92</td><td style=\"text-align: right;\">            553.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 658000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-07-06\n",
      "  done: false\n",
      "  episode_len_mean: 557.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.640000000000002\n",
      "  episode_reward_mean: -4.4969999999999395\n",
      "  episode_reward_min: -13.919999999999884\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2435\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5219246644498996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1507643779118855\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004518668293756534\n",
      "          policy_loss: -0.09714188393619326\n",
      "          total_loss: 0.0637438020358483\n",
      "          vf_explained_var: 0.2337433099746704\n",
      "          vf_loss: 0.17551625588093883\n",
      "    num_agent_steps_sampled: 658000\n",
      "    num_agent_steps_trained: 658000\n",
      "    num_steps_sampled: 658000\n",
      "    num_steps_trained: 658000\n",
      "  iterations_since_restore: 658\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.47391304347825\n",
      "    ram_util_percent: 34.57391304347827\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03690226180207607\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.38626223218382\n",
      "    mean_inference_ms: 2.1196269488220953\n",
      "    mean_raw_obs_processing_ms: 27.904494441869073\n",
      "  time_since_restore: 32240.622959136963\n",
      "  time_this_iter_s: 15.915067434310913\n",
      "  time_total_s: 32240.622959136963\n",
      "  timers:\n",
      "    learn_throughput: 1334.473\n",
      "    learn_time_ms: 749.359\n",
      "    load_throughput: 56074.482\n",
      "    load_time_ms: 17.833\n",
      "    sample_throughput: 50.053\n",
      "    sample_time_ms: 19978.932\n",
      "    update_time_ms: 3.657\n",
      "  timestamp: 1635314826\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 658000\n",
      "  training_iteration: 658\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   658</td><td style=\"text-align: right;\">         32240.6</td><td style=\"text-align: right;\">658000</td><td style=\"text-align: right;\">  -4.497</td><td style=\"text-align: right;\">                5.64</td><td style=\"text-align: right;\">              -13.92</td><td style=\"text-align: right;\">            557.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 659000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-07-40\n",
      "  done: false\n",
      "  episode_len_mean: 553.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.640000000000002\n",
      "  episode_reward_mean: -4.235299999999941\n",
      "  episode_reward_min: -11.129999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2437\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7609623322249498\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0398980299631755\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008720785534709681\n",
      "          policy_loss: -0.17780267426537144\n",
      "          total_loss: 0.004067097718103064\n",
      "          vf_explained_var: 0.04935775697231293\n",
      "          vf_loss: 0.19563256092886958\n",
      "    num_agent_steps_sampled: 659000\n",
      "    num_agent_steps_trained: 659000\n",
      "    num_steps_sampled: 659000\n",
      "    num_steps_trained: 659000\n",
      "  iterations_since_restore: 659\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.15208333333334\n",
      "    ram_util_percent: 34.4625\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036903685375227216\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.376542786428757\n",
      "    mean_inference_ms: 2.1196610445545234\n",
      "    mean_raw_obs_processing_ms: 27.861916282145003\n",
      "  time_since_restore: 32274.626055002213\n",
      "  time_this_iter_s: 34.003095865249634\n",
      "  time_total_s: 32274.626055002213\n",
      "  timers:\n",
      "    learn_throughput: 1334.797\n",
      "    learn_time_ms: 749.177\n",
      "    load_throughput: 53207.559\n",
      "    load_time_ms: 18.794\n",
      "    sample_throughput: 46.127\n",
      "    sample_time_ms: 21679.294\n",
      "    update_time_ms: 3.651\n",
      "  timestamp: 1635314860\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 659000\n",
      "  training_iteration: 659\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   659</td><td style=\"text-align: right;\">         32274.6</td><td style=\"text-align: right;\">659000</td><td style=\"text-align: right;\"> -4.2353</td><td style=\"text-align: right;\">                5.64</td><td style=\"text-align: right;\">              -11.13</td><td style=\"text-align: right;\">            553.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 660000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-07-56\n",
      "  done: false\n",
      "  episode_len_mean: 553.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.640000000000002\n",
      "  episode_reward_mean: -4.18719999999994\n",
      "  episode_reward_min: -10.839999999999895\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2438\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7609623322249498\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5968202047877842\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0038360238878844656\n",
      "          policy_loss: 0.042598000334368814\n",
      "          total_loss: 0.2533124081790447\n",
      "          vf_explained_var: 0.3211202621459961\n",
      "          vf_loss: 0.22376354306729304\n",
      "    num_agent_steps_sampled: 660000\n",
      "    num_agent_steps_trained: 660000\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "  iterations_since_restore: 660\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.80869565217392\n",
      "    ram_util_percent: 34.51304347826087\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03690439135049768\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.371749684076878\n",
      "    mean_inference_ms: 2.1196778998407124\n",
      "    mean_raw_obs_processing_ms: 27.840936727650828\n",
      "  time_since_restore: 32290.488302707672\n",
      "  time_this_iter_s: 15.862247705459595\n",
      "  time_total_s: 32290.488302707672\n",
      "  timers:\n",
      "    learn_throughput: 1337.825\n",
      "    learn_time_ms: 747.482\n",
      "    load_throughput: 57395.993\n",
      "    load_time_ms: 17.423\n",
      "    sample_throughput: 46.778\n",
      "    sample_time_ms: 21377.442\n",
      "    update_time_ms: 3.649\n",
      "  timestamp: 1635314876\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 660\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   660</td><td style=\"text-align: right;\">         32290.5</td><td style=\"text-align: right;\">660000</td><td style=\"text-align: right;\"> -4.1872</td><td style=\"text-align: right;\">                5.64</td><td style=\"text-align: right;\">              -10.84</td><td style=\"text-align: right;\">            553.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 661000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-08-47\n",
      "  done: false\n",
      "  episode_len_mean: 544.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.640000000000002\n",
      "  episode_reward_mean: -4.048399999999942\n",
      "  episode_reward_min: -10.839999999999895\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2442\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3804811661124749\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8646546403566997\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01196898998397372\n",
      "          policy_loss: 0.14529570697082414\n",
      "          total_loss: 0.32453779909345837\n",
      "          vf_explained_var: 0.5612603425979614\n",
      "          vf_loss: 0.19333466415603956\n",
      "    num_agent_steps_sampled: 661000\n",
      "    num_agent_steps_trained: 661000\n",
      "    num_steps_sampled: 661000\n",
      "    num_steps_trained: 661000\n",
      "  iterations_since_restore: 661\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.863013698630134\n",
      "    ram_util_percent: 34.464383561643835\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036907201154679666\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.35255626735726\n",
      "    mean_inference_ms: 2.1197451273745007\n",
      "    mean_raw_obs_processing_ms: 27.7590811291051\n",
      "  time_since_restore: 32341.75878405571\n",
      "  time_this_iter_s: 51.27048134803772\n",
      "  time_total_s: 32341.75878405571\n",
      "  timers:\n",
      "    learn_throughput: 1341.326\n",
      "    learn_time_ms: 745.531\n",
      "    load_throughput: 57988.121\n",
      "    load_time_ms: 17.245\n",
      "    sample_throughput: 40.484\n",
      "    sample_time_ms: 24700.938\n",
      "    update_time_ms: 3.543\n",
      "  timestamp: 1635314927\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 661000\n",
      "  training_iteration: 661\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   661</td><td style=\"text-align: right;\">         32341.8</td><td style=\"text-align: right;\">661000</td><td style=\"text-align: right;\"> -4.0484</td><td style=\"text-align: right;\">                5.64</td><td style=\"text-align: right;\">              -10.84</td><td style=\"text-align: right;\">            544.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 662000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-09-05\n",
      "  done: false\n",
      "  episode_len_mean: 549.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.640000000000002\n",
      "  episode_reward_mean: -4.171199999999941\n",
      "  episode_reward_min: -10.839999999999895\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2443\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3804811661124749\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2340592516793145\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01268817390906755\n",
      "          policy_loss: 0.042239536510573494\n",
      "          total_loss: 0.08775768793291516\n",
      "          vf_explained_var: 0.19583600759506226\n",
      "          vf_loss: 0.06303113052288406\n",
      "    num_agent_steps_sampled: 662000\n",
      "    num_agent_steps_trained: 662000\n",
      "    num_steps_sampled: 662000\n",
      "    num_steps_trained: 662000\n",
      "  iterations_since_restore: 662\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.86923076923077\n",
      "    ram_util_percent: 34.434615384615384\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03690790054407482\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.347793684189618\n",
      "    mean_inference_ms: 2.1197620437018285\n",
      "    mean_raw_obs_processing_ms: 27.738418585359522\n",
      "  time_since_restore: 32359.608959197998\n",
      "  time_this_iter_s: 17.850175142288208\n",
      "  time_total_s: 32359.608959197998\n",
      "  timers:\n",
      "    learn_throughput: 1341.421\n",
      "    learn_time_ms: 745.478\n",
      "    load_throughput: 58787.062\n",
      "    load_time_ms: 17.011\n",
      "    sample_throughput: 40.278\n",
      "    sample_time_ms: 24827.26\n",
      "    update_time_ms: 3.533\n",
      "  timestamp: 1635314945\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 662000\n",
      "  training_iteration: 662\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   662</td><td style=\"text-align: right;\">         32359.6</td><td style=\"text-align: right;\">662000</td><td style=\"text-align: right;\"> -4.1712</td><td style=\"text-align: right;\">                5.64</td><td style=\"text-align: right;\">              -10.84</td><td style=\"text-align: right;\">            549.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 663000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-09-21\n",
      "  done: false\n",
      "  episode_len_mean: 548.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.640000000000002\n",
      "  episode_reward_mean: -4.1912999999999405\n",
      "  episode_reward_min: -10.839999999999895\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2445\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3804811661124749\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.131210764249166\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007872316228565148\n",
      "          policy_loss: -0.03544088155031204\n",
      "          total_loss: 0.005657276511192322\n",
      "          vf_explained_var: 0.560409665107727\n",
      "          vf_loss: 0.059414996240391496\n",
      "    num_agent_steps_sampled: 663000\n",
      "    num_agent_steps_trained: 663000\n",
      "    num_steps_sampled: 663000\n",
      "    num_steps_trained: 663000\n",
      "  iterations_since_restore: 663\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.41304347826086\n",
      "    ram_util_percent: 34.3695652173913\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0369092939576121\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.338288217407623\n",
      "    mean_inference_ms: 2.1197971350462637\n",
      "    mean_raw_obs_processing_ms: 27.697234245533153\n",
      "  time_since_restore: 32376.107766389847\n",
      "  time_this_iter_s: 16.498807191848755\n",
      "  time_total_s: 32376.107766389847\n",
      "  timers:\n",
      "    learn_throughput: 1340.117\n",
      "    learn_time_ms: 746.204\n",
      "    load_throughput: 58804.04\n",
      "    load_time_ms: 17.006\n",
      "    sample_throughput: 40.269\n",
      "    sample_time_ms: 24832.812\n",
      "    update_time_ms: 3.592\n",
      "  timestamp: 1635314961\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 663000\n",
      "  training_iteration: 663\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   663</td><td style=\"text-align: right;\">         32376.1</td><td style=\"text-align: right;\">663000</td><td style=\"text-align: right;\"> -4.1913</td><td style=\"text-align: right;\">                5.64</td><td style=\"text-align: right;\">              -10.84</td><td style=\"text-align: right;\">            548.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 664000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-09-37\n",
      "  done: false\n",
      "  episode_len_mean: 548.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.640000000000002\n",
      "  episode_reward_mean: -4.23769999999994\n",
      "  episode_reward_min: -10.839999999999895\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2446\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3804811661124749\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0825994968414308\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008536493618354355\n",
      "          policy_loss: -0.05252789143058989\n",
      "          total_loss: 0.07936397675010894\n",
      "          vf_explained_var: -0.0498998798429966\n",
      "          vf_loss: 0.1494698897521529\n",
      "    num_agent_steps_sampled: 664000\n",
      "    num_agent_steps_trained: 664000\n",
      "    num_steps_sampled: 664000\n",
      "    num_steps_trained: 664000\n",
      "  iterations_since_restore: 664\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.90454545454544\n",
      "    ram_util_percent: 34.37727272727272\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036909988088404434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.333546772459187\n",
      "    mean_inference_ms: 2.119814608143023\n",
      "    mean_raw_obs_processing_ms: 27.67671185928751\n",
      "  time_since_restore: 32391.638205766678\n",
      "  time_this_iter_s: 15.530439376831055\n",
      "  time_total_s: 32391.638205766678\n",
      "  timers:\n",
      "    learn_throughput: 1343.144\n",
      "    learn_time_ms: 744.522\n",
      "    load_throughput: 63600.652\n",
      "    load_time_ms: 15.723\n",
      "    sample_throughput: 43.714\n",
      "    sample_time_ms: 22875.87\n",
      "    update_time_ms: 3.59\n",
      "  timestamp: 1635314977\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 664000\n",
      "  training_iteration: 664\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   664</td><td style=\"text-align: right;\">         32391.6</td><td style=\"text-align: right;\">664000</td><td style=\"text-align: right;\"> -4.2377</td><td style=\"text-align: right;\">                5.64</td><td style=\"text-align: right;\">              -10.84</td><td style=\"text-align: right;\">            548.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 665000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-10-30\n",
      "  done: false\n",
      "  episode_len_mean: 544.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.77\n",
      "  episode_reward_mean: -4.089999999999941\n",
      "  episode_reward_min: -10.319999999999933\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2450\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3804811661124749\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1392828596962823\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012324455836418401\n",
      "          policy_loss: -0.14409582227882411\n",
      "          total_loss: -0.04652640979944004\n",
      "          vf_explained_var: 0.0971694067120552\n",
      "          vf_loss: 0.11427301986453434\n",
      "    num_agent_steps_sampled: 665000\n",
      "    num_agent_steps_trained: 665000\n",
      "    num_steps_sampled: 665000\n",
      "    num_steps_trained: 665000\n",
      "  iterations_since_restore: 665\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.61818181818183\n",
      "    ram_util_percent: 34.298701298701296\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03691272538616355\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.31468336118547\n",
      "    mean_inference_ms: 2.11988328491036\n",
      "    mean_raw_obs_processing_ms: 27.59746991795282\n",
      "  time_since_restore: 32445.19408774376\n",
      "  time_this_iter_s: 53.5558819770813\n",
      "  time_total_s: 32445.19408774376\n",
      "  timers:\n",
      "    learn_throughput: 1340.601\n",
      "    learn_time_ms: 745.934\n",
      "    load_throughput: 63777.631\n",
      "    load_time_ms: 15.679\n",
      "    sample_throughput: 40.79\n",
      "    sample_time_ms: 24515.92\n",
      "    update_time_ms: 3.597\n",
      "  timestamp: 1635315030\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 665000\n",
      "  training_iteration: 665\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   665</td><td style=\"text-align: right;\">         32445.2</td><td style=\"text-align: right;\">665000</td><td style=\"text-align: right;\">   -4.09</td><td style=\"text-align: right;\">                5.77</td><td style=\"text-align: right;\">              -10.32</td><td style=\"text-align: right;\">            544.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 666000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-10-48\n",
      "  done: false\n",
      "  episode_len_mean: 542.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.77\n",
      "  episode_reward_mean: -4.112499999999941\n",
      "  episode_reward_min: -11.899999999999938\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2452\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3804811661124749\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.050948945681254\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013132507420850351\n",
      "          policy_loss: -0.014246786592735185\n",
      "          total_loss: 0.1804762090659804\n",
      "          vf_explained_var: 0.002632005373016\n",
      "          vf_loss: 0.21023581496103563\n",
      "    num_agent_steps_sampled: 666000\n",
      "    num_agent_steps_trained: 666000\n",
      "    num_steps_sampled: 666000\n",
      "    num_steps_trained: 666000\n",
      "  iterations_since_restore: 666\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.956\n",
      "    ram_util_percent: 34.492000000000004\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03691408774182795\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.305336822552935\n",
      "    mean_inference_ms: 2.1199173399199385\n",
      "    mean_raw_obs_processing_ms: 27.557701993964667\n",
      "  time_since_restore: 32463.016033172607\n",
      "  time_this_iter_s: 17.821945428848267\n",
      "  time_total_s: 32463.016033172607\n",
      "  timers:\n",
      "    learn_throughput: 1336.905\n",
      "    learn_time_ms: 747.996\n",
      "    load_throughput: 58916.625\n",
      "    load_time_ms: 16.973\n",
      "    sample_throughput: 40.435\n",
      "    sample_time_ms: 24731.258\n",
      "    update_time_ms: 3.694\n",
      "  timestamp: 1635315048\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 666000\n",
      "  training_iteration: 666\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   666</td><td style=\"text-align: right;\">           32463</td><td style=\"text-align: right;\">666000</td><td style=\"text-align: right;\"> -4.1125</td><td style=\"text-align: right;\">                5.77</td><td style=\"text-align: right;\">               -11.9</td><td style=\"text-align: right;\">            542.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 667000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-11-05\n",
      "  done: false\n",
      "  episode_len_mean: 541.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.77\n",
      "  episode_reward_mean: -4.1573999999999405\n",
      "  episode_reward_min: -11.899999999999938\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2453\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3804811661124749\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8542185929086474\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023811459012243377\n",
      "          policy_loss: -0.005411848487953345\n",
      "          total_loss: 0.3466683485441738\n",
      "          vf_explained_var: 0.4383002519607544\n",
      "          vf_loss: 0.3615625732888778\n",
      "    num_agent_steps_sampled: 667000\n",
      "    num_agent_steps_trained: 667000\n",
      "    num_steps_sampled: 667000\n",
      "    num_steps_trained: 667000\n",
      "  iterations_since_restore: 667\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.416\n",
      "    ram_util_percent: 34.5\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03691475819824604\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.300772628167934\n",
      "    mean_inference_ms: 2.1199341327253065\n",
      "    mean_raw_obs_processing_ms: 27.538117543642056\n",
      "  time_since_restore: 32480.06499361992\n",
      "  time_this_iter_s: 17.0489604473114\n",
      "  time_total_s: 32480.06499361992\n",
      "  timers:\n",
      "    learn_throughput: 1335.481\n",
      "    learn_time_ms: 748.794\n",
      "    load_throughput: 58950.493\n",
      "    load_time_ms: 16.963\n",
      "    sample_throughput: 40.384\n",
      "    sample_time_ms: 24762.036\n",
      "    update_time_ms: 3.708\n",
      "  timestamp: 1635315065\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 667000\n",
      "  training_iteration: 667\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   667</td><td style=\"text-align: right;\">         32480.1</td><td style=\"text-align: right;\">667000</td><td style=\"text-align: right;\"> -4.1574</td><td style=\"text-align: right;\">                5.77</td><td style=\"text-align: right;\">               -11.9</td><td style=\"text-align: right;\">             541.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 668000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-11-41\n",
      "  done: false\n",
      "  episode_len_mean: 542.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.77\n",
      "  episode_reward_mean: -4.16249999999994\n",
      "  episode_reward_min: -11.899999999999938\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2456\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5707217491687121\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9791273514429728\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006561141444149696\n",
      "          policy_loss: -0.11340210818582111\n",
      "          total_loss: 0.08719342740045653\n",
      "          vf_explained_var: 0.13761062920093536\n",
      "          vf_loss: 0.21664222586227375\n",
      "    num_agent_steps_sampled: 668000\n",
      "    num_agent_steps_trained: 668000\n",
      "    num_steps_sampled: 668000\n",
      "    num_steps_trained: 668000\n",
      "  iterations_since_restore: 668\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.92199999999999\n",
      "    ram_util_percent: 34.424\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03691677391852704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.287043785213463\n",
      "    mean_inference_ms: 2.1199845147263994\n",
      "    mean_raw_obs_processing_ms: 27.47929976935573\n",
      "  time_since_restore: 32515.598193645477\n",
      "  time_this_iter_s: 35.53320002555847\n",
      "  time_total_s: 32515.598193645477\n",
      "  timers:\n",
      "    learn_throughput: 1334.513\n",
      "    learn_time_ms: 749.337\n",
      "    load_throughput: 54624.144\n",
      "    load_time_ms: 18.307\n",
      "    sample_throughput: 37.422\n",
      "    sample_time_ms: 26721.971\n",
      "    update_time_ms: 3.711\n",
      "  timestamp: 1635315101\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 668000\n",
      "  training_iteration: 668\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   668</td><td style=\"text-align: right;\">         32515.6</td><td style=\"text-align: right;\">668000</td><td style=\"text-align: right;\"> -4.1625</td><td style=\"text-align: right;\">                5.77</td><td style=\"text-align: right;\">               -11.9</td><td style=\"text-align: right;\">            542.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 669000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-11-57\n",
      "  done: false\n",
      "  episode_len_mean: 543.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.77\n",
      "  episode_reward_mean: -4.18029999999994\n",
      "  episode_reward_min: -11.899999999999938\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2457\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5707217491687121\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0502599504258896\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01345231116664463\n",
      "          policy_loss: -0.01773590391708745\n",
      "          total_loss: 0.17098589258061514\n",
      "          vf_explained_var: 0.5851683020591736\n",
      "          vf_loss: 0.2015468688474761\n",
      "    num_agent_steps_sampled: 669000\n",
      "    num_agent_steps_trained: 669000\n",
      "    num_steps_sampled: 669000\n",
      "    num_steps_trained: 669000\n",
      "  iterations_since_restore: 669\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.22173913043478\n",
      "    ram_util_percent: 34.50869565217391\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03691744057946956\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.28246499582891\n",
      "    mean_inference_ms: 2.1200009666731034\n",
      "    mean_raw_obs_processing_ms: 27.459545511812557\n",
      "  time_since_restore: 32531.5689535141\n",
      "  time_this_iter_s: 15.970759868621826\n",
      "  time_total_s: 32531.5689535141\n",
      "  timers:\n",
      "    learn_throughput: 1333.944\n",
      "    learn_time_ms: 749.657\n",
      "    load_throughput: 59043.353\n",
      "    load_time_ms: 16.937\n",
      "    sample_throughput: 40.129\n",
      "    sample_time_ms: 24919.685\n",
      "    update_time_ms: 3.811\n",
      "  timestamp: 1635315117\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 669000\n",
      "  training_iteration: 669\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   669</td><td style=\"text-align: right;\">         32531.6</td><td style=\"text-align: right;\">669000</td><td style=\"text-align: right;\"> -4.1803</td><td style=\"text-align: right;\">                5.77</td><td style=\"text-align: right;\">               -11.9</td><td style=\"text-align: right;\">            543.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 670000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-12-14\n",
      "  done: false\n",
      "  episode_len_mean: 549.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.77\n",
      "  episode_reward_mean: -4.319799999999939\n",
      "  episode_reward_min: -11.899999999999938\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2459\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5707217491687121\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.841721060540941\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016740014998809918\n",
      "          policy_loss: -0.0001705709844827652\n",
      "          total_loss: 0.39079069155785773\n",
      "          vf_explained_var: 0.7041045427322388\n",
      "          vf_loss: 0.3998245779838827\n",
      "    num_agent_steps_sampled: 670000\n",
      "    num_agent_steps_trained: 670000\n",
      "    num_steps_sampled: 670000\n",
      "    num_steps_trained: 670000\n",
      "  iterations_since_restore: 670\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.78800000000001\n",
      "    ram_util_percent: 34.48\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036918789662297294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.273165894074797\n",
      "    mean_inference_ms: 2.120034292934021\n",
      "    mean_raw_obs_processing_ms: 27.41925076402889\n",
      "  time_since_restore: 32549.10574221611\n",
      "  time_this_iter_s: 17.53678870201111\n",
      "  time_total_s: 32549.10574221611\n",
      "  timers:\n",
      "    learn_throughput: 1332.728\n",
      "    learn_time_ms: 750.341\n",
      "    load_throughput: 57527.616\n",
      "    load_time_ms: 17.383\n",
      "    sample_throughput: 39.863\n",
      "    sample_time_ms: 25085.891\n",
      "    update_time_ms: 3.908\n",
      "  timestamp: 1635315134\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 670000\n",
      "  training_iteration: 670\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   670</td><td style=\"text-align: right;\">         32549.1</td><td style=\"text-align: right;\">670000</td><td style=\"text-align: right;\"> -4.3198</td><td style=\"text-align: right;\">                5.77</td><td style=\"text-align: right;\">               -11.9</td><td style=\"text-align: right;\">            549.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 671000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-13-09\n",
      "  done: false\n",
      "  episode_len_mean: 546.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.77\n",
      "  episode_reward_mean: -4.298799999999938\n",
      "  episode_reward_min: -11.899999999999938\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2463\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5707217491687121\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9637747685114542\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019724511993745825\n",
      "          policy_loss: 0.014592155896955067\n",
      "          total_loss: 0.2667197614494297\n",
      "          vf_explained_var: 0.5886826515197754\n",
      "          vf_loss: 0.2605081419356995\n",
      "    num_agent_steps_sampled: 671000\n",
      "    num_agent_steps_trained: 671000\n",
      "    num_steps_sampled: 671000\n",
      "    num_steps_trained: 671000\n",
      "  iterations_since_restore: 671\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.87820512820513\n",
      "    ram_util_percent: 34.48076923076923\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0369214740898034\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.254520165892973\n",
      "    mean_inference_ms: 2.120100164045614\n",
      "    mean_raw_obs_processing_ms: 27.340379284847877\n",
      "  time_since_restore: 32603.437228918076\n",
      "  time_this_iter_s: 54.33148670196533\n",
      "  time_total_s: 32603.437228918076\n",
      "  timers:\n",
      "    learn_throughput: 1329.754\n",
      "    learn_time_ms: 752.019\n",
      "    load_throughput: 57473.697\n",
      "    load_time_ms: 17.399\n",
      "    sample_throughput: 39.385\n",
      "    sample_time_ms: 25390.266\n",
      "    update_time_ms: 3.916\n",
      "  timestamp: 1635315189\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 671000\n",
      "  training_iteration: 671\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   671</td><td style=\"text-align: right;\">         32603.4</td><td style=\"text-align: right;\">671000</td><td style=\"text-align: right;\"> -4.2988</td><td style=\"text-align: right;\">                5.77</td><td style=\"text-align: right;\">               -11.9</td><td style=\"text-align: right;\">            546.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 672000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-13-42\n",
      "  done: false\n",
      "  episode_len_mean: 546.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.77\n",
      "  episode_reward_mean: -4.340599999999938\n",
      "  episode_reward_min: -11.899999999999938\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2465\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5707217491687121\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6010372890366449\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004728905553435137\n",
      "          policy_loss: -0.06257166531350877\n",
      "          total_loss: 0.022662999563746983\n",
      "          vf_explained_var: 0.1773223876953125\n",
      "          vf_loss: 0.09854614842010455\n",
      "    num_agent_steps_sampled: 672000\n",
      "    num_agent_steps_trained: 672000\n",
      "    num_steps_sampled: 672000\n",
      "    num_steps_trained: 672000\n",
      "  iterations_since_restore: 672\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.41489361702128\n",
      "    ram_util_percent: 34.51489361702128\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03692281162399605\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.245154405276857\n",
      "    mean_inference_ms: 2.120132875010326\n",
      "    mean_raw_obs_processing_ms: 27.301811652135147\n",
      "  time_since_restore: 32636.463443994522\n",
      "  time_this_iter_s: 33.02621507644653\n",
      "  time_total_s: 32636.463443994522\n",
      "  timers:\n",
      "    learn_throughput: 1326.087\n",
      "    learn_time_ms: 754.098\n",
      "    load_throughput: 55980.482\n",
      "    load_time_ms: 17.863\n",
      "    sample_throughput: 37.168\n",
      "    sample_time_ms: 26905.161\n",
      "    update_time_ms: 3.998\n",
      "  timestamp: 1635315222\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 672000\n",
      "  training_iteration: 672\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   672</td><td style=\"text-align: right;\">         32636.5</td><td style=\"text-align: right;\">672000</td><td style=\"text-align: right;\"> -4.3406</td><td style=\"text-align: right;\">                5.77</td><td style=\"text-align: right;\">               -11.9</td><td style=\"text-align: right;\">            546.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 673000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-14-19\n",
      "  done: false\n",
      "  episode_len_mean: 539.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.82\n",
      "  episode_reward_mean: -4.212299999999939\n",
      "  episode_reward_min: -11.899999999999938\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2467\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.28536087458435605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.855884755982293\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.026974936943672607\n",
      "          policy_loss: -0.07329390719532966\n",
      "          total_loss: 0.33934497783581413\n",
      "          vf_explained_var: 0.43131208419799805\n",
      "          vf_loss: 0.42350014914991335\n",
      "    num_agent_steps_sampled: 673000\n",
      "    num_agent_steps_trained: 673000\n",
      "    num_steps_sampled: 673000\n",
      "    num_steps_trained: 673000\n",
      "  iterations_since_restore: 673\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.87924528301887\n",
      "    ram_util_percent: 34.401886792452835\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036924143447067986\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.235893858066024\n",
      "    mean_inference_ms: 2.120165649461902\n",
      "    mean_raw_obs_processing_ms: 27.26390144297925\n",
      "  time_since_restore: 32673.499020814896\n",
      "  time_this_iter_s: 37.035576820373535\n",
      "  time_total_s: 32673.499020814896\n",
      "  timers:\n",
      "    learn_throughput: 1325.538\n",
      "    learn_time_ms: 754.411\n",
      "    load_throughput: 53014.544\n",
      "    load_time_ms: 18.863\n",
      "    sample_throughput: 34.533\n",
      "    sample_time_ms: 28957.607\n",
      "    update_time_ms: 3.928\n",
      "  timestamp: 1635315259\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 673000\n",
      "  training_iteration: 673\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   673</td><td style=\"text-align: right;\">         32673.5</td><td style=\"text-align: right;\">673000</td><td style=\"text-align: right;\"> -4.2123</td><td style=\"text-align: right;\">                5.82</td><td style=\"text-align: right;\">               -11.9</td><td style=\"text-align: right;\">            539.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 674000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-14-39\n",
      "  done: false\n",
      "  episode_len_mean: 539.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.82\n",
      "  episode_reward_mean: -4.205199999999937\n",
      "  episode_reward_min: -11.899999999999938\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2469\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.42804131187653416\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6271371748712329\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018796115818241314\n",
      "          policy_loss: -0.06939898932145702\n",
      "          total_loss: 0.2857761395474275\n",
      "          vf_explained_var: 0.4307310879230499\n",
      "          vf_loss: 0.3634009846382671\n",
      "    num_agent_steps_sampled: 674000\n",
      "    num_agent_steps_trained: 674000\n",
      "    num_steps_sampled: 674000\n",
      "    num_steps_trained: 674000\n",
      "  iterations_since_restore: 674\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.39285714285714\n",
      "    ram_util_percent: 34.38928571428571\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036925467464614704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.226833482408434\n",
      "    mean_inference_ms: 2.120198323368337\n",
      "    mean_raw_obs_processing_ms: 27.22602616279144\n",
      "  time_since_restore: 32693.469913959503\n",
      "  time_this_iter_s: 19.970893144607544\n",
      "  time_total_s: 32693.469913959503\n",
      "  timers:\n",
      "    learn_throughput: 1324.085\n",
      "    learn_time_ms: 755.239\n",
      "    load_throughput: 49082.709\n",
      "    load_time_ms: 20.374\n",
      "    sample_throughput: 34.014\n",
      "    sample_time_ms: 29399.293\n",
      "    update_time_ms: 3.937\n",
      "  timestamp: 1635315279\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 674000\n",
      "  training_iteration: 674\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   674</td><td style=\"text-align: right;\">         32693.5</td><td style=\"text-align: right;\">674000</td><td style=\"text-align: right;\"> -4.2052</td><td style=\"text-align: right;\">                5.82</td><td style=\"text-align: right;\">               -11.9</td><td style=\"text-align: right;\">            539.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 675000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-15-34\n",
      "  done: false\n",
      "  episode_len_mean: 528.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.82\n",
      "  episode_reward_mean: -4.0556999999999395\n",
      "  episode_reward_min: -11.899999999999938\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2473\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.42804131187653416\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8252229041523405\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015978128837623375\n",
      "          policy_loss: 0.04558517804576291\n",
      "          total_loss: 0.3387984982795186\n",
      "          vf_explained_var: 0.23309342563152313\n",
      "          vf_loss: 0.3046262539198829\n",
      "    num_agent_steps_sampled: 675000\n",
      "    num_agent_steps_trained: 675000\n",
      "    num_steps_sampled: 675000\n",
      "    num_steps_trained: 675000\n",
      "  iterations_since_restore: 675\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.94875\n",
      "    ram_util_percent: 34.37624999999999\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03692806024710947\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.209129118175124\n",
      "    mean_inference_ms: 2.120261182065727\n",
      "    mean_raw_obs_processing_ms: 27.15393013624607\n",
      "  time_since_restore: 32748.88366651535\n",
      "  time_this_iter_s: 55.41375255584717\n",
      "  time_total_s: 32748.88366651535\n",
      "  timers:\n",
      "    learn_throughput: 1324.909\n",
      "    learn_time_ms: 754.769\n",
      "    load_throughput: 49320.963\n",
      "    load_time_ms: 20.275\n",
      "    sample_throughput: 33.8\n",
      "    sample_time_ms: 29585.638\n",
      "    update_time_ms: 3.936\n",
      "  timestamp: 1635315334\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 675000\n",
      "  training_iteration: 675\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   675</td><td style=\"text-align: right;\">         32748.9</td><td style=\"text-align: right;\">675000</td><td style=\"text-align: right;\"> -4.0557</td><td style=\"text-align: right;\">                5.82</td><td style=\"text-align: right;\">               -11.9</td><td style=\"text-align: right;\">            528.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 676000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-15-57\n",
      "  done: false\n",
      "  episode_len_mean: 527.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.82\n",
      "  episode_reward_mean: -4.01349999999994\n",
      "  episode_reward_min: -11.899999999999938\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2475\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.42804131187653416\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8912239882681106\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01892120488632544\n",
      "          policy_loss: -0.12506351479225689\n",
      "          total_loss: 0.22379829953942035\n",
      "          vf_explained_var: 0.44513189792633057\n",
      "          vf_loss: 0.35967499679989284\n",
      "    num_agent_steps_sampled: 676000\n",
      "    num_agent_steps_trained: 676000\n",
      "    num_steps_sampled: 676000\n",
      "    num_steps_trained: 676000\n",
      "  iterations_since_restore: 676\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.203125\n",
      "    ram_util_percent: 34.45\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03692933170550852\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.200609425600383\n",
      "    mean_inference_ms: 2.120292383815606\n",
      "    mean_raw_obs_processing_ms: 27.118682559937596\n",
      "  time_since_restore: 32771.64787364006\n",
      "  time_this_iter_s: 22.764207124710083\n",
      "  time_total_s: 32771.64787364006\n",
      "  timers:\n",
      "    learn_throughput: 1325.556\n",
      "    learn_time_ms: 754.4\n",
      "    load_throughput: 49388.04\n",
      "    load_time_ms: 20.248\n",
      "    sample_throughput: 33.244\n",
      "    sample_time_ms: 30080.283\n",
      "    update_time_ms: 3.894\n",
      "  timestamp: 1635315357\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 676000\n",
      "  training_iteration: 676\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   676</td><td style=\"text-align: right;\">         32771.6</td><td style=\"text-align: right;\">676000</td><td style=\"text-align: right;\"> -4.0135</td><td style=\"text-align: right;\">                5.82</td><td style=\"text-align: right;\">               -11.9</td><td style=\"text-align: right;\">            527.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 677000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-17-33\n",
      "  done: false\n",
      "  episode_len_mean: 501.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.82\n",
      "  episode_reward_mean: -3.546499999999944\n",
      "  episode_reward_min: -11.899999999999938\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 2480\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.42804131187653416\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.657444088988834\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016407125862007682\n",
      "          policy_loss: -0.07703725563155281\n",
      "          total_loss: 0.416352052324348\n",
      "          vf_explained_var: 0.5254847407341003\n",
      "          vf_loss: 0.5029408186674118\n",
      "    num_agent_steps_sampled: 677000\n",
      "    num_agent_steps_trained: 677000\n",
      "    num_steps_sampled: 677000\n",
      "    num_steps_trained: 677000\n",
      "  iterations_since_restore: 677\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.065693430656935\n",
      "    ram_util_percent: 34.416058394160586\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036932389086212776\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.180313646090436\n",
      "    mean_inference_ms: 2.120368248151946\n",
      "    mean_raw_obs_processing_ms: 27.038859924183207\n",
      "  time_since_restore: 32867.35674905777\n",
      "  time_this_iter_s: 95.70887541770935\n",
      "  time_total_s: 32867.35674905777\n",
      "  timers:\n",
      "    learn_throughput: 1328.986\n",
      "    learn_time_ms: 752.453\n",
      "    load_throughput: 47515.033\n",
      "    load_time_ms: 21.046\n",
      "    sample_throughput: 26.352\n",
      "    sample_time_ms: 37947.346\n",
      "    update_time_ms: 3.965\n",
      "  timestamp: 1635315453\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 677000\n",
      "  training_iteration: 677\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   677</td><td style=\"text-align: right;\">         32867.4</td><td style=\"text-align: right;\">677000</td><td style=\"text-align: right;\"> -3.5465</td><td style=\"text-align: right;\">                5.82</td><td style=\"text-align: right;\">               -11.9</td><td style=\"text-align: right;\">            501.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 678000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-19-45\n",
      "  done: false\n",
      "  episode_len_mean: 463.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.82\n",
      "  episode_reward_mean: -2.9302999999999484\n",
      "  episode_reward_min: -11.899999999999938\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2488\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.42804131187653416\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8328896522521974\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016786612231768198\n",
      "          policy_loss: -0.09606869883007474\n",
      "          total_loss: 0.11147632383637958\n",
      "          vf_explained_var: 0.8659132122993469\n",
      "          vf_loss: 0.21868855402701431\n",
      "    num_agent_steps_sampled: 678000\n",
      "    num_agent_steps_trained: 678000\n",
      "    num_steps_sampled: 678000\n",
      "    num_steps_trained: 678000\n",
      "  iterations_since_restore: 678\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.090425531914896\n",
      "    ram_util_percent: 34.351063829787236\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03693690108834917\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.15139337355703\n",
      "    mean_inference_ms: 2.1204842711629563\n",
      "    mean_raw_obs_processing_ms: 26.9353418804641\n",
      "  time_since_restore: 32999.12854862213\n",
      "  time_this_iter_s: 131.77179956436157\n",
      "  time_total_s: 32999.12854862213\n",
      "  timers:\n",
      "    learn_throughput: 1328.279\n",
      "    learn_time_ms: 752.854\n",
      "    load_throughput: 47491.737\n",
      "    load_time_ms: 21.056\n",
      "    sample_throughput: 21.021\n",
      "    sample_time_ms: 47570.784\n",
      "    update_time_ms: 3.97\n",
      "  timestamp: 1635315585\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 678000\n",
      "  training_iteration: 678\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   678</td><td style=\"text-align: right;\">         32999.1</td><td style=\"text-align: right;\">678000</td><td style=\"text-align: right;\"> -2.9303</td><td style=\"text-align: right;\">                5.82</td><td style=\"text-align: right;\">               -11.9</td><td style=\"text-align: right;\">            463.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 679000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-20-27\n",
      "  done: false\n",
      "  episode_len_mean: 456.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.82\n",
      "  episode_reward_mean: -2.817999999999949\n",
      "  episode_reward_min: -11.899999999999938\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2491\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.42804131187653416\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8513098412089877\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012477636550987686\n",
      "          policy_loss: -0.13009324400789207\n",
      "          total_loss: 0.08876214838690227\n",
      "          vf_explained_var: 0.6607725620269775\n",
      "          vf_loss: 0.2320275461508168\n",
      "    num_agent_steps_sampled: 679000\n",
      "    num_agent_steps_trained: 679000\n",
      "    num_steps_sampled: 679000\n",
      "    num_steps_trained: 679000\n",
      "  iterations_since_restore: 679\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.3\n",
      "    ram_util_percent: 34.41166666666667\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03693846891611764\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.14167267015259\n",
      "    mean_inference_ms: 2.1205258184324975\n",
      "    mean_raw_obs_processing_ms: 26.90102266700127\n",
      "  time_since_restore: 33041.193256139755\n",
      "  time_this_iter_s: 42.0647075176239\n",
      "  time_total_s: 33041.193256139755\n",
      "  timers:\n",
      "    learn_throughput: 1328.216\n",
      "    learn_time_ms: 752.89\n",
      "    load_throughput: 44565.639\n",
      "    load_time_ms: 22.439\n",
      "    sample_throughput: 19.929\n",
      "    sample_time_ms: 50178.868\n",
      "    update_time_ms: 3.877\n",
      "  timestamp: 1635315627\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 679000\n",
      "  training_iteration: 679\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   679</td><td style=\"text-align: right;\">         33041.2</td><td style=\"text-align: right;\">679000</td><td style=\"text-align: right;\">  -2.818</td><td style=\"text-align: right;\">                5.82</td><td style=\"text-align: right;\">               -11.9</td><td style=\"text-align: right;\">            456.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 680000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-21-42\n",
      "  done: false\n",
      "  episode_len_mean: 436.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.82\n",
      "  episode_reward_mean: -2.473499999999952\n",
      "  episode_reward_min: -11.899999999999938\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2495\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.42804131187653416\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5311409685346815\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02248896829142003\n",
      "          policy_loss: -0.11474613580438826\n",
      "          total_loss: 0.3516004597561227\n",
      "          vf_explained_var: 0.6262279748916626\n",
      "          vf_loss: 0.47203180078003143\n",
      "    num_agent_steps_sampled: 680000\n",
      "    num_agent_steps_trained: 680000\n",
      "    num_steps_sampled: 680000\n",
      "    num_steps_trained: 680000\n",
      "  iterations_since_restore: 680\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.91495327102804\n",
      "    ram_util_percent: 34.40560747663551\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03694050673637662\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.129403603028816\n",
      "    mean_inference_ms: 2.1205804601396907\n",
      "    mean_raw_obs_processing_ms: 26.860275461479496\n",
      "  time_since_restore: 33116.31095266342\n",
      "  time_this_iter_s: 75.11769652366638\n",
      "  time_total_s: 33116.31095266342\n",
      "  timers:\n",
      "    learn_throughput: 1329.331\n",
      "    learn_time_ms: 752.258\n",
      "    load_throughput: 42733.22\n",
      "    load_time_ms: 23.401\n",
      "    sample_throughput: 17.877\n",
      "    sample_time_ms: 55936.645\n",
      "    update_time_ms: 3.867\n",
      "  timestamp: 1635315702\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 680000\n",
      "  training_iteration: 680\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   680</td><td style=\"text-align: right;\">         33116.3</td><td style=\"text-align: right;\">680000</td><td style=\"text-align: right;\"> -2.4735</td><td style=\"text-align: right;\">                5.82</td><td style=\"text-align: right;\">               -11.9</td><td style=\"text-align: right;\">            436.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 681000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-23-34\n",
      "  done: false\n",
      "  episode_len_mean: 409.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.590000000000005\n",
      "  episode_reward_mean: -2.0128999999999557\n",
      "  episode_reward_min: -11.899999999999938\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 2501\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6420619678148012\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.648984338177575\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.025692832660303425\n",
      "          policy_loss: -0.04824564109245936\n",
      "          total_loss: 0.4528419506218698\n",
      "          vf_explained_var: 0.7602860331535339\n",
      "          vf_loss: 0.5010810466276274\n",
      "    num_agent_steps_sampled: 681000\n",
      "    num_agent_steps_trained: 681000\n",
      "    num_steps_sampled: 681000\n",
      "    num_steps_trained: 681000\n",
      "  iterations_since_restore: 681\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.60187499999999\n",
      "    ram_util_percent: 34.42375\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03694337678586317\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.112613512930643\n",
      "    mean_inference_ms: 2.1206580393731604\n",
      "    mean_raw_obs_processing_ms: 26.812565899706456\n",
      "  time_since_restore: 33228.08621907234\n",
      "  time_this_iter_s: 111.77526640892029\n",
      "  time_total_s: 33228.08621907234\n",
      "  timers:\n",
      "    learn_throughput: 1331.487\n",
      "    learn_time_ms: 751.04\n",
      "    load_throughput: 42468.847\n",
      "    load_time_ms: 23.547\n",
      "    sample_throughput: 16.212\n",
      "    sample_time_ms: 61682.027\n",
      "    update_time_ms: 3.958\n",
      "  timestamp: 1635315814\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 681000\n",
      "  training_iteration: 681\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   681</td><td style=\"text-align: right;\">         33228.1</td><td style=\"text-align: right;\">681000</td><td style=\"text-align: right;\"> -2.0129</td><td style=\"text-align: right;\">                7.59</td><td style=\"text-align: right;\">               -11.9</td><td style=\"text-align: right;\">            409.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 682000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-24-11\n",
      "  done: false\n",
      "  episode_len_mean: 406.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.590000000000005\n",
      "  episode_reward_mean: -2.1052999999999544\n",
      "  episode_reward_min: -16.549999999999905\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2504\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9630929517222018\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.867239126894209\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011882039532151327\n",
      "          policy_loss: 0.03328957897093561\n",
      "          total_loss: 0.17133173843224844\n",
      "          vf_explained_var: -0.07546170800924301\n",
      "          vf_loss: 0.1452710456525286\n",
      "    num_agent_steps_sampled: 682000\n",
      "    num_agent_steps_trained: 682000\n",
      "    num_steps_sampled: 682000\n",
      "    num_steps_trained: 682000\n",
      "  iterations_since_restore: 682\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.73207547169811\n",
      "    ram_util_percent: 34.403773584905665\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036944740505723835\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.10473747018086\n",
      "    mean_inference_ms: 2.120694094548371\n",
      "    mean_raw_obs_processing_ms: 26.791568852875955\n",
      "  time_since_restore: 33265.61287856102\n",
      "  time_this_iter_s: 37.52665948867798\n",
      "  time_total_s: 33265.61287856102\n",
      "  timers:\n",
      "    learn_throughput: 1330.849\n",
      "    learn_time_ms: 751.4\n",
      "    load_throughput: 41672.958\n",
      "    load_time_ms: 23.996\n",
      "    sample_throughput: 16.095\n",
      "    sample_time_ms: 62131.428\n",
      "    update_time_ms: 3.87\n",
      "  timestamp: 1635315851\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 682000\n",
      "  training_iteration: 682\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   682</td><td style=\"text-align: right;\">         33265.6</td><td style=\"text-align: right;\">682000</td><td style=\"text-align: right;\"> -2.1053</td><td style=\"text-align: right;\">                7.59</td><td style=\"text-align: right;\">              -16.55</td><td style=\"text-align: right;\">            406.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 683000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-25-49\n",
      "  done: false\n",
      "  episode_len_mean: 383.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.590000000000005\n",
      "  episode_reward_mean: -1.6255999999999573\n",
      "  episode_reward_min: -16.549999999999905\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 2510\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9630929517222018\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7960275755988226\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017607526581673177\n",
      "          policy_loss: -0.030667294210029974\n",
      "          total_loss: 0.46598309717244574\n",
      "          vf_explained_var: 0.717583954334259\n",
      "          vf_loss: 0.49765299302008414\n",
      "    num_agent_steps_sampled: 683000\n",
      "    num_agent_steps_trained: 683000\n",
      "    num_steps_sampled: 683000\n",
      "    num_steps_trained: 683000\n",
      "  iterations_since_restore: 683\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.54460431654677\n",
      "    ram_util_percent: 34.45179856115109\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03694740179803734\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.090012461548977\n",
      "    mean_inference_ms: 2.120765333336705\n",
      "    mean_raw_obs_processing_ms: 26.759266324408262\n",
      "  time_since_restore: 33363.19649887085\n",
      "  time_this_iter_s: 97.58362030982971\n",
      "  time_total_s: 33363.19649887085\n",
      "  timers:\n",
      "    learn_throughput: 1331.247\n",
      "    learn_time_ms: 751.175\n",
      "    load_throughput: 42558.435\n",
      "    load_time_ms: 23.497\n",
      "    sample_throughput: 14.666\n",
      "    sample_time_ms: 68186.95\n",
      "    update_time_ms: 3.87\n",
      "  timestamp: 1635315949\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 683000\n",
      "  training_iteration: 683\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   683</td><td style=\"text-align: right;\">         33363.2</td><td style=\"text-align: right;\">683000</td><td style=\"text-align: right;\"> -1.6256</td><td style=\"text-align: right;\">                9.59</td><td style=\"text-align: right;\">              -16.55</td><td style=\"text-align: right;\">            383.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 684000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-28-02\n",
      "  done: false\n",
      "  episode_len_mean: 352.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.590000000000005\n",
      "  episode_reward_mean: -1.1272999999999604\n",
      "  episode_reward_min: -16.549999999999905\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2518\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9630929517222018\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8953650991121929\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01191405540861052\n",
      "          policy_loss: 0.09881714578304025\n",
      "          total_loss: 0.25914775265587703\n",
      "          vf_explained_var: 0.15185044705867767\n",
      "          vf_loss: 0.16780991343419172\n",
      "    num_agent_steps_sampled: 684000\n",
      "    num_agent_steps_trained: 684000\n",
      "    num_steps_sampled: 684000\n",
      "    num_steps_trained: 684000\n",
      "  iterations_since_restore: 684\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.2219895287958\n",
      "    ram_util_percent: 34.441361256544496\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036950694943049076\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.07262196185258\n",
      "    mean_inference_ms: 2.1208552253064283\n",
      "    mean_raw_obs_processing_ms: 26.73670447086928\n",
      "  time_since_restore: 33496.405234098434\n",
      "  time_this_iter_s: 133.20873522758484\n",
      "  time_total_s: 33496.405234098434\n",
      "  timers:\n",
      "    learn_throughput: 1329.14\n",
      "    learn_time_ms: 752.366\n",
      "    load_throughput: 42805.266\n",
      "    load_time_ms: 23.362\n",
      "    sample_throughput: 12.577\n",
      "    sample_time_ms: 79509.608\n",
      "    update_time_ms: 3.916\n",
      "  timestamp: 1635316082\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 684000\n",
      "  training_iteration: 684\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   684</td><td style=\"text-align: right;\">         33496.4</td><td style=\"text-align: right;\">684000</td><td style=\"text-align: right;\"> -1.1273</td><td style=\"text-align: right;\">                9.59</td><td style=\"text-align: right;\">              -16.55</td><td style=\"text-align: right;\">             352.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 685000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-28-22\n",
      "  done: false\n",
      "  episode_len_mean: 351.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.590000000000005\n",
      "  episode_reward_mean: -1.1222999999999606\n",
      "  episode_reward_min: -16.549999999999905\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2519\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9630929517222018\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.854928187529246\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010034494792532753\n",
      "          policy_loss: 0.04532608770661884\n",
      "          total_loss: 0.12251388629277547\n",
      "          vf_explained_var: 0.16824446618556976\n",
      "          vf_loss: 0.08607292585462953\n",
      "    num_agent_steps_sampled: 685000\n",
      "    num_agent_steps_trained: 685000\n",
      "    num_steps_sampled: 685000\n",
      "    num_steps_trained: 685000\n",
      "  iterations_since_restore: 685\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.32857142857142\n",
      "    ram_util_percent: 34.43571428571429\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03695108780232305\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.07060666315567\n",
      "    mean_inference_ms: 2.120866081995373\n",
      "    mean_raw_obs_processing_ms: 26.734637811568533\n",
      "  time_since_restore: 33516.25316953659\n",
      "  time_this_iter_s: 19.847935438156128\n",
      "  time_total_s: 33516.25316953659\n",
      "  timers:\n",
      "    learn_throughput: 1329.163\n",
      "    learn_time_ms: 752.353\n",
      "    load_throughput: 42820.474\n",
      "    load_time_ms: 23.353\n",
      "    sample_throughput: 13.166\n",
      "    sample_time_ms: 75953.075\n",
      "    update_time_ms: 3.909\n",
      "  timestamp: 1635316102\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 685000\n",
      "  training_iteration: 685\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   685</td><td style=\"text-align: right;\">         33516.3</td><td style=\"text-align: right;\">685000</td><td style=\"text-align: right;\"> -1.1223</td><td style=\"text-align: right;\">                9.59</td><td style=\"text-align: right;\">              -16.55</td><td style=\"text-align: right;\">            351.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 686000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-29-01\n",
      "  done: false\n",
      "  episode_len_mean: 346.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.590000000000005\n",
      "  episode_reward_mean: -1.0468999999999606\n",
      "  episode_reward_min: -16.549999999999905\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2522\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9630929517222018\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8650727536943223\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012152845934601824\n",
      "          policy_loss: -0.037176669016480444\n",
      "          total_loss: 0.2515990774664614\n",
      "          vf_explained_var: 0.6511785984039307\n",
      "          vf_loss: 0.2957221551901764\n",
      "    num_agent_steps_sampled: 686000\n",
      "    num_agent_steps_trained: 686000\n",
      "    num_steps_sampled: 686000\n",
      "    num_steps_trained: 686000\n",
      "  iterations_since_restore: 686\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.59999999999999\n",
      "    ram_util_percent: 34.45357142857143\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036952287139269895\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.064700424635088\n",
      "    mean_inference_ms: 2.1208994492316013\n",
      "    mean_raw_obs_processing_ms: 26.728843803996526\n",
      "  time_since_restore: 33555.69837331772\n",
      "  time_this_iter_s: 39.44520378112793\n",
      "  time_total_s: 33555.69837331772\n",
      "  timers:\n",
      "    learn_throughput: 1328.894\n",
      "    learn_time_ms: 752.505\n",
      "    load_throughput: 42704.287\n",
      "    load_time_ms: 23.417\n",
      "    sample_throughput: 12.883\n",
      "    sample_time_ms: 77621.015\n",
      "    update_time_ms: 3.863\n",
      "  timestamp: 1635316141\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 686000\n",
      "  training_iteration: 686\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   686</td><td style=\"text-align: right;\">         33555.7</td><td style=\"text-align: right;\">686000</td><td style=\"text-align: right;\"> -1.0469</td><td style=\"text-align: right;\">                9.59</td><td style=\"text-align: right;\">              -16.55</td><td style=\"text-align: right;\">            346.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 687000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-29-20\n",
      "  done: false\n",
      "  episode_len_mean: 343.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.590000000000005\n",
      "  episode_reward_mean: -1.0141999999999616\n",
      "  episode_reward_min: -16.549999999999905\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2524\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9630929517222018\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.659340308772193\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011798760311695577\n",
      "          policy_loss: -0.037814767782886824\n",
      "          total_loss: 0.27201157949037025\n",
      "          vf_explained_var: 0.6695032119750977\n",
      "          vf_loss: 0.3150564462567369\n",
      "    num_agent_steps_sampled: 687000\n",
      "    num_agent_steps_trained: 687000\n",
      "    num_steps_sampled: 687000\n",
      "    num_steps_trained: 687000\n",
      "  iterations_since_restore: 687\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.07777777777777\n",
      "    ram_util_percent: 34.53333333333334\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036953068947816456\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.06098481876863\n",
      "    mean_inference_ms: 2.1209211939629715\n",
      "    mean_raw_obs_processing_ms: 26.725715256631247\n",
      "  time_since_restore: 33574.75371646881\n",
      "  time_this_iter_s: 19.05534315109253\n",
      "  time_total_s: 33574.75371646881\n",
      "  timers:\n",
      "    learn_throughput: 1327.026\n",
      "    learn_time_ms: 753.565\n",
      "    load_throughput: 42423.658\n",
      "    load_time_ms: 23.572\n",
      "    sample_throughput: 14.295\n",
      "    sample_time_ms: 69954.549\n",
      "    update_time_ms: 3.779\n",
      "  timestamp: 1635316160\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 687000\n",
      "  training_iteration: 687\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   687</td><td style=\"text-align: right;\">         33574.8</td><td style=\"text-align: right;\">687000</td><td style=\"text-align: right;\"> -1.0142</td><td style=\"text-align: right;\">                9.59</td><td style=\"text-align: right;\">              -16.55</td><td style=\"text-align: right;\">            343.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 688000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-29-57\n",
      "  done: false\n",
      "  episode_len_mean: 339.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.8151999999999633\n",
      "  episode_reward_min: -16.549999999999905\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2527\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9630929517222018\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7897350192070007\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014356057182315082\n",
      "          policy_loss: -0.05559210106730461\n",
      "          total_loss: 0.7371922777758704\n",
      "          vf_explained_var: 0.5800309777259827\n",
      "          vf_loss: 0.7968555132548014\n",
      "    num_agent_steps_sampled: 688000\n",
      "    num_agent_steps_trained: 688000\n",
      "    num_steps_sampled: 688000\n",
      "    num_steps_trained: 688000\n",
      "  iterations_since_restore: 688\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.48653846153846\n",
      "    ram_util_percent: 34.4923076923077\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0369542314168792\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.0555215547007\n",
      "    mean_inference_ms: 2.1209538867396107\n",
      "    mean_raw_obs_processing_ms: 26.721554141377254\n",
      "  time_since_restore: 33611.21712756157\n",
      "  time_this_iter_s: 36.46341109275818\n",
      "  time_total_s: 33611.21712756157\n",
      "  timers:\n",
      "    learn_throughput: 1326.065\n",
      "    learn_time_ms: 754.111\n",
      "    load_throughput: 42167.031\n",
      "    load_time_ms: 23.715\n",
      "    sample_throughput: 16.55\n",
      "    sample_time_ms: 60423.021\n",
      "    update_time_ms: 3.773\n",
      "  timestamp: 1635316197\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 688000\n",
      "  training_iteration: 688\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   688</td><td style=\"text-align: right;\">         33611.2</td><td style=\"text-align: right;\">688000</td><td style=\"text-align: right;\"> -0.8152</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -16.55</td><td style=\"text-align: right;\">            339.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 689000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-30-14\n",
      "  done: false\n",
      "  episode_len_mean: 346.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.9840999999999621\n",
      "  episode_reward_min: -16.549999999999905\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2529\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9630929517222018\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.928501472208235\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008141472698315865\n",
      "          policy_loss: -0.017328790989187028\n",
      "          total_loss: 0.08446025202671686\n",
      "          vf_explained_var: 0.18094037473201752\n",
      "          vf_loss: 0.11323306971074393\n",
      "    num_agent_steps_sampled: 689000\n",
      "    num_agent_steps_trained: 689000\n",
      "    num_steps_sampled: 689000\n",
      "    num_steps_trained: 689000\n",
      "  iterations_since_restore: 689\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.33999999999999\n",
      "    ram_util_percent: 34.56399999999999\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036955006396897964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.051881826957466\n",
      "    mean_inference_ms: 2.120976069145283\n",
      "    mean_raw_obs_processing_ms: 26.718376165356275\n",
      "  time_since_restore: 33628.66962623596\n",
      "  time_this_iter_s: 17.4524986743927\n",
      "  time_total_s: 33628.66962623596\n",
      "  timers:\n",
      "    learn_throughput: 1326.111\n",
      "    learn_time_ms: 754.084\n",
      "    load_throughput: 43918.034\n",
      "    load_time_ms: 22.77\n",
      "    sample_throughput: 17.252\n",
      "    sample_time_ms: 57962.648\n",
      "    update_time_ms: 3.861\n",
      "  timestamp: 1635316214\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 689000\n",
      "  training_iteration: 689\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   689</td><td style=\"text-align: right;\">         33628.7</td><td style=\"text-align: right;\">689000</td><td style=\"text-align: right;\"> -0.9841</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -16.55</td><td style=\"text-align: right;\">            346.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 690000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-30-51\n",
      "  done: false\n",
      "  episode_len_mean: 341.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.9389999999999631\n",
      "  episode_reward_min: -16.549999999999905\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2531\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9630929517222018\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.76788121064504\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01679837746319595\n",
      "          policy_loss: 0.02172081189023124\n",
      "          total_loss: 0.37080883619685967\n",
      "          vf_explained_var: 0.7376570105552673\n",
      "          vf_loss: 0.3505884306298362\n",
      "    num_agent_steps_sampled: 690000\n",
      "    num_agent_steps_trained: 690000\n",
      "    num_steps_sampled: 690000\n",
      "    num_steps_trained: 690000\n",
      "  iterations_since_restore: 690\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.090566037735854\n",
      "    ram_util_percent: 34.562264150943385\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0369557913963291\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.04826812324002\n",
      "    mean_inference_ms: 2.1209987693321706\n",
      "    mean_raw_obs_processing_ms: 26.71533134419482\n",
      "  time_since_restore: 33665.36464571953\n",
      "  time_this_iter_s: 36.695019483566284\n",
      "  time_total_s: 33665.36464571953\n",
      "  timers:\n",
      "    learn_throughput: 1324.992\n",
      "    learn_time_ms: 754.721\n",
      "    load_throughput: 43993.489\n",
      "    load_time_ms: 22.731\n",
      "    sample_throughput: 18.478\n",
      "    sample_time_ms: 54119.863\n",
      "    update_time_ms: 3.78\n",
      "  timestamp: 1635316251\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 690000\n",
      "  training_iteration: 690\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   690</td><td style=\"text-align: right;\">         33665.4</td><td style=\"text-align: right;\">690000</td><td style=\"text-align: right;\">  -0.939</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -16.55</td><td style=\"text-align: right;\">            341.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 691000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-31-48\n",
      "  done: false\n",
      "  episode_len_mean: 326.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.6598999999999644\n",
      "  episode_reward_min: -16.549999999999905\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2535\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9630929517222018\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.798192142115699\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009605856347308163\n",
      "          policy_loss: -0.08857068282862504\n",
      "          total_loss: 0.3905190067158805\n",
      "          vf_explained_var: 0.7948357462882996\n",
      "          vf_loss: 0.48782026867071787\n",
      "    num_agent_steps_sampled: 691000\n",
      "    num_agent_steps_trained: 691000\n",
      "    num_steps_sampled: 691000\n",
      "    num_steps_trained: 691000\n",
      "  iterations_since_restore: 691\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.497530864197536\n",
      "    ram_util_percent: 34.538271604938274\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03695731621993179\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.041713136793565\n",
      "    mean_inference_ms: 2.1210435525382065\n",
      "    mean_raw_obs_processing_ms: 26.71304873541145\n",
      "  time_since_restore: 33722.50865769386\n",
      "  time_this_iter_s: 57.14401197433472\n",
      "  time_total_s: 33722.50865769386\n",
      "  timers:\n",
      "    learn_throughput: 1321.575\n",
      "    learn_time_ms: 756.673\n",
      "    load_throughput: 43995.98\n",
      "    load_time_ms: 22.729\n",
      "    sample_throughput: 20.553\n",
      "    sample_time_ms: 48654.79\n",
      "    update_time_ms: 3.765\n",
      "  timestamp: 1635316308\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 691000\n",
      "  training_iteration: 691\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   691</td><td style=\"text-align: right;\">         33722.5</td><td style=\"text-align: right;\">691000</td><td style=\"text-align: right;\"> -0.6599</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -16.55</td><td style=\"text-align: right;\">            326.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 692000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-32-05\n",
      "  done: false\n",
      "  episode_len_mean: 330.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.8\n",
      "  episode_reward_mean: -0.773499999999963\n",
      "  episode_reward_min: -16.549999999999905\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2537\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9630929517222018\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9726742294099595\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006433253718623197\n",
      "          policy_loss: -0.04014385239117675\n",
      "          total_loss: 0.09295808294167121\n",
      "          vf_explained_var: 0.47040700912475586\n",
      "          vf_loss: 0.14663285387472974\n",
      "    num_agent_steps_sampled: 692000\n",
      "    num_agent_steps_trained: 692000\n",
      "    num_steps_sampled: 692000\n",
      "    num_steps_trained: 692000\n",
      "  iterations_since_restore: 692\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.45\n",
      "    ram_util_percent: 34.50833333333333\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03695807898340649\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.038551925541995\n",
      "    mean_inference_ms: 2.121066158093853\n",
      "    mean_raw_obs_processing_ms: 26.71181379710324\n",
      "  time_since_restore: 33738.963376283646\n",
      "  time_this_iter_s: 16.454718589782715\n",
      "  time_total_s: 33738.963376283646\n",
      "  timers:\n",
      "    learn_throughput: 1323.26\n",
      "    learn_time_ms: 755.709\n",
      "    load_throughput: 45875.236\n",
      "    load_time_ms: 21.798\n",
      "    sample_throughput: 21.483\n",
      "    sample_time_ms: 46549.488\n",
      "    update_time_ms: 3.768\n",
      "  timestamp: 1635316325\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 692000\n",
      "  training_iteration: 692\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   692</td><td style=\"text-align: right;\">           33739</td><td style=\"text-align: right;\">692000</td><td style=\"text-align: right;\"> -0.7735</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">              -16.55</td><td style=\"text-align: right;\">            330.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 693000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-34-53\n",
      "  done: false\n",
      "  episode_len_mean: 296.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.83\n",
      "  episode_reward_mean: -0.11329999999996807\n",
      "  episode_reward_min: -16.549999999999905\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2545\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9630929517222018\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.738901674747467\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011050175367577205\n",
      "          policy_loss: -0.06097895304361979\n",
      "          total_loss: 0.6533853554891216\n",
      "          vf_explained_var: 0.569085419178009\n",
      "          vf_loss: 0.7211109762390454\n",
      "    num_agent_steps_sampled: 693000\n",
      "    num_agent_steps_trained: 693000\n",
      "    num_steps_sampled: 693000\n",
      "    num_steps_trained: 693000\n",
      "  iterations_since_restore: 693\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.20291666666666\n",
      "    ram_util_percent: 34.43999999999999\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036960977773936365\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.027525431334112\n",
      "    mean_inference_ms: 2.1211529211833464\n",
      "    mean_raw_obs_processing_ms: 26.725225289792938\n",
      "  time_since_restore: 33907.46780586243\n",
      "  time_this_iter_s: 168.50442957878113\n",
      "  time_total_s: 33907.46780586243\n",
      "  timers:\n",
      "    learn_throughput: 1323.211\n",
      "    learn_time_ms: 755.738\n",
      "    load_throughput: 44847.978\n",
      "    load_time_ms: 22.298\n",
      "    sample_throughput: 18.642\n",
      "    sample_time_ms: 53641.029\n",
      "    update_time_ms: 3.774\n",
      "  timestamp: 1635316493\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 693000\n",
      "  training_iteration: 693\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   693</td><td style=\"text-align: right;\">         33907.5</td><td style=\"text-align: right;\">693000</td><td style=\"text-align: right;\"> -0.1133</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">              -16.55</td><td style=\"text-align: right;\">            296.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 694000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-35-53\n",
      "  done: false\n",
      "  episode_len_mean: 295.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.83\n",
      "  episode_reward_mean: -0.06929999999996844\n",
      "  episode_reward_min: -16.549999999999905\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2549\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9630929517222018\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8044409698910184\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008584411252221565\n",
      "          policy_loss: -0.14549937637315857\n",
      "          total_loss: 0.019253236800432207\n",
      "          vf_explained_var: 0.818865954875946\n",
      "          vf_loss: 0.17452943885388475\n",
      "    num_agent_steps_sampled: 694000\n",
      "    num_agent_steps_trained: 694000\n",
      "    num_steps_sampled: 694000\n",
      "    num_steps_trained: 694000\n",
      "  iterations_since_restore: 694\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.516279069767435\n",
      "    ram_util_percent: 34.437209302325584\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036962369267062235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.02318363034192\n",
      "    mean_inference_ms: 2.121191932975173\n",
      "    mean_raw_obs_processing_ms: 26.735802177287873\n",
      "  time_since_restore: 33967.33608818054\n",
      "  time_this_iter_s: 59.868282318115234\n",
      "  time_total_s: 33967.33608818054\n",
      "  timers:\n",
      "    learn_throughput: 1323.824\n",
      "    learn_time_ms: 755.388\n",
      "    load_throughput: 44959.364\n",
      "    load_time_ms: 22.242\n",
      "    sample_throughput: 21.595\n",
      "    sample_time_ms: 46307.458\n",
      "    update_time_ms: 3.724\n",
      "  timestamp: 1635316553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 694000\n",
      "  training_iteration: 694\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   694</td><td style=\"text-align: right;\">         33967.3</td><td style=\"text-align: right;\">694000</td><td style=\"text-align: right;\"> -0.0693</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">              -16.55</td><td style=\"text-align: right;\">            295.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 695000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-38-09\n",
      "  done: false\n",
      "  episode_len_mean: 264.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.84\n",
      "  episode_reward_mean: 0.6448000000000276\n",
      "  episode_reward_min: -16.549999999999905\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2557\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9630929517222018\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7059478627310858\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012646876599681584\n",
      "          policy_loss: 0.036088656220171184\n",
      "          total_loss: 0.6570510551333427\n",
      "          vf_explained_var: 0.8526515364646912\n",
      "          vf_loss: 0.6258417603042391\n",
      "    num_agent_steps_sampled: 695000\n",
      "    num_agent_steps_trained: 695000\n",
      "    num_steps_sampled: 695000\n",
      "    num_steps_trained: 695000\n",
      "  iterations_since_restore: 695\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.45025906735752\n",
      "    ram_util_percent: 34.4222797927461\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036965042507755785\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.01610424275195\n",
      "    mean_inference_ms: 2.1212676695535944\n",
      "    mean_raw_obs_processing_ms: 26.772060006172733\n",
      "  time_since_restore: 34102.67737817764\n",
      "  time_this_iter_s: 135.34128999710083\n",
      "  time_total_s: 34102.67737817764\n",
      "  timers:\n",
      "    learn_throughput: 1322.484\n",
      "    learn_time_ms: 756.153\n",
      "    load_throughput: 45006.546\n",
      "    load_time_ms: 22.219\n",
      "    sample_throughput: 17.284\n",
      "    sample_time_ms: 57855.995\n",
      "    update_time_ms: 3.773\n",
      "  timestamp: 1635316689\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 695000\n",
      "  training_iteration: 695\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   695</td><td style=\"text-align: right;\">         34102.7</td><td style=\"text-align: right;\">695000</td><td style=\"text-align: right;\">  0.6448</td><td style=\"text-align: right;\">                9.84</td><td style=\"text-align: right;\">              -16.55</td><td style=\"text-align: right;\">            264.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 696000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-39-21\n",
      "  done: false\n",
      "  episode_len_mean: 253.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.84\n",
      "  episode_reward_mean: 0.8992000000000256\n",
      "  episode_reward_min: -16.549999999999905\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2561\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9630929517222018\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9381562617090013\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006587357221642145\n",
      "          policy_loss: 0.020509166684415606\n",
      "          total_loss: 0.34248749253650507\n",
      "          vf_explained_var: 0.7431150078773499\n",
      "          vf_loss: 0.3350156499719661\n",
      "    num_agent_steps_sampled: 696000\n",
      "    num_agent_steps_trained: 696000\n",
      "    num_steps_sampled: 696000\n",
      "    num_steps_trained: 696000\n",
      "  iterations_since_restore: 696\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.655339805825236\n",
      "    ram_util_percent: 34.38834951456311\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036966290156495954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.01334947748295\n",
      "    mean_inference_ms: 2.1213033298828226\n",
      "    mean_raw_obs_processing_ms: 26.79548185230744\n",
      "  time_since_restore: 34175.127220630646\n",
      "  time_this_iter_s: 72.44984245300293\n",
      "  time_total_s: 34175.127220630646\n",
      "  timers:\n",
      "    learn_throughput: 1324.607\n",
      "    learn_time_ms: 754.941\n",
      "    load_throughput: 44923.441\n",
      "    load_time_ms: 22.26\n",
      "    sample_throughput: 16.351\n",
      "    sample_time_ms: 61157.578\n",
      "    update_time_ms: 3.839\n",
      "  timestamp: 1635316761\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 696000\n",
      "  training_iteration: 696\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   696</td><td style=\"text-align: right;\">         34175.1</td><td style=\"text-align: right;\">696000</td><td style=\"text-align: right;\">  0.8992</td><td style=\"text-align: right;\">                9.84</td><td style=\"text-align: right;\">              -16.55</td><td style=\"text-align: right;\">            253.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 697000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-39-57\n",
      "  done: false\n",
      "  episode_len_mean: 254.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.84\n",
      "  episode_reward_mean: 0.8937000000000255\n",
      "  episode_reward_min: -16.549999999999905\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2563\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9630929517222018\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.935840862327152\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00826335086024401\n",
      "          policy_loss: -0.06732040262884564\n",
      "          total_loss: 0.13874624135593575\n",
      "          vf_explained_var: 0.7688034772872925\n",
      "          vf_loss: 0.21746667864111563\n",
      "    num_agent_steps_sampled: 697000\n",
      "    num_agent_steps_trained: 697000\n",
      "    num_steps_sampled: 697000\n",
      "    num_steps_trained: 697000\n",
      "  iterations_since_restore: 697\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.42884615384615\n",
      "    ram_util_percent: 34.51346153846154\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03696692928732785\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.011906742422294\n",
      "    mean_inference_ms: 2.1213213924477903\n",
      "    mean_raw_obs_processing_ms: 26.80680060229983\n",
      "  time_since_restore: 34211.27970790863\n",
      "  time_this_iter_s: 36.15248727798462\n",
      "  time_total_s: 34211.27970790863\n",
      "  timers:\n",
      "    learn_throughput: 1323.102\n",
      "    learn_time_ms: 755.799\n",
      "    load_throughput: 44915.551\n",
      "    load_time_ms: 22.264\n",
      "    sample_throughput: 15.907\n",
      "    sample_time_ms: 62866.417\n",
      "    update_time_ms: 3.848\n",
      "  timestamp: 1635316797\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 697000\n",
      "  training_iteration: 697\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   697</td><td style=\"text-align: right;\">         34211.3</td><td style=\"text-align: right;\">697000</td><td style=\"text-align: right;\">  0.8937</td><td style=\"text-align: right;\">                9.84</td><td style=\"text-align: right;\">              -16.55</td><td style=\"text-align: right;\">             254.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 698000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-42-28\n",
      "  done: false\n",
      "  episode_len_mean: 230.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.84\n",
      "  episode_reward_mean: 1.4235000000000224\n",
      "  episode_reward_min: -16.549999999999905\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2572\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9630929517222018\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.866947399245368\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011181511279640283\n",
      "          policy_loss: 0.007141991125212775\n",
      "          total_loss: 0.5127639980779753\n",
      "          vf_explained_var: 0.3895585834980011\n",
      "          vf_loss: 0.5135226407631611\n",
      "    num_agent_steps_sampled: 698000\n",
      "    num_agent_steps_trained: 698000\n",
      "    num_steps_sampled: 698000\n",
      "    num_steps_trained: 698000\n",
      "  iterations_since_restore: 698\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.51203703703704\n",
      "    ram_util_percent: 34.46388888888889\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03696962977955717\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.00672528777363\n",
      "    mean_inference_ms: 2.1213987271081725\n",
      "    mean_raw_obs_processing_ms: 26.8746944244579\n",
      "  time_since_restore: 34362.51883149147\n",
      "  time_this_iter_s: 151.23912358283997\n",
      "  time_total_s: 34362.51883149147\n",
      "  timers:\n",
      "    learn_throughput: 1321.456\n",
      "    learn_time_ms: 756.741\n",
      "    load_throughput: 45298.871\n",
      "    load_time_ms: 22.076\n",
      "    sample_throughput: 13.451\n",
      "    sample_time_ms: 74343.172\n",
      "    update_time_ms: 3.854\n",
      "  timestamp: 1635316948\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 698000\n",
      "  training_iteration: 698\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   698</td><td style=\"text-align: right;\">         34362.5</td><td style=\"text-align: right;\">698000</td><td style=\"text-align: right;\">  1.4235</td><td style=\"text-align: right;\">                9.84</td><td style=\"text-align: right;\">              -16.55</td><td style=\"text-align: right;\">            230.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 699000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-44-03\n",
      "  done: false\n",
      "  episode_len_mean: 221.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.84\n",
      "  episode_reward_mean: 1.6028000000000207\n",
      "  episode_reward_min: -16.549999999999905\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 2577\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9630929517222018\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8325038843684727\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02882485621135247\n",
      "          policy_loss: -0.0400610723429256\n",
      "          total_loss: 0.43159455731511115\n",
      "          vf_explained_var: 0.7034435868263245\n",
      "          vf_loss: 0.4622196550998423\n",
      "    num_agent_steps_sampled: 699000\n",
      "    num_agent_steps_trained: 699000\n",
      "    num_steps_sampled: 699000\n",
      "    num_steps_trained: 699000\n",
      "  iterations_since_restore: 699\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.71333333333334\n",
      "    ram_util_percent: 34.41777777777778\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036971044120371784\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.004035812242456\n",
      "    mean_inference_ms: 2.121438996790632\n",
      "    mean_raw_obs_processing_ms: 26.91641418974638\n",
      "  time_since_restore: 34457.45620679855\n",
      "  time_this_iter_s: 94.93737530708313\n",
      "  time_total_s: 34457.45620679855\n",
      "  timers:\n",
      "    learn_throughput: 1319.385\n",
      "    learn_time_ms: 757.929\n",
      "    load_throughput: 43517.097\n",
      "    load_time_ms: 22.979\n",
      "    sample_throughput: 12.182\n",
      "    sample_time_ms: 82089.674\n",
      "    update_time_ms: 3.761\n",
      "  timestamp: 1635317043\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 699000\n",
      "  training_iteration: 699\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   699</td><td style=\"text-align: right;\">         34457.5</td><td style=\"text-align: right;\">699000</td><td style=\"text-align: right;\">  1.6028</td><td style=\"text-align: right;\">                9.84</td><td style=\"text-align: right;\">              -16.55</td><td style=\"text-align: right;\">            221.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 700000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-44-44\n",
      "  done: false\n",
      "  episode_len_mean: 231.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.84\n",
      "  episode_reward_mean: 1.4273000000000218\n",
      "  episode_reward_min: -16.549999999999905\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2580\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4446394275833025\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9192852179209392\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011001882275053889\n",
      "          policy_loss: -0.0014994765321413677\n",
      "          total_loss: 0.23192674457612966\n",
      "          vf_explained_var: 0.7356089353561401\n",
      "          vf_loss: 0.23672531354758475\n",
      "    num_agent_steps_sampled: 700000\n",
      "    num_agent_steps_trained: 700000\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 700000\n",
      "  iterations_since_restore: 700\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.891525423728815\n",
      "    ram_util_percent: 34.45423728813559\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03697191072550937\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.002488394631328\n",
      "    mean_inference_ms: 2.1214638726624973\n",
      "    mean_raw_obs_processing_ms: 26.939991360807994\n",
      "  time_since_restore: 34498.34671783447\n",
      "  time_this_iter_s: 40.89051103591919\n",
      "  time_total_s: 34498.34671783447\n",
      "  timers:\n",
      "    learn_throughput: 1318.491\n",
      "    learn_time_ms: 758.443\n",
      "    load_throughput: 43356.64\n",
      "    load_time_ms: 23.065\n",
      "    sample_throughput: 12.12\n",
      "    sample_time_ms: 82508.636\n",
      "    update_time_ms: 3.756\n",
      "  timestamp: 1635317084\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 700\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   700</td><td style=\"text-align: right;\">         34498.3</td><td style=\"text-align: right;\">700000</td><td style=\"text-align: right;\">  1.4273</td><td style=\"text-align: right;\">                9.84</td><td style=\"text-align: right;\">              -16.55</td><td style=\"text-align: right;\">             231.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 701000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-46-21\n",
      "  done: false\n",
      "  episode_len_mean: 231.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.84\n",
      "  episode_reward_mean: 1.3981000000000219\n",
      "  episode_reward_min: -16.549999999999905\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 2586\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4446394275833025\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9589485552575854\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005342268053399055\n",
      "          policy_loss: 0.03249286909898122\n",
      "          total_loss: 0.11093225520518091\n",
      "          vf_explained_var: 0.6941036581993103\n",
      "          vf_loss: 0.09031122506906589\n",
      "    num_agent_steps_sampled: 701000\n",
      "    num_agent_steps_trained: 701000\n",
      "    num_steps_sampled: 701000\n",
      "    num_steps_trained: 701000\n",
      "  iterations_since_restore: 701\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.93333333333334\n",
      "    ram_util_percent: 34.49782608695653\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03697366915726473\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.999518606169936\n",
      "    mean_inference_ms: 2.1215143136813857\n",
      "    mean_raw_obs_processing_ms: 26.983734666743995\n",
      "  time_since_restore: 34595.30432844162\n",
      "  time_this_iter_s: 96.95761060714722\n",
      "  time_total_s: 34595.30432844162\n",
      "  timers:\n",
      "    learn_throughput: 1320.174\n",
      "    learn_time_ms: 757.476\n",
      "    load_throughput: 43538.78\n",
      "    load_time_ms: 22.968\n",
      "    sample_throughput: 11.562\n",
      "    sample_time_ms: 86491.143\n",
      "    update_time_ms: 3.676\n",
      "  timestamp: 1635317181\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 701000\n",
      "  training_iteration: 701\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   701</td><td style=\"text-align: right;\">         34595.3</td><td style=\"text-align: right;\">701000</td><td style=\"text-align: right;\">  1.3981</td><td style=\"text-align: right;\">                9.84</td><td style=\"text-align: right;\">              -16.55</td><td style=\"text-align: right;\">            231.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 702000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-48-15\n",
      "  done: false\n",
      "  episode_len_mean: 225.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.84\n",
      "  episode_reward_mean: 1.5088000000000208\n",
      "  episode_reward_min: -16.549999999999905\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2593\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4446394275833025\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.802543694443173\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009255306020844204\n",
      "          policy_loss: 0.03670970714754528\n",
      "          total_loss: 0.41344810290676026\n",
      "          vf_explained_var: 0.3904972970485687\n",
      "          vf_loss: 0.38139325338933205\n",
      "    num_agent_steps_sampled: 702000\n",
      "    num_agent_steps_trained: 702000\n",
      "    num_steps_sampled: 702000\n",
      "    num_steps_trained: 702000\n",
      "  iterations_since_restore: 702\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.61913580246913\n",
      "    ram_util_percent: 34.44814814814814\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03697572341807397\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.99590202780312\n",
      "    mean_inference_ms: 2.1215729879135976\n",
      "    mean_raw_obs_processing_ms: 27.040997698580878\n",
      "  time_since_restore: 34708.527750730515\n",
      "  time_this_iter_s: 113.22342228889465\n",
      "  time_total_s: 34708.527750730515\n",
      "  timers:\n",
      "    learn_throughput: 1322.379\n",
      "    learn_time_ms: 756.213\n",
      "    load_throughput: 41534.054\n",
      "    load_time_ms: 24.077\n",
      "    sample_throughput: 10.398\n",
      "    sample_time_ms: 96168.175\n",
      "    update_time_ms: 3.683\n",
      "  timestamp: 1635317295\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 702000\n",
      "  training_iteration: 702\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   702</td><td style=\"text-align: right;\">         34708.5</td><td style=\"text-align: right;\">702000</td><td style=\"text-align: right;\">  1.5088</td><td style=\"text-align: right;\">                9.84</td><td style=\"text-align: right;\">              -16.55</td><td style=\"text-align: right;\">            225.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 703000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-48-35\n",
      "  done: false\n",
      "  episode_len_mean: 230.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.84\n",
      "  episode_reward_mean: 1.4566000000000212\n",
      "  episode_reward_min: -16.549999999999905\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2594\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4446394275833025\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8237268447875976\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004733394846409153\n",
      "          policy_loss: 0.019756270986464288\n",
      "          total_loss: 0.2619074409206708\n",
      "          vf_explained_var: 0.6701077222824097\n",
      "          vf_loss: 0.25355039010238317\n",
      "    num_agent_steps_sampled: 703000\n",
      "    num_agent_steps_trained: 703000\n",
      "    num_steps_sampled: 703000\n",
      "    num_steps_trained: 703000\n",
      "  iterations_since_restore: 703\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.3896551724138\n",
      "    ram_util_percent: 34.56896551724138\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03697601880866526\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.99537765525375\n",
      "    mean_inference_ms: 2.1215813668734826\n",
      "    mean_raw_obs_processing_ms: 27.048524946326843\n",
      "  time_since_restore: 34729.102554798126\n",
      "  time_this_iter_s: 20.574804067611694\n",
      "  time_total_s: 34729.102554798126\n",
      "  timers:\n",
      "    learn_throughput: 1322.741\n",
      "    learn_time_ms: 756.006\n",
      "    load_throughput: 41628.082\n",
      "    load_time_ms: 24.022\n",
      "    sample_throughput: 12.289\n",
      "    sample_time_ms: 81375.404\n",
      "    update_time_ms: 3.767\n",
      "  timestamp: 1635317315\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 703000\n",
      "  training_iteration: 703\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   703</td><td style=\"text-align: right;\">         34729.1</td><td style=\"text-align: right;\">703000</td><td style=\"text-align: right;\">  1.4566</td><td style=\"text-align: right;\">                9.84</td><td style=\"text-align: right;\">              -16.55</td><td style=\"text-align: right;\">            230.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 704000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-52-27\n",
      "  done: false\n",
      "  episode_len_mean: 211.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.84\n",
      "  episode_reward_mean: 1.980500000000018\n",
      "  episode_reward_min: -11.849999999999888\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 2607\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7223197137916513\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6983348608016968\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02700420856977445\n",
      "          policy_loss: 0.055152416643169194\n",
      "          total_loss: 1.0164231037100155\n",
      "          vf_explained_var: 0.7760406732559204\n",
      "          vf_loss: 0.9587483657730951\n",
      "    num_agent_steps_sampled: 704000\n",
      "    num_agent_steps_trained: 704000\n",
      "    num_steps_sampled: 704000\n",
      "    num_steps_trained: 704000\n",
      "  iterations_since_restore: 704\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.23383685800604\n",
      "    ram_util_percent: 34.489728096676735\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03697980448629137\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.99004198585356\n",
      "    mean_inference_ms: 2.121690569662282\n",
      "    mean_raw_obs_processing_ms: 27.166318913976557\n",
      "  time_since_restore: 34960.939643621445\n",
      "  time_this_iter_s: 231.83708882331848\n",
      "  time_total_s: 34960.939643621445\n",
      "  timers:\n",
      "    learn_throughput: 1324.681\n",
      "    learn_time_ms: 754.898\n",
      "    load_throughput: 41507.707\n",
      "    load_time_ms: 24.092\n",
      "    sample_throughput: 10.145\n",
      "    sample_time_ms: 98573.025\n",
      "    update_time_ms: 4.064\n",
      "  timestamp: 1635317547\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 704000\n",
      "  training_iteration: 704\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   704</td><td style=\"text-align: right;\">         34960.9</td><td style=\"text-align: right;\">704000</td><td style=\"text-align: right;\">  1.9805</td><td style=\"text-align: right;\">                9.84</td><td style=\"text-align: right;\">              -11.85</td><td style=\"text-align: right;\">            211.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 705000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-53-26\n",
      "  done: false\n",
      "  episode_len_mean: 217.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.84\n",
      "  episode_reward_mean: 1.9520000000000184\n",
      "  episode_reward_min: -11.849999999999888\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2610\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0834795706874765\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9864767723613315\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010007914815692824\n",
      "          policy_loss: -0.08992382919208872\n",
      "          total_loss: 0.2559319055794428\n",
      "          vf_explained_var: 0.4410989582538605\n",
      "          vf_loss: 0.35487712663469007\n",
      "    num_agent_steps_sampled: 705000\n",
      "    num_agent_steps_trained: 705000\n",
      "    num_steps_sampled: 705000\n",
      "    num_steps_trained: 705000\n",
      "  iterations_since_restore: 705\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.61428571428571\n",
      "    ram_util_percent: 34.45833333333333\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03698068119654294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.988884424943308\n",
      "    mean_inference_ms: 2.1217155520124136\n",
      "    mean_raw_obs_processing_ms: 27.19223543504584\n",
      "  time_since_restore: 35020.067793130875\n",
      "  time_this_iter_s: 59.12814950942993\n",
      "  time_total_s: 35020.067793130875\n",
      "  timers:\n",
      "    learn_throughput: 1325.729\n",
      "    learn_time_ms: 754.302\n",
      "    load_throughput: 41348.327\n",
      "    load_time_ms: 24.185\n",
      "    sample_throughput: 10.995\n",
      "    sample_time_ms: 90952.231\n",
      "    update_time_ms: 4.014\n",
      "  timestamp: 1635317606\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 705000\n",
      "  training_iteration: 705\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   705</td><td style=\"text-align: right;\">         35020.1</td><td style=\"text-align: right;\">705000</td><td style=\"text-align: right;\">   1.952</td><td style=\"text-align: right;\">                9.84</td><td style=\"text-align: right;\">              -11.85</td><td style=\"text-align: right;\">            217.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 706000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-56-36\n",
      "  done: false\n",
      "  episode_len_mean: 203.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.84\n",
      "  episode_reward_mean: 2.2974000000000174\n",
      "  episode_reward_min: -11.849999999999888\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2621\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0834795706874765\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9398520827293395\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008999892309340554\n",
      "          policy_loss: 0.002968894276354048\n",
      "          total_loss: 0.3946640650431315\n",
      "          vf_explained_var: 0.6447061896324158\n",
      "          vf_loss: 0.4013424867919336\n",
      "    num_agent_steps_sampled: 706000\n",
      "    num_agent_steps_trained: 706000\n",
      "    num_steps_sampled: 706000\n",
      "    num_steps_trained: 706000\n",
      "  iterations_since_restore: 706\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.193357933579335\n",
      "    ram_util_percent: 34.51328413284132\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03698388856748382\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.984825848809034\n",
      "    mean_inference_ms: 2.121805762247378\n",
      "    mean_raw_obs_processing_ms: 27.296886656444226\n",
      "  time_since_restore: 35209.81578564644\n",
      "  time_this_iter_s: 189.74799251556396\n",
      "  time_total_s: 35209.81578564644\n",
      "  timers:\n",
      "    learn_throughput: 1323.07\n",
      "    learn_time_ms: 755.818\n",
      "    load_throughput: 41361.253\n",
      "    load_time_ms: 24.177\n",
      "    sample_throughput: 9.739\n",
      "    sample_time_ms: 102680.604\n",
      "    update_time_ms: 3.94\n",
      "  timestamp: 1635317796\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 706000\n",
      "  training_iteration: 706\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   706</td><td style=\"text-align: right;\">         35209.8</td><td style=\"text-align: right;\">706000</td><td style=\"text-align: right;\">  2.2974</td><td style=\"text-align: right;\">                9.84</td><td style=\"text-align: right;\">              -11.85</td><td style=\"text-align: right;\">            203.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 707000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-57-57\n",
      "  done: false\n",
      "  episode_len_mean: 192.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.84\n",
      "  episode_reward_mean: 2.5077000000000176\n",
      "  episode_reward_min: -11.849999999999888\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 2626\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0834795706874765\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.700221339861552\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.04137024904029343\n",
      "          policy_loss: 0.12465812584592237\n",
      "          total_loss: 0.8854792550206184\n",
      "          vf_explained_var: 0.4800715446472168\n",
      "          vf_loss: 0.7329995108975305\n",
      "    num_agent_steps_sampled: 707000\n",
      "    num_agent_steps_trained: 707000\n",
      "    num_steps_sampled: 707000\n",
      "    num_steps_trained: 707000\n",
      "  iterations_since_restore: 707\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.19741379310345\n",
      "    ram_util_percent: 34.614655172413805\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03698524441126652\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.983580158632197\n",
      "    mean_inference_ms: 2.1218440395460774\n",
      "    mean_raw_obs_processing_ms: 27.350257271359247\n",
      "  time_since_restore: 35290.631558179855\n",
      "  time_this_iter_s: 80.81577253341675\n",
      "  time_total_s: 35290.631558179855\n",
      "  timers:\n",
      "    learn_throughput: 1325.538\n",
      "    learn_time_ms: 754.41\n",
      "    load_throughput: 41113.332\n",
      "    load_time_ms: 24.323\n",
      "    sample_throughput: 9.333\n",
      "    sample_time_ms: 107148.195\n",
      "    update_time_ms: 3.936\n",
      "  timestamp: 1635317877\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 707000\n",
      "  training_iteration: 707\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   707</td><td style=\"text-align: right;\">         35290.6</td><td style=\"text-align: right;\">707000</td><td style=\"text-align: right;\">  2.5077</td><td style=\"text-align: right;\">                9.84</td><td style=\"text-align: right;\">              -11.85</td><td style=\"text-align: right;\">            192.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 708000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-58-38\n",
      "  done: false\n",
      "  episode_len_mean: 192.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.84\n",
      "  episode_reward_mean: 2.463000000000017\n",
      "  episode_reward_min: -11.849999999999888\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2628\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6252193560312151\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9317261656125386\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004422798065093477\n",
      "          policy_loss: 0.01274473046263059\n",
      "          total_loss: 0.2949050074236261\n",
      "          vf_explained_var: 0.3114481270313263\n",
      "          vf_loss: 0.2942895198447837\n",
      "    num_agent_steps_sampled: 708000\n",
      "    num_agent_steps_trained: 708000\n",
      "    num_steps_sampled: 708000\n",
      "    num_steps_trained: 708000\n",
      "  iterations_since_restore: 708\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.68275862068966\n",
      "    ram_util_percent: 34.55172413793103\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0369857861107149\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.983242488715714\n",
      "    mean_inference_ms: 2.1218589702498556\n",
      "    mean_raw_obs_processing_ms: 27.37203107327423\n",
      "  time_since_restore: 35331.34630584717\n",
      "  time_this_iter_s: 40.71474766731262\n",
      "  time_total_s: 35331.34630584717\n",
      "  timers:\n",
      "    learn_throughput: 1327.336\n",
      "    learn_time_ms: 753.389\n",
      "    load_throughput: 40796.971\n",
      "    load_time_ms: 24.512\n",
      "    sample_throughput: 10.406\n",
      "    sample_time_ms: 96096.578\n",
      "    update_time_ms: 4.02\n",
      "  timestamp: 1635317918\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 708000\n",
      "  training_iteration: 708\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   708</td><td style=\"text-align: right;\">         35331.3</td><td style=\"text-align: right;\">708000</td><td style=\"text-align: right;\">   2.463</td><td style=\"text-align: right;\">                9.84</td><td style=\"text-align: right;\">              -11.85</td><td style=\"text-align: right;\">            192.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 709000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-59-32\n",
      "  done: false\n",
      "  episode_len_mean: 185.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.84\n",
      "  episode_reward_mean: 2.8038000000000154\n",
      "  episode_reward_min: -9.439999999999898\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2632\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8126096780156076\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0317314002248974\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004280611522728033\n",
      "          policy_loss: -0.08174333497881889\n",
      "          total_loss: -0.012674666568636894\n",
      "          vf_explained_var: 0.863280713558197\n",
      "          vf_loss: 0.08590751788417239\n",
      "    num_agent_steps_sampled: 709000\n",
      "    num_agent_steps_trained: 709000\n",
      "    num_steps_sampled: 709000\n",
      "    num_steps_trained: 709000\n",
      "  iterations_since_restore: 709\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.852564102564116\n",
      "    ram_util_percent: 34.55641025641025\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036986851534021155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.982642766858184\n",
      "    mean_inference_ms: 2.121887820653801\n",
      "    mean_raw_obs_processing_ms: 27.41709647054725\n",
      "  time_since_restore: 35386.14000940323\n",
      "  time_this_iter_s: 54.79370355606079\n",
      "  time_total_s: 35386.14000940323\n",
      "  timers:\n",
      "    learn_throughput: 1328.819\n",
      "    learn_time_ms: 752.548\n",
      "    load_throughput: 40758.754\n",
      "    load_time_ms: 24.535\n",
      "    sample_throughput: 10.86\n",
      "    sample_time_ms: 92083.054\n",
      "    update_time_ms: 4.021\n",
      "  timestamp: 1635317972\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 709000\n",
      "  training_iteration: 709\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   709</td><td style=\"text-align: right;\">         35386.1</td><td style=\"text-align: right;\">709000</td><td style=\"text-align: right;\">  2.8038</td><td style=\"text-align: right;\">                9.84</td><td style=\"text-align: right;\">               -9.44</td><td style=\"text-align: right;\">            185.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 710000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_06-59-54\n",
      "  done: false\n",
      "  episode_len_mean: 192.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.84\n",
      "  episode_reward_mean: 2.605000000000016\n",
      "  episode_reward_min: -9.439999999999898\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2634\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4063048390078038\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8573611153496636\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016802576038401934\n",
      "          policy_loss: 0.007771159294578764\n",
      "          total_loss: 0.6145131144258711\n",
      "          vf_explained_var: 0.3493298292160034\n",
      "          vf_loss: 0.6184885970400905\n",
      "    num_agent_steps_sampled: 710000\n",
      "    num_agent_steps_trained: 710000\n",
      "    num_steps_sampled: 710000\n",
      "    num_steps_trained: 710000\n",
      "  iterations_since_restore: 710\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.29354838709679\n",
      "    ram_util_percent: 34.6\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036987383443829146\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.982365804122125\n",
      "    mean_inference_ms: 2.121902335188453\n",
      "    mean_raw_obs_processing_ms: 27.438709867921045\n",
      "  time_since_restore: 35407.54964399338\n",
      "  time_this_iter_s: 21.409634590148926\n",
      "  time_total_s: 35407.54964399338\n",
      "  timers:\n",
      "    learn_throughput: 1327.846\n",
      "    learn_time_ms: 753.1\n",
      "    load_throughput: 40959.6\n",
      "    load_time_ms: 24.414\n",
      "    sample_throughput: 11.095\n",
      "    sample_time_ms: 90134.45\n",
      "    update_time_ms: 4.097\n",
      "  timestamp: 1635317994\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 710000\n",
      "  training_iteration: 710\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   710</td><td style=\"text-align: right;\">         35407.5</td><td style=\"text-align: right;\">710000</td><td style=\"text-align: right;\">   2.605</td><td style=\"text-align: right;\">                9.84</td><td style=\"text-align: right;\">               -9.44</td><td style=\"text-align: right;\">            192.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 711000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-01-28\n",
      "  done: false\n",
      "  episode_len_mean: 182.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.84\n",
      "  episode_reward_mean: 2.765400000000015\n",
      "  episode_reward_min: -9.439999999999898\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 2639\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4063048390078038\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9934691058264837\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011385392406667734\n",
      "          policy_loss: -0.0344393152743578\n",
      "          total_loss: 0.17729424074706104\n",
      "          vf_explained_var: 0.2460564523935318\n",
      "          vf_loss: 0.22704230188392102\n",
      "    num_agent_steps_sampled: 711000\n",
      "    num_agent_steps_trained: 711000\n",
      "    num_steps_sampled: 711000\n",
      "    num_steps_trained: 711000\n",
      "  iterations_since_restore: 711\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.00592592592592\n",
      "    ram_util_percent: 34.58518518518518\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0369886844426258\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.981899894633983\n",
      "    mean_inference_ms: 2.1219370643627187\n",
      "    mean_raw_obs_processing_ms: 27.49408003697279\n",
      "  time_since_restore: 35502.17898988724\n",
      "  time_this_iter_s: 94.62934589385986\n",
      "  time_total_s: 35502.17898988724\n",
      "  timers:\n",
      "    learn_throughput: 1329.585\n",
      "    learn_time_ms: 752.115\n",
      "    load_throughput: 40555.044\n",
      "    load_time_ms: 24.658\n",
      "    sample_throughput: 11.123\n",
      "    sample_time_ms: 89902.372\n",
      "    update_time_ms: 4.105\n",
      "  timestamp: 1635318088\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 711000\n",
      "  training_iteration: 711\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   711</td><td style=\"text-align: right;\">         35502.2</td><td style=\"text-align: right;\">711000</td><td style=\"text-align: right;\">  2.7654</td><td style=\"text-align: right;\">                9.84</td><td style=\"text-align: right;\">               -9.44</td><td style=\"text-align: right;\">            182.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 712000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-02-29\n",
      "  done: false\n",
      "  episode_len_mean: 193.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.84\n",
      "  episode_reward_mean: 2.581700000000016\n",
      "  episode_reward_min: -9.439999999999898\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2643\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4063048390078038\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8787417703204685\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008194442705258058\n",
      "          policy_loss: -0.03278174367215898\n",
      "          total_loss: 0.2128481738269329\n",
      "          vf_explained_var: 0.6141959428787231\n",
      "          vf_loss: 0.26108789008317723\n",
      "    num_agent_steps_sampled: 712000\n",
      "    num_agent_steps_trained: 712000\n",
      "    num_steps_sampled: 712000\n",
      "    num_steps_trained: 712000\n",
      "  iterations_since_restore: 712\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.839080459770116\n",
      "    ram_util_percent: 34.53793103448275\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036989734131793045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.98170685447966\n",
      "    mean_inference_ms: 2.12196474126271\n",
      "    mean_raw_obs_processing_ms: 27.535039953564862\n",
      "  time_since_restore: 35563.07932305336\n",
      "  time_this_iter_s: 60.90033316612244\n",
      "  time_total_s: 35563.07932305336\n",
      "  timers:\n",
      "    learn_throughput: 1329.24\n",
      "    learn_time_ms: 752.31\n",
      "    load_throughput: 40446.831\n",
      "    load_time_ms: 24.724\n",
      "    sample_throughput: 11.811\n",
      "    sample_time_ms: 84669.822\n",
      "    update_time_ms: 4.098\n",
      "  timestamp: 1635318149\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 712000\n",
      "  training_iteration: 712\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   712</td><td style=\"text-align: right;\">         35563.1</td><td style=\"text-align: right;\">712000</td><td style=\"text-align: right;\">  2.5817</td><td style=\"text-align: right;\">                9.84</td><td style=\"text-align: right;\">               -9.44</td><td style=\"text-align: right;\">            193.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 713000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-04-22\n",
      "  done: false\n",
      "  episode_len_mean: 186.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.84\n",
      "  episode_reward_mean: 2.722500000000016\n",
      "  episode_reward_min: -9.439999999999898\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2650\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4063048390078038\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9181078208817377\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012518095258913349\n",
      "          policy_loss: -0.013826393998331493\n",
      "          total_loss: 0.33710721714629066\n",
      "          vf_explained_var: 0.1785270720720291\n",
      "          vf_loss: 0.365028523405393\n",
      "    num_agent_steps_sampled: 713000\n",
      "    num_agent_steps_trained: 713000\n",
      "    num_steps_sampled: 713000\n",
      "    num_steps_trained: 713000\n",
      "  iterations_since_restore: 713\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.53125\n",
      "    ram_util_percent: 34.66499999999999\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0369915042128277\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.981122783716884\n",
      "    mean_inference_ms: 2.1220127473921218\n",
      "    mean_raw_obs_processing_ms: 27.611078335543795\n",
      "  time_since_restore: 35675.597645282745\n",
      "  time_this_iter_s: 112.51832222938538\n",
      "  time_total_s: 35675.597645282745\n",
      "  timers:\n",
      "    learn_throughput: 1329.576\n",
      "    learn_time_ms: 752.119\n",
      "    load_throughput: 40488.491\n",
      "    load_time_ms: 24.698\n",
      "    sample_throughput: 10.654\n",
      "    sample_time_ms: 93864.372\n",
      "    update_time_ms: 4.007\n",
      "  timestamp: 1635318262\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 713000\n",
      "  training_iteration: 713\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   713</td><td style=\"text-align: right;\">         35675.6</td><td style=\"text-align: right;\">713000</td><td style=\"text-align: right;\">  2.7225</td><td style=\"text-align: right;\">                9.84</td><td style=\"text-align: right;\">               -9.44</td><td style=\"text-align: right;\">            186.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 714000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-08-44\n",
      "  done: false\n",
      "  episode_len_mean: 166.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.82\n",
      "  episode_reward_mean: 3.1399000000000132\n",
      "  episode_reward_min: -9.439999999999898\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 2664\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4063048390078038\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7984726044866775\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012442108640800214\n",
      "          policy_loss: -0.11583531267113156\n",
      "          total_loss: 0.0896812666621473\n",
      "          vf_explained_var: 0.7594797015190125\n",
      "          vf_loss: 0.21844601237939465\n",
      "    num_agent_steps_sampled: 714000\n",
      "    num_agent_steps_trained: 714000\n",
      "    num_steps_sampled: 714000\n",
      "    num_steps_trained: 714000\n",
      "  iterations_since_restore: 714\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.40454545454546\n",
      "    ram_util_percent: 34.65187165775401\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03699486732464696\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.979443918126925\n",
      "    mean_inference_ms: 2.122104321159773\n",
      "    mean_raw_obs_processing_ms: 27.786360244245426\n",
      "  time_since_restore: 35937.260459661484\n",
      "  time_this_iter_s: 261.6628143787384\n",
      "  time_total_s: 35937.260459661484\n",
      "  timers:\n",
      "    learn_throughput: 1329.026\n",
      "    learn_time_ms: 752.431\n",
      "    load_throughput: 40471.614\n",
      "    load_time_ms: 24.709\n",
      "    sample_throughput: 10.326\n",
      "    sample_time_ms: 96846.843\n",
      "    update_time_ms: 3.783\n",
      "  timestamp: 1635318524\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 714000\n",
      "  training_iteration: 714\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   714</td><td style=\"text-align: right;\">         35937.3</td><td style=\"text-align: right;\">714000</td><td style=\"text-align: right;\">  3.1399</td><td style=\"text-align: right;\">                9.82</td><td style=\"text-align: right;\">               -9.44</td><td style=\"text-align: right;\">            166.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 715000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-10-38\n",
      "  done: false\n",
      "  episode_len_mean: 170.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.850000000000001\n",
      "  episode_reward_mean: 3.105900000000014\n",
      "  episode_reward_min: -9.439999999999898\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2671\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4063048390078038\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.179816256629096\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011597126269801184\n",
      "          policy_loss: 0.03521317119399706\n",
      "          total_loss: 0.08817293850911988\n",
      "          vf_explained_var: 0.3897024691104889\n",
      "          vf_loss: 0.07004595741681341\n",
      "    num_agent_steps_sampled: 715000\n",
      "    num_agent_steps_trained: 715000\n",
      "    num_steps_sampled: 715000\n",
      "    num_steps_trained: 715000\n",
      "  iterations_since_restore: 715\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.807975460122705\n",
      "    ram_util_percent: 34.680981595092014\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0369964762300607\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.979007142613803\n",
      "    mean_inference_ms: 2.1221482602760107\n",
      "    mean_raw_obs_processing_ms: 27.870216523459828\n",
      "  time_since_restore: 36051.77341890335\n",
      "  time_this_iter_s: 114.51295924186707\n",
      "  time_total_s: 36051.77341890335\n",
      "  timers:\n",
      "    learn_throughput: 1329.545\n",
      "    learn_time_ms: 752.137\n",
      "    load_throughput: 40575.405\n",
      "    load_time_ms: 24.645\n",
      "    sample_throughput: 9.767\n",
      "    sample_time_ms: 102385.697\n",
      "    update_time_ms: 3.79\n",
      "  timestamp: 1635318638\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 715000\n",
      "  training_iteration: 715\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   715</td><td style=\"text-align: right;\">         36051.8</td><td style=\"text-align: right;\">715000</td><td style=\"text-align: right;\">  3.1059</td><td style=\"text-align: right;\">                9.85</td><td style=\"text-align: right;\">               -9.44</td><td style=\"text-align: right;\">            170.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 716000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-11-53\n",
      "  done: false\n",
      "  episode_len_mean: 175.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.850000000000001\n",
      "  episode_reward_mean: 3.0320000000000142\n",
      "  episode_reward_min: -9.439999999999898\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2675\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4063048390078038\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.058385287390815\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015559234362657924\n",
      "          policy_loss: -0.09076774418354035\n",
      "          total_loss: 0.23373633246454928\n",
      "          vf_explained_var: 0.39792415499687195\n",
      "          vf_loss: 0.33876613988929116\n",
      "    num_agent_steps_sampled: 716000\n",
      "    num_agent_steps_trained: 716000\n",
      "    num_steps_sampled: 716000\n",
      "    num_steps_trained: 716000\n",
      "  iterations_since_restore: 716\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.34485981308412\n",
      "    ram_util_percent: 34.728037383177565\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03699742105969629\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.97864028334199\n",
      "    mean_inference_ms: 2.122174042827984\n",
      "    mean_raw_obs_processing_ms: 27.917654813169843\n",
      "  time_since_restore: 36126.635254621506\n",
      "  time_this_iter_s: 74.86183571815491\n",
      "  time_total_s: 36126.635254621506\n",
      "  timers:\n",
      "    learn_throughput: 1329.786\n",
      "    learn_time_ms: 752.001\n",
      "    load_throughput: 40556.024\n",
      "    load_time_ms: 24.657\n",
      "    sample_throughput: 11.001\n",
      "    sample_time_ms: 90897.218\n",
      "    update_time_ms: 3.783\n",
      "  timestamp: 1635318713\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 716000\n",
      "  training_iteration: 716\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   716</td><td style=\"text-align: right;\">         36126.6</td><td style=\"text-align: right;\">716000</td><td style=\"text-align: right;\">   3.032</td><td style=\"text-align: right;\">                9.85</td><td style=\"text-align: right;\">               -9.44</td><td style=\"text-align: right;\">            175.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 717000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-12-13\n",
      "  done: false\n",
      "  episode_len_mean: 181.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.850000000000001\n",
      "  episode_reward_mean: 2.9266000000000143\n",
      "  episode_reward_min: -9.439999999999898\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2677\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4063048390078038\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.139799486266242\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007120471064441889\n",
      "          policy_loss: -0.04696333466304673\n",
      "          total_loss: 0.02492158862037791\n",
      "          vf_explained_var: 0.6038731932640076\n",
      "          vf_loss: 0.09038984018067518\n",
      "    num_agent_steps_sampled: 717000\n",
      "    num_agent_steps_trained: 717000\n",
      "    num_steps_sampled: 717000\n",
      "    num_steps_trained: 717000\n",
      "  iterations_since_restore: 717\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.59655172413794\n",
      "    ram_util_percent: 34.80344827586206\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036997915833301975\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.978399099232302\n",
      "    mean_inference_ms: 2.1221875481083416\n",
      "    mean_raw_obs_processing_ms: 27.940257811352776\n",
      "  time_since_restore: 36146.734166145325\n",
      "  time_this_iter_s: 20.09891152381897\n",
      "  time_total_s: 36146.734166145325\n",
      "  timers:\n",
      "    learn_throughput: 1329.474\n",
      "    learn_time_ms: 752.177\n",
      "    load_throughput: 40765.528\n",
      "    load_time_ms: 24.531\n",
      "    sample_throughput: 11.789\n",
      "    sample_time_ms: 84825.513\n",
      "    update_time_ms: 3.774\n",
      "  timestamp: 1635318733\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 717000\n",
      "  training_iteration: 717\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   717</td><td style=\"text-align: right;\">         36146.7</td><td style=\"text-align: right;\">717000</td><td style=\"text-align: right;\">  2.9266</td><td style=\"text-align: right;\">                9.85</td><td style=\"text-align: right;\">               -9.44</td><td style=\"text-align: right;\">            181.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 718000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-14-04\n",
      "  done: false\n",
      "  episode_len_mean: 170.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 3.241000000000014\n",
      "  episode_reward_min: -6.959999999999947\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2684\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4063048390078038\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1500759959220885\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006215196098207541\n",
      "          policy_loss: 0.04995158513387044\n",
      "          total_loss: 0.07335389355818431\n",
      "          vf_explained_var: 0.6562740802764893\n",
      "          vf_loss: 0.042377802733002075\n",
      "    num_agent_steps_sampled: 718000\n",
      "    num_agent_steps_trained: 718000\n",
      "    num_steps_sampled: 718000\n",
      "    num_steps_trained: 718000\n",
      "  iterations_since_restore: 718\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.30253164556962\n",
      "    ram_util_percent: 34.71012658227848\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03699959895006083\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.97733134865959\n",
      "    mean_inference_ms: 2.1222323869061106\n",
      "    mean_raw_obs_processing_ms: 28.023611531941583\n",
      "  time_since_restore: 36257.64346194267\n",
      "  time_this_iter_s: 110.90929579734802\n",
      "  time_total_s: 36257.64346194267\n",
      "  timers:\n",
      "    learn_throughput: 1328.389\n",
      "    learn_time_ms: 752.792\n",
      "    load_throughput: 40920.279\n",
      "    load_time_ms: 24.438\n",
      "    sample_throughput: 10.888\n",
      "    sample_time_ms: 91844.523\n",
      "    update_time_ms: 3.686\n",
      "  timestamp: 1635318844\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 718000\n",
      "  training_iteration: 718\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   718</td><td style=\"text-align: right;\">         36257.6</td><td style=\"text-align: right;\">718000</td><td style=\"text-align: right;\">   3.241</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            170.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 719000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-14-24\n",
      "  done: false\n",
      "  episode_len_mean: 181.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 3.0370000000000146\n",
      "  episode_reward_min: -6.959999999999947\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2686\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4063048390078038\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1947734938727486\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007454553215171522\n",
      "          policy_loss: -0.01265585840576225\n",
      "          total_loss: 0.08251333816183938\n",
      "          vf_explained_var: 0.0809330865740776\n",
      "          vf_loss: 0.11408811169159082\n",
      "    num_agent_steps_sampled: 719000\n",
      "    num_agent_steps_trained: 719000\n",
      "    num_steps_sampled: 719000\n",
      "    num_steps_trained: 719000\n",
      "  iterations_since_restore: 719\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.50714285714285\n",
      "    ram_util_percent: 34.739285714285714\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03700009707538139\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.976946022319268\n",
      "    mean_inference_ms: 2.1222454704578007\n",
      "    mean_raw_obs_processing_ms: 28.046063210396532\n",
      "  time_since_restore: 36277.19047522545\n",
      "  time_this_iter_s: 19.54701328277588\n",
      "  time_total_s: 36277.19047522545\n",
      "  timers:\n",
      "    learn_throughput: 1328.731\n",
      "    learn_time_ms: 752.598\n",
      "    load_throughput: 40609.741\n",
      "    load_time_ms: 24.625\n",
      "    sample_throughput: 11.322\n",
      "    sample_time_ms: 88319.848\n",
      "    update_time_ms: 3.688\n",
      "  timestamp: 1635318864\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 719000\n",
      "  training_iteration: 719\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   719</td><td style=\"text-align: right;\">         36277.2</td><td style=\"text-align: right;\">719000</td><td style=\"text-align: right;\">   3.037</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            181.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 720000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-14-43\n",
      "  done: false\n",
      "  episode_len_mean: 184.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.9856000000000154\n",
      "  episode_reward_min: -6.959999999999947\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2687\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4063048390078038\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0824122813012864\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012090687474139215\n",
      "          policy_loss: -0.0077882569283247\n",
      "          total_loss: 0.23963862732052804\n",
      "          vf_explained_var: 0.4019591808319092\n",
      "          vf_loss: 0.26333850373565737\n",
      "    num_agent_steps_sampled: 720000\n",
      "    num_agent_steps_trained: 720000\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "  iterations_since_restore: 720\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.95555555555556\n",
      "    ram_util_percent: 34.75185185185185\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03700034815103527\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.976722736112038\n",
      "    mean_inference_ms: 2.1222520056791017\n",
      "    mean_raw_obs_processing_ms: 28.05597555184572\n",
      "  time_since_restore: 36296.377984285355\n",
      "  time_this_iter_s: 19.187509059906006\n",
      "  time_total_s: 36296.377984285355\n",
      "  timers:\n",
      "    learn_throughput: 1330.699\n",
      "    learn_time_ms: 751.485\n",
      "    load_throughput: 40415.068\n",
      "    load_time_ms: 24.743\n",
      "    sample_throughput: 11.351\n",
      "    sample_time_ms: 88098.716\n",
      "    update_time_ms: 3.61\n",
      "  timestamp: 1635318883\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 720\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   720</td><td style=\"text-align: right;\">         36296.4</td><td style=\"text-align: right;\">720000</td><td style=\"text-align: right;\">  2.9856</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            184.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 721000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-15-01\n",
      "  done: false\n",
      "  episode_len_mean: 189.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.844100000000017\n",
      "  episode_reward_min: -6.959999999999947\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2689\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4063048390078038\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0588588370217216\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013989775468915416\n",
      "          policy_loss: 0.013371829522980584\n",
      "          total_loss: 0.16438154987990855\n",
      "          vf_explained_var: 0.5487565994262695\n",
      "          vf_loss: 0.16591419581737782\n",
      "    num_agent_steps_sampled: 721000\n",
      "    num_agent_steps_trained: 721000\n",
      "    num_steps_sampled: 721000\n",
      "    num_steps_trained: 721000\n",
      "  iterations_since_restore: 721\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.52307692307693\n",
      "    ram_util_percent: 34.707692307692305\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03700087701071879\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.976166234447245\n",
      "    mean_inference_ms: 2.122265792968189\n",
      "    mean_raw_obs_processing_ms: 28.07501688926652\n",
      "  time_since_restore: 36314.6010158062\n",
      "  time_this_iter_s: 18.223031520843506\n",
      "  time_total_s: 36314.6010158062\n",
      "  timers:\n",
      "    learn_throughput: 1329.764\n",
      "    learn_time_ms: 752.013\n",
      "    load_throughput: 40798.797\n",
      "    load_time_ms: 24.511\n",
      "    sample_throughput: 12.429\n",
      "    sample_time_ms: 80457.788\n",
      "    update_time_ms: 3.596\n",
      "  timestamp: 1635318901\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 721000\n",
      "  training_iteration: 721\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   721</td><td style=\"text-align: right;\">         36314.6</td><td style=\"text-align: right;\">721000</td><td style=\"text-align: right;\">  2.8441</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            189.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 722000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-16-51\n",
      "  done: false\n",
      "  episode_len_mean: 188.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.9050000000000167\n",
      "  episode_reward_min: -6.959999999999947\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2696\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4063048390078038\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.145207138856252\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006522062470496327\n",
      "          policy_loss: 0.19901537067360348\n",
      "          total_loss: 0.23075979070530997\n",
      "          vf_explained_var: 0.5727096796035767\n",
      "          vf_loss: 0.050546546984050006\n",
      "    num_agent_steps_sampled: 722000\n",
      "    num_agent_steps_trained: 722000\n",
      "    num_steps_sampled: 722000\n",
      "    num_steps_trained: 722000\n",
      "  iterations_since_restore: 722\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.91528662420382\n",
      "    ram_util_percent: 34.70573248407644\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037002774178696\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.973711150169752\n",
      "    mean_inference_ms: 2.1223141785709614\n",
      "    mean_raw_obs_processing_ms: 28.143994160520585\n",
      "  time_since_restore: 36424.55117559433\n",
      "  time_this_iter_s: 109.95015978813171\n",
      "  time_total_s: 36424.55117559433\n",
      "  timers:\n",
      "    learn_throughput: 1327.929\n",
      "    learn_time_ms: 753.052\n",
      "    load_throughput: 41092.668\n",
      "    load_time_ms: 24.335\n",
      "    sample_throughput: 11.715\n",
      "    sample_time_ms: 85361.888\n",
      "    update_time_ms: 3.596\n",
      "  timestamp: 1635319011\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 722000\n",
      "  training_iteration: 722\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   722</td><td style=\"text-align: right;\">         36424.6</td><td style=\"text-align: right;\">722000</td><td style=\"text-align: right;\">   2.905</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            188.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 723000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-17-34\n",
      "  done: false\n",
      "  episode_len_mean: 193.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.7650000000000183\n",
      "  episode_reward_min: -6.959999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 2699\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4063048390078038\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.810859089427524\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009608062201040192\n",
      "          policy_loss: -0.20424191719955867\n",
      "          total_loss: -0.0016555264592170715\n",
      "          vf_explained_var: 0.6615210771560669\n",
      "          vf_loss: 0.21679118434484634\n",
      "    num_agent_steps_sampled: 723000\n",
      "    num_agent_steps_trained: 723000\n",
      "    num_steps_sampled: 723000\n",
      "    num_steps_trained: 723000\n",
      "  iterations_since_restore: 723\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.29838709677419\n",
      "    ram_util_percent: 34.78387096774193\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037003584404226485\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.972635719383646\n",
      "    mean_inference_ms: 2.122334528595504\n",
      "    mean_raw_obs_processing_ms: 28.168423170186927\n",
      "  time_since_restore: 36467.60573720932\n",
      "  time_this_iter_s: 43.054561614990234\n",
      "  time_total_s: 36467.60573720932\n",
      "  timers:\n",
      "    learn_throughput: 1327.549\n",
      "    learn_time_ms: 753.268\n",
      "    load_throughput: 41034.857\n",
      "    load_time_ms: 24.37\n",
      "    sample_throughput: 12.753\n",
      "    sample_time_ms: 78415.358\n",
      "    update_time_ms: 3.602\n",
      "  timestamp: 1635319054\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 723000\n",
      "  training_iteration: 723\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   723</td><td style=\"text-align: right;\">         36467.6</td><td style=\"text-align: right;\">723000</td><td style=\"text-align: right;\">   2.765</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -6.96</td><td style=\"text-align: right;\">            193.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 724000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-17-52\n",
      "  done: false\n",
      "  episode_len_mean: 198.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.6377000000000193\n",
      "  episode_reward_min: -7.239999999999912\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2700\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4063048390078038\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0522508475515577\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.022714232490879956\n",
      "          policy_loss: -0.011901494943433337\n",
      "          total_loss: 0.28174048070278435\n",
      "          vf_explained_var: 0.4891766905784607\n",
      "          vf_loss: 0.3049355829755465\n",
      "    num_agent_steps_sampled: 724000\n",
      "    num_agent_steps_trained: 724000\n",
      "    num_steps_sampled: 724000\n",
      "    num_steps_trained: 724000\n",
      "  iterations_since_restore: 724\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.04615384615384\n",
      "    ram_util_percent: 34.79230769230769\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037003868464661825\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.972224095079724\n",
      "    mean_inference_ms: 2.1223416264560573\n",
      "    mean_raw_obs_processing_ms: 28.17617500943956\n",
      "  time_since_restore: 36485.884924411774\n",
      "  time_this_iter_s: 18.279187202453613\n",
      "  time_total_s: 36485.884924411774\n",
      "  timers:\n",
      "    learn_throughput: 1326.817\n",
      "    learn_time_ms: 753.683\n",
      "    load_throughput: 41175.528\n",
      "    load_time_ms: 24.286\n",
      "    sample_throughput: 18.492\n",
      "    sample_time_ms: 54076.763\n",
      "    update_time_ms: 3.527\n",
      "  timestamp: 1635319072\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 724000\n",
      "  training_iteration: 724\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   724</td><td style=\"text-align: right;\">         36485.9</td><td style=\"text-align: right;\">724000</td><td style=\"text-align: right;\">  2.6377</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -7.24</td><td style=\"text-align: right;\">            198.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 725000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-20-20\n",
      "  done: false\n",
      "  episode_len_mean: 203.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.4894000000000203\n",
      "  episode_reward_min: -7.239999999999912\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2709\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6094572585117055\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1503883547253078\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010081878347757142\n",
      "          policy_loss: 0.1592652339902189\n",
      "          total_loss: 0.2653091029988395\n",
      "          vf_explained_var: 0.7540789842605591\n",
      "          vf_loss: 0.12140327787233723\n",
      "    num_agent_steps_sampled: 725000\n",
      "    num_agent_steps_trained: 725000\n",
      "    num_steps_sampled: 725000\n",
      "    num_steps_trained: 725000\n",
      "  iterations_since_restore: 725\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.59476190476191\n",
      "    ram_util_percent: 34.75\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03700652065072271\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.967975343650718\n",
      "    mean_inference_ms: 2.122408392468557\n",
      "    mean_raw_obs_processing_ms: 28.258293718790384\n",
      "  time_since_restore: 36633.565262794495\n",
      "  time_this_iter_s: 147.68033838272095\n",
      "  time_total_s: 36633.565262794495\n",
      "  timers:\n",
      "    learn_throughput: 1328.869\n",
      "    learn_time_ms: 752.52\n",
      "    load_throughput: 41322.459\n",
      "    load_time_ms: 24.2\n",
      "    sample_throughput: 17.423\n",
      "    sample_time_ms: 57394.765\n",
      "    update_time_ms: 3.521\n",
      "  timestamp: 1635319220\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 725000\n",
      "  training_iteration: 725\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   725</td><td style=\"text-align: right;\">         36633.6</td><td style=\"text-align: right;\">725000</td><td style=\"text-align: right;\">  2.4894</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -7.24</td><td style=\"text-align: right;\">            203.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 726000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-21-55\n",
      "  done: false\n",
      "  episode_len_mean: 204.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.42400000000002\n",
      "  episode_reward_min: -7.239999999999912\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 2714\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6094572585117055\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0564845111634997\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010903669723092611\n",
      "          policy_loss: -0.05735332284950548\n",
      "          total_loss: 0.18341700616810058\n",
      "          vf_explained_var: 0.7807391881942749\n",
      "          vf_loss: 0.25468985291404855\n",
      "    num_agent_steps_sampled: 726000\n",
      "    num_agent_steps_trained: 726000\n",
      "    num_steps_sampled: 726000\n",
      "    num_steps_trained: 726000\n",
      "  iterations_since_restore: 726\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.36617647058823\n",
      "    ram_util_percent: 34.749264705882354\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03700795121136082\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.965488599940883\n",
      "    mean_inference_ms: 2.122444347512349\n",
      "    mean_raw_obs_processing_ms: 28.29864151259111\n",
      "  time_since_restore: 36728.47700333595\n",
      "  time_this_iter_s: 94.91174054145813\n",
      "  time_total_s: 36728.47700333595\n",
      "  timers:\n",
      "    learn_throughput: 1330.807\n",
      "    learn_time_ms: 751.424\n",
      "    load_throughput: 41365.25\n",
      "    load_time_ms: 24.175\n",
      "    sample_throughput: 16.835\n",
      "    sample_time_ms: 59400.85\n",
      "    update_time_ms: 3.534\n",
      "  timestamp: 1635319315\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 726000\n",
      "  training_iteration: 726\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   726</td><td style=\"text-align: right;\">         36728.5</td><td style=\"text-align: right;\">726000</td><td style=\"text-align: right;\">   2.424</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -7.24</td><td style=\"text-align: right;\">            204.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 727000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-22-14\n",
      "  done: false\n",
      "  episode_len_mean: 216.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.1375000000000215\n",
      "  episode_reward_min: -14.6899999999999\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2716\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6094572585117055\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.054129378663169\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015149213767432742\n",
      "          policy_loss: -0.10249002522064579\n",
      "          total_loss: 0.14471476835509142\n",
      "          vf_explained_var: 0.524273693561554\n",
      "          vf_loss: 0.24851329144504336\n",
      "    num_agent_steps_sampled: 727000\n",
      "    num_agent_steps_trained: 727000\n",
      "    num_steps_sampled: 727000\n",
      "    num_steps_trained: 727000\n",
      "  iterations_since_restore: 727\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.8\n",
      "    ram_util_percent: 34.77407407407408\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037008558266321614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.964402514376722\n",
      "    mean_inference_ms: 2.1224595925560155\n",
      "    mean_raw_obs_processing_ms: 28.313200964683702\n",
      "  time_since_restore: 36747.204307079315\n",
      "  time_this_iter_s: 18.727303743362427\n",
      "  time_total_s: 36747.204307079315\n",
      "  timers:\n",
      "    learn_throughput: 1327.985\n",
      "    learn_time_ms: 753.02\n",
      "    load_throughput: 41462.9\n",
      "    load_time_ms: 24.118\n",
      "    sample_throughput: 16.874\n",
      "    sample_time_ms: 59262.102\n",
      "    update_time_ms: 3.534\n",
      "  timestamp: 1635319334\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 727000\n",
      "  training_iteration: 727\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   727</td><td style=\"text-align: right;\">         36747.2</td><td style=\"text-align: right;\">727000</td><td style=\"text-align: right;\">  2.1375</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">              -14.69</td><td style=\"text-align: right;\">            216.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 728000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-22-32\n",
      "  done: false\n",
      "  episode_len_mean: 227.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 1.9553000000000227\n",
      "  episode_reward_min: -14.6899999999999\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2718\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6094572585117055\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0961328744888306\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006986476958237967\n",
      "          policy_loss: -0.06829314322935211\n",
      "          total_loss: 0.041580950675739185\n",
      "          vf_explained_var: 0.462147980928421\n",
      "          vf_loss: 0.12657746368398268\n",
      "    num_agent_steps_sampled: 728000\n",
      "    num_agent_steps_trained: 728000\n",
      "    num_steps_sampled: 728000\n",
      "    num_steps_trained: 728000\n",
      "  iterations_since_restore: 728\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.83846153846153\n",
      "    ram_util_percent: 34.74230769230769\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03700919460363366\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.963205654462076\n",
      "    mean_inference_ms: 2.1224753791852886\n",
      "    mean_raw_obs_processing_ms: 28.326980240498898\n",
      "  time_since_restore: 36765.32555317879\n",
      "  time_this_iter_s: 18.121246099472046\n",
      "  time_total_s: 36765.32555317879\n",
      "  timers:\n",
      "    learn_throughput: 1329.369\n",
      "    learn_time_ms: 752.237\n",
      "    load_throughput: 41614.906\n",
      "    load_time_ms: 24.03\n",
      "    sample_throughput: 20.006\n",
      "    sample_time_ms: 49984.153\n",
      "    update_time_ms: 3.532\n",
      "  timestamp: 1635319352\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 728000\n",
      "  training_iteration: 728\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   728</td><td style=\"text-align: right;\">         36765.3</td><td style=\"text-align: right;\">728000</td><td style=\"text-align: right;\">  1.9553</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">              -14.69</td><td style=\"text-align: right;\">            227.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 729000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-24-06\n",
      "  done: false\n",
      "  episode_len_mean: 219.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 1.982500000000022\n",
      "  episode_reward_min: -14.6899999999999\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 2723\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6094572585117055\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8294503331184386\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0395376857755606\n",
      "          policy_loss: 0.009085222168101205\n",
      "          total_loss: 0.5538960584335857\n",
      "          vf_explained_var: 0.6538557410240173\n",
      "          vf_loss: 0.5390088039967749\n",
      "    num_agent_steps_sampled: 729000\n",
      "    num_agent_steps_trained: 729000\n",
      "    num_steps_sampled: 729000\n",
      "    num_steps_trained: 729000\n",
      "  iterations_since_restore: 729\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.28134328358209\n",
      "    ram_util_percent: 34.70223880597015\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701082561269067\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.96000941503497\n",
      "    mean_inference_ms: 2.1225154689506236\n",
      "    mean_raw_obs_processing_ms: 28.363728939407565\n",
      "  time_since_restore: 36859.15738248825\n",
      "  time_this_iter_s: 93.8318293094635\n",
      "  time_total_s: 36859.15738248825\n",
      "  timers:\n",
      "    learn_throughput: 1329.216\n",
      "    learn_time_ms: 752.323\n",
      "    load_throughput: 42154.36\n",
      "    load_time_ms: 23.722\n",
      "    sample_throughput: 17.418\n",
      "    sample_time_ms: 57412.84\n",
      "    update_time_ms: 3.541\n",
      "  timestamp: 1635319446\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 729000\n",
      "  training_iteration: 729\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   729</td><td style=\"text-align: right;\">         36859.2</td><td style=\"text-align: right;\">729000</td><td style=\"text-align: right;\">  1.9825</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">              -14.69</td><td style=\"text-align: right;\">            219.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 730000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-26-58\n",
      "  done: false\n",
      "  episode_len_mean: 205.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.1854000000000213\n",
      "  episode_reward_min: -14.6899999999999\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2733\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9141858877675587\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1080799327956306\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009381430031458965\n",
      "          policy_loss: 0.08796524107456208\n",
      "          total_loss: 0.20447831758194498\n",
      "          vf_explained_var: 0.8600515723228455\n",
      "          vf_loss: 0.12901750243165427\n",
      "    num_agent_steps_sampled: 730000\n",
      "    num_agent_steps_trained: 730000\n",
      "    num_steps_sampled: 730000\n",
      "    num_steps_trained: 730000\n",
      "  iterations_since_restore: 730\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.673469387755105\n",
      "    ram_util_percent: 34.739999999999995\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701396136783885\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.953905446291675\n",
      "    mean_inference_ms: 2.122592689370623\n",
      "    mean_raw_obs_processing_ms: 28.45191469635004\n",
      "  time_since_restore: 37031.15851306915\n",
      "  time_this_iter_s: 172.0011305809021\n",
      "  time_total_s: 37031.15851306915\n",
      "  timers:\n",
      "    learn_throughput: 1328.15\n",
      "    learn_time_ms: 752.927\n",
      "    load_throughput: 42251.348\n",
      "    load_time_ms: 23.668\n",
      "    sample_throughput: 13.756\n",
      "    sample_time_ms: 72693.627\n",
      "    update_time_ms: 3.552\n",
      "  timestamp: 1635319618\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 730000\n",
      "  training_iteration: 730\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   730</td><td style=\"text-align: right;\">         37031.2</td><td style=\"text-align: right;\">730000</td><td style=\"text-align: right;\">  2.1854</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">              -14.69</td><td style=\"text-align: right;\">            205.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 731000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-27-55\n",
      "  done: false\n",
      "  episode_len_mean: 205.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.246900000000021\n",
      "  episode_reward_min: -14.6899999999999\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2737\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9141858877675587\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1533545838461983\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009345377839099249\n",
      "          policy_loss: 0.05358679696089692\n",
      "          total_loss: 0.16919915891355938\n",
      "          vf_explained_var: 0.6090323328971863\n",
      "          vf_loss: 0.12860249512725405\n",
      "    num_agent_steps_sampled: 731000\n",
      "    num_agent_steps_trained: 731000\n",
      "    num_steps_sampled: 731000\n",
      "    num_steps_trained: 731000\n",
      "  iterations_since_restore: 731\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.203658536585365\n",
      "    ram_util_percent: 34.774390243902445\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701514834096144\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.95158463587085\n",
      "    mean_inference_ms: 2.122621698799091\n",
      "    mean_raw_obs_processing_ms: 28.48701637452133\n",
      "  time_since_restore: 37088.68912816048\n",
      "  time_this_iter_s: 57.53061509132385\n",
      "  time_total_s: 37088.68912816048\n",
      "  timers:\n",
      "    learn_throughput: 1329.383\n",
      "    learn_time_ms: 752.229\n",
      "    load_throughput: 41859.572\n",
      "    load_time_ms: 23.889\n",
      "    sample_throughput: 13.051\n",
      "    sample_time_ms: 76624.842\n",
      "    update_time_ms: 3.566\n",
      "  timestamp: 1635319675\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 731000\n",
      "  training_iteration: 731\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   731</td><td style=\"text-align: right;\">         37088.7</td><td style=\"text-align: right;\">731000</td><td style=\"text-align: right;\">  2.2469</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">              -14.69</td><td style=\"text-align: right;\">            205.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 732000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-29-49\n",
      "  done: false\n",
      "  episode_len_mean: 199.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.4418000000000193\n",
      "  episode_reward_min: -14.6899999999999\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 2743\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9141858877675587\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7579672469033136\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014071166289565657\n",
      "          policy_loss: -0.03140449523925781\n",
      "          total_loss: 0.30538491333524387\n",
      "          vf_explained_var: 0.7761499285697937\n",
      "          vf_loss: 0.34150542285707264\n",
      "    num_agent_steps_sampled: 732000\n",
      "    num_agent_steps_trained: 732000\n",
      "    num_steps_sampled: 732000\n",
      "    num_steps_trained: 732000\n",
      "  iterations_since_restore: 732\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.05185185185185\n",
      "    ram_util_percent: 34.77098765432098\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701695966436159\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.94805264344624\n",
      "    mean_inference_ms: 2.1226664597019296\n",
      "    mean_raw_obs_processing_ms: 28.54327761818896\n",
      "  time_since_restore: 37201.89300608635\n",
      "  time_this_iter_s: 113.2038779258728\n",
      "  time_total_s: 37201.89300608635\n",
      "  timers:\n",
      "    learn_throughput: 1329.702\n",
      "    learn_time_ms: 752.048\n",
      "    load_throughput: 41974.227\n",
      "    load_time_ms: 23.824\n",
      "    sample_throughput: 12.995\n",
      "    sample_time_ms: 76950.469\n",
      "    update_time_ms: 3.57\n",
      "  timestamp: 1635319789\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 732000\n",
      "  training_iteration: 732\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   732</td><td style=\"text-align: right;\">         37201.9</td><td style=\"text-align: right;\">732000</td><td style=\"text-align: right;\">  2.4418</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">              -14.69</td><td style=\"text-align: right;\">            199.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 733000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-31-07\n",
      "  done: false\n",
      "  episode_len_mean: 205.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.31340000000002\n",
      "  episode_reward_min: -14.6899999999999\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 2748\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9141858877675587\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.968635728624132\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016693687215163584\n",
      "          policy_loss: 0.005043549877074029\n",
      "          total_loss: 0.6485633505715265\n",
      "          vf_explained_var: 0.5911524295806885\n",
      "          vf_loss: 0.6479450298680199\n",
      "    num_agent_steps_sampled: 733000\n",
      "    num_agent_steps_trained: 733000\n",
      "    num_steps_sampled: 733000\n",
      "    num_steps_trained: 733000\n",
      "  iterations_since_restore: 733\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.57747747747748\n",
      "    ram_util_percent: 34.80450450450452\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037018468290103385\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.94531775191234\n",
      "    mean_inference_ms: 2.1227037150711405\n",
      "    mean_raw_obs_processing_ms: 28.5871829004304\n",
      "  time_since_restore: 37280.17918610573\n",
      "  time_this_iter_s: 78.28618001937866\n",
      "  time_total_s: 37280.17918610573\n",
      "  timers:\n",
      "    learn_throughput: 1331.683\n",
      "    learn_time_ms: 750.93\n",
      "    load_throughput: 42256.328\n",
      "    load_time_ms: 23.665\n",
      "    sample_throughput: 12.426\n",
      "    sample_time_ms: 80474.901\n",
      "    update_time_ms: 3.563\n",
      "  timestamp: 1635319867\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 733000\n",
      "  training_iteration: 733\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   733</td><td style=\"text-align: right;\">         37280.2</td><td style=\"text-align: right;\">733000</td><td style=\"text-align: right;\">  2.3134</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">              -14.69</td><td style=\"text-align: right;\">            205.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 734000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-33-21\n",
      "  done: false\n",
      "  episode_len_mean: 202.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.4724000000000195\n",
      "  episode_reward_min: -14.6899999999999\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2756\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9141858877675587\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9440480788548788\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015688442157895822\n",
      "          policy_loss: -0.13469574517673916\n",
      "          total_loss: 0.12121579895416895\n",
      "          vf_explained_var: 0.651210606098175\n",
      "          vf_loss: 0.2610098714629809\n",
      "    num_agent_steps_sampled: 734000\n",
      "    num_agent_steps_trained: 734000\n",
      "    num_steps_sampled: 734000\n",
      "    num_steps_trained: 734000\n",
      "  iterations_since_restore: 734\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.30260416666666\n",
      "    ram_util_percent: 34.72083333333333\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702092134940651\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.94142596605772\n",
      "    mean_inference_ms: 2.1227652171239884\n",
      "    mean_raw_obs_processing_ms: 28.64823596928054\n",
      "  time_since_restore: 37414.29860186577\n",
      "  time_this_iter_s: 134.11941576004028\n",
      "  time_total_s: 37414.29860186577\n",
      "  timers:\n",
      "    learn_throughput: 1330.883\n",
      "    learn_time_ms: 751.381\n",
      "    load_throughput: 42337.37\n",
      "    load_time_ms: 23.62\n",
      "    sample_throughput: 10.863\n",
      "    sample_time_ms: 92058.516\n",
      "    update_time_ms: 3.561\n",
      "  timestamp: 1635320001\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 734000\n",
      "  training_iteration: 734\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   734</td><td style=\"text-align: right;\">         37414.3</td><td style=\"text-align: right;\">734000</td><td style=\"text-align: right;\">  2.4724</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">              -14.69</td><td style=\"text-align: right;\">             202.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 735000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-35-00\n",
      "  done: false\n",
      "  episode_len_mean: 212.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.35500000000002\n",
      "  episode_reward_min: -14.6899999999999\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 2762\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9141858877675587\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9672598454687331\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0047801995795878505\n",
      "          policy_loss: -0.0021758156311180855\n",
      "          total_loss: 0.0932696555637651\n",
      "          vf_explained_var: 0.8771694898605347\n",
      "          vf_loss: 0.1107480781359805\n",
      "    num_agent_steps_sampled: 735000\n",
      "    num_agent_steps_trained: 735000\n",
      "    num_steps_sampled: 735000\n",
      "    num_steps_trained: 735000\n",
      "  iterations_since_restore: 735\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.99787234042554\n",
      "    ram_util_percent: 34.756028368794325\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702283366542675\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.938807204401083\n",
      "    mean_inference_ms: 2.1228130408148185\n",
      "    mean_raw_obs_processing_ms: 28.69319832793227\n",
      "  time_since_restore: 37513.290299892426\n",
      "  time_this_iter_s: 98.9916980266571\n",
      "  time_total_s: 37513.290299892426\n",
      "  timers:\n",
      "    learn_throughput: 1328.972\n",
      "    learn_time_ms: 752.461\n",
      "    load_throughput: 42160.546\n",
      "    load_time_ms: 23.719\n",
      "    sample_throughput: 11.469\n",
      "    sample_time_ms: 87188.353\n",
      "    update_time_ms: 3.656\n",
      "  timestamp: 1635320100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 735000\n",
      "  training_iteration: 735\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   735</td><td style=\"text-align: right;\">         37513.3</td><td style=\"text-align: right;\">735000</td><td style=\"text-align: right;\">   2.355</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">              -14.69</td><td style=\"text-align: right;\">            212.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 736000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-38-33\n",
      "  done: false\n",
      "  episode_len_mean: 203.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.493700000000019\n",
      "  episode_reward_min: -14.6899999999999\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 2774\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45709294388377936\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.059646240870158\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010465863910445903\n",
      "          policy_loss: 0.21376775279641153\n",
      "          total_loss: 0.3257918042441209\n",
      "          vf_explained_var: 0.5918826460838318\n",
      "          vf_loss: 0.12783664042750995\n",
      "    num_agent_steps_sampled: 736000\n",
      "    num_agent_steps_trained: 736000\n",
      "    num_steps_sampled: 736000\n",
      "    num_steps_trained: 736000\n",
      "  iterations_since_restore: 736\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.33737704918032\n",
      "    ram_util_percent: 34.79081967213115\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702661810587291\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.934418485606685\n",
      "    mean_inference_ms: 2.1229077644173615\n",
      "    mean_raw_obs_processing_ms: 28.798734593506232\n",
      "  time_since_restore: 37726.61387038231\n",
      "  time_this_iter_s: 213.32357048988342\n",
      "  time_total_s: 37726.61387038231\n",
      "  timers:\n",
      "    learn_throughput: 1328.492\n",
      "    learn_time_ms: 752.733\n",
      "    load_throughput: 42152.199\n",
      "    load_time_ms: 23.724\n",
      "    sample_throughput: 10.098\n",
      "    sample_time_ms: 99029.269\n",
      "    update_time_ms: 3.653\n",
      "  timestamp: 1635320313\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 736000\n",
      "  training_iteration: 736\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   736</td><td style=\"text-align: right;\">         37726.6</td><td style=\"text-align: right;\">736000</td><td style=\"text-align: right;\">  2.4937</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">              -14.69</td><td style=\"text-align: right;\">            203.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 737000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-41-07\n",
      "  done: false\n",
      "  episode_len_mean: 187.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 2.892000000000017\n",
      "  episode_reward_min: -14.6899999999999\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2782\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45709294388377936\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9620773315429687\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021592750277414356\n",
      "          policy_loss: -0.054062761242191\n",
      "          total_loss: 0.6531439579195446\n",
      "          vf_explained_var: 0.7974109053611755\n",
      "          vf_loss: 0.7169576042228275\n",
      "    num_agent_steps_sampled: 737000\n",
      "    num_agent_steps_trained: 737000\n",
      "    num_steps_sampled: 737000\n",
      "    num_steps_trained: 737000\n",
      "  iterations_since_restore: 737\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.065753424657544\n",
      "    ram_util_percent: 34.81232876712329\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037028981246111474\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.932018600498928\n",
      "    mean_inference_ms: 2.1229658683454864\n",
      "    mean_raw_obs_processing_ms: 28.87523187530952\n",
      "  time_since_restore: 37880.64243578911\n",
      "  time_this_iter_s: 154.02856540679932\n",
      "  time_total_s: 37880.64243578911\n",
      "  timers:\n",
      "    learn_throughput: 1331.378\n",
      "    learn_time_ms: 751.101\n",
      "    load_throughput: 42069.543\n",
      "    load_time_ms: 23.77\n",
      "    sample_throughput: 8.884\n",
      "    sample_time_ms: 112560.99\n",
      "    update_time_ms: 3.668\n",
      "  timestamp: 1635320467\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 737000\n",
      "  training_iteration: 737\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   737</td><td style=\"text-align: right;\">         37880.6</td><td style=\"text-align: right;\">737000</td><td style=\"text-align: right;\">   2.892</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">              -14.69</td><td style=\"text-align: right;\">            187.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 738000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-43-08\n",
      "  done: false\n",
      "  episode_len_mean: 168.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 3.337100000000015\n",
      "  episode_reward_min: -14.6899999999999\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2789\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.685639415825669\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.53244336909718\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012591112457372274\n",
      "          policy_loss: 0.02763838627272182\n",
      "          total_loss: 0.45827020530899365\n",
      "          vf_explained_var: 0.773103654384613\n",
      "          vf_loss: 0.4373232910202609\n",
      "    num_agent_steps_sampled: 738000\n",
      "    num_agent_steps_trained: 738000\n",
      "    num_steps_sampled: 738000\n",
      "    num_steps_trained: 738000\n",
      "  iterations_since_restore: 738\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.42890173410405\n",
      "    ram_util_percent: 34.79826589595376\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037030954448659094\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.930891824128015\n",
      "    mean_inference_ms: 2.123014395969099\n",
      "    mean_raw_obs_processing_ms: 28.950298927247303\n",
      "  time_since_restore: 38001.60047507286\n",
      "  time_this_iter_s: 120.95803928375244\n",
      "  time_total_s: 38001.60047507286\n",
      "  timers:\n",
      "    learn_throughput: 1330.103\n",
      "    learn_time_ms: 751.821\n",
      "    load_throughput: 41685.383\n",
      "    load_time_ms: 23.989\n",
      "    sample_throughput: 8.14\n",
      "    sample_time_ms: 122843.675\n",
      "    update_time_ms: 3.667\n",
      "  timestamp: 1635320588\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 738000\n",
      "  training_iteration: 738\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   738</td><td style=\"text-align: right;\">         38001.6</td><td style=\"text-align: right;\">738000</td><td style=\"text-align: right;\">  3.3371</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">              -14.69</td><td style=\"text-align: right;\">             168.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 739000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-44-06\n",
      "  done: false\n",
      "  episode_len_mean: 167.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 3.411000000000014\n",
      "  episode_reward_min: -14.6899999999999\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 2793\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.685639415825669\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.060557406478458\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020480916479152376\n",
      "          policy_loss: -0.018849377644558747\n",
      "          total_loss: 0.5009258433348602\n",
      "          vf_explained_var: 0.7030490636825562\n",
      "          vf_loss: 0.5263382744044065\n",
      "    num_agent_steps_sampled: 739000\n",
      "    num_agent_steps_trained: 739000\n",
      "    num_steps_sampled: 739000\n",
      "    num_steps_trained: 739000\n",
      "  iterations_since_restore: 739\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.35487804878049\n",
      "    ram_util_percent: 34.83292682926829\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03703198992285779\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.930698521135398\n",
      "    mean_inference_ms: 2.123041374494583\n",
      "    mean_raw_obs_processing_ms: 28.992507941485982\n",
      "  time_since_restore: 38058.73012971878\n",
      "  time_this_iter_s: 57.1296546459198\n",
      "  time_total_s: 38058.73012971878\n",
      "  timers:\n",
      "    learn_throughput: 1330.372\n",
      "    learn_time_ms: 751.669\n",
      "    load_throughput: 41574.688\n",
      "    load_time_ms: 24.053\n",
      "    sample_throughput: 8.391\n",
      "    sample_time_ms: 119173.474\n",
      "    update_time_ms: 3.723\n",
      "  timestamp: 1635320646\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 739000\n",
      "  training_iteration: 739\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   739</td><td style=\"text-align: right;\">         38058.7</td><td style=\"text-align: right;\">739000</td><td style=\"text-align: right;\">   3.411</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">              -14.69</td><td style=\"text-align: right;\">            167.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 740000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-49-31\n",
      "  done: false\n",
      "  episode_len_mean: 145.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 3.9193000000000113\n",
      "  episode_reward_min: -14.6899999999999\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 2811\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0284591237385035\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7921739803420174\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013910682881626732\n",
      "          policy_loss: 0.12955369071827993\n",
      "          total_loss: 0.6645222745007939\n",
      "          vf_explained_var: 0.7185719609260559\n",
      "          vf_loss: 0.5385837554931641\n",
      "    num_agent_steps_sampled: 740000\n",
      "    num_agent_steps_trained: 740000\n",
      "    num_steps_sampled: 740000\n",
      "    num_steps_trained: 740000\n",
      "  iterations_since_restore: 740\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.677105831533474\n",
      "    ram_util_percent: 34.82051835853131\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03703631229354367\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.931661723541232\n",
      "    mean_inference_ms: 2.123155653348561\n",
      "    mean_raw_obs_processing_ms: 29.238026021163318\n",
      "  time_since_restore: 38383.74622249603\n",
      "  time_this_iter_s: 325.0160927772522\n",
      "  time_total_s: 38383.74622249603\n",
      "  timers:\n",
      "    learn_throughput: 1328.795\n",
      "    learn_time_ms: 752.562\n",
      "    load_throughput: 41368.228\n",
      "    load_time_ms: 24.173\n",
      "    sample_throughput: 7.436\n",
      "    sample_time_ms: 134473.859\n",
      "    update_time_ms: 3.805\n",
      "  timestamp: 1635320971\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 740000\n",
      "  training_iteration: 740\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   740</td><td style=\"text-align: right;\">         38383.7</td><td style=\"text-align: right;\">740000</td><td style=\"text-align: right;\">  3.9193</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">              -14.69</td><td style=\"text-align: right;\">            145.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 741000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-56-21\n",
      "  done: false\n",
      "  episode_len_mean: 111.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 4.641100000000008\n",
      "  episode_reward_min: -5.479999999999963\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 2832\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0284591237385035\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2216966403855218\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0166178440213801\n",
      "          policy_loss: -0.12588545928398767\n",
      "          total_loss: 0.6401444392899672\n",
      "          vf_explained_var: 0.8098100423812866\n",
      "          vf_loss: 0.7611560934119754\n",
      "    num_agent_steps_sampled: 741000\n",
      "    num_agent_steps_trained: 741000\n",
      "    num_steps_sampled: 741000\n",
      "    num_steps_trained: 741000\n",
      "  iterations_since_restore: 741\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.46786324786325\n",
      "    ram_util_percent: 34.79897435897436\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037040132561331394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.9383111359816\n",
      "    mean_inference_ms: 2.1232644887714116\n",
      "    mean_raw_obs_processing_ms: 29.593238311942223\n",
      "  time_since_restore: 38793.656777858734\n",
      "  time_this_iter_s: 409.9105553627014\n",
      "  time_total_s: 38793.656777858734\n",
      "  timers:\n",
      "    learn_throughput: 1327.44\n",
      "    learn_time_ms: 753.33\n",
      "    load_throughput: 41670.722\n",
      "    load_time_ms: 23.998\n",
      "    sample_throughput: 5.892\n",
      "    sample_time_ms: 169711.254\n",
      "    update_time_ms: 3.801\n",
      "  timestamp: 1635321381\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 741000\n",
      "  training_iteration: 741\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   741</td><td style=\"text-align: right;\">         38793.7</td><td style=\"text-align: right;\">741000</td><td style=\"text-align: right;\">  4.6411</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -5.48</td><td style=\"text-align: right;\">            111.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 742000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_07-58-53\n",
      "  done: false\n",
      "  episode_len_mean: 101.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 4.910300000000006\n",
      "  episode_reward_min: -4.779999999999942\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 2840\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0284591237385035\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.992130524582333\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008910946412724977\n",
      "          policy_loss: -0.021373681227366128\n",
      "          total_loss: 0.1745378517028358\n",
      "          vf_explained_var: 0.6921438574790955\n",
      "          vf_loss: 0.20666829256547822\n",
      "    num_agent_steps_sampled: 742000\n",
      "    num_agent_steps_trained: 742000\n",
      "    num_steps_sampled: 742000\n",
      "    num_steps_trained: 742000\n",
      "  iterations_since_restore: 742\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.75622119815667\n",
      "    ram_util_percent: 34.872811059907825\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03704141818056095\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.941054834850714\n",
      "    mean_inference_ms: 2.123302332671625\n",
      "    mean_raw_obs_processing_ms: 29.728043343894697\n",
      "  time_since_restore: 38945.686629772186\n",
      "  time_this_iter_s: 152.02985191345215\n",
      "  time_total_s: 38945.686629772186\n",
      "  timers:\n",
      "    learn_throughput: 1328.838\n",
      "    learn_time_ms: 752.537\n",
      "    load_throughput: 41882.352\n",
      "    load_time_ms: 23.876\n",
      "    sample_throughput: 5.761\n",
      "    sample_time_ms: 173594.748\n",
      "    update_time_ms: 3.796\n",
      "  timestamp: 1635321533\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 742000\n",
      "  training_iteration: 742\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   742</td><td style=\"text-align: right;\">         38945.7</td><td style=\"text-align: right;\">742000</td><td style=\"text-align: right;\">  4.9103</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -4.78</td><td style=\"text-align: right;\">            101.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 743000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_08-03-57\n",
      "  done: false\n",
      "  episode_len_mean: 87.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 5.122200000000005\n",
      "  episode_reward_min: -4.679999999999936\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 2857\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0284591237385035\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6677563124232822\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012100414294178914\n",
      "          policy_loss: -0.021371917095449236\n",
      "          total_loss: 0.449612835711903\n",
      "          vf_explained_var: 0.6410874724388123\n",
      "          vf_loss: 0.4752175337738461\n",
      "    num_agent_steps_sampled: 743000\n",
      "    num_agent_steps_trained: 743000\n",
      "    num_steps_sampled: 743000\n",
      "    num_steps_trained: 743000\n",
      "  iterations_since_restore: 743\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.535862068965514\n",
      "    ram_util_percent: 34.86436781609195\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037043803332356434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.947049443408694\n",
      "    mean_inference_ms: 2.123375259245263\n",
      "    mean_raw_obs_processing_ms: 30.046622541245675\n",
      "  time_since_restore: 39250.14628338814\n",
      "  time_this_iter_s: 304.45965361595154\n",
      "  time_total_s: 39250.14628338814\n",
      "  timers:\n",
      "    learn_throughput: 1327.894\n",
      "    learn_time_ms: 753.072\n",
      "    load_throughput: 41707.02\n",
      "    load_time_ms: 23.977\n",
      "    sample_throughput: 5.097\n",
      "    sample_time_ms: 196211.351\n",
      "    update_time_ms: 3.895\n",
      "  timestamp: 1635321837\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 743000\n",
      "  training_iteration: 743\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   743</td><td style=\"text-align: right;\">         39250.1</td><td style=\"text-align: right;\">743000</td><td style=\"text-align: right;\">  5.1222</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -4.68</td><td style=\"text-align: right;\">             87.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 744000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_08-11-53\n",
      "  done: false\n",
      "  episode_len_mean: 69.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 5.693000000000001\n",
      "  episode_reward_min: -4.679999999999936\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 2881\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0284591237385035\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5741184923383924\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010281340760209387\n",
      "          policy_loss: -0.10389486948649089\n",
      "          total_loss: 0.26603835026423134\n",
      "          vf_explained_var: 0.9427388310432434\n",
      "          vf_loss: 0.3751004661122958\n",
      "    num_agent_steps_sampled: 744000\n",
      "    num_agent_steps_trained: 744000\n",
      "    num_steps_sampled: 744000\n",
      "    num_steps_trained: 744000\n",
      "  iterations_since_restore: 744\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.95265486725663\n",
      "    ram_util_percent: 34.9051622418879\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03704655336471592\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.95397269810514\n",
      "    mean_inference_ms: 2.1234655668461557\n",
      "    mean_raw_obs_processing_ms: 30.553426323892197\n",
      "  time_since_restore: 39725.54395413399\n",
      "  time_this_iter_s: 475.3976707458496\n",
      "  time_total_s: 39725.54395413399\n",
      "  timers:\n",
      "    learn_throughput: 1326.586\n",
      "    learn_time_ms: 753.814\n",
      "    load_throughput: 41562.453\n",
      "    load_time_ms: 24.06\n",
      "    sample_throughput: 4.341\n",
      "    sample_time_ms: 230338.276\n",
      "    update_time_ms: 3.952\n",
      "  timestamp: 1635322313\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 744000\n",
      "  training_iteration: 744\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   744</td><td style=\"text-align: right;\">         39725.5</td><td style=\"text-align: right;\">744000</td><td style=\"text-align: right;\">   5.693</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -4.68</td><td style=\"text-align: right;\">             69.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 745000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_08-13-28\n",
      "  done: false\n",
      "  episode_len_mean: 70.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 5.755200000000005\n",
      "  episode_reward_min: -4.679999999999936\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 2887\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0284591237385035\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7730942659907871\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017943060230685544\n",
      "          policy_loss: 0.018055709016819795\n",
      "          total_loss: 0.9281984562675158\n",
      "          vf_explained_var: 0.5373560190200806\n",
      "          vf_loss: 0.8994199835591846\n",
      "    num_agent_steps_sampled: 745000\n",
      "    num_agent_steps_trained: 745000\n",
      "    num_steps_sampled: 745000\n",
      "    num_steps_trained: 745000\n",
      "  iterations_since_restore: 745\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.83529411764706\n",
      "    ram_util_percent: 34.91764705882353\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037047188852024314\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.95533048491349\n",
      "    mean_inference_ms: 2.123488538397602\n",
      "    mean_raw_obs_processing_ms: 30.67048340145396\n",
      "  time_since_restore: 39820.9047293663\n",
      "  time_this_iter_s: 95.36077523231506\n",
      "  time_total_s: 39820.9047293663\n",
      "  timers:\n",
      "    learn_throughput: 1325.261\n",
      "    learn_time_ms: 754.569\n",
      "    load_throughput: 41738.397\n",
      "    load_time_ms: 23.959\n",
      "    sample_throughput: 4.348\n",
      "    sample_time_ms: 229974.649\n",
      "    update_time_ms: 3.855\n",
      "  timestamp: 1635322408\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 745000\n",
      "  training_iteration: 745\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   745</td><td style=\"text-align: right;\">         39820.9</td><td style=\"text-align: right;\">745000</td><td style=\"text-align: right;\">  5.7552</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -4.68</td><td style=\"text-align: right;\">             70.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 746000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_08-24-18\n",
      "  done: false\n",
      "  episode_len_mean: 55.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 6.125400000000004\n",
      "  episode_reward_min: -13.439999999999907\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 2921\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0284591237385035\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2180959781010945\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009928658851891786\n",
      "          policy_loss: -0.01989862819512685\n",
      "          total_loss: 0.4501788259794315\n",
      "          vf_explained_var: 0.9155179262161255\n",
      "          vf_loss: 0.4720472087462743\n",
      "    num_agent_steps_sampled: 746000\n",
      "    num_agent_steps_trained: 746000\n",
      "    num_steps_sampled: 746000\n",
      "    num_steps_trained: 746000\n",
      "  iterations_since_restore: 746\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.89655172413793\n",
      "    ram_util_percent: 34.91691810344827\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03705030738515899\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.963910602329065\n",
      "    mean_inference_ms: 2.1236030532076935\n",
      "    mean_raw_obs_processing_ms: 31.445145931302378\n",
      "  time_since_restore: 40470.72933411598\n",
      "  time_this_iter_s: 649.8246047496796\n",
      "  time_total_s: 40470.72933411598\n",
      "  timers:\n",
      "    learn_throughput: 1324.455\n",
      "    learn_time_ms: 755.027\n",
      "    load_throughput: 41737.732\n",
      "    load_time_ms: 23.959\n",
      "    sample_throughput: 3.655\n",
      "    sample_time_ms: 273624.276\n",
      "    update_time_ms: 3.851\n",
      "  timestamp: 1635323058\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 746000\n",
      "  training_iteration: 746\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   746</td><td style=\"text-align: right;\">         40470.7</td><td style=\"text-align: right;\">746000</td><td style=\"text-align: right;\">  6.1254</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">              -13.44</td><td style=\"text-align: right;\">             55.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 747000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_08-31-57\n",
      "  done: false\n",
      "  episode_len_mean: 47.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 6.124500000000004\n",
      "  episode_reward_min: -13.439999999999907\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 2945\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0284591237385035\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6463824894693162\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013423880131678424\n",
      "          policy_loss: 0.04819792484243711\n",
      "          total_loss: 0.640614265203476\n",
      "          vf_explained_var: 0.7275803685188293\n",
      "          vf_loss: 0.5950742509629992\n",
      "    num_agent_steps_sampled: 747000\n",
      "    num_agent_steps_trained: 747000\n",
      "    num_steps_sampled: 747000\n",
      "    num_steps_trained: 747000\n",
      "  iterations_since_restore: 747\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.96661585365853\n",
      "    ram_util_percent: 35.00411585365853\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037052298415451594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.969516376137356\n",
      "    mean_inference_ms: 2.1236765245753673\n",
      "    mean_raw_obs_processing_ms: 31.990564757765604\n",
      "  time_since_restore: 40930.17932796478\n",
      "  time_this_iter_s: 459.44999384880066\n",
      "  time_total_s: 40930.17932796478\n",
      "  timers:\n",
      "    learn_throughput: 1322.365\n",
      "    learn_time_ms: 756.221\n",
      "    load_throughput: 41481.845\n",
      "    load_time_ms: 24.107\n",
      "    sample_throughput: 3.288\n",
      "    sample_time_ms: 304165.067\n",
      "    update_time_ms: 3.846\n",
      "  timestamp: 1635323517\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 747000\n",
      "  training_iteration: 747\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   747</td><td style=\"text-align: right;\">         40930.2</td><td style=\"text-align: right;\">747000</td><td style=\"text-align: right;\">  6.1245</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">              -13.44</td><td style=\"text-align: right;\">             47.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 748000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_08-37-06\n",
      "  done: false\n",
      "  episode_len_mean: 47.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 6.104000000000003\n",
      "  episode_reward_min: -13.439999999999907\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 2961\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0284591237385035\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6931263433562385\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009083431561483144\n",
      "          policy_loss: -0.05648494238654773\n",
      "          total_loss: 0.4655565997792615\n",
      "          vf_explained_var: 0.6714658737182617\n",
      "          vf_loss: 0.5296308649910821\n",
      "    num_agent_steps_sampled: 748000\n",
      "    num_agent_steps_trained: 748000\n",
      "    num_steps_sampled: 748000\n",
      "    num_steps_trained: 748000\n",
      "  iterations_since_restore: 748\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.54454545454545\n",
      "    ram_util_percent: 35.04795454545455\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370534981060407\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.973632606364077\n",
      "    mean_inference_ms: 2.1237226256904207\n",
      "    mean_raw_obs_processing_ms: 32.33251908247725\n",
      "  time_since_restore: 41238.56617760658\n",
      "  time_this_iter_s: 308.3868496417999\n",
      "  time_total_s: 41238.56617760658\n",
      "  timers:\n",
      "    learn_throughput: 1324.398\n",
      "    learn_time_ms: 755.06\n",
      "    load_throughput: 41499.493\n",
      "    load_time_ms: 24.097\n",
      "    sample_throughput: 3.097\n",
      "    sample_time_ms: 322909.187\n",
      "    update_time_ms: 3.848\n",
      "  timestamp: 1635323826\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 748000\n",
      "  training_iteration: 748\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   748</td><td style=\"text-align: right;\">         41238.6</td><td style=\"text-align: right;\">748000</td><td style=\"text-align: right;\">   6.104</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">              -13.44</td><td style=\"text-align: right;\">             47.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 749000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_08-45-41\n",
      "  done: false\n",
      "  episode_len_mean: 36.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 6.259400000000002\n",
      "  episode_reward_min: -6.769999999999918\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2988\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0284591237385035\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.572575909561581\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007792227176334008\n",
      "          policy_loss: -0.15502512355645498\n",
      "          total_loss: 0.05584768628080686\n",
      "          vf_explained_var: 0.6487732529640198\n",
      "          vf_loss: 0.2185845836997032\n",
      "    num_agent_steps_sampled: 749000\n",
      "    num_agent_steps_trained: 749000\n",
      "    num_steps_sampled: 749000\n",
      "    num_steps_trained: 749000\n",
      "  iterations_since_restore: 749\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.93755102040817\n",
      "    ram_util_percent: 35.0991836734694\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037055506526670834\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.981488932588544\n",
      "    mean_inference_ms: 2.123801645026613\n",
      "    mean_raw_obs_processing_ms: 32.94761141290269\n",
      "  time_since_restore: 41753.92626142502\n",
      "  time_this_iter_s: 515.3600838184357\n",
      "  time_total_s: 41753.92626142502\n",
      "  timers:\n",
      "    learn_throughput: 1324.414\n",
      "    learn_time_ms: 755.051\n",
      "    load_throughput: 41396.646\n",
      "    load_time_ms: 24.157\n",
      "    sample_throughput: 2.712\n",
      "    sample_time_ms: 368732.252\n",
      "    update_time_ms: 3.779\n",
      "  timestamp: 1635324341\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 749000\n",
      "  training_iteration: 749\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   749</td><td style=\"text-align: right;\">         41753.9</td><td style=\"text-align: right;\">749000</td><td style=\"text-align: right;\">  6.2594</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -6.77</td><td style=\"text-align: right;\">             36.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 750000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_08-53-19\n",
      "  done: false\n",
      "  episode_len_mean: 42.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 6.171800000000003\n",
      "  episode_reward_min: -6.769999999999918\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 3012\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0284591237385035\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7435408552487692\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008761329437059541\n",
      "          policy_loss: 0.06898166769080692\n",
      "          total_loss: 0.41190418668298256\n",
      "          vf_explained_var: 0.742293119430542\n",
      "          vf_loss: 0.35134725632766883\n",
      "    num_agent_steps_sampled: 750000\n",
      "    num_agent_steps_trained: 750000\n",
      "    num_steps_sampled: 750000\n",
      "    num_steps_trained: 750000\n",
      "  iterations_since_restore: 750\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.103058103975535\n",
      "    ram_util_percent: 35.13272171253823\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037056975839905824\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.98714685116015\n",
      "    mean_inference_ms: 2.1238576901018416\n",
      "    mean_raw_obs_processing_ms: 33.42961375509126\n",
      "  time_since_restore: 42211.812576532364\n",
      "  time_this_iter_s: 457.8863151073456\n",
      "  time_total_s: 42211.812576532364\n",
      "  timers:\n",
      "    learn_throughput: 1325.542\n",
      "    learn_time_ms: 754.409\n",
      "    load_throughput: 41634.115\n",
      "    load_time_ms: 24.019\n",
      "    sample_throughput: 2.618\n",
      "    sample_time_ms: 382020.164\n",
      "    update_time_ms: 3.686\n",
      "  timestamp: 1635324799\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 750000\n",
      "  training_iteration: 750\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   750</td><td style=\"text-align: right;\">         42211.8</td><td style=\"text-align: right;\">750000</td><td style=\"text-align: right;\">  6.1718</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -6.77</td><td style=\"text-align: right;\">             42.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 751000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_09-00-16\n",
      "  done: false\n",
      "  episode_len_mean: 43.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 6.299000000000001\n",
      "  episode_reward_min: -6.769999999999918\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 3034\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0284591237385035\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.705246619383494\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0055842168169287255\n",
      "          policy_loss: 0.016472029272052977\n",
      "          total_loss: 0.20134410055147278\n",
      "          vf_explained_var: 0.7280028462409973\n",
      "          vf_loss: 0.1961813996028569\n",
      "    num_agent_steps_sampled: 751000\n",
      "    num_agent_steps_trained: 751000\n",
      "    num_steps_sampled: 751000\n",
      "    num_steps_trained: 751000\n",
      "  iterations_since_restore: 751\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.016302521008406\n",
      "    ram_util_percent: 35.161344537815125\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037058517262461906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.99162305731036\n",
      "    mean_inference_ms: 2.123917858771612\n",
      "    mean_raw_obs_processing_ms: 33.907460473818446\n",
      "  time_since_restore: 42629.33833408356\n",
      "  time_this_iter_s: 417.52575755119324\n",
      "  time_total_s: 42629.33833408356\n",
      "  timers:\n",
      "    learn_throughput: 1325.824\n",
      "    learn_time_ms: 754.248\n",
      "    load_throughput: 41610.283\n",
      "    load_time_ms: 24.033\n",
      "    sample_throughput: 2.612\n",
      "    sample_time_ms: 382781.825\n",
      "    update_time_ms: 3.682\n",
      "  timestamp: 1635325216\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 751000\n",
      "  training_iteration: 751\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   751</td><td style=\"text-align: right;\">         42629.3</td><td style=\"text-align: right;\">751000</td><td style=\"text-align: right;\">   6.299</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -6.77</td><td style=\"text-align: right;\">              43.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 752000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_09-15-04\n",
      "  done: false\n",
      "  episode_len_mean: 31.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 6.3004999999999995\n",
      "  episode_reward_min: -2.8399999999999817\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 3080\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0284591237385035\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2037145945760939\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006237258920978306\n",
      "          policy_loss: -0.1570768487950166\n",
      "          total_loss: 0.17006737250420784\n",
      "          vf_explained_var: 0.9406247735023499\n",
      "          vf_loss: 0.33276660442352296\n",
      "    num_agent_steps_sampled: 752000\n",
      "    num_agent_steps_trained: 752000\n",
      "    num_steps_sampled: 752000\n",
      "    num_steps_trained: 752000\n",
      "  iterations_since_restore: 752\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.536543014996056\n",
      "    ram_util_percent: 35.222178374112076\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03706134982070251\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.99797515321284\n",
      "    mean_inference_ms: 2.1240444237378022\n",
      "    mean_raw_obs_processing_ms: 35.0802647023659\n",
      "  time_since_restore: 43516.82575964928\n",
      "  time_this_iter_s: 887.4874255657196\n",
      "  time_total_s: 43516.82575964928\n",
      "  timers:\n",
      "    learn_throughput: 1324.249\n",
      "    learn_time_ms: 755.145\n",
      "    load_throughput: 41369.33\n",
      "    load_time_ms: 24.172\n",
      "    sample_throughput: 2.191\n",
      "    sample_time_ms: 456326.482\n",
      "    update_time_ms: 3.698\n",
      "  timestamp: 1635326104\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 752000\n",
      "  training_iteration: 752\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   752</td><td style=\"text-align: right;\">         43516.8</td><td style=\"text-align: right;\">752000</td><td style=\"text-align: right;\">  6.3005</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -2.84</td><td style=\"text-align: right;\">             31.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 753000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_09-24-49\n",
      "  done: false\n",
      "  episode_len_mean: 30.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 6.3027999999999995\n",
      "  episode_reward_min: -2.269999999999985\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 3111\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0284591237385035\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6424958321783278\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007804187516406063\n",
      "          policy_loss: 0.12148361139827304\n",
      "          total_loss: 0.2686003998749786\n",
      "          vf_explained_var: 0.7055225372314453\n",
      "          vf_loss: 0.1555154585176044\n",
      "    num_agent_steps_sampled: 753000\n",
      "    num_agent_steps_trained: 753000\n",
      "    num_steps_sampled: 753000\n",
      "    num_steps_trained: 753000\n",
      "  iterations_since_restore: 753\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.71305389221556\n",
      "    ram_util_percent: 35.36467065868263\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03706307014345612\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.00059104335817\n",
      "    mean_inference_ms: 2.1241211520749834\n",
      "    mean_raw_obs_processing_ms: 35.8292517049927\n",
      "  time_since_restore: 44102.31162071228\n",
      "  time_this_iter_s: 585.4858610630035\n",
      "  time_total_s: 44102.31162071228\n",
      "  timers:\n",
      "    learn_throughput: 1321.531\n",
      "    learn_time_ms: 756.698\n",
      "    load_throughput: 41154.318\n",
      "    load_time_ms: 24.299\n",
      "    sample_throughput: 2.064\n",
      "    sample_time_ms: 484427.506\n",
      "    update_time_ms: 3.602\n",
      "  timestamp: 1635326689\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 753000\n",
      "  training_iteration: 753\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   753</td><td style=\"text-align: right;\">         44102.3</td><td style=\"text-align: right;\">753000</td><td style=\"text-align: right;\">  6.3028</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -2.27</td><td style=\"text-align: right;\">             30.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 754000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_09-33-05\n",
      "  done: false\n",
      "  episode_len_mean: 25.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 6.559199999999999\n",
      "  episode_reward_min: -1.969999999999987\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3136\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0284591237385035\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3808399240175884\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010127520928129267\n",
      "          policy_loss: -0.06347404496951235\n",
      "          total_loss: 0.5071262286769019\n",
      "          vf_explained_var: 0.9375001788139343\n",
      "          vf_loss: 0.5739929325050778\n",
      "    num_agent_steps_sampled: 754000\n",
      "    num_agent_steps_trained: 754000\n",
      "    num_steps_sampled: 754000\n",
      "    num_steps_trained: 754000\n",
      "  iterations_since_restore: 754\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.18531073446327\n",
      "    ram_util_percent: 35.425847457627114\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03706430574494755\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.003271000042684\n",
      "    mean_inference_ms: 2.1241794721436635\n",
      "    mean_raw_obs_processing_ms: 36.405482583130194\n",
      "  time_since_restore: 44597.921441078186\n",
      "  time_this_iter_s: 495.60982036590576\n",
      "  time_total_s: 44597.921441078186\n",
      "  timers:\n",
      "    learn_throughput: 1322.924\n",
      "    learn_time_ms: 755.901\n",
      "    load_throughput: 41253.407\n",
      "    load_time_ms: 24.24\n",
      "    sample_throughput: 2.056\n",
      "    sample_time_ms: 486449.631\n",
      "    update_time_ms: 3.55\n",
      "  timestamp: 1635327185\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 754000\n",
      "  training_iteration: 754\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   754</td><td style=\"text-align: right;\">         44597.9</td><td style=\"text-align: right;\">754000</td><td style=\"text-align: right;\">  6.5592</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -1.97</td><td style=\"text-align: right;\">             25.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 755000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_09-51-04\n",
      "  done: false\n",
      "  episode_len_mean: 22.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 6.770200000000001\n",
      "  episode_reward_min: -8.749999999999941\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 3193\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0284591237385035\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.106926425298055\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009805262302564977\n",
      "          policy_loss: -0.010308127767509884\n",
      "          total_loss: 0.3862929257667727\n",
      "          vf_explained_var: 0.9436254501342773\n",
      "          vf_loss: 0.39758600923750137\n",
      "    num_agent_steps_sampled: 755000\n",
      "    num_agent_steps_trained: 755000\n",
      "    num_steps_sampled: 755000\n",
      "    num_steps_trained: 755000\n",
      "  iterations_since_restore: 755\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.474333983105915\n",
      "    ram_util_percent: 35.481026640675765\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037066909271655464\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.01213604771525\n",
      "    mean_inference_ms: 2.1243042747382086\n",
      "    mean_raw_obs_processing_ms: 37.804980377577024\n",
      "  time_since_restore: 45676.72850084305\n",
      "  time_this_iter_s: 1078.807059764862\n",
      "  time_total_s: 45676.72850084305\n",
      "  timers:\n",
      "    learn_throughput: 1322.539\n",
      "    learn_time_ms: 756.121\n",
      "    load_throughput: 41095.607\n",
      "    load_time_ms: 24.334\n",
      "    sample_throughput: 1.71\n",
      "    sample_time_ms: 584792.201\n",
      "    update_time_ms: 5.232\n",
      "  timestamp: 1635328264\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 755000\n",
      "  training_iteration: 755\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   755</td><td style=\"text-align: right;\">         45676.7</td><td style=\"text-align: right;\">755000</td><td style=\"text-align: right;\">  6.7702</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -8.75</td><td style=\"text-align: right;\">             22.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 756000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_10-01-50\n",
      "  done: false\n",
      "  episode_len_mean: 23.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 6.533100000000002\n",
      "  episode_reward_min: -8.749999999999941\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 3226\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0284591237385035\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3327583743466271\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00858303467276621\n",
      "          policy_loss: -0.1677325223882993\n",
      "          total_loss: 0.34670161364807023\n",
      "          vf_explained_var: 0.8935757279396057\n",
      "          vf_loss: 0.518934428691864\n",
      "    num_agent_steps_sampled: 756000\n",
      "    num_agent_steps_trained: 756000\n",
      "    num_steps_sampled: 756000\n",
      "    num_steps_trained: 756000\n",
      "  iterations_since_restore: 756\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.770639219935\n",
      "    ram_util_percent: 35.56110509209101\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037068285258232074\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.017019163048058\n",
      "    mean_inference_ms: 2.1243757725305823\n",
      "    mean_raw_obs_processing_ms: 38.60582442166782\n",
      "  time_since_restore: 46323.228024959564\n",
      "  time_this_iter_s: 646.4995241165161\n",
      "  time_total_s: 46323.228024959564\n",
      "  timers:\n",
      "    learn_throughput: 1321.793\n",
      "    learn_time_ms: 756.548\n",
      "    load_throughput: 41130.588\n",
      "    load_time_ms: 24.313\n",
      "    sample_throughput: 1.711\n",
      "    sample_time_ms: 584459.279\n",
      "    update_time_ms: 5.227\n",
      "  timestamp: 1635328910\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 756000\n",
      "  training_iteration: 756\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   756</td><td style=\"text-align: right;\">         46323.2</td><td style=\"text-align: right;\">756000</td><td style=\"text-align: right;\">  6.5331</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -8.75</td><td style=\"text-align: right;\">             23.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 757000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_10-11-19\n",
      "  done: false\n",
      "  episode_len_mean: 22.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 6.384100000000001\n",
      "  episode_reward_min: -8.859999999999916\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 3256\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0284591237385035\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1776649514834086\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004960693482602628\n",
      "          policy_loss: -0.13355912276440196\n",
      "          total_loss: 0.04280459417237176\n",
      "          vf_explained_var: 0.962972104549408\n",
      "          vf_loss: 0.18303849757131602\n",
      "    num_agent_steps_sampled: 757000\n",
      "    num_agent_steps_trained: 757000\n",
      "    num_steps_sampled: 757000\n",
      "    num_steps_trained: 757000\n",
      "  iterations_since_restore: 757\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.81035758323058\n",
      "    ram_util_percent: 35.68803945745992\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03706944883814638\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.019167852625348\n",
      "    mean_inference_ms: 2.124435175443213\n",
      "    mean_raw_obs_processing_ms: 39.17232093368907\n",
      "  time_since_restore: 46891.97247052193\n",
      "  time_this_iter_s: 568.7444455623627\n",
      "  time_total_s: 46891.97247052193\n",
      "  timers:\n",
      "    learn_throughput: 1322.521\n",
      "    learn_time_ms: 756.132\n",
      "    load_throughput: 41406.985\n",
      "    load_time_ms: 24.151\n",
      "    sample_throughput: 1.68\n",
      "    sample_time_ms: 595389.31\n",
      "    update_time_ms: 5.216\n",
      "  timestamp: 1635329479\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 757000\n",
      "  training_iteration: 757\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   757</td><td style=\"text-align: right;\">           46892</td><td style=\"text-align: right;\">757000</td><td style=\"text-align: right;\">  6.3841</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -8.86</td><td style=\"text-align: right;\">             22.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 758000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_10-18-50\n",
      "  done: false\n",
      "  episode_len_mean: 28.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 6.325100000000001\n",
      "  episode_reward_min: -8.859999999999916\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 3280\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5142295618692517\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6582619398832321\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004313865618542955\n",
      "          policy_loss: -0.08758004539542728\n",
      "          total_loss: 0.15103682238194677\n",
      "          vf_explained_var: 0.7271302938461304\n",
      "          vf_loss: 0.2429811742570665\n",
      "    num_agent_steps_sampled: 758000\n",
      "    num_agent_steps_trained: 758000\n",
      "    num_steps_sampled: 758000\n",
      "    num_steps_trained: 758000\n",
      "  iterations_since_restore: 758\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.963509316770185\n",
      "    ram_util_percent: 35.731521739130436\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03707069892764445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.020093422137794\n",
      "    mean_inference_ms: 2.124496906840481\n",
      "    mean_raw_obs_processing_ms: 39.64149162969888\n",
      "  time_since_restore: 47343.124775886536\n",
      "  time_this_iter_s: 451.15230536460876\n",
      "  time_total_s: 47343.124775886536\n",
      "  timers:\n",
      "    learn_throughput: 1321.966\n",
      "    learn_time_ms: 756.449\n",
      "    load_throughput: 41512.842\n",
      "    load_time_ms: 24.089\n",
      "    sample_throughput: 1.64\n",
      "    sample_time_ms: 609665.575\n",
      "    update_time_ms: 5.218\n",
      "  timestamp: 1635329930\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 758000\n",
      "  training_iteration: 758\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   758</td><td style=\"text-align: right;\">         47343.1</td><td style=\"text-align: right;\">758000</td><td style=\"text-align: right;\">  6.3251</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -8.86</td><td style=\"text-align: right;\">             28.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 759000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_10-28-40\n",
      "  done: false\n",
      "  episode_len_mean: 35.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 6.469900000000003\n",
      "  episode_reward_min: -8.859999999999916\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 3311\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.25711478093462586\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5731874045398501\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0059890003338760895\n",
      "          policy_loss: -0.004597075697448518\n",
      "          total_loss: 0.1565076174835364\n",
      "          vf_explained_var: 0.5778098106384277\n",
      "          vf_loss: 0.16529670912358496\n",
      "    num_agent_steps_sampled: 759000\n",
      "    num_agent_steps_trained: 759000\n",
      "    num_steps_sampled: 759000\n",
      "    num_steps_trained: 759000\n",
      "  iterations_since_restore: 759\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.67158145065399\n",
      "    ram_util_percent: 35.79357907253271\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037072609428716936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.020466349692782\n",
      "    mean_inference_ms: 2.1245911355464666\n",
      "    mean_raw_obs_processing_ms: 40.32403793421386\n",
      "  time_since_restore: 47932.601969480515\n",
      "  time_this_iter_s: 589.4771935939789\n",
      "  time_total_s: 47932.601969480515\n",
      "  timers:\n",
      "    learn_throughput: 1319.381\n",
      "    learn_time_ms: 757.931\n",
      "    load_throughput: 41526.2\n",
      "    load_time_ms: 24.081\n",
      "    sample_throughput: 1.621\n",
      "    sample_time_ms: 617075.817\n",
      "    update_time_ms: 5.212\n",
      "  timestamp: 1635330520\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 759000\n",
      "  training_iteration: 759\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   759</td><td style=\"text-align: right;\">         47932.6</td><td style=\"text-align: right;\">759000</td><td style=\"text-align: right;\">  6.4699</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -8.86</td><td style=\"text-align: right;\">             35.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 760000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_10-46-26\n",
      "  done: false\n",
      "  episode_len_mean: 25.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 6.431600000000002\n",
      "  episode_reward_min: -7.569999999999883\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 3366\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.25711478093462586\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9162454254097409\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014657706737238247\n",
      "          policy_loss: 0.027333931831849945\n",
      "          total_loss: 0.37570661641657355\n",
      "          vf_explained_var: 0.9413928389549255\n",
      "          vf_loss: 0.353766429093149\n",
      "    num_agent_steps_sampled: 760000\n",
      "    num_agent_steps_trained: 760000\n",
      "    num_steps_sampled: 760000\n",
      "    num_steps_trained: 760000\n",
      "  iterations_since_restore: 760\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.92253613666229\n",
      "    ram_util_percent: 36.07496714848883\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370757407257038\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.022917074526617\n",
      "    mean_inference_ms: 2.1247623361379384\n",
      "    mean_raw_obs_processing_ms: 41.76102791085847\n",
      "  time_since_restore: 48999.11782860756\n",
      "  time_this_iter_s: 1066.5158591270447\n",
      "  time_total_s: 48999.11782860756\n",
      "  timers:\n",
      "    learn_throughput: 1313.034\n",
      "    learn_time_ms: 761.595\n",
      "    load_throughput: 41505.653\n",
      "    load_time_ms: 24.093\n",
      "    sample_throughput: 1.475\n",
      "    sample_time_ms: 677935.047\n",
      "    update_time_ms: 5.215\n",
      "  timestamp: 1635331586\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 760000\n",
      "  training_iteration: 760\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   760</td><td style=\"text-align: right;\">         48999.1</td><td style=\"text-align: right;\">760000</td><td style=\"text-align: right;\">  6.4316</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -7.57</td><td style=\"text-align: right;\">             25.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 761000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_11-08-37\n",
      "  done: false\n",
      "  episode_len_mean: 14.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 6.5298\n",
      "  episode_reward_min: 3.9\n",
      "  episodes_this_iter: 69\n",
      "  episodes_total: 3435\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.25711478093462586\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9381727370950911\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011240292624796026\n",
      "          policy_loss: 0.011750735425286823\n",
      "          total_loss: 0.23806521706283093\n",
      "          vf_explained_var: 0.9613524675369263\n",
      "          vf_loss: 0.23280616501967114\n",
      "    num_agent_steps_sampled: 761000\n",
      "    num_agent_steps_trained: 761000\n",
      "    num_steps_sampled: 761000\n",
      "    num_steps_trained: 761000\n",
      "  iterations_since_restore: 761\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.01169036334914\n",
      "    ram_util_percent: 36.31916798314903\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03707811428274368\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.031180554956613\n",
      "    mean_inference_ms: 2.1249290402742855\n",
      "    mean_raw_obs_processing_ms: 43.59148633411278\n",
      "  time_since_restore: 50329.26967263222\n",
      "  time_this_iter_s: 1330.1518440246582\n",
      "  time_total_s: 50329.26967263222\n",
      "  timers:\n",
      "    learn_throughput: 1313.42\n",
      "    learn_time_ms: 761.371\n",
      "    load_throughput: 41513.746\n",
      "    load_time_ms: 24.088\n",
      "    sample_throughput: 1.3\n",
      "    sample_time_ms: 769197.839\n",
      "    update_time_ms: 5.21\n",
      "  timestamp: 1635332917\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 761000\n",
      "  training_iteration: 761\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   761</td><td style=\"text-align: right;\">         50329.3</td><td style=\"text-align: right;\">761000</td><td style=\"text-align: right;\">  6.5298</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">                 3.9</td><td style=\"text-align: right;\">             14.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 762000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_11-33-02\n",
      "  done: false\n",
      "  episode_len_mean: 13.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 6.7549\n",
      "  episode_reward_min: 3.9\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 3511\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.25711478093462586\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7022062109576331\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008772746244168792\n",
      "          policy_loss: 0.011485522985458374\n",
      "          total_loss: 0.15635514474577375\n",
      "          vf_explained_var: 0.9790595769882202\n",
      "          vf_loss: 0.14963608160614966\n",
      "    num_agent_steps_sampled: 762000\n",
      "    num_agent_steps_trained: 762000\n",
      "    num_steps_sampled: 762000\n",
      "    num_steps_trained: 762000\n",
      "  iterations_since_restore: 762\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.240363462458156\n",
      "    ram_util_percent: 36.50707795313247\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03707981090679589\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.040812908488252\n",
      "    mean_inference_ms: 2.12508098554211\n",
      "    mean_raw_obs_processing_ms: 45.49333107985831\n",
      "  time_since_restore: 51795.116354227066\n",
      "  time_this_iter_s: 1465.8466815948486\n",
      "  time_total_s: 51795.116354227066\n",
      "  timers:\n",
      "    learn_throughput: 1313.963\n",
      "    learn_time_ms: 761.056\n",
      "    load_throughput: 41316.394\n",
      "    load_time_ms: 24.203\n",
      "    sample_throughput: 1.209\n",
      "    sample_time_ms: 827033.964\n",
      "    update_time_ms: 5.198\n",
      "  timestamp: 1635334382\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 762000\n",
      "  training_iteration: 762\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   762</td><td style=\"text-align: right;\">         51795.1</td><td style=\"text-align: right;\">762000</td><td style=\"text-align: right;\">  6.7549</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">                 3.9</td><td style=\"text-align: right;\">             13.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 763000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_11-54-57\n",
      "  done: false\n",
      "  episode_len_mean: 14.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 6.8098\n",
      "  episode_reward_min: 0.4300000000000044\n",
      "  episodes_this_iter: 68\n",
      "  episodes_total: 3579\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.25711478093462586\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8489180492030249\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01757746285561102\n",
      "          policy_loss: 0.02965494067304664\n",
      "          total_loss: 0.21583045263671213\n",
      "          vf_explained_var: 0.9727251529693604\n",
      "          vf_loss: 0.19014526986413532\n",
      "    num_agent_steps_sampled: 763000\n",
      "    num_agent_steps_trained: 763000\n",
      "    num_steps_sampled: 763000\n",
      "    num_steps_trained: 763000\n",
      "  iterations_since_restore: 763\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.31263326226013\n",
      "    ram_util_percent: 36.64093816631131\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708132394533638\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.04851569130593\n",
      "    mean_inference_ms: 2.1252217218722347\n",
      "    mean_raw_obs_processing_ms: 47.03701804551481\n",
      "  time_since_restore: 53109.3143825531\n",
      "  time_this_iter_s: 1314.1980283260345\n",
      "  time_total_s: 53109.3143825531\n",
      "  timers:\n",
      "    learn_throughput: 1318.039\n",
      "    learn_time_ms: 758.703\n",
      "    load_throughput: 41568.425\n",
      "    load_time_ms: 24.057\n",
      "    sample_throughput: 1.111\n",
      "    sample_time_ms: 899907.63\n",
      "    update_time_ms: 5.19\n",
      "  timestamp: 1635335697\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 763000\n",
      "  training_iteration: 763\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   763</td><td style=\"text-align: right;\">         53109.3</td><td style=\"text-align: right;\">763000</td><td style=\"text-align: right;\">  6.8098</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                0.43</td><td style=\"text-align: right;\">             14.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 764000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_12-19-10\n",
      "  done: false\n",
      "  episode_len_mean: 13.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 7.140499999999999\n",
      "  episode_reward_min: 3.83\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 3654\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.25711478093462586\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6164531545506583\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006433831452506884\n",
      "          policy_loss: 0.0635380859590239\n",
      "          total_loss: 0.1851467865208785\n",
      "          vf_explained_var: 0.9805253744125366\n",
      "          vf_loss: 0.12611899822950362\n",
      "    num_agent_steps_sampled: 764000\n",
      "    num_agent_steps_trained: 764000\n",
      "    num_steps_sampled: 764000\n",
      "    num_steps_trained: 764000\n",
      "  iterations_since_restore: 764\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.35870718765075\n",
      "    ram_util_percent: 36.81394114809455\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708312434098364\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.059540907005715\n",
      "    mean_inference_ms: 2.1253932010525998\n",
      "    mean_raw_obs_processing_ms: 48.90643658292238\n",
      "  time_since_restore: 54562.197350263596\n",
      "  time_this_iter_s: 1452.882967710495\n",
      "  time_total_s: 54562.197350263596\n",
      "  timers:\n",
      "    learn_throughput: 1320.152\n",
      "    learn_time_ms: 757.489\n",
      "    load_throughput: 41307.931\n",
      "    load_time_ms: 24.208\n",
      "    sample_throughput: 1.004\n",
      "    sample_time_ms: 995635.923\n",
      "    update_time_ms: 5.179\n",
      "  timestamp: 1635337150\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 764000\n",
      "  training_iteration: 764\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   764</td><td style=\"text-align: right;\">         54562.2</td><td style=\"text-align: right;\">764000</td><td style=\"text-align: right;\">  7.1405</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                3.83</td><td style=\"text-align: right;\">             13.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 765000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_12-41-27\n",
      "  done: false\n",
      "  episode_len_mean: 14.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 6.912599999999998\n",
      "  episode_reward_min: 3.8200000000000003\n",
      "  episodes_this_iter: 68\n",
      "  episodes_total: 3722\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.25711478093462586\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8132523953914642\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011403479508872567\n",
      "          policy_loss: -0.05740652937028143\n",
      "          total_loss: 0.14232147940331036\n",
      "          vf_explained_var: 0.9609628319740295\n",
      "          vf_loss: 0.20492853191163804\n",
      "    num_agent_steps_sampled: 765000\n",
      "    num_agent_steps_trained: 765000\n",
      "    num_steps_sampled: 765000\n",
      "    num_steps_trained: 765000\n",
      "  iterations_since_restore: 765\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.675117862755364\n",
      "    ram_util_percent: 38.25007857517024\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037086612569090294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.070299616320217\n",
      "    mean_inference_ms: 2.125655321844623\n",
      "    mean_raw_obs_processing_ms: 50.47339974476101\n",
      "  time_since_restore: 55899.66209316254\n",
      "  time_this_iter_s: 1337.464742898941\n",
      "  time_total_s: 55899.66209316254\n",
      "  timers:\n",
      "    learn_throughput: 1313.533\n",
      "    learn_time_ms: 761.306\n",
      "    load_throughput: 41360.926\n",
      "    load_time_ms: 24.177\n",
      "    sample_throughput: 0.979\n",
      "    sample_time_ms: 1021499.408\n",
      "    update_time_ms: 3.496\n",
      "  timestamp: 1635338487\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 765000\n",
      "  training_iteration: 765\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   765</td><td style=\"text-align: right;\">         55899.7</td><td style=\"text-align: right;\">765000</td><td style=\"text-align: right;\">  6.9126</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">                3.82</td><td style=\"text-align: right;\">             14.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 766000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_13-06-31\n",
      "  done: false\n",
      "  episode_len_mean: 13.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 7.007299999999998\n",
      "  episode_reward_min: 4.770000000000001\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 3799\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.25711478093462586\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6670134835773044\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007790741306554239\n",
      "          policy_loss: 0.006299662672811084\n",
      "          total_loss: 0.14109856221410963\n",
      "          vf_explained_var: 0.9762188196182251\n",
      "          vf_loss: 0.13946592124799886\n",
      "    num_agent_steps_sampled: 766000\n",
      "    num_agent_steps_trained: 766000\n",
      "    num_steps_sampled: 766000\n",
      "    num_steps_trained: 766000\n",
      "  iterations_since_restore: 766\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.47618825722274\n",
      "    ram_util_percent: 38.51486486486486\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037094071369356094\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.08411978581053\n",
      "    mean_inference_ms: 2.1259510175844856\n",
      "    mean_raw_obs_processing_ms: 52.42748980611205\n",
      "  time_since_restore: 57403.89861249924\n",
      "  time_this_iter_s: 1504.2365193367004\n",
      "  time_total_s: 57403.89861249924\n",
      "  timers:\n",
      "    learn_throughput: 1314.548\n",
      "    learn_time_ms: 760.718\n",
      "    load_throughput: 41317.941\n",
      "    load_time_ms: 24.203\n",
      "    sample_throughput: 0.903\n",
      "    sample_time_ms: 1107273.581\n",
      "    update_time_ms: 3.505\n",
      "  timestamp: 1635339991\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 766000\n",
      "  training_iteration: 766\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   766</td><td style=\"text-align: right;\">         57403.9</td><td style=\"text-align: right;\">766000</td><td style=\"text-align: right;\">  7.0073</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                4.77</td><td style=\"text-align: right;\">             13.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 767000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_13-22-09\n",
      "  done: false\n",
      "  episode_len_mean: 16.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 7.186099999999999\n",
      "  episode_reward_min: 2.520000000000012\n",
      "  episodes_this_iter: 48\n",
      "  episodes_total: 3847\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.25711478093462586\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6788589000701905\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00937454644690868\n",
      "          policy_loss: 0.07940566167235374\n",
      "          total_loss: 0.3916327837440703\n",
      "          vf_explained_var: 0.9504621624946594\n",
      "          vf_loss: 0.3166053710712327\n",
      "    num_agent_steps_sampled: 767000\n",
      "    num_agent_steps_trained: 767000\n",
      "    num_steps_sampled: 767000\n",
      "    num_steps_trained: 767000\n",
      "  iterations_since_restore: 767\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.77535474234503\n",
      "    ram_util_percent: 38.21553398058253\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370968436525677\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.090766029442275\n",
      "    mean_inference_ms: 2.1261037313593363\n",
      "    mean_raw_obs_processing_ms: 53.390502045088766\n",
      "  time_since_restore: 58341.859411001205\n",
      "  time_this_iter_s: 937.9607985019684\n",
      "  time_total_s: 58341.859411001205\n",
      "  timers:\n",
      "    learn_throughput: 1313.032\n",
      "    learn_time_ms: 761.596\n",
      "    load_throughput: 41465.933\n",
      "    load_time_ms: 24.116\n",
      "    sample_throughput: 0.874\n",
      "    sample_time_ms: 1144194.399\n",
      "    update_time_ms: 3.513\n",
      "  timestamp: 1635340929\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 767000\n",
      "  training_iteration: 767\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>RUNNING </td><td>192.168.3.5:206</td><td style=\"text-align: right;\">   767</td><td style=\"text-align: right;\">         58341.9</td><td style=\"text-align: right;\">767000</td><td style=\"text-align: right;\">  7.1861</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                2.52</td><td style=\"text-align: right;\">             16.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=201)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "2021-10-27 13:26:31,013\tERROR trial_runner.py:773 -- Trial PPO_my_env_f560f_00000: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 739, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 746, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 82, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/worker.py\", line 1621, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(timeout): \u001b[36mray::PPO.train()\u001b[39m (pid=206, ip=192.168.3.5, repr=PPO)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 648, in train\n",
      "    raise e\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 637, in train\n",
      "    result = Trainable.train(self)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/tune/trainable.py\", line 237, in train\n",
      "    result = self.step()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\", line 193, in step\n",
      "    res = next(self.train_exec_impl)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 756, in __next__\n",
      "    return next(self.built_iterator)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "    for item in it:\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "    for item in it:\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 876, in apply_flatten\n",
      "    for item in it:\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 828, in add_wait_hooks\n",
      "    item = next(it)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 471, in base_iterator\n",
      "    yield ray.get(futures, timeout=timeout)\n",
      "ray.exceptions.RayTaskError(timeout): \u001b[36mray::RolloutWorker.par_iter_next()\u001b[39m (pid=201, ip=192.168.3.5, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fe755175048>)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "    return next(self.local_it)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 346, in gen_rollouts\n",
      "    yield self.sample()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 744, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\n",
      "    item = next(self.rollout_provider)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 615, in _env_runner\n",
      "    sample_collector=sample_collector,\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 940, in _process_observations\n",
      "    env_id)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/env/base_env.py\", line 370, in try_reset\n",
      "    return {_DUMMY_AGENT_ID: self.vector_env.reset_at(env_id)}\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/rllib/env/vector_env.py\", line 167, in reset_at\n",
      "    return self.envs[index].reset()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/core.py\", line 277, in reset\n",
      "    return self.env.reset(**kwargs)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/core.py\", line 237, in reset\n",
      "    return self.env.reset(**kwargs)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/core.py\", line 237, in reset\n",
      "    return self.env.reset(**kwargs)\n",
      "  File \"/IGLU-Minecraft/wrappers_2.py\", line 68, in reset\n",
      "    return self.observation(super().reset())\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/core.py\", line 237, in reset\n",
      "    return self.env.reset(**kwargs)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/wrappers/time_limit.py\", line 25, in reset\n",
      "    return self.env.reset(**kwargs)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/iglu/env.py\", line 107, in reset\n",
      "    obs = self.real_reset()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/iglu/env.py\", line 127, in real_reset\n",
      "    obs = super().reset()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/minerl_patched-0.1.0-py3.7-linux-x86_64.egg/minerl_patched/env/_singleagent.py\", line 23, in reset\n",
      "    multi_obs = super().reset()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/minerl_patched-0.1.0-py3.7-linux-x86_64.egg/minerl_patched/env/_multiagent.py\", line 477, in reset\n",
      "    return self._peek_obs()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/minerl_patched-0.1.0-py3.7-linux-x86_64.egg/minerl_patched/env/_multiagent.py\", line 636, in _peek_obs\n",
      "    obs = comms.recv_message(instance.client_socket)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/minerl_patched-0.1.0-py3.7-linux-x86_64.egg/minerl_patched/env/comms.py\", line 63, in recv_message\n",
      "    lengthbuf = recvall(sock, 4)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/minerl_patched-0.1.0-py3.7-linux-x86_64.egg/minerl_patched/env/comms.py\", line 73, in recvall\n",
      "    newbuf = sock.recv(count)\n",
      "socket.timeout: timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f560f_00000:\n",
      "  agent_timesteps_total: 767000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-27_13-22-09\n",
      "  done: false\n",
      "  episode_len_mean: 16.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 7.186099999999999\n",
      "  episode_reward_min: 2.520000000000012\n",
      "  episodes_this_iter: 48\n",
      "  episodes_total: 3847\n",
      "  experiment_id: f127dc46046f4004934563dcc31441c2\n",
      "  experiment_tag: '0'\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.25711478093462586\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6788589000701905\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00937454644690868\n",
      "          policy_loss: 0.07940566167235374\n",
      "          total_loss: 0.3916327837440703\n",
      "          vf_explained_var: 0.9504621624946594\n",
      "          vf_loss: 0.3166053710712327\n",
      "    num_agent_steps_sampled: 767000\n",
      "    num_agent_steps_trained: 767000\n",
      "    num_steps_sampled: 767000\n",
      "    num_steps_trained: 767000\n",
      "  iterations_since_restore: 767\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.77535474234503\n",
      "    ram_util_percent: 38.21553398058253\n",
      "  pid: 206\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370968436525677\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.090766029442275\n",
      "    mean_inference_ms: 2.1261037313593363\n",
      "    mean_raw_obs_processing_ms: 53.390502045088766\n",
      "  time_since_restore: 58341.859411001205\n",
      "  time_this_iter_s: 937.9607985019684\n",
      "  time_total_s: 58341.859411001205\n",
      "  timers:\n",
      "    learn_throughput: 1313.032\n",
      "    learn_time_ms: 761.596\n",
      "    load_throughput: 41465.933\n",
      "    load_time_ms: 24.116\n",
      "    sample_throughput: 0.874\n",
      "    sample_time_ms: 1144194.399\n",
      "    update_time_ms: 3.513\n",
      "  timestamp: 1635340929\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 767000\n",
      "  training_iteration: 767\n",
      "  trial_id: f560f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 410<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b55ecbc2d784c27bd4b9987902d2f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-27 13:26:41,031\tWARNING util.py:164 -- The `process_trial` operation took 10.018 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 ERROR)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">   767</td><td style=\"text-align: right;\">         58341.9</td><td style=\"text-align: right;\">767000</td><td style=\"text-align: right;\">  7.1861</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                2.52</td><td style=\"text-align: right;\">             16.88</td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                      </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td style=\"text-align: right;\">           1</td><td>/root/ray_results/PPO_2021-10-26_21-09-08/PPO_my_env_f560f_00000_0_2021-10-26_21-09-08/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/27.7 GiB heap, 0.0/13.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-26_21-09-08<br>Number of trials: 1/1 (1 ERROR)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">   767</td><td style=\"text-align: right;\">         58341.9</td><td style=\"text-align: right;\">767000</td><td style=\"text-align: right;\">  7.1861</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                2.52</td><td style=\"text-align: right;\">             16.88</td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                      </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f560f_00000</td><td style=\"text-align: right;\">           1</td><td>/root/ray_results/PPO_2021-10-26_21-09-08/PPO_my_env_f560f_00000_0_2021-10-26_21-09-08/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PPO_my_env_f560f_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_89/2646892878.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         },\n\u001b[1;32m     29\u001b[0m         \u001b[0mloggers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWandbLogger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         checkpoint_at_end=True)\n\u001b[0m",
      "\u001b[0;32m~/miniconda/envs/py37/lib/python3.7/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint, _remote)\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [PPO_my_env_f560f_00000])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-28 14:45:24,660\tERROR worker.py:475 -- print_logs: Connection closed by server.\n",
      "2021-10-28 14:45:24,661\tERROR worker.py:1217 -- listen_error_messages_raylet: Connection closed by server.\n",
      "2021-10-28 14:45:24,661\tERROR import_thread.py:88 -- ImportThread: Connection closed by server.\n"
     ]
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "analysis = tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 1,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 1000,\n",
    "             #\"gamma\": 0.99,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO MultiTask (C3, C17, C32) pretrained (AngelaCNN) (3 noops after placement) r: -0.01\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger],\n",
    "        checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
