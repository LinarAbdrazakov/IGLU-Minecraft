{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "#from models import VisualEncoder\n",
    "from train import *\n",
    "from wrappers_2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9fd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=2, stride=2, padding=0),  \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(), \n",
    "            nn.Conv2d(128, 256, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import one_hot\n",
    "\n",
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        visual_features_dim = 512\n",
    "        target_features_dim = 9 * 11 * 11 \n",
    "        self.visual_encoder = VisualEncoder()\n",
    "        self.visual_encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AngelaCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.target_encoder = nn.Sequential(\n",
    "            nn.Conv3d(7, 1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        policy_hidden_dim = 256 \n",
    "        self.policy_network = nn.Sequential(\n",
    "            nn.Linear(visual_features_dim + target_features_dim, 2048),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, policy_hidden_dim),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.action_head = nn.Linear(policy_hidden_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(policy_hidden_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.visual_encoder.cuda()\n",
    "            self.target_encoder.cuda()\n",
    "            self.policy_network.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs']\n",
    "        pov = obs['pov'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        target = one_hot(obs['target_grid'].long(), num_classes=7).permute(0, 4, 1, 2, 3).float()\n",
    "        if self.use_cuda:\n",
    "            pov.cuda()\n",
    "            target.cuda()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            visual_features = self.visual_encoder(pov)\n",
    "            \n",
    "        target_features = self.target_encoder(target)\n",
    "        target_features = target_features.reshape(target_features.shape[0], -1)\n",
    "        features = torch.cat([visual_features, target_features], dim=1)\n",
    "        features = self.policy_network(features)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8668add4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11609600"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual_features_dim = 512\n",
    "target_features_dim = 9 * 11 * 11\n",
    "policy_hidden_dim = 256 \n",
    "\n",
    "policy_network = nn.Sequential(\n",
    "    nn.Linear(visual_features_dim + target_features_dim, 2048),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(2048, 2048),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(512, 512),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(256, policy_hidden_dim),\n",
    "    nn.ELU(),\n",
    ")\n",
    "\n",
    "sum(p.numel() for p in policy_network.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc09c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualObservationWrapper(ObsWrapper):\n",
    "    def __init__(self, env, include_target=False):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = {   \n",
    "            'pov': gym.spaces.Box(low=0, high=255, shape=(64, 64, 3)),\n",
    "            'inventory': gym.spaces.Box(low=0.0, high=20.0, shape=(6,)),\n",
    "            'compass': gym.spaces.Box(low=-180.0, high=180.0, shape=(1,))\n",
    "        }\n",
    "        if include_target:\n",
    "            self.observation_space['target_grid'] = \\\n",
    "                gym.spaces.Box(low=0, high=6, shape=(9, 11, 11))\n",
    "        self.observation_space = gym.spaces.Dict(self.observation_space)\n",
    "\n",
    "    def observation(self, obs, reward=None, done=None, info=None):\n",
    "        if info is not None:\n",
    "            if 'target_grid' in info:\n",
    "                target_grid = info['target_grid']\n",
    "                del info['target_grid']\n",
    "            else:\n",
    "                logger.error(f'info: {info}')\n",
    "                if hasattr(self.unwrapped, 'should_reset'):\n",
    "                    self.unwrapped.should_reset(True)\n",
    "                target_grid = self.env.unwrapped.tasks.current.target_grid\n",
    "        else:\n",
    "            target_grid = self.env.unwrapped.tasks.current.target_grid\n",
    "        return {\n",
    "            'pov': obs['pov'].astype(np.float32),\n",
    "            'inventory': obs['inventory'],\n",
    "            'compass': np.array([obs['compass']['angle'].item()]),\n",
    "            'target_grid': target_grid\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "class RewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def reward(self, rew):\n",
    "        if rew == 0:\n",
    "            rew = -0.01\n",
    "        return rew\n",
    "    \n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "    env.update_taskset(TaskSet(preset=['C3', 'C17', 'C20', 'C22', 'C32', 'C40', 'C85', 'C87', 'C93']))\n",
    "    #env = PovOnlyWrapper(env)\n",
    "    env = VisualObservationWrapper(env, include_target=True)\n",
    "    env = SelectAndPlace(env)\n",
    "    env = Discretization(env, flat_action_space('human-level'))\n",
    "    env = RewardWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n",
      "2021-10-29 21:04:12,888\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-10-29 21:04:12,902\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=625567)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625567)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO MultiTask <=10 pretrained (AngelaCNN + MLP 8) (3 noops after placement) r: -0.01</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/c4887_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/c4887_00000</a><br/>\n",
       "                Run data is saved locally in <code>/IGLU-Minecraft/wandb/run-20211029_210413-c4887_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625567)\u001b[0m 2021-10-29 21:04:16,352\tWARNING ppo.py:143 -- `train_batch_size` (5000) cannot be achieved with your other settings (num_workers=3 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1666.\n",
      "\u001b[2m\u001b[36m(pid=625567)\u001b[0m 2021-10-29 21:04:16,352\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=625567)\u001b[0m 2021-10-29 21:04:16,352\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625567)\u001b[0m 2021-10-29 21:04:22,574\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warn(RuntimeWarning(msg))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warn(RuntimeWarning(msg))\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 9996\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-08-02\n",
      "  done: false\n",
      "  episode_len_mean: 404.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.789999999999963\n",
      "  episode_reward_mean: -9.165833333333303\n",
      "  episode_reward_min: -19.480000000000178\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 24\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.8832698318693373\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.005976090252176761\n",
      "          policy_loss: -0.008823641306824155\n",
      "          total_loss: 0.36435518509939185\n",
      "          vf_explained_var: -0.370869904756546\n",
      "          vf_loss: 0.40081630689854403\n",
      "    num_agent_steps_sampled: 9996\n",
      "    num_agent_steps_trained: 9996\n",
      "    num_steps_sampled: 9996\n",
      "    num_steps_trained: 9996\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.33492063492064\n",
      "    ram_util_percent: 42.73396825396826\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04759408531337753\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 54.87159231040749\n",
      "    mean_inference_ms: 4.822155251146758\n",
      "    mean_raw_obs_processing_ms: 0.2682924103720186\n",
      "  time_since_restore: 220.17458963394165\n",
      "  time_this_iter_s: 220.17458963394165\n",
      "  time_total_s: 220.17458963394165\n",
      "  timers:\n",
      "    learn_throughput: 592.048\n",
      "    learn_time_ms: 16883.762\n",
      "    load_throughput: 88560.957\n",
      "    load_time_ms: 112.871\n",
      "    sample_throughput: 49.213\n",
      "    sample_time_ms: 203116.724\n",
      "    update_time_ms: 28.891\n",
      "  timestamp: 1635541682\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9996\n",
      "  training_iteration: 1\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         220.175</td><td style=\"text-align: right;\">9996</td><td style=\"text-align: right;\">-9.16583</td><td style=\"text-align: right;\">               -3.79</td><td style=\"text-align: right;\">              -19.48</td><td style=\"text-align: right;\">               404</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 19992\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-10-04\n",
      "  done: false\n",
      "  episode_len_mean: 404.3333333333333\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.789999999999963\n",
      "  episode_reward_mean: -7.538541666666632\n",
      "  episode_reward_min: -19.480000000000178\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 48\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.8783106999519545\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.005715139597830428\n",
      "          policy_loss: -0.006147194440420876\n",
      "          total_loss: 0.16474733425375934\n",
      "          vf_explained_var: 0.02605714462697506\n",
      "          vf_loss: 0.19853460734643433\n",
      "    num_agent_steps_sampled: 19992\n",
      "    num_agent_steps_trained: 19992\n",
      "    num_steps_sampled: 19992\n",
      "    num_steps_trained: 19992\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.13930635838149\n",
      "    ram_util_percent: 48.998843930635836\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04816603277714368\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 47.567635630527825\n",
      "    mean_inference_ms: 4.811930938104623\n",
      "    mean_raw_obs_processing_ms: 0.2608588058444137\n",
      "  time_since_restore: 341.95588541030884\n",
      "  time_this_iter_s: 121.78129577636719\n",
      "  time_total_s: 341.95588541030884\n",
      "  timers:\n",
      "    learn_throughput: 589.644\n",
      "    learn_time_ms: 16952.607\n",
      "    load_throughput: 88502.164\n",
      "    load_time_ms: 112.946\n",
      "    sample_throughput: 64.968\n",
      "    sample_time_ms: 153859.931\n",
      "    update_time_ms: 25.46\n",
      "  timestamp: 1635541804\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19992\n",
      "  training_iteration: 2\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         341.956</td><td style=\"text-align: right;\">19992</td><td style=\"text-align: right;\">-7.53854</td><td style=\"text-align: right;\">               -3.79</td><td style=\"text-align: right;\">              -19.48</td><td style=\"text-align: right;\">           404.333</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 29988\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-12-04\n",
      "  done: false\n",
      "  episode_len_mean: 402.18055555555554\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.619999999999967\n",
      "  episode_reward_mean: -6.589583333333296\n",
      "  episode_reward_min: -19.480000000000178\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 72\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.87183359321366\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.005993062470161649\n",
      "          policy_loss: -0.00039545998701618777\n",
      "          total_loss: 0.08547777000050515\n",
      "          vf_explained_var: -0.030956359580159187\n",
      "          vf_loss: 0.11339295119732846\n",
      "    num_agent_steps_sampled: 29988\n",
      "    num_agent_steps_trained: 29988\n",
      "    num_steps_sampled: 29988\n",
      "    num_steps_trained: 29988\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.07093023255814\n",
      "    ram_util_percent: 49.10406976744187\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04819038444378024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 43.49646400375005\n",
      "    mean_inference_ms: 4.806447949059468\n",
      "    mean_raw_obs_processing_ms: 0.2568867186273118\n",
      "  time_since_restore: 462.29682874679565\n",
      "  time_this_iter_s: 120.34094333648682\n",
      "  time_total_s: 462.29682874679565\n",
      "  timers:\n",
      "    learn_throughput: 590.069\n",
      "    learn_time_ms: 16940.397\n",
      "    load_throughput: 89615.584\n",
      "    load_time_ms: 111.543\n",
      "    sample_throughput: 72.965\n",
      "    sample_time_ms: 136997.666\n",
      "    update_time_ms: 22.976\n",
      "  timestamp: 1635541924\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29988\n",
      "  training_iteration: 3\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         462.297</td><td style=\"text-align: right;\">29988</td><td style=\"text-align: right;\">-6.58958</td><td style=\"text-align: right;\">               -3.62</td><td style=\"text-align: right;\">              -19.48</td><td style=\"text-align: right;\">           402.181</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 39984\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-14-28\n",
      "  done: false\n",
      "  episode_len_mean: 400.7878787878788\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4699999999999718\n",
      "  episode_reward_mean: -6.128989898989861\n",
      "  episode_reward_min: -19.480000000000178\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 99\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.8719291316138373\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.005865483286475509\n",
      "          policy_loss: -0.005399203996579998\n",
      "          total_loss: 0.06425475380706808\n",
      "          vf_explained_var: -0.12049980461597443\n",
      "          vf_loss: 0.09720015220489617\n",
      "    num_agent_steps_sampled: 39984\n",
      "    num_agent_steps_trained: 39984\n",
      "    num_steps_sampled: 39984\n",
      "    num_steps_trained: 39984\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.95073170731708\n",
      "    ram_util_percent: 48.81707317073171\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04811428208220197\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.719928611899476\n",
      "    mean_inference_ms: 4.803768276058415\n",
      "    mean_raw_obs_processing_ms: 0.6167868647429129\n",
      "  time_since_restore: 605.8582272529602\n",
      "  time_this_iter_s: 143.56139850616455\n",
      "  time_total_s: 605.8582272529602\n",
      "  timers:\n",
      "    learn_throughput: 589.217\n",
      "    learn_time_ms: 16964.875\n",
      "    load_throughput: 89658.017\n",
      "    load_time_ms: 111.49\n",
      "    sample_throughput: 74.407\n",
      "    sample_time_ms: 134342.542\n",
      "    update_time_ms: 20.287\n",
      "  timestamp: 1635542068\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39984\n",
      "  training_iteration: 4\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         605.858</td><td style=\"text-align: right;\">39984</td><td style=\"text-align: right;\">-6.12899</td><td style=\"text-align: right;\">               -2.47</td><td style=\"text-align: right;\">              -19.48</td><td style=\"text-align: right;\">           400.788</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 49980\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-16-37\n",
      "  done: false\n",
      "  episode_len_mean: 398.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.7700000000000019\n",
      "  episode_reward_mean: -4.96849999999996\n",
      "  episode_reward_min: -14.849999999999962\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 123\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.859299398895003\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.006903323522985729\n",
      "          policy_loss: -0.007857704982131671\n",
      "          total_loss: 0.0786986000640875\n",
      "          vf_explained_var: -0.19872279465198517\n",
      "          vf_loss: 0.11376863251679245\n",
      "    num_agent_steps_sampled: 49980\n",
      "    num_agent_steps_trained: 49980\n",
      "    num_steps_sampled: 49980\n",
      "    num_steps_trained: 49980\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.15652173913043\n",
      "    ram_util_percent: 50.25978260869565\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04832553264783799\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.40603696111289\n",
      "    mean_inference_ms: 4.80376888448745\n",
      "    mean_raw_obs_processing_ms: 0.8641615813239828\n",
      "  time_since_restore: 734.5988454818726\n",
      "  time_this_iter_s: 128.74061822891235\n",
      "  time_total_s: 734.5988454818726\n",
      "  timers:\n",
      "    learn_throughput: 589.279\n",
      "    learn_time_ms: 16963.114\n",
      "    load_throughput: 89502.496\n",
      "    load_time_ms: 111.684\n",
      "    sample_throughput: 77.01\n",
      "    sample_time_ms: 129800.716\n",
      "    update_time_ms: 18.965\n",
      "  timestamp: 1635542197\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49980\n",
      "  training_iteration: 5\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         734.599</td><td style=\"text-align: right;\">49980</td><td style=\"text-align: right;\"> -4.9685</td><td style=\"text-align: right;\">               -0.77</td><td style=\"text-align: right;\">              -14.85</td><td style=\"text-align: right;\">            398.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 59976\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-18-47\n",
      "  done: false\n",
      "  episode_len_mean: 396.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.7700000000000019\n",
      "  episode_reward_mean: -4.635599999999959\n",
      "  episode_reward_min: -14.849999999999962\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 149\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.8567220208991286\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.007402953224623023\n",
      "          policy_loss: -0.008427571315859627\n",
      "          total_loss: 0.014109141534815232\n",
      "          vf_explained_var: -0.12967818975448608\n",
      "          vf_loss: 0.049623341272736536\n",
      "    num_agent_steps_sampled: 59976\n",
      "    num_agent_steps_trained: 59976\n",
      "    num_steps_sampled: 59976\n",
      "    num_steps_trained: 59976\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.00972972972973\n",
      "    ram_util_percent: 51.14216216216216\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0483764692781766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.041744267316886\n",
      "    mean_inference_ms: 4.814451780425329\n",
      "    mean_raw_obs_processing_ms: 1.0938638443563569\n",
      "  time_since_restore: 864.7073605060577\n",
      "  time_this_iter_s: 130.10851502418518\n",
      "  time_total_s: 864.7073605060577\n",
      "  timers:\n",
      "    learn_throughput: 589.429\n",
      "    learn_time_ms: 16958.777\n",
      "    load_throughput: 89463.465\n",
      "    load_time_ms: 111.733\n",
      "    sample_throughput: 78.706\n",
      "    sample_time_ms: 127003.726\n",
      "    update_time_ms: 18.768\n",
      "  timestamp: 1635542327\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 59976\n",
      "  training_iteration: 6\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         864.707</td><td style=\"text-align: right;\">59976</td><td style=\"text-align: right;\"> -4.6356</td><td style=\"text-align: right;\">               -0.77</td><td style=\"text-align: right;\">              -14.85</td><td style=\"text-align: right;\">            396.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 69972\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-20-50\n",
      "  done: false\n",
      "  episode_len_mean: 394.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.7700000000000019\n",
      "  episode_reward_mean: -4.46909999999996\n",
      "  episode_reward_min: -14.849999999999962\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 174\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.843265289119166\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.00916006100470442\n",
      "          policy_loss: -0.012616257203949823\n",
      "          total_loss: -0.018872724291987907\n",
      "          vf_explained_var: 0.24371007084846497\n",
      "          vf_loss: 0.020344172762724287\n",
      "    num_agent_steps_sampled: 69972\n",
      "    num_agent_steps_trained: 69972\n",
      "    num_steps_sampled: 69972\n",
      "    num_steps_trained: 69972\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.43863636363636\n",
      "    ram_util_percent: 51.23352272727272\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.048448477360390596\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.933337904915025\n",
      "    mean_inference_ms: 4.824292455552104\n",
      "    mean_raw_obs_processing_ms: 1.2562412900406974\n",
      "  time_since_restore: 987.9815740585327\n",
      "  time_this_iter_s: 123.27421355247498\n",
      "  time_total_s: 987.9815740585327\n",
      "  timers:\n",
      "    learn_throughput: 589.541\n",
      "    learn_time_ms: 16955.565\n",
      "    load_throughput: 89329.768\n",
      "    load_time_ms: 111.9\n",
      "    sample_throughput: 80.595\n",
      "    sample_time_ms: 124028.049\n",
      "    update_time_ms: 18.854\n",
      "  timestamp: 1635542450\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 69972\n",
      "  training_iteration: 7\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         987.982</td><td style=\"text-align: right;\">69972</td><td style=\"text-align: right;\"> -4.4691</td><td style=\"text-align: right;\">               -0.77</td><td style=\"text-align: right;\">              -14.85</td><td style=\"text-align: right;\">            394.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 79968\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-23-09\n",
      "  done: false\n",
      "  episode_len_mean: 393.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.1999999999999758\n",
      "  episode_reward_mean: -4.2322999999999595\n",
      "  episode_reward_min: -13.509999999999955\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 200\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.8299141918492112\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.009429405921508866\n",
      "          policy_loss: -0.011609189207546222\n",
      "          total_loss: -0.028535528980896004\n",
      "          vf_explained_var: 0.31521379947662354\n",
      "          vf_loss: 0.009486920256655202\n",
      "    num_agent_steps_sampled: 79968\n",
      "    num_agent_steps_trained: 79968\n",
      "    num_steps_sampled: 79968\n",
      "    num_steps_trained: 79968\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.8075376884422\n",
      "    ram_util_percent: 50.928643216080395\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04854454364742102\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.118734595896058\n",
      "    mean_inference_ms: 4.831927953011932\n",
      "    mean_raw_obs_processing_ms: 1.2590163198423172\n",
      "  time_since_restore: 1127.2069709300995\n",
      "  time_this_iter_s: 139.22539687156677\n",
      "  time_total_s: 1127.2069709300995\n",
      "  timers:\n",
      "    learn_throughput: 589.331\n",
      "    learn_time_ms: 16961.598\n",
      "    load_throughput: 89504.488\n",
      "    load_time_ms: 111.682\n",
      "    sample_throughput: 80.755\n",
      "    sample_time_ms: 123781.949\n",
      "    update_time_ms: 20.533\n",
      "  timestamp: 1635542589\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79968\n",
      "  training_iteration: 8\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         1127.21</td><td style=\"text-align: right;\">79968</td><td style=\"text-align: right;\"> -4.2323</td><td style=\"text-align: right;\">                -3.2</td><td style=\"text-align: right;\">              -13.51</td><td style=\"text-align: right;\">            393.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 89964\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-25-09\n",
      "  done: false\n",
      "  episode_len_mean: 391.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.1999999999999758\n",
      "  episode_reward_mean: -4.057299999999961\n",
      "  episode_reward_min: -11.36999999999996\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 226\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.823943393658369\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.00990417727956674\n",
      "          policy_loss: -0.014482465195350158\n",
      "          total_loss: -0.033047017614301455\n",
      "          vf_explained_var: 0.48553383350372314\n",
      "          vf_loss: 0.007694045563465239\n",
      "    num_agent_steps_sampled: 89964\n",
      "    num_agent_steps_trained: 89964\n",
      "    num_steps_sampled: 89964\n",
      "    num_steps_trained: 89964\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.32117647058823\n",
      "    ram_util_percent: 51.39529411764706\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.048426139632079215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.46197324489718\n",
      "    mean_inference_ms: 4.831061514190423\n",
      "    mean_raw_obs_processing_ms: 1.2935815919083256\n",
      "  time_since_restore: 1246.8037357330322\n",
      "  time_this_iter_s: 119.59676480293274\n",
      "  time_total_s: 1246.8037357330322\n",
      "  timers:\n",
      "    learn_throughput: 589.357\n",
      "    learn_time_ms: 16960.852\n",
      "    load_throughput: 89576.142\n",
      "    load_time_ms: 111.592\n",
      "    sample_throughput: 82.33\n",
      "    sample_time_ms: 121413.562\n",
      "    update_time_ms: 22.937\n",
      "  timestamp: 1635542709\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 89964\n",
      "  training_iteration: 9\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">          1246.8</td><td style=\"text-align: right;\">89964</td><td style=\"text-align: right;\"> -4.0573</td><td style=\"text-align: right;\">                -3.2</td><td style=\"text-align: right;\">              -11.37</td><td style=\"text-align: right;\">            391.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 99960\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-27-09\n",
      "  done: false\n",
      "  episode_len_mean: 386.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.1999999999999758\n",
      "  episode_reward_mean: -3.885499999999961\n",
      "  episode_reward_min: -5.819999999999958\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 253\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.8010744956823497\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.011831489042012096\n",
      "          policy_loss: -0.016278078254216757\n",
      "          total_loss: -0.03441328588459227\n",
      "          vf_explained_var: 0.6044422388076782\n",
      "          vf_loss: 0.007509238083407076\n",
      "    num_agent_steps_sampled: 99960\n",
      "    num_agent_steps_trained: 99960\n",
      "    num_steps_sampled: 99960\n",
      "    num_steps_trained: 99960\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.42807017543858\n",
      "    ram_util_percent: 51.505847953216374\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04824593819425532\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.856522995593036\n",
      "    mean_inference_ms: 4.822588279026037\n",
      "    mean_raw_obs_processing_ms: 1.3448657469950507\n",
      "  time_since_restore: 1366.2603578567505\n",
      "  time_this_iter_s: 119.45662212371826\n",
      "  time_total_s: 1366.2603578567505\n",
      "  timers:\n",
      "    learn_throughput: 589.285\n",
      "    learn_time_ms: 16962.921\n",
      "    load_throughput: 89473.826\n",
      "    load_time_ms: 111.72\n",
      "    sample_throughput: 83.645\n",
      "    sample_time_ms: 119504.383\n",
      "    update_time_ms: 22.618\n",
      "  timestamp: 1635542829\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 99960\n",
      "  training_iteration: 10\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         1366.26</td><td style=\"text-align: right;\">99960</td><td style=\"text-align: right;\"> -3.8855</td><td style=\"text-align: right;\">                -3.2</td><td style=\"text-align: right;\">               -5.82</td><td style=\"text-align: right;\">            386.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 109956\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-29-26\n",
      "  done: false\n",
      "  episode_len_mean: 379.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.189999999999976\n",
      "  episode_reward_mean: -3.7997999999999625\n",
      "  episode_reward_min: -4.209999999999955\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 280\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.7708889447725737\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.014445696190251737\n",
      "          policy_loss: -0.014678755771910024\n",
      "          total_loss: -0.0317896332559932\n",
      "          vf_explained_var: 0.5103092193603516\n",
      "          vf_loss: 0.007708872475812578\n",
      "    num_agent_steps_sampled: 109956\n",
      "    num_agent_steps_trained: 109956\n",
      "    num_steps_sampled: 109956\n",
      "    num_steps_trained: 109956\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.96836734693879\n",
      "    ram_util_percent: 51.21683673469388\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0481249304654888\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.371413264362758\n",
      "    mean_inference_ms: 4.814471078866251\n",
      "    mean_raw_obs_processing_ms: 1.494201939914591\n",
      "  time_since_restore: 1504.12273812294\n",
      "  time_this_iter_s: 137.86238026618958\n",
      "  time_total_s: 1504.12273812294\n",
      "  timers:\n",
      "    learn_throughput: 589.153\n",
      "    learn_time_ms: 16966.724\n",
      "    load_throughput: 89461.149\n",
      "    load_time_ms: 111.736\n",
      "    sample_throughput: 89.836\n",
      "    sample_time_ms: 111269.7\n",
      "    update_time_ms: 23.183\n",
      "  timestamp: 1635542966\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 109956\n",
      "  training_iteration: 11\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         1504.12</td><td style=\"text-align: right;\">109956</td><td style=\"text-align: right;\"> -3.7998</td><td style=\"text-align: right;\">               -3.19</td><td style=\"text-align: right;\">               -4.21</td><td style=\"text-align: right;\">            379.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 119952\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-31-27\n",
      "  done: false\n",
      "  episode_len_mean: 374.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.189999999999976\n",
      "  episode_reward_mean: -3.7497999999999636\n",
      "  episode_reward_min: -4.089999999999957\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 306\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.748139734757252\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.013918578106001488\n",
      "          policy_loss: -0.013981476343340344\n",
      "          total_loss: -0.031169562809105614\n",
      "          vf_explained_var: 0.5569139719009399\n",
      "          vf_loss: 0.007509594696061985\n",
      "    num_agent_steps_sampled: 119952\n",
      "    num_agent_steps_trained: 119952\n",
      "    num_steps_sampled: 119952\n",
      "    num_steps_trained: 119952\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.20173410404625\n",
      "    ram_util_percent: 51.34855491329479\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0480435385885629\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.994316762312483\n",
      "    mean_inference_ms: 4.809116439805045\n",
      "    mean_raw_obs_processing_ms: 1.4977670605107465\n",
      "  time_since_restore: 1624.8881466388702\n",
      "  time_this_iter_s: 120.76540851593018\n",
      "  time_total_s: 1624.8881466388702\n",
      "  timers:\n",
      "    learn_throughput: 588.898\n",
      "    learn_time_ms: 16974.08\n",
      "    load_throughput: 89523.193\n",
      "    load_time_ms: 111.658\n",
      "    sample_throughput: 89.925\n",
      "    sample_time_ms: 111159.899\n",
      "    update_time_ms: 23.924\n",
      "  timestamp: 1635543087\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 119952\n",
      "  training_iteration: 12\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         1624.89</td><td style=\"text-align: right;\">119952</td><td style=\"text-align: right;\"> -3.7498</td><td style=\"text-align: right;\">               -3.19</td><td style=\"text-align: right;\">               -4.09</td><td style=\"text-align: right;\">            374.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 129948\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-33-26\n",
      "  done: false\n",
      "  episode_len_mean: 369.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.189999999999976\n",
      "  episode_reward_mean: -3.6993999999999647\n",
      "  episode_reward_min: -4.089999999999957\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 333\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.7288184948456595\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.016153578655671876\n",
      "          policy_loss: -0.015599416610267427\n",
      "          total_loss: -0.031735727802301064\n",
      "          vf_explained_var: 0.47395259141921997\n",
      "          vf_loss: 0.007921157692584934\n",
      "    num_agent_steps_sampled: 129948\n",
      "    num_agent_steps_trained: 129948\n",
      "    num_steps_sampled: 129948\n",
      "    num_steps_trained: 129948\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.48343195266273\n",
      "    ram_util_percent: 51.337869822485196\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047962071641804854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.67172009133802\n",
      "    mean_inference_ms: 4.805528317195704\n",
      "    mean_raw_obs_processing_ms: 1.5137049019553788\n",
      "  time_since_restore: 1743.2510681152344\n",
      "  time_this_iter_s: 118.36292147636414\n",
      "  time_total_s: 1743.2510681152344\n",
      "  timers:\n",
      "    learn_throughput: 588.549\n",
      "    learn_time_ms: 16984.154\n",
      "    load_throughput: 89372.797\n",
      "    load_time_ms: 111.846\n",
      "    sample_throughput: 90.093\n",
      "    sample_time_ms: 110952.301\n",
      "    update_time_ms: 23.789\n",
      "  timestamp: 1635543206\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 129948\n",
      "  training_iteration: 13\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         1743.25</td><td style=\"text-align: right;\">129948</td><td style=\"text-align: right;\"> -3.6994</td><td style=\"text-align: right;\">               -3.19</td><td style=\"text-align: right;\">               -4.09</td><td style=\"text-align: right;\">            369.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 139944\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-35-42\n",
      "  done: false\n",
      "  episode_len_mean: 366.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.139999999999977\n",
      "  episode_reward_mean: -3.660899999999966\n",
      "  episode_reward_min: -4.089999999999957\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 362\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.697187589580177\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.016535257152983613\n",
      "          policy_loss: -0.018392473618444215\n",
      "          total_loss: -0.033025577078517686\n",
      "          vf_explained_var: 0.4784652292728424\n",
      "          vf_loss: 0.009031720794229888\n",
      "    num_agent_steps_sampled: 139944\n",
      "    num_agent_steps_trained: 139944\n",
      "    num_steps_sampled: 139944\n",
      "    num_steps_trained: 139944\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.18505154639176\n",
      "    ram_util_percent: 51.03865979381443\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047891370734560006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.37503644608737\n",
      "    mean_inference_ms: 4.801687658502809\n",
      "    mean_raw_obs_processing_ms: 1.6058188114206182\n",
      "  time_since_restore: 1879.2360339164734\n",
      "  time_this_iter_s: 135.984965801239\n",
      "  time_total_s: 1879.2360339164734\n",
      "  timers:\n",
      "    learn_throughput: 588.684\n",
      "    learn_time_ms: 16980.241\n",
      "    load_throughput: 89252.08\n",
      "    load_time_ms: 111.997\n",
      "    sample_throughput: 90.71\n",
      "    sample_time_ms: 110197.783\n",
      "    update_time_ms: 24.291\n",
      "  timestamp: 1635543342\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 139944\n",
      "  training_iteration: 14\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         1879.24</td><td style=\"text-align: right;\">139944</td><td style=\"text-align: right;\"> -3.6609</td><td style=\"text-align: right;\">               -3.14</td><td style=\"text-align: right;\">               -4.09</td><td style=\"text-align: right;\">            366.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 149940\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-37-42\n",
      "  done: false\n",
      "  episode_len_mean: 363.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.139999999999977\n",
      "  episode_reward_mean: -3.6370999999999656\n",
      "  episode_reward_min: -4.059999999999958\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 389\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.6840950142624034\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.014594825186529912\n",
      "          policy_loss: -0.018851986059393637\n",
      "          total_loss: -0.03432929736808834\n",
      "          vf_explained_var: 0.5043028593063354\n",
      "          vf_loss: 0.00844467373089228\n",
      "    num_agent_steps_sampled: 149940\n",
      "    num_agent_steps_trained: 149940\n",
      "    num_steps_sampled: 149940\n",
      "    num_steps_trained: 149940\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.26725146198831\n",
      "    ram_util_percent: 50.91929824561404\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047847635098276144\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.13598608630582\n",
      "    mean_inference_ms: 4.799454079202097\n",
      "    mean_raw_obs_processing_ms: 1.6057741847016436\n",
      "  time_since_restore: 1999.4183735847473\n",
      "  time_this_iter_s: 120.18233966827393\n",
      "  time_total_s: 1999.4183735847473\n",
      "  timers:\n",
      "    learn_throughput: 588.627\n",
      "    learn_time_ms: 16981.885\n",
      "    load_throughput: 89422.129\n",
      "    load_time_ms: 111.784\n",
      "    sample_throughput: 91.421\n",
      "    sample_time_ms: 109340.25\n",
      "    update_time_ms: 24.843\n",
      "  timestamp: 1635543462\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 149940\n",
      "  training_iteration: 15\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         1999.42</td><td style=\"text-align: right;\">149940</td><td style=\"text-align: right;\"> -3.6371</td><td style=\"text-align: right;\">               -3.14</td><td style=\"text-align: right;\">               -4.06</td><td style=\"text-align: right;\">            363.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 159936\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-39-40\n",
      "  done: false\n",
      "  episode_len_mean: 359.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.139999999999977\n",
      "  episode_reward_mean: -3.5958999999999666\n",
      "  episode_reward_min: -3.989999999999959\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 417\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.673925769634736\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.018525136458072508\n",
      "          policy_loss: -0.020327709812639105\n",
      "          total_loss: -0.03250559601518843\n",
      "          vf_explained_var: 0.36277905106544495\n",
      "          vf_loss: 0.010856343619326431\n",
      "    num_agent_steps_sampled: 159936\n",
      "    num_agent_steps_trained: 159936\n",
      "    num_steps_sampled: 159936\n",
      "    num_steps_trained: 159936\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.06568047337278\n",
      "    ram_util_percent: 51.35739644970414\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047809897732292245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.911399902783373\n",
      "    mean_inference_ms: 4.797204185688022\n",
      "    mean_raw_obs_processing_ms: 1.6152269326158046\n",
      "  time_since_restore: 2117.493831396103\n",
      "  time_this_iter_s: 118.07545781135559\n",
      "  time_total_s: 2117.493831396103\n",
      "  timers:\n",
      "    learn_throughput: 588.547\n",
      "    learn_time_ms: 16984.194\n",
      "    load_throughput: 89227.805\n",
      "    load_time_ms: 112.028\n",
      "    sample_throughput: 92.442\n",
      "    sample_time_ms: 108132.258\n",
      "    update_time_ms: 27.065\n",
      "  timestamp: 1635543580\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 159936\n",
      "  training_iteration: 16\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         2117.49</td><td style=\"text-align: right;\">159936</td><td style=\"text-align: right;\"> -3.5959</td><td style=\"text-align: right;\">               -3.14</td><td style=\"text-align: right;\">               -3.99</td><td style=\"text-align: right;\">            359.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 169932\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-41-39\n",
      "  done: false\n",
      "  episode_len_mean: 356.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.139999999999977\n",
      "  episode_reward_mean: -3.5651999999999675\n",
      "  episode_reward_min: -3.989999999999959\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 446\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.650360490521814\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.0159370044839628\n",
      "          policy_loss: -0.020528173917888574\n",
      "          total_loss: -0.03395871111215689\n",
      "          vf_explained_var: 0.4250655770301819\n",
      "          vf_loss: 0.009885665684521525\n",
      "    num_agent_steps_sampled: 169932\n",
      "    num_agent_steps_trained: 169932\n",
      "    num_steps_sampled: 169932\n",
      "    num_steps_trained: 169932\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.36117647058823\n",
      "    ram_util_percent: 51.280588235294125\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04779163319972499\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.714398541591734\n",
      "    mean_inference_ms: 4.794594834435032\n",
      "    mean_raw_obs_processing_ms: 1.5846522990783802\n",
      "  time_since_restore: 2236.5749678611755\n",
      "  time_this_iter_s: 119.08113646507263\n",
      "  time_total_s: 2236.5749678611755\n",
      "  timers:\n",
      "    learn_throughput: 588.382\n",
      "    learn_time_ms: 16988.956\n",
      "    load_throughput: 89224.653\n",
      "    load_time_ms: 112.032\n",
      "    sample_throughput: 92.807\n",
      "    sample_time_ms: 107707.793\n",
      "    update_time_ms: 28.727\n",
      "  timestamp: 1635543699\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 169932\n",
      "  training_iteration: 17\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         2236.57</td><td style=\"text-align: right;\">169932</td><td style=\"text-align: right;\"> -3.5652</td><td style=\"text-align: right;\">               -3.14</td><td style=\"text-align: right;\">               -3.99</td><td style=\"text-align: right;\">            356.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 179928\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-43-57\n",
      "  done: false\n",
      "  episode_len_mean: 351.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.869999999999983\n",
      "  episode_reward_mean: -3.5102999999999684\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 476\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.647235590779883\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.0336255600042519\n",
      "          policy_loss: 0.0020575098088409147\n",
      "          total_loss: -0.005411271585358514\n",
      "          vf_explained_var: 0.3502059578895569\n",
      "          vf_loss: 0.012278460971309612\n",
      "    num_agent_steps_sampled: 179928\n",
      "    num_agent_steps_trained: 179928\n",
      "    num_steps_sampled: 179928\n",
      "    num_steps_trained: 179928\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.27295918367346\n",
      "    ram_util_percent: 50.99795918367347\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047784481129403\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.54920443218375\n",
      "    mean_inference_ms: 4.792267534884948\n",
      "    mean_raw_obs_processing_ms: 1.5838960631227579\n",
      "  time_since_restore: 2374.5931441783905\n",
      "  time_this_iter_s: 138.01817631721497\n",
      "  time_total_s: 2374.5931441783905\n",
      "  timers:\n",
      "    learn_throughput: 588.667\n",
      "    learn_time_ms: 16980.732\n",
      "    load_throughput: 89077.131\n",
      "    load_time_ms: 112.217\n",
      "    sample_throughput: 92.904\n",
      "    sample_time_ms: 107594.834\n",
      "    update_time_ms: 28.592\n",
      "  timestamp: 1635543837\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 179928\n",
      "  training_iteration: 18\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         2374.59</td><td style=\"text-align: right;\">179928</td><td style=\"text-align: right;\"> -3.5103</td><td style=\"text-align: right;\">               -2.87</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            351.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 189924\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-45-58\n",
      "  done: false\n",
      "  episode_len_mean: 346.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.869999999999983\n",
      "  episode_reward_mean: -3.4651999999999696\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 504\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.6203505950096324\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015328205244416018\n",
      "          policy_loss: -0.019133502178085154\n",
      "          total_loss: -0.031046132240285223\n",
      "          vf_explained_var: 0.42118608951568604\n",
      "          vf_loss: 0.009692413643499763\n",
      "    num_agent_steps_sampled: 189924\n",
      "    num_agent_steps_trained: 189924\n",
      "    num_steps_sampled: 189924\n",
      "    num_steps_trained: 189924\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.18728323699423\n",
      "    ram_util_percent: 51.33410404624276\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047753014245420715\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.424882811432784\n",
      "    mean_inference_ms: 4.789753136111636\n",
      "    mean_raw_obs_processing_ms: 1.5900480494590574\n",
      "  time_since_restore: 2495.368109703064\n",
      "  time_this_iter_s: 120.77496552467346\n",
      "  time_total_s: 2495.368109703064\n",
      "  timers:\n",
      "    learn_throughput: 588.616\n",
      "    learn_time_ms: 16982.205\n",
      "    load_throughput: 89068.899\n",
      "    load_time_ms: 112.228\n",
      "    sample_throughput: 92.802\n",
      "    sample_time_ms: 107713.7\n",
      "    update_time_ms: 26.383\n",
      "  timestamp: 1635543958\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 189924\n",
      "  training_iteration: 19\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         2495.37</td><td style=\"text-align: right;\">189924</td><td style=\"text-align: right;\"> -3.4652</td><td style=\"text-align: right;\">               -2.87</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            346.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 199920\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-47-58\n",
      "  done: false\n",
      "  episode_len_mean: 343.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.869999999999983\n",
      "  episode_reward_mean: -3.43449999999997\n",
      "  episode_reward_min: -3.9999999999999587\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 533\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.6142915246833085\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.016774899511858316\n",
      "          policy_loss: -0.019028127702892337\n",
      "          total_loss: -0.02764210909222945\n",
      "          vf_explained_var: 0.14600670337677002\n",
      "          vf_loss: 0.01249646338740907\n",
      "    num_agent_steps_sampled: 199920\n",
      "    num_agent_steps_trained: 199920\n",
      "    num_steps_sampled: 199920\n",
      "    num_steps_trained: 199920\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.24709302325581\n",
      "    ram_util_percent: 51.275581395348844\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0477313603693527\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.3129420512828\n",
      "    mean_inference_ms: 4.7872531391483335\n",
      "    mean_raw_obs_processing_ms: 1.6010947415930246\n",
      "  time_since_restore: 2615.833432674408\n",
      "  time_this_iter_s: 120.465322971344\n",
      "  time_total_s: 2615.833432674408\n",
      "  timers:\n",
      "    learn_throughput: 588.589\n",
      "    learn_time_ms: 16982.998\n",
      "    load_throughput: 89297.95\n",
      "    load_time_ms: 111.94\n",
      "    sample_throughput: 92.716\n",
      "    sample_time_ms: 107813.107\n",
      "    update_time_ms: 27.354\n",
      "  timestamp: 1635544078\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 199920\n",
      "  training_iteration: 20\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         2615.83</td><td style=\"text-align: right;\">199920</td><td style=\"text-align: right;\"> -3.4345</td><td style=\"text-align: right;\">               -2.87</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            343.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 209916\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-50-18\n",
      "  done: false\n",
      "  episode_len_mean: 339.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.9299999999999815\n",
      "  episode_reward_mean: -3.393099999999971\n",
      "  episode_reward_min: -3.9999999999999587\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 563\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.6072287276259853\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.013898418321317264\n",
      "          policy_loss: -0.018560377530689932\n",
      "          total_loss: -0.02918115337817078\n",
      "          vf_explained_var: 0.24773888289928436\n",
      "          vf_loss: 0.011281984345514813\n",
      "    num_agent_steps_sampled: 209916\n",
      "    num_agent_steps_trained: 209916\n",
      "    num_steps_sampled: 209916\n",
      "    num_steps_trained: 209916\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.4795\n",
      "    ram_util_percent: 50.97300000000001\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04773946106137403\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.22315892133928\n",
      "    mean_inference_ms: 4.78452898664789\n",
      "    mean_raw_obs_processing_ms: 1.6427118715246518\n",
      "  time_since_restore: 2755.8372292518616\n",
      "  time_this_iter_s: 140.0037965774536\n",
      "  time_total_s: 2755.8372292518616\n",
      "  timers:\n",
      "    learn_throughput: 588.525\n",
      "    learn_time_ms: 16984.842\n",
      "    load_throughput: 89485.685\n",
      "    load_time_ms: 111.705\n",
      "    sample_throughput: 92.532\n",
      "    sample_time_ms: 108027.173\n",
      "    update_time_ms: 25.691\n",
      "  timestamp: 1635544218\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 209916\n",
      "  training_iteration: 21\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         2755.84</td><td style=\"text-align: right;\">209916</td><td style=\"text-align: right;\"> -3.3931</td><td style=\"text-align: right;\">               -2.93</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            339.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 219912\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-52-20\n",
      "  done: false\n",
      "  episode_len_mean: 336.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.9699999999999807\n",
      "  episode_reward_mean: -3.367599999999972\n",
      "  episode_reward_min: -3.9999999999999587\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 593\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.605514992811741\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.016487242803742856\n",
      "          policy_loss: -0.01916974249940652\n",
      "          total_loss: -0.02913717405918317\n",
      "          vf_explained_var: 0.35976043343544006\n",
      "          vf_loss: 0.01114154502429301\n",
      "    num_agent_steps_sampled: 219912\n",
      "    num_agent_steps_trained: 219912\n",
      "    num_steps_sampled: 219912\n",
      "    num_steps_trained: 219912\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.46918604651161\n",
      "    ram_util_percent: 51.31976744186046\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047751297128381054\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.149508090782547\n",
      "    mean_inference_ms: 4.782643447485546\n",
      "    mean_raw_obs_processing_ms: 1.6478530759579764\n",
      "  time_since_restore: 2877.039470911026\n",
      "  time_this_iter_s: 121.20224165916443\n",
      "  time_total_s: 2877.039470911026\n",
      "  timers:\n",
      "    learn_throughput: 588.981\n",
      "    learn_time_ms: 16971.689\n",
      "    load_throughput: 89575.753\n",
      "    load_time_ms: 111.593\n",
      "    sample_throughput: 92.483\n",
      "    sample_time_ms: 108084.947\n",
      "    update_time_ms: 25.03\n",
      "  timestamp: 1635544340\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 219912\n",
      "  training_iteration: 22\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         2877.04</td><td style=\"text-align: right;\">219912</td><td style=\"text-align: right;\"> -3.3676</td><td style=\"text-align: right;\">               -2.97</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            336.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 229908\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-54-21\n",
      "  done: false\n",
      "  episode_len_mean: 335.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.9699999999999807\n",
      "  episode_reward_mean: -3.3585999999999725\n",
      "  episode_reward_min: -3.799999999999963\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 623\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.590216198334327\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015808230927384606\n",
      "          policy_loss: -0.02228547969243975\n",
      "          total_loss: -0.033759151271775235\n",
      "          vf_explained_var: 0.47682833671569824\n",
      "          vf_loss: 0.009686020790048553\n",
      "    num_agent_steps_sampled: 229908\n",
      "    num_agent_steps_trained: 229908\n",
      "    num_steps_sampled: 229908\n",
      "    num_steps_trained: 229908\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.34425287356322\n",
      "    ram_util_percent: 51.38563218390804\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04772529866045667\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.086682323033116\n",
      "    mean_inference_ms: 4.7804017314177045\n",
      "    mean_raw_obs_processing_ms: 1.6581220109655523\n",
      "  time_since_restore: 2998.876156806946\n",
      "  time_this_iter_s: 121.8366858959198\n",
      "  time_total_s: 2998.876156806946\n",
      "  timers:\n",
      "    learn_throughput: 588.918\n",
      "    learn_time_ms: 16973.494\n",
      "    load_throughput: 89528.718\n",
      "    load_time_ms: 111.651\n",
      "    sample_throughput: 92.188\n",
      "    sample_time_ms: 108430.835\n",
      "    update_time_ms: 24.856\n",
      "  timestamp: 1635544461\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 229908\n",
      "  training_iteration: 23\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         2998.88</td><td style=\"text-align: right;\">229908</td><td style=\"text-align: right;\"> -3.3586</td><td style=\"text-align: right;\">               -2.97</td><td style=\"text-align: right;\">                -3.8</td><td style=\"text-align: right;\">            335.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 239904\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-56-42\n",
      "  done: false\n",
      "  episode_len_mean: 335.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.9199999999999817\n",
      "  episode_reward_mean: -3.353899999999973\n",
      "  episode_reward_min: -3.759999999999964\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 653\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.5340582069168742\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.016402513513692492\n",
      "          policy_loss: -0.015112484507581108\n",
      "          total_loss: -0.024588220302238423\n",
      "          vf_explained_var: 0.36747103929519653\n",
      "          vf_loss: 0.010944092055047866\n",
      "    num_agent_steps_sampled: 239904\n",
      "    num_agent_steps_trained: 239904\n",
      "    num_steps_sampled: 239904\n",
      "    num_steps_trained: 239904\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.10597014925372\n",
      "    ram_util_percent: 51.08905472636817\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047696487918501854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.03260825418836\n",
      "    mean_inference_ms: 4.777969144827413\n",
      "    mean_raw_obs_processing_ms: 1.6874061604409811\n",
      "  time_since_restore: 3139.6415390968323\n",
      "  time_this_iter_s: 140.76538228988647\n",
      "  time_total_s: 3139.6415390968323\n",
      "  timers:\n",
      "    learn_throughput: 589.022\n",
      "    learn_time_ms: 16970.498\n",
      "    load_throughput: 89848.493\n",
      "    load_time_ms: 111.254\n",
      "    sample_throughput: 91.782\n",
      "    sample_time_ms: 108909.992\n",
      "    update_time_ms: 27.029\n",
      "  timestamp: 1635544602\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 239904\n",
      "  training_iteration: 24\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         3139.64</td><td style=\"text-align: right;\">239904</td><td style=\"text-align: right;\"> -3.3539</td><td style=\"text-align: right;\">               -2.92</td><td style=\"text-align: right;\">               -3.76</td><td style=\"text-align: right;\">            335.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 249900\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_21-58-46\n",
      "  done: false\n",
      "  episode_len_mean: 334.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.9199999999999817\n",
      "  episode_reward_mean: -3.342599999999972\n",
      "  episode_reward_min: -3.759999999999964\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 683\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.5581381331142197\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.016281500909050877\n",
      "          policy_loss: -0.0224372102950628\n",
      "          total_loss: -0.032254301450955564\n",
      "          vf_explained_var: 0.38476794958114624\n",
      "          vf_loss: 0.01087983891144783\n",
      "    num_agent_steps_sampled: 249900\n",
      "    num_agent_steps_trained: 249900\n",
      "    num_steps_sampled: 249900\n",
      "    num_steps_trained: 249900\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.52429378531075\n",
      "    ram_util_percent: 51.424858757062154\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04767001007345687\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.993526027132265\n",
      "    mean_inference_ms: 4.776030368021995\n",
      "    mean_raw_obs_processing_ms: 1.690868998272822\n",
      "  time_since_restore: 3263.7169075012207\n",
      "  time_this_iter_s: 124.07536840438843\n",
      "  time_total_s: 3263.7169075012207\n",
      "  timers:\n",
      "    learn_throughput: 589.09\n",
      "    learn_time_ms: 16968.558\n",
      "    load_throughput: 89620.788\n",
      "    load_time_ms: 111.537\n",
      "    sample_throughput: 91.455\n",
      "    sample_time_ms: 109299.083\n",
      "    update_time_ms: 28.388\n",
      "  timestamp: 1635544726\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 249900\n",
      "  training_iteration: 25\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         3263.72</td><td style=\"text-align: right;\">249900</td><td style=\"text-align: right;\"> -3.3426</td><td style=\"text-align: right;\">               -2.92</td><td style=\"text-align: right;\">               -3.76</td><td style=\"text-align: right;\">            334.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 259896\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-00-51\n",
      "  done: false\n",
      "  episode_len_mean: 330.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.9199999999999817\n",
      "  episode_reward_mean: -3.3035999999999732\n",
      "  episode_reward_min: -3.839999999999962\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 714\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.514530855977637\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01693860484762962\n",
      "          policy_loss: -0.02450768023920365\n",
      "          total_loss: -0.033990137291769695\n",
      "          vf_explained_var: 0.46373456716537476\n",
      "          vf_loss: 0.01058126967790851\n",
      "    num_agent_steps_sampled: 259896\n",
      "    num_agent_steps_trained: 259896\n",
      "    num_steps_sampled: 259896\n",
      "    num_steps_trained: 259896\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.5376404494382\n",
      "    ram_util_percent: 51.45730337078651\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047651575610425566\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.975370252222415\n",
      "    mean_inference_ms: 4.774231550944741\n",
      "    mean_raw_obs_processing_ms: 1.696506403487847\n",
      "  time_since_restore: 3388.4217069149017\n",
      "  time_this_iter_s: 124.70479941368103\n",
      "  time_total_s: 3388.4217069149017\n",
      "  timers:\n",
      "    learn_throughput: 589.151\n",
      "    learn_time_ms: 16966.787\n",
      "    load_throughput: 89868.599\n",
      "    load_time_ms: 111.229\n",
      "    sample_throughput: 90.901\n",
      "    sample_time_ms: 109965.737\n",
      "    update_time_ms: 26.881\n",
      "  timestamp: 1635544851\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 259896\n",
      "  training_iteration: 26\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         3388.42</td><td style=\"text-align: right;\">259896</td><td style=\"text-align: right;\"> -3.3036</td><td style=\"text-align: right;\">               -2.92</td><td style=\"text-align: right;\">               -3.84</td><td style=\"text-align: right;\">            330.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 269892\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-03-12\n",
      "  done: false\n",
      "  episode_len_mean: 325.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.719999999999986\n",
      "  episode_reward_mean: -3.2563999999999744\n",
      "  episode_reward_min: -3.839999999999962\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 745\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.5747376853584223\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01836315547892131\n",
      "          policy_loss: -0.02160740277578688\n",
      "          total_loss: -0.03002023630672031\n",
      "          vf_explained_var: 0.3663577437400818\n",
      "          vf_loss: 0.011825596110180863\n",
      "    num_agent_steps_sampled: 269892\n",
      "    num_agent_steps_trained: 269892\n",
      "    num_steps_sampled: 269892\n",
      "    num_steps_trained: 269892\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.54950495049505\n",
      "    ram_util_percent: 51.2539603960396\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04764496936181132\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.964209330071895\n",
      "    mean_inference_ms: 4.772475482641353\n",
      "    mean_raw_obs_processing_ms: 1.71358374942089\n",
      "  time_since_restore: 3529.778862476349\n",
      "  time_this_iter_s: 141.35715556144714\n",
      "  time_total_s: 3529.778862476349\n",
      "  timers:\n",
      "    learn_throughput: 589.306\n",
      "    learn_time_ms: 16962.327\n",
      "    load_throughput: 89981.547\n",
      "    load_time_ms: 111.089\n",
      "    sample_throughput: 89.091\n",
      "    sample_time_ms: 112199.308\n",
      "    update_time_ms: 25.408\n",
      "  timestamp: 1635544992\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 269892\n",
      "  training_iteration: 27\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         3529.78</td><td style=\"text-align: right;\">269892</td><td style=\"text-align: right;\"> -3.2564</td><td style=\"text-align: right;\">               -2.72</td><td style=\"text-align: right;\">               -3.84</td><td style=\"text-align: right;\">            325.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 279888\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-05-15\n",
      "  done: false\n",
      "  episode_len_mean: 326.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.719999999999986\n",
      "  episode_reward_mean: -3.264399999999974\n",
      "  episode_reward_min: -3.839999999999962\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 775\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.543680451058934\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01792138740122211\n",
      "          policy_loss: -0.020016354685410475\n",
      "          total_loss: -0.028538150359422733\n",
      "          vf_explained_var: 0.3204682767391205\n",
      "          vf_loss: 0.011538591295277748\n",
      "    num_agent_steps_sampled: 279888\n",
      "    num_agent_steps_trained: 279888\n",
      "    num_steps_sampled: 279888\n",
      "    num_steps_trained: 279888\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.27485714285713\n",
      "    ram_util_percent: 51.5017142857143\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047634835507615304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.953449639253655\n",
      "    mean_inference_ms: 4.7712488022757595\n",
      "    mean_raw_obs_processing_ms: 1.7145076378348978\n",
      "  time_since_restore: 3652.7370340824127\n",
      "  time_this_iter_s: 122.95817160606384\n",
      "  time_total_s: 3652.7370340824127\n",
      "  timers:\n",
      "    learn_throughput: 589.22\n",
      "    learn_time_ms: 16964.792\n",
      "    load_throughput: 90106.88\n",
      "    load_time_ms: 110.935\n",
      "    sample_throughput: 90.305\n",
      "    sample_time_ms: 110691.464\n",
      "    update_time_ms: 24.671\n",
      "  timestamp: 1635545115\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 279888\n",
      "  training_iteration: 28\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         3652.74</td><td style=\"text-align: right;\">279888</td><td style=\"text-align: right;\"> -3.2644</td><td style=\"text-align: right;\">               -2.72</td><td style=\"text-align: right;\">               -3.84</td><td style=\"text-align: right;\">            326.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 289884\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-07-37\n",
      "  done: false\n",
      "  episode_len_mean: 321.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.719999999999986\n",
      "  episode_reward_mean: -3.214799999999975\n",
      "  episode_reward_min: -3.839999999999962\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 808\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.480481813707922\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.0168978026015408\n",
      "          policy_loss: -0.02002618005578844\n",
      "          total_loss: -0.028826697801168148\n",
      "          vf_explained_var: 0.4878488779067993\n",
      "          vf_loss: 0.010934959393445876\n",
      "    num_agent_steps_sampled: 289884\n",
      "    num_agent_steps_trained: 289884\n",
      "    num_steps_sampled: 289884\n",
      "    num_steps_trained: 289884\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.87772277227724\n",
      "    ram_util_percent: 51.43910891089109\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04762191666580174\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.94796362718318\n",
      "    mean_inference_ms: 4.770145217660042\n",
      "    mean_raw_obs_processing_ms: 1.7533920677048815\n",
      "  time_since_restore: 3794.0472309589386\n",
      "  time_this_iter_s: 141.31019687652588\n",
      "  time_total_s: 3794.0472309589386\n",
      "  timers:\n",
      "    learn_throughput: 589.293\n",
      "    learn_time_ms: 16962.711\n",
      "    load_throughput: 89977.684\n",
      "    load_time_ms: 111.094\n",
      "    sample_throughput: 88.66\n",
      "    sample_time_ms: 112745.633\n",
      "    update_time_ms: 25.359\n",
      "  timestamp: 1635545257\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 289884\n",
      "  training_iteration: 29\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         3794.05</td><td style=\"text-align: right;\">289884</td><td style=\"text-align: right;\"> -3.2148</td><td style=\"text-align: right;\">               -2.72</td><td style=\"text-align: right;\">               -3.84</td><td style=\"text-align: right;\">            321.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 299880\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-09-57\n",
      "  done: false\n",
      "  episode_len_mean: 320.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7299999999999858\n",
      "  episode_reward_mean: -3.205099999999975\n",
      "  episode_reward_min: -3.839999999999962\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 838\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.462966432530656\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01843382190936982\n",
      "          policy_loss: -0.019856300478817052\n",
      "          total_loss: -0.027921302498787895\n",
      "          vf_explained_var: 0.41814863681793213\n",
      "          vf_loss: 0.01103451513154452\n",
      "    num_agent_steps_sampled: 299880\n",
      "    num_agent_steps_trained: 299880\n",
      "    num_steps_sampled: 299880\n",
      "    num_steps_trained: 299880\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.02900000000001\n",
      "    ram_util_percent: 51.32150000000001\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04757799427069944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.941621916565627\n",
      "    mean_inference_ms: 4.768400834765327\n",
      "    mean_raw_obs_processing_ms: 1.7654432571568328\n",
      "  time_since_restore: 3934.1663920879364\n",
      "  time_this_iter_s: 140.1191611289978\n",
      "  time_total_s: 3934.1663920879364\n",
      "  timers:\n",
      "    learn_throughput: 589.521\n",
      "    learn_time_ms: 16956.135\n",
      "    load_throughput: 89770.099\n",
      "    load_time_ms: 111.351\n",
      "    sample_throughput: 87.135\n",
      "    sample_time_ms: 114718.457\n",
      "    update_time_ms: 24.029\n",
      "  timestamp: 1635545397\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 299880\n",
      "  training_iteration: 30\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         3934.17</td><td style=\"text-align: right;\">299880</td><td style=\"text-align: right;\"> -3.2051</td><td style=\"text-align: right;\">               -2.73</td><td style=\"text-align: right;\">               -3.84</td><td style=\"text-align: right;\">            320.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 309876\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-12-04\n",
      "  done: false\n",
      "  episode_len_mean: 315.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7299999999999858\n",
      "  episode_reward_mean: -3.1524999999999763\n",
      "  episode_reward_min: -3.6399999999999664\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 871\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.486479832779648\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.04357411827604883\n",
      "          policy_loss: -0.0015788276735534015\n",
      "          total_loss: 0.0003608049872594002\n",
      "          vf_explained_var: 0.284025639295578\n",
      "          vf_loss: 0.013732194087147697\n",
      "    num_agent_steps_sampled: 309876\n",
      "    num_agent_steps_trained: 309876\n",
      "    num_steps_sampled: 309876\n",
      "    num_steps_trained: 309876\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.35193370165746\n",
      "    ram_util_percent: 51.53922651933701\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04757242523795651\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.95259690312669\n",
      "    mean_inference_ms: 4.76737680647087\n",
      "    mean_raw_obs_processing_ms: 1.7653941534180217\n",
      "  time_since_restore: 4060.875206232071\n",
      "  time_this_iter_s: 126.70881414413452\n",
      "  time_total_s: 4060.875206232071\n",
      "  timers:\n",
      "    learn_throughput: 589.491\n",
      "    learn_time_ms: 16956.994\n",
      "    load_throughput: 89616.708\n",
      "    load_time_ms: 111.542\n",
      "    sample_throughput: 88.159\n",
      "    sample_time_ms: 113386.374\n",
      "    update_time_ms: 25.839\n",
      "  timestamp: 1635545524\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 309876\n",
      "  training_iteration: 31\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         4060.88</td><td style=\"text-align: right;\">309876</td><td style=\"text-align: right;\"> -3.1525</td><td style=\"text-align: right;\">               -2.73</td><td style=\"text-align: right;\">               -3.64</td><td style=\"text-align: right;\">            315.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 319872\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-14-27\n",
      "  done: false\n",
      "  episode_len_mean: 312.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7499999999999853\n",
      "  episode_reward_mean: -3.1297999999999773\n",
      "  episode_reward_min: -3.6399999999999664\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 904\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.4833546893209473\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.017415653507915407\n",
      "          policy_loss: -0.012616865066254241\n",
      "          total_loss: -0.016658770598662206\n",
      "          vf_explained_var: 0.3220987915992737\n",
      "          vf_loss: 0.012954597231768058\n",
      "    num_agent_steps_sampled: 319872\n",
      "    num_agent_steps_trained: 319872\n",
      "    num_steps_sampled: 319872\n",
      "    num_steps_trained: 319872\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.93951219512196\n",
      "    ram_util_percent: 51.41073170731707\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04754125117245892\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.959328076216362\n",
      "    mean_inference_ms: 4.765034816956149\n",
      "    mean_raw_obs_processing_ms: 1.7836934858900355\n",
      "  time_since_restore: 4204.483672380447\n",
      "  time_this_iter_s: 143.60846614837646\n",
      "  time_total_s: 4204.483672380447\n",
      "  timers:\n",
      "    learn_throughput: 589.187\n",
      "    learn_time_ms: 16965.749\n",
      "    load_throughput: 89463.459\n",
      "    load_time_ms: 111.733\n",
      "    sample_throughput: 86.457\n",
      "    sample_time_ms: 115618.057\n",
      "    update_time_ms: 26.072\n",
      "  timestamp: 1635545667\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 319872\n",
      "  training_iteration: 32\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         4204.48</td><td style=\"text-align: right;\">319872</td><td style=\"text-align: right;\"> -3.1298</td><td style=\"text-align: right;\">               -2.75</td><td style=\"text-align: right;\">               -3.64</td><td style=\"text-align: right;\">            312.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 329868\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-16-36\n",
      "  done: false\n",
      "  episode_len_mean: 312.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7899999999999845\n",
      "  episode_reward_mean: -3.1282999999999777\n",
      "  episode_reward_min: -3.5999999999999672\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 935\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.4905378690132727\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.014784408288709201\n",
      "          policy_loss: -0.01881795882159828\n",
      "          total_loss: -0.025536895466920655\n",
      "          vf_explained_var: 0.3332158029079437\n",
      "          vf_loss: 0.011533457410654141\n",
      "    num_agent_steps_sampled: 329868\n",
      "    num_agent_steps_trained: 329868\n",
      "    num_steps_sampled: 329868\n",
      "    num_steps_trained: 329868\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.1928961748634\n",
      "    ram_util_percent: 51.5120218579235\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04752817895216424\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.971176402222735\n",
      "    mean_inference_ms: 4.763701100856032\n",
      "    mean_raw_obs_processing_ms: 1.7905313909809961\n",
      "  time_since_restore: 4332.914197683334\n",
      "  time_this_iter_s: 128.43052530288696\n",
      "  time_total_s: 4332.914197683334\n",
      "  timers:\n",
      "    learn_throughput: 589.557\n",
      "    learn_time_ms: 16955.093\n",
      "    load_throughput: 89317.449\n",
      "    load_time_ms: 111.915\n",
      "    sample_throughput: 85.959\n",
      "    sample_time_ms: 116287.437\n",
      "    update_time_ms: 26.55\n",
      "  timestamp: 1635545796\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 329868\n",
      "  training_iteration: 33\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         4332.91</td><td style=\"text-align: right;\">329868</td><td style=\"text-align: right;\"> -3.1283</td><td style=\"text-align: right;\">               -2.79</td><td style=\"text-align: right;\">                -3.6</td><td style=\"text-align: right;\">            312.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 339864\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-18-43\n",
      "  done: false\n",
      "  episode_len_mean: 311.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7399999999999856\n",
      "  episode_reward_mean: -3.1186999999999774\n",
      "  episode_reward_min: -3.5999999999999672\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 967\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.455656281291929\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015142638221916953\n",
      "          policy_loss: -0.02143822324772676\n",
      "          total_loss: -0.02736675529780551\n",
      "          vf_explained_var: 0.3762071430683136\n",
      "          vf_loss: 0.011813842216454363\n",
      "    num_agent_steps_sampled: 339864\n",
      "    num_agent_steps_trained: 339864\n",
      "    num_steps_sampled: 339864\n",
      "    num_steps_trained: 339864\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.25714285714284\n",
      "    ram_util_percent: 51.677472527472524\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04751238434737235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.97958537741517\n",
      "    mean_inference_ms: 4.762479058251558\n",
      "    mean_raw_obs_processing_ms: 1.7935565648649012\n",
      "  time_since_restore: 4460.254989862442\n",
      "  time_this_iter_s: 127.34079217910767\n",
      "  time_total_s: 4460.254989862442\n",
      "  timers:\n",
      "    learn_throughput: 589.767\n",
      "    learn_time_ms: 16949.063\n",
      "    load_throughput: 89078.38\n",
      "    load_time_ms: 112.216\n",
      "    sample_throughput: 86.958\n",
      "    sample_time_ms: 114951.737\n",
      "    update_time_ms: 25.532\n",
      "  timestamp: 1635545923\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 339864\n",
      "  training_iteration: 34\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         4460.25</td><td style=\"text-align: right;\">339864</td><td style=\"text-align: right;\"> -3.1187</td><td style=\"text-align: right;\">               -2.74</td><td style=\"text-align: right;\">                -3.6</td><td style=\"text-align: right;\">            311.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 349860\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-21-07\n",
      "  done: false\n",
      "  episode_len_mean: 312.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.679999999999987\n",
      "  episode_reward_mean: -3.1238999999999777\n",
      "  episode_reward_min: -3.649999999999966\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 999\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.4785326240409136\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01757062327916766\n",
      "          policy_loss: -0.02220239574010046\n",
      "          total_loss: -0.027840495367462817\n",
      "          vf_explained_var: 0.4516887366771698\n",
      "          vf_loss: 0.011240446177643573\n",
      "    num_agent_steps_sampled: 349860\n",
      "    num_agent_steps_trained: 349860\n",
      "    num_steps_sampled: 349860\n",
      "    num_steps_trained: 349860\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.60390243902438\n",
      "    ram_util_percent: 51.415121951219504\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04750509499955306\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.986350855735914\n",
      "    mean_inference_ms: 4.761420030709628\n",
      "    mean_raw_obs_processing_ms: 1.7999454554084422\n",
      "  time_since_restore: 4603.979830980301\n",
      "  time_this_iter_s: 143.7248411178589\n",
      "  time_total_s: 4603.979830980301\n",
      "  timers:\n",
      "    learn_throughput: 589.803\n",
      "    learn_time_ms: 16948.044\n",
      "    load_throughput: 89216.261\n",
      "    load_time_ms: 112.042\n",
      "    sample_throughput: 85.495\n",
      "    sample_time_ms: 116918.975\n",
      "    update_time_ms: 24.235\n",
      "  timestamp: 1635546067\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 349860\n",
      "  training_iteration: 35\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         4603.98</td><td style=\"text-align: right;\">349860</td><td style=\"text-align: right;\"> -3.1239</td><td style=\"text-align: right;\">               -2.68</td><td style=\"text-align: right;\">               -3.65</td><td style=\"text-align: right;\">            312.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 359856\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-23-14\n",
      "  done: false\n",
      "  episode_len_mean: 312.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.679999999999987\n",
      "  episode_reward_mean: -3.120999999999977\n",
      "  episode_reward_min: -3.649999999999966\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 1031\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.465301500630175\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.014987771715023178\n",
      "          policy_loss: -0.020971454151420512\n",
      "          total_loss: -0.028570239981397603\n",
      "          vf_explained_var: 0.43308722972869873\n",
      "          vf_loss: 0.010309731472446782\n",
      "    num_agent_steps_sampled: 359856\n",
      "    num_agent_steps_trained: 359856\n",
      "    num_steps_sampled: 359856\n",
      "    num_steps_trained: 359856\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.31436464088397\n",
      "    ram_util_percent: 51.519889502762425\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04749016806883754\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.994302990216294\n",
      "    mean_inference_ms: 4.760394558525946\n",
      "    mean_raw_obs_processing_ms: 1.8009796694687867\n",
      "  time_since_restore: 4730.7979691028595\n",
      "  time_this_iter_s: 126.8181381225586\n",
      "  time_total_s: 4730.7979691028595\n",
      "  timers:\n",
      "    learn_throughput: 589.654\n",
      "    learn_time_ms: 16952.31\n",
      "    load_throughput: 89161.618\n",
      "    load_time_ms: 112.111\n",
      "    sample_throughput: 85.344\n",
      "    sample_time_ms: 117126.106\n",
      "    update_time_ms: 23.837\n",
      "  timestamp: 1635546194\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 359856\n",
      "  training_iteration: 36\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">          4730.8</td><td style=\"text-align: right;\">359856</td><td style=\"text-align: right;\">  -3.121</td><td style=\"text-align: right;\">               -2.68</td><td style=\"text-align: right;\">               -3.65</td><td style=\"text-align: right;\">             312.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 369852\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-25-22\n",
      "  done: false\n",
      "  episode_len_mean: 312.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.679999999999987\n",
      "  episode_reward_mean: -3.128699999999978\n",
      "  episode_reward_min: -3.649999999999966\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 1064\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.4439613786518066\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015856081691068277\n",
      "          policy_loss: -0.020754738303267548\n",
      "          total_loss: -0.027710292099887488\n",
      "          vf_explained_var: 0.4735862910747528\n",
      "          vf_loss: 0.010348822307796815\n",
      "    num_agent_steps_sampled: 369852\n",
      "    num_agent_steps_trained: 369852\n",
      "    num_steps_sampled: 369852\n",
      "    num_steps_trained: 369852\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.1344262295082\n",
      "    ram_util_percent: 51.70273224043717\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047471616922016395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.004415236959836\n",
      "    mean_inference_ms: 4.75941882434969\n",
      "    mean_raw_obs_processing_ms: 1.8035093167560488\n",
      "  time_since_restore: 4859.533709049225\n",
      "  time_this_iter_s: 128.73573994636536\n",
      "  time_total_s: 4859.533709049225\n",
      "  timers:\n",
      "    learn_throughput: 589.691\n",
      "    learn_time_ms: 16951.259\n",
      "    load_throughput: 89346.057\n",
      "    load_time_ms: 111.88\n",
      "    sample_throughput: 86.273\n",
      "    sample_time_ms: 115865.382\n",
      "    update_time_ms: 23.499\n",
      "  timestamp: 1635546322\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 369852\n",
      "  training_iteration: 37\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         4859.53</td><td style=\"text-align: right;\">369852</td><td style=\"text-align: right;\"> -3.1287</td><td style=\"text-align: right;\">               -2.68</td><td style=\"text-align: right;\">               -3.65</td><td style=\"text-align: right;\">            312.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 379848\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-28-02\n",
      "  done: false\n",
      "  episode_len_mean: 309.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4099999999999926\n",
      "  episode_reward_mean: -3.0975999999999773\n",
      "  episode_reward_min: -3.649999999999966\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 1097\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.374099533170716\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.014943379940512772\n",
      "          policy_loss: -0.018988210274877713\n",
      "          total_loss: -0.026447981076999607\n",
      "          vf_explained_var: 0.5625393390655518\n",
      "          vf_loss: 0.009556703569599364\n",
      "    num_agent_steps_sampled: 379848\n",
      "    num_agent_steps_trained: 379848\n",
      "    num_steps_sampled: 379848\n",
      "    num_steps_trained: 379848\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.95350877192983\n",
      "    ram_util_percent: 51.479385964912275\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047442511563659334\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.01080218528342\n",
      "    mean_inference_ms: 4.758094835240863\n",
      "    mean_raw_obs_processing_ms: 1.8093137533494954\n",
      "  time_since_restore: 5018.976005792618\n",
      "  time_this_iter_s: 159.44229674339294\n",
      "  time_total_s: 5018.976005792618\n",
      "  timers:\n",
      "    learn_throughput: 589.622\n",
      "    learn_time_ms: 16953.241\n",
      "    load_throughput: 89363.215\n",
      "    load_time_ms: 111.858\n",
      "    sample_throughput: 83.641\n",
      "    sample_time_ms: 119510.423\n",
      "    update_time_ms: 24.077\n",
      "  timestamp: 1635546482\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 379848\n",
      "  training_iteration: 38\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         5018.98</td><td style=\"text-align: right;\">379848</td><td style=\"text-align: right;\"> -3.0976</td><td style=\"text-align: right;\">               -2.41</td><td style=\"text-align: right;\">               -3.65</td><td style=\"text-align: right;\">            309.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 389844\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-30-14\n",
      "  done: false\n",
      "  episode_len_mean: 304.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4099999999999926\n",
      "  episode_reward_mean: -3.0489999999999795\n",
      "  episode_reward_min: -3.6399999999999664\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 1129\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.3354024942104634\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015965880162440018\n",
      "          policy_loss: -0.024223454298180902\n",
      "          total_loss: -0.031201171717391563\n",
      "          vf_explained_var: 0.4857080578804016\n",
      "          vf_loss: 0.00919166130959514\n",
      "    num_agent_steps_sampled: 389844\n",
      "    num_agent_steps_trained: 389844\n",
      "    num_steps_sampled: 389844\n",
      "    num_steps_trained: 389844\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.35212765957448\n",
      "    ram_util_percent: 51.586170212765964\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047439412108978105\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.028273442963574\n",
      "    mean_inference_ms: 4.757050528062705\n",
      "    mean_raw_obs_processing_ms: 1.811921552027419\n",
      "  time_since_restore: 5150.7001938819885\n",
      "  time_this_iter_s: 131.72418808937073\n",
      "  time_total_s: 5150.7001938819885\n",
      "  timers:\n",
      "    learn_throughput: 589.631\n",
      "    learn_time_ms: 16952.964\n",
      "    load_throughput: 89357.445\n",
      "    load_time_ms: 111.865\n",
      "    sample_throughput: 84.317\n",
      "    sample_time_ms: 118552.358\n",
      "    update_time_ms: 24.341\n",
      "  timestamp: 1635546614\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 389844\n",
      "  training_iteration: 39\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">          5150.7</td><td style=\"text-align: right;\">389844</td><td style=\"text-align: right;\">  -3.049</td><td style=\"text-align: right;\">               -2.41</td><td style=\"text-align: right;\">               -3.64</td><td style=\"text-align: right;\">             304.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 399840\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-32-21\n",
      "  done: false\n",
      "  episode_len_mean: 304.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4099999999999926\n",
      "  episode_reward_mean: -3.048399999999979\n",
      "  episode_reward_min: -3.5999999999999672\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 1162\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.3881343299507076\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.016355600907076107\n",
      "          policy_loss: -0.017645637458588322\n",
      "          total_loss: -0.023507251065128888\n",
      "          vf_explained_var: 0.4070455729961395\n",
      "          vf_loss: 0.01065970875743705\n",
      "    num_agent_steps_sampled: 399840\n",
      "    num_agent_steps_trained: 399840\n",
      "    num_steps_sampled: 399840\n",
      "    num_steps_trained: 399840\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.43406593406593\n",
      "    ram_util_percent: 51.69450549450549\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04743040033227493\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.046174919514506\n",
      "    mean_inference_ms: 4.755690646797292\n",
      "    mean_raw_obs_processing_ms: 1.8156923890907424\n",
      "  time_since_restore: 5278.507030248642\n",
      "  time_this_iter_s: 127.80683636665344\n",
      "  time_total_s: 5278.507030248642\n",
      "  timers:\n",
      "    learn_throughput: 589.532\n",
      "    learn_time_ms: 16955.814\n",
      "    load_throughput: 89375.846\n",
      "    load_time_ms: 111.842\n",
      "    sample_throughput: 85.204\n",
      "    sample_time_ms: 117317.769\n",
      "    update_time_ms: 25.068\n",
      "  timestamp: 1635546741\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 399840\n",
      "  training_iteration: 40\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         5278.51</td><td style=\"text-align: right;\">399840</td><td style=\"text-align: right;\"> -3.0484</td><td style=\"text-align: right;\">               -2.41</td><td style=\"text-align: right;\">                -3.6</td><td style=\"text-align: right;\">            304.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 409836\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-34-43\n",
      "  done: false\n",
      "  episode_len_mean: 307.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.0702999999999783\n",
      "  episode_reward_min: -3.699999999999965\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 1194\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.3994258157208432\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015232055339597816\n",
      "          policy_loss: -0.01932046360726285\n",
      "          total_loss: -0.026096213363811502\n",
      "          vf_explained_var: 0.3386276066303253\n",
      "          vf_loss: 0.01036408297803639\n",
      "    num_agent_steps_sampled: 409836\n",
      "    num_agent_steps_trained: 409836\n",
      "    num_steps_sampled: 409836\n",
      "    num_steps_trained: 409836\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.67524752475248\n",
      "    ram_util_percent: 51.38663366336635\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047413598678891826\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.05439082638851\n",
      "    mean_inference_ms: 4.75441609682892\n",
      "    mean_raw_obs_processing_ms: 1.8202230476991252\n",
      "  time_since_restore: 5420.2137751579285\n",
      "  time_this_iter_s: 141.7067449092865\n",
      "  time_total_s: 5420.2137751579285\n",
      "  timers:\n",
      "    learn_throughput: 589.493\n",
      "    learn_time_ms: 16956.932\n",
      "    load_throughput: 89555.931\n",
      "    load_time_ms: 111.617\n",
      "    sample_throughput: 84.128\n",
      "    sample_time_ms: 118818.721\n",
      "    update_time_ms: 23.254\n",
      "  timestamp: 1635546883\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 409836\n",
      "  training_iteration: 41\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         5420.21</td><td style=\"text-align: right;\">409836</td><td style=\"text-align: right;\"> -3.0703</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">                -3.7</td><td style=\"text-align: right;\">            307.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 419832\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-36-49\n",
      "  done: false\n",
      "  episode_len_mean: 311.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.1148999999999774\n",
      "  episode_reward_min: -3.789999999999963\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 1226\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.3764708171542894\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.014758756555181315\n",
      "          policy_loss: -0.021561736691520256\n",
      "          total_loss: -0.028459307317359326\n",
      "          vf_explained_var: 0.3527114689350128\n",
      "          vf_loss: 0.01022569637393139\n",
      "    num_agent_steps_sampled: 419832\n",
      "    num_agent_steps_trained: 419832\n",
      "    num_steps_sampled: 419832\n",
      "    num_steps_trained: 419832\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.35698324022347\n",
      "    ram_util_percent: 51.682122905027924\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047428241986161496\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.0548108181584\n",
      "    mean_inference_ms: 4.754082295155629\n",
      "    mean_raw_obs_processing_ms: 1.8210008308202306\n",
      "  time_since_restore: 5545.871598958969\n",
      "  time_this_iter_s: 125.65782380104065\n",
      "  time_total_s: 5545.871598958969\n",
      "  timers:\n",
      "    learn_throughput: 589.91\n",
      "    learn_time_ms: 16944.953\n",
      "    load_throughput: 89428.347\n",
      "    load_time_ms: 111.777\n",
      "    sample_throughput: 85.41\n",
      "    sample_time_ms: 117035.456\n",
      "    update_time_ms: 23.316\n",
      "  timestamp: 1635547009\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 419832\n",
      "  training_iteration: 42\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         5545.87</td><td style=\"text-align: right;\">419832</td><td style=\"text-align: right;\"> -3.1149</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.79</td><td style=\"text-align: right;\">            311.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 429828\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-39-12\n",
      "  done: false\n",
      "  episode_len_mean: 309.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.0992999999999773\n",
      "  episode_reward_min: -3.789999999999963\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 1259\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.338917745076693\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015512373400396457\n",
      "          policy_loss: -0.016073763950003518\n",
      "          total_loss: -0.022079420538666922\n",
      "          vf_explained_var: 0.3974491059780121\n",
      "          vf_loss: 0.010402953043482047\n",
      "    num_agent_steps_sampled: 429828\n",
      "    num_agent_steps_trained: 429828\n",
      "    num_steps_sampled: 429828\n",
      "    num_steps_trained: 429828\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.65951219512195\n",
      "    ram_util_percent: 51.72975609756098\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047433502223834516\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.04731022325058\n",
      "    mean_inference_ms: 4.753254004054388\n",
      "    mean_raw_obs_processing_ms: 1.8484047857729171\n",
      "  time_since_restore: 5689.4286341667175\n",
      "  time_this_iter_s: 143.5570352077484\n",
      "  time_total_s: 5689.4286341667175\n",
      "  timers:\n",
      "    learn_throughput: 589.922\n",
      "    learn_time_ms: 16944.613\n",
      "    load_throughput: 89448.094\n",
      "    load_time_ms: 111.752\n",
      "    sample_throughput: 84.321\n",
      "    sample_time_ms: 118546.84\n",
      "    update_time_ms: 24.682\n",
      "  timestamp: 1635547152\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 429828\n",
      "  training_iteration: 43\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         5689.43</td><td style=\"text-align: right;\">429828</td><td style=\"text-align: right;\"> -3.0993</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.79</td><td style=\"text-align: right;\">            309.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 439824\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-41-34\n",
      "  done: false\n",
      "  episode_len_mean: 305.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.0598999999999785\n",
      "  episode_reward_min: -3.789999999999963\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 1292\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.304757446814806\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.014093102431892898\n",
      "          policy_loss: -0.018636864076694872\n",
      "          total_loss: -0.02538163314263026\n",
      "          vf_explained_var: 0.43813446164131165\n",
      "          vf_loss: 0.009960909416743864\n",
      "    num_agent_steps_sampled: 439824\n",
      "    num_agent_steps_trained: 439824\n",
      "    num_steps_sampled: 439824\n",
      "    num_steps_trained: 439824\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.13663366336633\n",
      "    ram_util_percent: 51.66881188118811\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047461221252956515\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.05154032274254\n",
      "    mean_inference_ms: 4.752460323366586\n",
      "    mean_raw_obs_processing_ms: 1.8510592193715354\n",
      "  time_since_restore: 5831.271499633789\n",
      "  time_this_iter_s: 141.84286546707153\n",
      "  time_total_s: 5831.271499633789\n",
      "  timers:\n",
      "    learn_throughput: 589.713\n",
      "    learn_time_ms: 16950.612\n",
      "    load_throughput: 89598.514\n",
      "    load_time_ms: 111.564\n",
      "    sample_throughput: 83.306\n",
      "    sample_time_ms: 119991.653\n",
      "    update_time_ms: 24.713\n",
      "  timestamp: 1635547294\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 439824\n",
      "  training_iteration: 44\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         5831.27</td><td style=\"text-align: right;\">439824</td><td style=\"text-align: right;\"> -3.0599</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.79</td><td style=\"text-align: right;\">            305.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 449820\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-43-43\n",
      "  done: false\n",
      "  episode_len_mean: 303.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.399999999999993\n",
      "  episode_reward_mean: -3.031399999999979\n",
      "  episode_reward_min: -3.749999999999964\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 1325\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.3633854591948356\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.018280340920817456\n",
      "          policy_loss: -0.01769227715384247\n",
      "          total_loss: -0.022249851245273893\n",
      "          vf_explained_var: 0.3553963005542755\n",
      "          vf_loss: 0.010850126716123225\n",
      "    num_agent_steps_sampled: 449820\n",
      "    num_agent_steps_trained: 449820\n",
      "    num_steps_sampled: 449820\n",
      "    num_steps_trained: 449820\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.43532608695651\n",
      "    ram_util_percent: 51.86847826086957\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047457223134457945\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.060741561535178\n",
      "    mean_inference_ms: 4.751017425305167\n",
      "    mean_raw_obs_processing_ms: 1.8525562719526272\n",
      "  time_since_restore: 5959.862508773804\n",
      "  time_this_iter_s: 128.59100914001465\n",
      "  time_total_s: 5959.862508773804\n",
      "  timers:\n",
      "    learn_throughput: 589.808\n",
      "    learn_time_ms: 16947.876\n",
      "    load_throughput: 89505.916\n",
      "    load_time_ms: 111.68\n",
      "    sample_throughput: 84.367\n",
      "    sample_time_ms: 118482.407\n",
      "    update_time_ms: 23.968\n",
      "  timestamp: 1635547423\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 449820\n",
      "  training_iteration: 45\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         5959.86</td><td style=\"text-align: right;\">449820</td><td style=\"text-align: right;\"> -3.0314</td><td style=\"text-align: right;\">                -2.4</td><td style=\"text-align: right;\">               -3.75</td><td style=\"text-align: right;\">            303.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 459816\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-46-11\n",
      "  done: false\n",
      "  episode_len_mean: 300.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4199999999999924\n",
      "  episode_reward_mean: -3.0021999999999798\n",
      "  episode_reward_min: -3.7299999999999645\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 1359\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.289929468611367\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.0162376785452844\n",
      "          policy_loss: -0.019333535767136475\n",
      "          total_loss: -0.024323479009744447\n",
      "          vf_explained_var: 0.43398892879486084\n",
      "          vf_loss: 0.010602397071525979\n",
      "    num_agent_steps_sampled: 459816\n",
      "    num_agent_steps_trained: 459816\n",
      "    num_steps_sampled: 459816\n",
      "    num_steps_trained: 459816\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.75402843601896\n",
      "    ram_util_percent: 51.71753554502369\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04746262269553899\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.081191199557107\n",
      "    mean_inference_ms: 4.7497854202189504\n",
      "    mean_raw_obs_processing_ms: 1.8665318789847491\n",
      "  time_since_restore: 6107.61078619957\n",
      "  time_this_iter_s: 147.748277425766\n",
      "  time_total_s: 6107.61078619957\n",
      "  timers:\n",
      "    learn_throughput: 589.648\n",
      "    learn_time_ms: 16952.491\n",
      "    load_throughput: 89565.994\n",
      "    load_time_ms: 111.605\n",
      "    sample_throughput: 82.906\n",
      "    sample_time_ms: 120570.001\n",
      "    update_time_ms: 25.189\n",
      "  timestamp: 1635547571\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 459816\n",
      "  training_iteration: 46\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         6107.61</td><td style=\"text-align: right;\">459816</td><td style=\"text-align: right;\"> -3.0022</td><td style=\"text-align: right;\">               -2.42</td><td style=\"text-align: right;\">               -3.73</td><td style=\"text-align: right;\">            300.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 469812\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-48-20\n",
      "  done: false\n",
      "  episode_len_mean: 300.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4199999999999924\n",
      "  episode_reward_mean: -3.005799999999979\n",
      "  episode_reward_min: -3.4899999999999696\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 1392\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.2836106721152607\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015097520499631933\n",
      "          policy_loss: -0.01865168148572119\n",
      "          total_loss: -0.024397287288537392\n",
      "          vf_explained_var: 0.46282070875167847\n",
      "          vf_loss: 0.010296616560886972\n",
      "    num_agent_steps_sampled: 469812\n",
      "    num_agent_steps_trained: 469812\n",
      "    num_steps_sampled: 469812\n",
      "    num_steps_trained: 469812\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.58756756756755\n",
      "    ram_util_percent: 51.78324324324324\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04747713854051552\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.102723258523202\n",
      "    mean_inference_ms: 4.749361747798061\n",
      "    mean_raw_obs_processing_ms: 1.86856852866566\n",
      "  time_since_restore: 6237.193314552307\n",
      "  time_this_iter_s: 129.58252835273743\n",
      "  time_total_s: 6237.193314552307\n",
      "  timers:\n",
      "    learn_throughput: 589.648\n",
      "    learn_time_ms: 16952.498\n",
      "    load_throughput: 89481.197\n",
      "    load_time_ms: 111.711\n",
      "    sample_throughput: 82.849\n",
      "    sample_time_ms: 120653.654\n",
      "    update_time_ms: 26.1\n",
      "  timestamp: 1635547700\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 469812\n",
      "  training_iteration: 47\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         6237.19</td><td style=\"text-align: right;\">469812</td><td style=\"text-align: right;\"> -3.0058</td><td style=\"text-align: right;\">               -2.42</td><td style=\"text-align: right;\">               -3.49</td><td style=\"text-align: right;\">            300.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 479808\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-50-30\n",
      "  done: false\n",
      "  episode_len_mean: 300.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4199999999999924\n",
      "  episode_reward_mean: -3.00249999999998\n",
      "  episode_reward_min: -3.4899999999999696\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 1425\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.31308002461735\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01651572449124078\n",
      "          policy_loss: -0.018782907869252893\n",
      "          total_loss: -0.02424049158540801\n",
      "          vf_explained_var: 0.4735865890979767\n",
      "          vf_loss: 0.010241139664053002\n",
      "    num_agent_steps_sampled: 479808\n",
      "    num_agent_steps_trained: 479808\n",
      "    num_steps_sampled: 479808\n",
      "    num_steps_trained: 479808\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.58702702702702\n",
      "    ram_util_percent: 51.905945945945945\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04751639113004732\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.125125408009488\n",
      "    mean_inference_ms: 4.749124895647634\n",
      "    mean_raw_obs_processing_ms: 1.8706622183021375\n",
      "  time_since_restore: 6366.747564792633\n",
      "  time_this_iter_s: 129.55425024032593\n",
      "  time_total_s: 6366.747564792633\n",
      "  timers:\n",
      "    learn_throughput: 589.411\n",
      "    learn_time_ms: 16959.307\n",
      "    load_throughput: 89346.191\n",
      "    load_time_ms: 111.879\n",
      "    sample_throughput: 84.958\n",
      "    sample_time_ms: 117658.27\n",
      "    update_time_ms: 26.584\n",
      "  timestamp: 1635547830\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 479808\n",
      "  training_iteration: 48\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         6366.75</td><td style=\"text-align: right;\">479808</td><td style=\"text-align: right;\"> -3.0025</td><td style=\"text-align: right;\">               -2.42</td><td style=\"text-align: right;\">               -3.49</td><td style=\"text-align: right;\">            300.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 489804\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-52-57\n",
      "  done: false\n",
      "  episode_len_mean: 299.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -2.9934999999999805\n",
      "  episode_reward_min: -3.4899999999999696\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 1458\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.3147392332044423\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015496314287551114\n",
      "          policy_loss: -0.018402710299079236\n",
      "          total_loss: -0.02376255417226726\n",
      "          vf_explained_var: 0.4498850107192993\n",
      "          vf_loss: 0.010814205956178654\n",
      "    num_agent_steps_sampled: 489804\n",
      "    num_agent_steps_trained: 489804\n",
      "    num_steps_sampled: 489804\n",
      "    num_steps_trained: 489804\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.98333333333333\n",
      "    ram_util_percent: 51.65761904761904\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047534411341416\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.1432305879238\n",
      "    mean_inference_ms: 4.748607475187473\n",
      "    mean_raw_obs_processing_ms: 1.871784126755796\n",
      "  time_since_restore: 6514.099457740784\n",
      "  time_this_iter_s: 147.35189294815063\n",
      "  time_total_s: 6514.099457740784\n",
      "  timers:\n",
      "    learn_throughput: 589.239\n",
      "    learn_time_ms: 16964.268\n",
      "    load_throughput: 89286.027\n",
      "    load_time_ms: 111.955\n",
      "    sample_throughput: 83.847\n",
      "    sample_time_ms: 119216.528\n",
      "    update_time_ms: 25.836\n",
      "  timestamp: 1635547977\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 489804\n",
      "  training_iteration: 49\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">          6514.1</td><td style=\"text-align: right;\">489804</td><td style=\"text-align: right;\"> -2.9935</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.49</td><td style=\"text-align: right;\">            299.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 499800\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-55-11\n",
      "  done: false\n",
      "  episode_len_mean: 295.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -2.95559999999998\n",
      "  episode_reward_min: -3.46999999999997\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 1492\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.1793257379124307\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.016372420892667997\n",
      "          policy_loss: -0.019051698620757486\n",
      "          total_loss: -0.02399324507922189\n",
      "          vf_explained_var: 0.619396984577179\n",
      "          vf_loss: 0.0094841219370132\n",
      "    num_agent_steps_sampled: 499800\n",
      "    num_agent_steps_trained: 499800\n",
      "    num_steps_sampled: 499800\n",
      "    num_steps_trained: 499800\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.17539267015707\n",
      "    ram_util_percent: 51.7219895287958\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04756172689680347\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.17147181090295\n",
      "    mean_inference_ms: 4.748597488850394\n",
      "    mean_raw_obs_processing_ms: 1.8721900630966628\n",
      "  time_since_restore: 6647.962825059891\n",
      "  time_this_iter_s: 133.86336731910706\n",
      "  time_total_s: 6647.962825059891\n",
      "  timers:\n",
      "    learn_throughput: 589.122\n",
      "    learn_time_ms: 16967.62\n",
      "    load_throughput: 89251.605\n",
      "    load_time_ms: 111.998\n",
      "    sample_throughput: 83.425\n",
      "    sample_time_ms: 119819.493\n",
      "    update_time_ms: 25.136\n",
      "  timestamp: 1635548111\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 499800\n",
      "  training_iteration: 50\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         6647.96</td><td style=\"text-align: right;\">499800</td><td style=\"text-align: right;\"> -2.9556</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.47</td><td style=\"text-align: right;\">            295.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 509796\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_22-57-40\n",
      "  done: false\n",
      "  episode_len_mean: 290.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -2.902999999999982\n",
      "  episode_reward_min: -3.46999999999997\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 1528\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.2030139334181436\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.013451662042164204\n",
      "          policy_loss: -0.015658262507337282\n",
      "          total_loss: -0.021766033434332945\n",
      "          vf_explained_var: 0.5676963329315186\n",
      "          vf_loss: 0.009869120108972614\n",
      "    num_agent_steps_sampled: 509796\n",
      "    num_agent_steps_trained: 509796\n",
      "    num_steps_sampled: 509796\n",
      "    num_steps_trained: 509796\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.67358490566038\n",
      "    ram_util_percent: 51.662264150943415\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04750414318497205\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.20195161631525\n",
      "    mean_inference_ms: 4.74689253890861\n",
      "    mean_raw_obs_processing_ms: 1.8960102245547774\n",
      "  time_since_restore: 6796.435227632523\n",
      "  time_this_iter_s: 148.47240257263184\n",
      "  time_total_s: 6796.435227632523\n",
      "  timers:\n",
      "    learn_throughput: 589.025\n",
      "    learn_time_ms: 16970.42\n",
      "    load_throughput: 89040.052\n",
      "    load_time_ms: 112.264\n",
      "    sample_throughput: 82.96\n",
      "    sample_time_ms: 120491.542\n",
      "    update_time_ms: 26.388\n",
      "  timestamp: 1635548260\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 509796\n",
      "  training_iteration: 51\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         6796.44</td><td style=\"text-align: right;\">509796</td><td style=\"text-align: right;\">  -2.903</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.47</td><td style=\"text-align: right;\">             290.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 519792\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_23-00-07\n",
      "  done: false\n",
      "  episode_len_mean: 289.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -2.899399999999982\n",
      "  episode_reward_min: -3.46999999999997\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 1563\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.222625909707485\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01802687505469138\n",
      "          policy_loss: -0.020581856132763574\n",
      "          total_loss: -0.02484679869097522\n",
      "          vf_explained_var: 0.5447456240653992\n",
      "          vf_loss: 0.009849222769744845\n",
      "    num_agent_steps_sampled: 519792\n",
      "    num_agent_steps_trained: 519792\n",
      "    num_steps_sampled: 519792\n",
      "    num_steps_trained: 519792\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.94497607655502\n",
      "    ram_util_percent: 51.55406698564593\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047451406142144784\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.23171236640285\n",
      "    mean_inference_ms: 4.745177073792182\n",
      "    mean_raw_obs_processing_ms: 1.8990727148640025\n",
      "  time_since_restore: 6943.322082042694\n",
      "  time_this_iter_s: 146.8868544101715\n",
      "  time_total_s: 6943.322082042694\n",
      "  timers:\n",
      "    learn_throughput: 589.021\n",
      "    learn_time_ms: 16970.525\n",
      "    load_throughput: 89202.29\n",
      "    load_time_ms: 112.06\n",
      "    sample_throughput: 81.523\n",
      "    sample_time_ms: 122615.098\n",
      "    update_time_ms: 25.55\n",
      "  timestamp: 1635548407\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 519792\n",
      "  training_iteration: 52\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         6943.32</td><td style=\"text-align: right;\">519792</td><td style=\"text-align: right;\"> -2.8994</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -3.47</td><td style=\"text-align: right;\">            289.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 529788\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_23-02-20\n",
      "  done: false\n",
      "  episode_len_mean: 288.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -2.8868999999999825\n",
      "  episode_reward_min: -3.279999999999974\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 1597\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.1713716874774707\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01634742466014644\n",
      "          policy_loss: -0.02297049639189345\n",
      "          total_loss: -0.028771786796104194\n",
      "          vf_explained_var: 0.5805609822273254\n",
      "          vf_loss: 0.00855608444280811\n",
      "    num_agent_steps_sampled: 529788\n",
      "    num_agent_steps_trained: 529788\n",
      "    num_steps_sampled: 529788\n",
      "    num_steps_trained: 529788\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.30471204188483\n",
      "    ram_util_percent: 51.841884816753925\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04744900588719962\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.265207518865726\n",
      "    mean_inference_ms: 4.74418877929555\n",
      "    mean_raw_obs_processing_ms: 1.8971866064367802\n",
      "  time_since_restore: 7076.757798194885\n",
      "  time_this_iter_s: 133.43571615219116\n",
      "  time_total_s: 7076.757798194885\n",
      "  timers:\n",
      "    learn_throughput: 588.81\n",
      "    learn_time_ms: 16976.627\n",
      "    load_throughput: 89397.761\n",
      "    load_time_ms: 111.815\n",
      "    sample_throughput: 82.205\n",
      "    sample_time_ms: 121598.295\n",
      "    update_time_ms: 24.148\n",
      "  timestamp: 1635548540\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 529788\n",
      "  training_iteration: 53\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         7076.76</td><td style=\"text-align: right;\">529788</td><td style=\"text-align: right;\"> -2.8869</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -3.28</td><td style=\"text-align: right;\">            288.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 539784\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_23-04-47\n",
      "  done: false\n",
      "  episode_len_mean: 291.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -2.9123999999999812\n",
      "  episode_reward_min: -3.429999999999971\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 1630\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.1629161366030702\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.018812124458975198\n",
      "          policy_loss: -0.024726992937871534\n",
      "          total_loss: -0.029555408128051675\n",
      "          vf_explained_var: 0.6291961669921875\n",
      "          vf_loss: 0.008335290178626728\n",
      "    num_agent_steps_sampled: 539784\n",
      "    num_agent_steps_trained: 539784\n",
      "    num_steps_sampled: 539784\n",
      "    num_steps_trained: 539784\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.21244019138757\n",
      "    ram_util_percent: 51.72296650717703\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04749098062789355\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.29226963115893\n",
      "    mean_inference_ms: 4.743942349988057\n",
      "    mean_raw_obs_processing_ms: 1.9091432105842603\n",
      "  time_since_restore: 7223.872467756271\n",
      "  time_this_iter_s: 147.1146695613861\n",
      "  time_total_s: 7223.872467756271\n",
      "  timers:\n",
      "    learn_throughput: 589.011\n",
      "    learn_time_ms: 16970.811\n",
      "    load_throughput: 89262.531\n",
      "    load_time_ms: 111.984\n",
      "    sample_throughput: 81.847\n",
      "    sample_time_ms: 122130.127\n",
      "    update_time_ms: 24.807\n",
      "  timestamp: 1635548687\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 539784\n",
      "  training_iteration: 54\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         7223.87</td><td style=\"text-align: right;\">539784</td><td style=\"text-align: right;\"> -2.9124</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.43</td><td style=\"text-align: right;\">            291.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 549780\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_23-06-59\n",
      "  done: false\n",
      "  episode_len_mean: 293.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -2.936299999999981\n",
      "  episode_reward_min: -3.45999999999997\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 1664\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.190102571911282\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.017827942630340057\n",
      "          policy_loss: -0.022195738372512354\n",
      "          total_loss: -0.02813126426190138\n",
      "          vf_explained_var: 0.612973690032959\n",
      "          vf_loss: 0.007942924573923\n",
      "    num_agent_steps_sampled: 549780\n",
      "    num_agent_steps_trained: 549780\n",
      "    num_steps_sampled: 549780\n",
      "    num_steps_trained: 549780\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.48941798941799\n",
      "    ram_util_percent: 51.68306878306878\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04751356138489525\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.320360198925936\n",
      "    mean_inference_ms: 4.743854212443593\n",
      "    mean_raw_obs_processing_ms: 1.9091810736801427\n",
      "  time_since_restore: 7355.816533327103\n",
      "  time_this_iter_s: 131.9440655708313\n",
      "  time_total_s: 7355.816533327103\n",
      "  timers:\n",
      "    learn_throughput: 588.926\n",
      "    learn_time_ms: 16973.274\n",
      "    load_throughput: 89198.779\n",
      "    load_time_ms: 112.064\n",
      "    sample_throughput: 81.625\n",
      "    sample_time_ms: 122462.377\n",
      "    update_time_ms: 25.286\n",
      "  timestamp: 1635548819\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 549780\n",
      "  training_iteration: 55\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         7355.82</td><td style=\"text-align: right;\">549780</td><td style=\"text-align: right;\"> -2.9363</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.46</td><td style=\"text-align: right;\">            293.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 559776\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_23-09-12\n",
      "  done: false\n",
      "  episode_len_mean: 294.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -2.946899999999981\n",
      "  episode_reward_min: -3.45999999999997\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 1699\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.0848699658344954\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01594373286038608\n",
      "          policy_loss: -0.022048574284865306\n",
      "          total_loss: -0.02707297321822908\n",
      "          vf_explained_var: 0.5545592904090881\n",
      "          vf_loss: 0.008649621297491906\n",
      "    num_agent_steps_sampled: 559776\n",
      "    num_agent_steps_trained: 559776\n",
      "    num_steps_sampled: 559776\n",
      "    num_steps_trained: 559776\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.04126984126982\n",
      "    ram_util_percent: 51.844444444444434\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047482097142920836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.339682322265244\n",
      "    mean_inference_ms: 4.742919923592529\n",
      "    mean_raw_obs_processing_ms: 1.909730544893486\n",
      "  time_since_restore: 7488.346534013748\n",
      "  time_this_iter_s: 132.5300006866455\n",
      "  time_total_s: 7488.346534013748\n",
      "  timers:\n",
      "    learn_throughput: 589.265\n",
      "    learn_time_ms: 16963.517\n",
      "    load_throughput: 89061.899\n",
      "    load_time_ms: 112.237\n",
      "    sample_throughput: 82.645\n",
      "    sample_time_ms: 120950.567\n",
      "    update_time_ms: 23.958\n",
      "  timestamp: 1635548952\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 559776\n",
      "  training_iteration: 56\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         7488.35</td><td style=\"text-align: right;\">559776</td><td style=\"text-align: right;\"> -2.9469</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -3.46</td><td style=\"text-align: right;\">            294.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 569772\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_23-11-40\n",
      "  done: false\n",
      "  episode_len_mean: 291.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.913299999999982\n",
      "  episode_reward_min: -3.379999999999972\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 1733\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.1093609109902993\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015910196046946223\n",
      "          policy_loss: -0.021765779404558688\n",
      "          total_loss: -0.02769345939796195\n",
      "          vf_explained_var: 0.5633935332298279\n",
      "          vf_loss: 0.008006340928244381\n",
      "    num_agent_steps_sampled: 569772\n",
      "    num_agent_steps_trained: 569772\n",
      "    num_steps_sampled: 569772\n",
      "    num_steps_trained: 569772\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.24691943127961\n",
      "    ram_util_percent: 51.53222748815166\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04745226029216635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.364031887933763\n",
      "    mean_inference_ms: 4.742094719954512\n",
      "    mean_raw_obs_processing_ms: 1.9108065951922115\n",
      "  time_since_restore: 7636.566299676895\n",
      "  time_this_iter_s: 148.21976566314697\n",
      "  time_total_s: 7636.566299676895\n",
      "  timers:\n",
      "    learn_throughput: 589.044\n",
      "    learn_time_ms: 16969.884\n",
      "    load_throughput: 88810.703\n",
      "    load_time_ms: 112.554\n",
      "    sample_throughput: 81.396\n",
      "    sample_time_ms: 122807.431\n",
      "    update_time_ms: 24.175\n",
      "  timestamp: 1635549100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 569772\n",
      "  training_iteration: 57\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         7636.57</td><td style=\"text-align: right;\">569772</td><td style=\"text-align: right;\"> -2.9133</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -3.38</td><td style=\"text-align: right;\">            291.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 579768\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_23-13-54\n",
      "  done: false\n",
      "  episode_len_mean: 286.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.8688999999999827\n",
      "  episode_reward_min: -3.289999999999974\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 1769\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.07872747366245\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.0175620039529285\n",
      "          policy_loss: -0.017759458028162138\n",
      "          total_loss: -0.02163725811828915\n",
      "          vf_explained_var: 0.5480151772499084\n",
      "          vf_loss: 0.009006572793174185\n",
      "    num_agent_steps_sampled: 579768\n",
      "    num_agent_steps_trained: 579768\n",
      "    num_steps_sampled: 579768\n",
      "    num_steps_trained: 579768\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.2625\n",
      "    ram_util_percent: 51.813020833333326\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04741976769942689\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.39138769491422\n",
      "    mean_inference_ms: 4.741011450996831\n",
      "    mean_raw_obs_processing_ms: 1.9136443850768614\n",
      "  time_since_restore: 7770.844778299332\n",
      "  time_this_iter_s: 134.27847862243652\n",
      "  time_total_s: 7770.844778299332\n",
      "  timers:\n",
      "    learn_throughput: 589.502\n",
      "    learn_time_ms: 16956.679\n",
      "    load_throughput: 88763.941\n",
      "    load_time_ms: 112.613\n",
      "    sample_throughput: 81.074\n",
      "    sample_time_ms: 123294.852\n",
      "    update_time_ms: 22.911\n",
      "  timestamp: 1635549234\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 579768\n",
      "  training_iteration: 58\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         7770.84</td><td style=\"text-align: right;\">579768</td><td style=\"text-align: right;\"> -2.8689</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -3.29</td><td style=\"text-align: right;\">            286.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 589764\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_23-16-27\n",
      "  done: false\n",
      "  episode_len_mean: 282.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -2.826699999999984\n",
      "  episode_reward_min: -3.289999999999974\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 1805\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.0517369370175222\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01644579320157949\n",
      "          policy_loss: -0.018114786362673482\n",
      "          total_loss: -0.02239180891177593\n",
      "          vf_explained_var: 0.5638426542282104\n",
      "          vf_loss: 0.008839739460456304\n",
      "    num_agent_steps_sampled: 589764\n",
      "    num_agent_steps_trained: 589764\n",
      "    num_steps_sampled: 589764\n",
      "    num_steps_trained: 589764\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.19357798165136\n",
      "    ram_util_percent: 51.8137614678899\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047396495494546095\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.427797267847186\n",
      "    mean_inference_ms: 4.739905045569563\n",
      "    mean_raw_obs_processing_ms: 1.9419046518558931\n",
      "  time_since_restore: 7923.634266376495\n",
      "  time_this_iter_s: 152.7894880771637\n",
      "  time_total_s: 7923.634266376495\n",
      "  timers:\n",
      "    learn_throughput: 589.098\n",
      "    learn_time_ms: 16968.305\n",
      "    load_throughput: 88839.834\n",
      "    load_time_ms: 112.517\n",
      "    sample_throughput: 80.725\n",
      "    sample_time_ms: 123827.352\n",
      "    update_time_ms: 22.578\n",
      "  timestamp: 1635549387\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 589764\n",
      "  training_iteration: 59\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         7923.63</td><td style=\"text-align: right;\">589764</td><td style=\"text-align: right;\"> -2.8267</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">               -3.29</td><td style=\"text-align: right;\">            282.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 599760\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_23-18-45\n",
      "  done: false\n",
      "  episode_len_mean: 283.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -2.837899999999983\n",
      "  episode_reward_min: -3.419999999999971\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 1839\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.065214273868463\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.014123745496059291\n",
      "          policy_loss: -0.021670338177146056\n",
      "          total_loss: -0.027071743842182507\n",
      "          vf_explained_var: 0.5465595722198486\n",
      "          vf_loss: 0.00889505126989152\n",
      "    num_agent_steps_sampled: 599760\n",
      "    num_agent_steps_trained: 599760\n",
      "    num_steps_sampled: 599760\n",
      "    num_steps_trained: 599760\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.35051020408162\n",
      "    ram_util_percent: 51.895408163265316\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04741519385053539\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.469910305130135\n",
      "    mean_inference_ms: 4.740329982554515\n",
      "    mean_raw_obs_processing_ms: 1.9438246969447777\n",
      "  time_since_restore: 8061.409993886948\n",
      "  time_this_iter_s: 137.77572751045227\n",
      "  time_total_s: 8061.409993886948\n",
      "  timers:\n",
      "    learn_throughput: 588.951\n",
      "    learn_time_ms: 16972.558\n",
      "    load_throughput: 89224.577\n",
      "    load_time_ms: 112.032\n",
      "    sample_throughput: 80.475\n",
      "    sample_time_ms: 124212.957\n",
      "    update_time_ms: 23.395\n",
      "  timestamp: 1635549525\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 599760\n",
      "  training_iteration: 60\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         8061.41</td><td style=\"text-align: right;\">599760</td><td style=\"text-align: right;\"> -2.8379</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">               -3.42</td><td style=\"text-align: right;\">            283.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 609756\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_23-20-58\n",
      "  done: false\n",
      "  episode_len_mean: 285.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -2.858399999999983\n",
      "  episode_reward_min: -3.5499999999999683\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 1874\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.0670559655906806\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.04254650097345052\n",
      "          policy_loss: -0.0050093069768104796\n",
      "          total_loss: 0.007067105479729482\n",
      "          vf_explained_var: 0.4369203746318817\n",
      "          vf_loss: 0.013601046668029287\n",
      "    num_agent_steps_sampled: 609756\n",
      "    num_agent_steps_trained: 609756\n",
      "    num_steps_sampled: 609756\n",
      "    num_steps_trained: 609756\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.40684210526315\n",
      "    ram_util_percent: 52.060526315789495\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047441982386878016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.510749272544754\n",
      "    mean_inference_ms: 4.741135025804309\n",
      "    mean_raw_obs_processing_ms: 1.941074802881174\n",
      "  time_since_restore: 8194.168719291687\n",
      "  time_this_iter_s: 132.75872540473938\n",
      "  time_total_s: 8194.168719291687\n",
      "  timers:\n",
      "    learn_throughput: 588.844\n",
      "    learn_time_ms: 16975.641\n",
      "    load_throughput: 89170.228\n",
      "    load_time_ms: 112.1\n",
      "    sample_throughput: 81.507\n",
      "    sample_time_ms: 122639.082\n",
      "    update_time_ms: 22.596\n",
      "  timestamp: 1635549658\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 609756\n",
      "  training_iteration: 61\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         8194.17</td><td style=\"text-align: right;\">609756</td><td style=\"text-align: right;\"> -2.8584</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">               -3.55</td><td style=\"text-align: right;\">            285.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 619752\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_23-23-40\n",
      "  done: false\n",
      "  episode_len_mean: 288.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.299999999999995\n",
      "  episode_reward_mean: -2.8859999999999815\n",
      "  episode_reward_min: -3.5499999999999683\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 1908\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999999\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.0855036985160957\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.012300640591275499\n",
      "          policy_loss: -0.021298991414344208\n",
      "          total_loss: -0.02258242990375839\n",
      "          vf_explained_var: 0.41605859994888306\n",
      "          vf_loss: 0.011268665338866413\n",
      "    num_agent_steps_sampled: 619752\n",
      "    num_agent_steps_trained: 619752\n",
      "    num_steps_sampled: 619752\n",
      "    num_steps_trained: 619752\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.21379310344827\n",
      "    ram_util_percent: 51.880603448275856\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047466359066990566\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.536329912867494\n",
      "    mean_inference_ms: 4.741481128632275\n",
      "    mean_raw_obs_processing_ms: 1.9414696376115144\n",
      "  time_since_restore: 8356.641201257706\n",
      "  time_this_iter_s: 162.47248196601868\n",
      "  time_total_s: 8356.641201257706\n",
      "  timers:\n",
      "    learn_throughput: 588.566\n",
      "    learn_time_ms: 16983.639\n",
      "    load_throughput: 89141.713\n",
      "    load_time_ms: 112.136\n",
      "    sample_throughput: 80.49\n",
      "    sample_time_ms: 124189.535\n",
      "    update_time_ms: 22.559\n",
      "  timestamp: 1635549820\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 619752\n",
      "  training_iteration: 62\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         8356.64</td><td style=\"text-align: right;\">619752</td><td style=\"text-align: right;\">  -2.886</td><td style=\"text-align: right;\">                -2.3</td><td style=\"text-align: right;\">               -3.55</td><td style=\"text-align: right;\">             288.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 629748\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_23-25-54\n",
      "  done: false\n",
      "  episode_len_mean: 288.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.299999999999995\n",
      "  episode_reward_mean: -2.881899999999982\n",
      "  episode_reward_min: -3.5499999999999683\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 1944\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999999\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.0460161824511665\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.010634216407628147\n",
      "          policy_loss: -0.01714293073830951\n",
      "          total_loss: -0.020729632476647185\n",
      "          vf_explained_var: 0.47515279054641724\n",
      "          vf_loss: 0.009695363039614505\n",
      "    num_agent_steps_sampled: 629748\n",
      "    num_agent_steps_trained: 629748\n",
      "    num_steps_sampled: 629748\n",
      "    num_steps_trained: 629748\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.47277486910994\n",
      "    ram_util_percent: 51.962827225130894\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04743061355338516\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.55427972978026\n",
      "    mean_inference_ms: 4.740563177761464\n",
      "    mean_raw_obs_processing_ms: 1.943426954851027\n",
      "  time_since_restore: 8490.910821199417\n",
      "  time_this_iter_s: 134.26961994171143\n",
      "  time_total_s: 8490.910821199417\n",
      "  timers:\n",
      "    learn_throughput: 588.347\n",
      "    learn_time_ms: 16989.967\n",
      "    load_throughput: 88939.057\n",
      "    load_time_ms: 112.392\n",
      "    sample_throughput: 80.44\n",
      "    sample_time_ms: 124267.213\n",
      "    update_time_ms: 21.819\n",
      "  timestamp: 1635549954\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 629748\n",
      "  training_iteration: 63\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         8490.91</td><td style=\"text-align: right;\">629748</td><td style=\"text-align: right;\"> -2.8819</td><td style=\"text-align: right;\">                -2.3</td><td style=\"text-align: right;\">               -3.55</td><td style=\"text-align: right;\">            288.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 639744\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_23-28-24\n",
      "  done: false\n",
      "  episode_len_mean: 287.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.299999999999995\n",
      "  episode_reward_mean: -2.8740999999999826\n",
      "  episode_reward_min: -3.369999999999972\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 1978\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999999\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.045762222139244\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.011985253630256007\n",
      "          policy_loss: -0.01930820350973015\n",
      "          total_loss: -0.022359022666883263\n",
      "          vf_explained_var: 0.5474748015403748\n",
      "          vf_loss: 0.009316756012332108\n",
      "    num_agent_steps_sampled: 639744\n",
      "    num_agent_steps_trained: 639744\n",
      "    num_steps_sampled: 639744\n",
      "    num_steps_trained: 639744\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.09624413145539\n",
      "    ram_util_percent: 52.055868544600955\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04743195478355089\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.57477782056567\n",
      "    mean_inference_ms: 4.740293655953406\n",
      "    mean_raw_obs_processing_ms: 1.9595503783455428\n",
      "  time_since_restore: 8640.021589040756\n",
      "  time_this_iter_s: 149.1107678413391\n",
      "  time_total_s: 8640.021589040756\n",
      "  timers:\n",
      "    learn_throughput: 587.916\n",
      "    learn_time_ms: 17002.422\n",
      "    load_throughput: 88791.706\n",
      "    load_time_ms: 112.578\n",
      "    sample_throughput: 80.318\n",
      "    sample_time_ms: 124455.714\n",
      "    update_time_ms: 20.606\n",
      "  timestamp: 1635550104\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 639744\n",
      "  training_iteration: 64\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         8640.02</td><td style=\"text-align: right;\">639744</td><td style=\"text-align: right;\"> -2.8741</td><td style=\"text-align: right;\">                -2.3</td><td style=\"text-align: right;\">               -3.37</td><td style=\"text-align: right;\">            287.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 649740\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_23-30-52\n",
      "  done: false\n",
      "  episode_len_mean: 285.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3699999999999934\n",
      "  episode_reward_mean: -2.8572999999999826\n",
      "  episode_reward_min: -3.289999999999974\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 2013\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999999\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.028498539659712\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.012719947084878499\n",
      "          policy_loss: -0.019898597004576624\n",
      "          total_loss: -0.02132507791249161\n",
      "          vf_explained_var: 0.5349735617637634\n",
      "          vf_loss: 0.010272539911132991\n",
      "    num_agent_steps_sampled: 649740\n",
      "    num_agent_steps_trained: 649740\n",
      "    num_steps_sampled: 649740\n",
      "    num_steps_trained: 649740\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 95.39526066350712\n",
      "    ram_util_percent: 51.920853080568726\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047426989198728384\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.599955867471184\n",
      "    mean_inference_ms: 4.740293594804004\n",
      "    mean_raw_obs_processing_ms: 1.9601208413894478\n",
      "  time_since_restore: 8788.174357652664\n",
      "  time_this_iter_s: 148.15276861190796\n",
      "  time_total_s: 8788.174357652664\n",
      "  timers:\n",
      "    learn_throughput: 587.523\n",
      "    learn_time_ms: 17013.796\n",
      "    load_throughput: 89014.834\n",
      "    load_time_ms: 112.296\n",
      "    sample_throughput: 79.292\n",
      "    sample_time_ms: 126065.894\n",
      "    update_time_ms: 20.518\n",
      "  timestamp: 1635550252\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 649740\n",
      "  training_iteration: 65\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         8788.17</td><td style=\"text-align: right;\">649740</td><td style=\"text-align: right;\"> -2.8573</td><td style=\"text-align: right;\">               -2.37</td><td style=\"text-align: right;\">               -3.29</td><td style=\"text-align: right;\">            285.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 659736\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_23-33-10\n",
      "  done: false\n",
      "  episode_len_mean: 283.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3699999999999934\n",
      "  episode_reward_mean: -2.8306999999999833\n",
      "  episode_reward_min: -3.289999999999974\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 2049\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999999\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 1.9929540474190672\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.011212341012001959\n",
      "          policy_loss: -0.014168124184267133\n",
      "          total_loss: -0.01581960196296374\n",
      "          vf_explained_var: 0.5183500051498413\n",
      "          vf_loss: 0.010709732177848411\n",
      "    num_agent_steps_sampled: 659736\n",
      "    num_agent_steps_trained: 659736\n",
      "    num_steps_sampled: 659736\n",
      "    num_steps_trained: 659736\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.38434343434342\n",
      "    ram_util_percent: 52.03737373737374\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047428502352147746\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.630507280200263\n",
      "    mean_inference_ms: 4.740257298564576\n",
      "    mean_raw_obs_processing_ms: 1.9571762834716515\n",
      "  time_since_restore: 8926.486211776733\n",
      "  time_this_iter_s: 138.3118541240692\n",
      "  time_total_s: 8926.486211776733\n",
      "  timers:\n",
      "    learn_throughput: 587.384\n",
      "    learn_time_ms: 17017.837\n",
      "    load_throughput: 89225.792\n",
      "    load_time_ms: 112.03\n",
      "    sample_throughput: 78.932\n",
      "    sample_time_ms: 126640.711\n",
      "    update_time_ms: 20.996\n",
      "  timestamp: 1635550390\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 659736\n",
      "  training_iteration: 66\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         8926.49</td><td style=\"text-align: right;\">659736</td><td style=\"text-align: right;\"> -2.8307</td><td style=\"text-align: right;\">               -2.37</td><td style=\"text-align: right;\">               -3.29</td><td style=\"text-align: right;\">            283.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625565)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625572)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=625561)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 669732\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_23-35-52\n",
      "  done: false\n",
      "  episode_len_mean: 283.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3699999999999934\n",
      "  episode_reward_mean: -2.8337999999999837\n",
      "  episode_reward_min: -3.46999999999997\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 2084\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999999\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 1.9871881185433804\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01419904472908332\n",
      "          policy_loss: -0.017674300724115126\n",
      "          total_loss: -0.01631366869386954\n",
      "          vf_explained_var: 0.392121285200119\n",
      "          vf_loss: 0.011648157352092079\n",
      "    num_agent_steps_sampled: 669732\n",
      "    num_agent_steps_trained: 669732\n",
      "    num_steps_sampled: 669732\n",
      "    num_steps_trained: 669732\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.5948051948052\n",
      "    ram_util_percent: 51.813419913419914\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04740330785851413\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.653190340206717\n",
      "    mean_inference_ms: 4.7393193107329035\n",
      "    mean_raw_obs_processing_ms: 1.967398201614273\n",
      "  time_since_restore: 9088.343566656113\n",
      "  time_this_iter_s: 161.85735487937927\n",
      "  time_total_s: 9088.343566656113\n",
      "  timers:\n",
      "    learn_throughput: 587.448\n",
      "    learn_time_ms: 17015.966\n",
      "    load_throughput: 89109.145\n",
      "    load_time_ms: 112.177\n",
      "    sample_throughput: 78.089\n",
      "    sample_time_ms: 128007.181\n",
      "    update_time_ms: 20.012\n",
      "  timestamp: 1635550552\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 669732\n",
      "  training_iteration: 67\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         9088.34</td><td style=\"text-align: right;\">669732</td><td style=\"text-align: right;\"> -2.8338</td><td style=\"text-align: right;\">               -2.37</td><td style=\"text-align: right;\">               -3.47</td><td style=\"text-align: right;\">            283.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4887_00000:\n",
      "  agent_timesteps_total: 679728\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-29_23-38-07\n",
      "  done: false\n",
      "  episode_len_mean: 283.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4599999999999915\n",
      "  episode_reward_mean: -2.8319999999999834\n",
      "  episode_reward_min: -3.46999999999997\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 2119\n",
      "  experiment_id: 39255c2c32a148c1abc3e53c967b4ad5\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999999\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 1.9528093075140929\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01206058641523219\n",
      "          policy_loss: -0.013660110616021686\n",
      "          total_loss: -0.014016258070229465\n",
      "          vf_explained_var: 0.3820677697658539\n",
      "          vf_loss: 0.011031048572408313\n",
      "    num_agent_steps_sampled: 679728\n",
      "    num_agent_steps_trained: 679728\n",
      "    num_steps_sampled: 679728\n",
      "    num_steps_trained: 679728\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.43886010362695\n",
      "    ram_util_percent: 51.88963730569949\n",
      "  pid: 625567\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047410120662582184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.68007808591598\n",
      "    mean_inference_ms: 4.739016748623345\n",
      "    mean_raw_obs_processing_ms: 1.9691346661504798\n",
      "  time_since_restore: 9223.72510266304\n",
      "  time_this_iter_s: 135.3815360069275\n",
      "  time_total_s: 9223.72510266304\n",
      "  timers:\n",
      "    learn_throughput: 587.069\n",
      "    learn_time_ms: 17026.954\n",
      "    load_throughput: 89396.675\n",
      "    load_time_ms: 111.816\n",
      "    sample_throughput: 78.029\n",
      "    sample_time_ms: 128106.354\n",
      "    update_time_ms: 20.496\n",
      "  timestamp: 1635550687\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 679728\n",
      "  training_iteration: 68\n",
      "  trial_id: c4887_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/22.75 GiB heap, 0.0/11.37 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-29_21-04-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4887_00000</td><td>RUNNING </td><td>192.168.3.5:625567</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         9223.73</td><td style=\"text-align: right;\">679728</td><td style=\"text-align: right;\">  -2.832</td><td style=\"text-align: right;\">               -2.46</td><td style=\"text-align: right;\">               -3.47</td><td style=\"text-align: right;\">             283.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "analysis = tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 3,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 5_000,\n",
    "             \"lr\": 1e-4,\n",
    "             #\"gamma\": 0.99,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO MultiTask <=10 pretrained (AngelaCNN + MLP 8) (3 noops after placement) r: -0.01\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
