{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "#from models import VisualEncoder\n",
    "from train import *\n",
    "from wrappers_2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9fd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=2, stride=2, padding=0),  \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(), \n",
    "            nn.Conv2d(128, 256, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import one_hot\n",
    "\n",
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        visual_features_dim = 512\n",
    "        target_features_dim = 9 * 11 * 11 \n",
    "        self.visual_encoder = VisualEncoder()\n",
    "        self.visual_encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AngelaCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.target_encoder = nn.Sequential(\n",
    "            nn.Conv3d(7, 1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        policy_hidden_dim = 256 \n",
    "        self.policy_network = nn.Sequential(\n",
    "            nn.Linear(visual_features_dim + target_features_dim, 512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, policy_hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.action_head = nn.Linear(policy_hidden_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(policy_hidden_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.visual_encoder.cuda()\n",
    "            self.target_encoder.cuda()\n",
    "            self.policy_network.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs']\n",
    "        pov = obs['pov'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        target = one_hot(obs['target_grid'].long(), num_classes=7).permute(0, 4, 1, 2, 3).float()\n",
    "        if self.use_cuda:\n",
    "            pov.cuda()\n",
    "            target.cuda()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            visual_features = self.visual_encoder(pov)\n",
    "            \n",
    "        target_features = self.target_encoder(target)\n",
    "        target_features = target_features.reshape(target_features.shape[0], -1)\n",
    "        features = torch.cat([visual_features, target_features], dim=1)\n",
    "        features = self.policy_network(features)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc09c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualObservationWrapper(ObsWrapper):\n",
    "    def __init__(self, env, include_target=False):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = {   \n",
    "            'pov': gym.spaces.Box(low=0, high=255, shape=(64, 64, 3)),\n",
    "            'inventory': gym.spaces.Box(low=0.0, high=20.0, shape=(6,)),\n",
    "            'compass': gym.spaces.Box(low=-180.0, high=180.0, shape=(1,))\n",
    "        }\n",
    "        if include_target:\n",
    "            self.observation_space['target_grid'] = \\\n",
    "                gym.spaces.Box(low=0, high=6, shape=(9, 11, 11))\n",
    "        self.observation_space = gym.spaces.Dict(self.observation_space)\n",
    "\n",
    "    def observation(self, obs, reward=None, done=None, info=None):\n",
    "        if info is not None:\n",
    "            if 'target_grid' in info:\n",
    "                target_grid = info['target_grid']\n",
    "                del info['target_grid']\n",
    "            else:\n",
    "                logger.error(f'info: {info}')\n",
    "                if hasattr(self.unwrapped, 'should_reset'):\n",
    "                    self.unwrapped.should_reset(True)\n",
    "                target_grid = self.env.unwrapped.tasks.current.target_grid\n",
    "        else:\n",
    "            target_grid = self.env.unwrapped.tasks.current.target_grid\n",
    "        return {\n",
    "            'pov': obs['pov'].astype(np.float32),\n",
    "            'inventory': obs['inventory'],\n",
    "            'compass': np.array([obs['compass']['angle'].item()]),\n",
    "            'target_grid': target_grid\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "class RewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def reward(self, rew):\n",
    "        if rew == 0:\n",
    "            rew = -0.01\n",
    "        return rew\n",
    "    \n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "    env.update_taskset(TaskSet(preset=['C3', 'C17', 'C20', 'C22', 'C32', 'C40', 'C85', 'C87', 'C93']))\n",
    "    #env = PovOnlyWrapper(env)\n",
    "    env = VisualObservationWrapper(env, include_target=True)\n",
    "    env = SelectAndPlace(env)\n",
    "    env = Discretization(env, flat_action_space('human-level'))\n",
    "    env = RewardWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-24 13:21:27,928\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-10-24 13:21:27,939\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to auto resume run with id 722d3_00000 but id 4b4a2_00000 is set.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=380543)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380543)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO MultiTask <=10 pretrained (AngelaCNN + changed policy) (3 noops after placement) r: -0.01</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/4b4a2_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/4b4a2_00000</a><br/>\n",
       "                Run data is saved locally in <code>/IGLU-Minecraft/wandb/run-20211024_132128-4b4a2_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=380543)\u001b[0m 2021-10-24 13:21:31,457\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=380543)\u001b[0m 2021-10-24 13:21:31,457\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=380543)\u001b[0m 2021-10-24 13:21:37,493\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-22-45\n",
      "  done: false\n",
      "  episode_len_mean: 407.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4.189999999999955\n",
      "  episode_reward_mean: -9.379999999999951\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.88430388768514\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005359336429532312\n",
      "          policy_loss: 0.1058047072754966\n",
      "          total_loss: 0.3807262102762858\n",
      "          vf_explained_var: -0.13551174104213715\n",
      "          vf_loss: 0.30269267728435806\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.23367346938777\n",
      "    ram_util_percent: 48.98877551020408\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04191522474412794\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 64.78542285961109\n",
      "    mean_inference_ms: 2.2800838077937686\n",
      "    mean_raw_obs_processing_ms: 0.2254336983054787\n",
      "  time_since_restore: 68.33106994628906\n",
      "  time_this_iter_s: 68.33106994628906\n",
      "  time_total_s: 68.33106994628906\n",
      "  timers:\n",
      "    learn_throughput: 1228.444\n",
      "    learn_time_ms: 814.038\n",
      "    load_throughput: 38663.594\n",
      "    load_time_ms: 25.864\n",
      "    sample_throughput: 14.82\n",
      "    sample_time_ms: 67477.676\n",
      "    update_time_ms: 7.79\n",
      "  timestamp: 1635081765\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         68.3311</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">   -9.38</td><td style=\"text-align: right;\">               -4.19</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">             407.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-23-13\n",
      "  done: false\n",
      "  episode_len_mean: 409.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4.089999999999957\n",
      "  episode_reward_mean: -6.744999999999954\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 4\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.880343461036682\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0038223870718520118\n",
      "          policy_loss: 0.03334708702233102\n",
      "          total_loss: 0.21720773958497577\n",
      "          vf_explained_var: -0.259689062833786\n",
      "          vf_loss: 0.21189961622779568\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.3948717948718\n",
      "    ram_util_percent: 50.83333333333333\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04247658701941437\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 54.56472731433459\n",
      "    mean_inference_ms: 2.3005822272830123\n",
      "    mean_raw_obs_processing_ms: 0.22724552088636107\n",
      "  time_since_restore: 95.79855823516846\n",
      "  time_this_iter_s: 27.467488288879395\n",
      "  time_total_s: 95.79855823516846\n",
      "  timers:\n",
      "    learn_throughput: 1227.029\n",
      "    learn_time_ms: 814.977\n",
      "    load_throughput: 39087.138\n",
      "    load_time_ms: 25.584\n",
      "    sample_throughput: 21.257\n",
      "    sample_time_ms: 47043.14\n",
      "    update_time_ms: 10.259\n",
      "  timestamp: 1635081793\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         95.7986</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">  -6.745</td><td style=\"text-align: right;\">               -4.09</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">            409.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-23-38\n",
      "  done: false\n",
      "  episode_len_mean: 411.42857142857144\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4.089999999999957\n",
      "  episode_reward_mean: -6.1671428571428075\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 7\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8637402693430585\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007777223787296271\n",
      "          policy_loss: 0.08529037208192879\n",
      "          total_loss: 0.06441561614887582\n",
      "          vf_explained_var: 0.3617481589317322\n",
      "          vf_loss: 0.0069849242068206275\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.06944444444444\n",
      "    ram_util_percent: 50.57222222222222\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042729181684020845\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 46.90423642149574\n",
      "    mean_inference_ms: 2.3074295384204784\n",
      "    mean_raw_obs_processing_ms: 0.23218732861484578\n",
      "  time_since_restore: 120.7206346988678\n",
      "  time_this_iter_s: 24.92207646369934\n",
      "  time_total_s: 120.7206346988678\n",
      "  timers:\n",
      "    learn_throughput: 1220.349\n",
      "    learn_time_ms: 819.438\n",
      "    load_throughput: 44692.842\n",
      "    load_time_ms: 22.375\n",
      "    sample_throughput: 25.39\n",
      "    sample_time_ms: 39385.295\n",
      "    update_time_ms: 8.093\n",
      "  timestamp: 1635081818\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         120.721</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">-6.16714</td><td style=\"text-align: right;\">               -4.09</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           411.429</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-24-02\n",
      "  done: false\n",
      "  episode_len_mean: 415.6666666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4.0299999999999585\n",
      "  episode_reward_mean: -5.753333333333284\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 9\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8657065709431966\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0029981814722072286\n",
      "          policy_loss: 0.051370571470922895\n",
      "          total_loss: 0.028304285638862187\n",
      "          vf_explained_var: 0.3005284368991852\n",
      "          vf_loss: 0.005290961389740308\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.16470588235295\n",
      "    ram_util_percent: 50.49705882352941\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04287144320385036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 43.724285405132754\n",
      "    mean_inference_ms: 2.306538117754079\n",
      "    mean_raw_obs_processing_ms: 0.23197911106153907\n",
      "  time_since_restore: 144.4785578250885\n",
      "  time_this_iter_s: 23.757923126220703\n",
      "  time_total_s: 144.4785578250885\n",
      "  timers:\n",
      "    learn_throughput: 1219.698\n",
      "    learn_time_ms: 819.875\n",
      "    load_throughput: 42984.336\n",
      "    load_time_ms: 23.264\n",
      "    sample_throughput: 28.359\n",
      "    sample_time_ms: 35262.631\n",
      "    update_time_ms: 9.166\n",
      "  timestamp: 1635081842\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         144.479</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-5.75333</td><td style=\"text-align: right;\">               -4.03</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           415.667</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-24-26\n",
      "  done: false\n",
      "  episode_len_mean: 412.9166666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8699999999999615\n",
      "  episode_reward_mean: -5.891666666666617\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 12\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8637919452455307\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009324571621031967\n",
      "          policy_loss: 0.02014780773056878\n",
      "          total_loss: 0.08817347702052858\n",
      "          vf_explained_var: 0.29073312878608704\n",
      "          vf_loss: 0.09619736303057935\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 5000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.68571428571428\n",
      "    ram_util_percent: 50.465714285714284\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04295468060228475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.37353707387081\n",
      "    mean_inference_ms: 2.3033355141931096\n",
      "    mean_raw_obs_processing_ms: 0.23388719199214583\n",
      "  time_since_restore: 169.2180814743042\n",
      "  time_this_iter_s: 24.7395236492157\n",
      "  time_total_s: 169.2180814743042\n",
      "  timers:\n",
      "    learn_throughput: 1220.321\n",
      "    learn_time_ms: 819.456\n",
      "    load_throughput: 41726.064\n",
      "    load_time_ms: 23.966\n",
      "    sample_throughput: 30.317\n",
      "    sample_time_ms: 32984.968\n",
      "    update_time_ms: 10.524\n",
      "  timestamp: 1635081866\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         169.218</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">-5.89167</td><td style=\"text-align: right;\">               -3.87</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           412.917</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-24-50\n",
      "  done: false\n",
      "  episode_len_mean: 412.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8699999999999615\n",
      "  episode_reward_mean: -5.6307142857142365\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 14\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.857915931277805\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0067264830165445835\n",
      "          policy_loss: -0.07643999200728205\n",
      "          total_loss: -0.09400515688790215\n",
      "          vf_explained_var: 0.15042069554328918\n",
      "          vf_loss: 0.010677671308318775\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.57878787878786\n",
      "    ram_util_percent: 50.481818181818184\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04298143523972162\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.69006939439295\n",
      "    mean_inference_ms: 2.299444478640903\n",
      "    mean_raw_obs_processing_ms: 0.23409369673150385\n",
      "  time_since_restore: 192.47433280944824\n",
      "  time_this_iter_s: 23.256251335144043\n",
      "  time_total_s: 192.47433280944824\n",
      "  timers:\n",
      "    learn_throughput: 1229.984\n",
      "    learn_time_ms: 813.019\n",
      "    load_throughput: 43168.745\n",
      "    load_time_ms: 23.165\n",
      "    sample_throughput: 32.023\n",
      "    sample_time_ms: 31227.305\n",
      "    update_time_ms: 10.648\n",
      "  timestamp: 1635081890\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         192.474</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">-5.63071</td><td style=\"text-align: right;\">               -3.87</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">               412</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 7000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-25-14\n",
      "  done: false\n",
      "  episode_len_mean: 410.88235294117646\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8699999999999615\n",
      "  episode_reward_mean: -5.352941176470542\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 17\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.86157689359453\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00586386673848417\n",
      "          policy_loss: 0.014785119808382458\n",
      "          total_loss: -0.004635833617713716\n",
      "          vf_explained_var: -0.07290228456258774\n",
      "          vf_loss: 0.008901621838514175\n",
      "    num_agent_steps_sampled: 7000\n",
      "    num_agent_steps_trained: 7000\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 7000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.41714285714285\n",
      "    ram_util_percent: 50.488571428571426\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04299947202900809\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 36.710951464747986\n",
      "    mean_inference_ms: 2.2946659619485987\n",
      "    mean_raw_obs_processing_ms: 0.23469865198858614\n",
      "  time_since_restore: 216.71785950660706\n",
      "  time_this_iter_s: 24.243526697158813\n",
      "  time_total_s: 216.71785950660706\n",
      "  timers:\n",
      "    learn_throughput: 1231.637\n",
      "    learn_time_ms: 811.927\n",
      "    load_throughput: 42335.612\n",
      "    load_time_ms: 23.621\n",
      "    sample_throughput: 33.212\n",
      "    sample_time_ms: 30109.188\n",
      "    update_time_ms: 10.143\n",
      "  timestamp: 1635081914\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         216.718</td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\">-5.35294</td><td style=\"text-align: right;\">               -3.87</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           410.882</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-25-36\n",
      "  done: false\n",
      "  episode_len_mean: 410.4736842105263\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8699999999999615\n",
      "  episode_reward_mean: -5.217894736842059\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 19\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8467625591490004\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0054967777837697206\n",
      "          policy_loss: 0.09801394848359955\n",
      "          total_loss: 0.07302356039484342\n",
      "          vf_explained_var: 0.7789921164512634\n",
      "          vf_loss: 0.0032023975632101713\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.51249999999999\n",
      "    ram_util_percent: 50.403125\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042993894317821406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.627682070554776\n",
      "    mean_inference_ms: 2.2912362170514884\n",
      "    mean_raw_obs_processing_ms: 0.2346217391228342\n",
      "  time_since_restore: 239.1404891014099\n",
      "  time_this_iter_s: 22.422629594802856\n",
      "  time_total_s: 239.1404891014099\n",
      "  timers:\n",
      "    learn_throughput: 1226.734\n",
      "    learn_time_ms: 815.172\n",
      "    load_throughput: 42153.756\n",
      "    load_time_ms: 23.723\n",
      "    sample_throughput: 34.437\n",
      "    sample_time_ms: 29038.558\n",
      "    update_time_ms: 10.328\n",
      "  timestamp: 1635081936\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">          239.14</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-5.21789</td><td style=\"text-align: right;\">               -3.87</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           410.474</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-25-57\n",
      "  done: false\n",
      "  episode_len_mean: 411.76190476190476\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8699999999999615\n",
      "  episode_reward_mean: -5.124761904761859\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 21\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.802092525694105\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009422847857244222\n",
      "          policy_loss: -0.05939520647128423\n",
      "          total_loss: -0.0815585041211711\n",
      "          vf_explained_var: 0.7241853475570679\n",
      "          vf_loss: 0.00538648307038885\n",
      "    num_agent_steps_sampled: 9000\n",
      "    num_agent_steps_trained: 9000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.4965517241379\n",
      "    ram_util_percent: 50.400000000000006\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04299987431234206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 34.654152124553065\n",
      "    mean_inference_ms: 2.287684325718607\n",
      "    mean_raw_obs_processing_ms: 0.2342738603179001\n",
      "  time_since_restore: 259.7845389842987\n",
      "  time_this_iter_s: 20.644049882888794\n",
      "  time_total_s: 259.7845389842987\n",
      "  timers:\n",
      "    learn_throughput: 1230.887\n",
      "    learn_time_ms: 812.422\n",
      "    load_throughput: 43852.118\n",
      "    load_time_ms: 22.804\n",
      "    sample_throughput: 35.696\n",
      "    sample_time_ms: 28014.078\n",
      "    update_time_ms: 10.983\n",
      "  timestamp: 1635081957\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         259.785</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">-5.12476</td><td style=\"text-align: right;\">               -3.87</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           411.762</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-26-16\n",
      "  done: false\n",
      "  episode_len_mean: 416.2916666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8699999999999615\n",
      "  episode_reward_mean: -5.044166666666619\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 24\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.79072376092275\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008623493259970121\n",
      "          policy_loss: -0.06873717043134901\n",
      "          total_loss: -0.09117124279340108\n",
      "          vf_explained_var: 0.7958502173423767\n",
      "          vf_loss: 0.005041990479609618\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.34285714285714\n",
      "    ram_util_percent: 50.346428571428575\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043018755763296966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.377327314513586\n",
      "    mean_inference_ms: 2.282394124666134\n",
      "    mean_raw_obs_processing_ms: 0.23395789934986252\n",
      "  time_since_restore: 278.93045592308044\n",
      "  time_this_iter_s: 19.14591693878174\n",
      "  time_total_s: 278.93045592308044\n",
      "  timers:\n",
      "    learn_throughput: 1228.793\n",
      "    learn_time_ms: 813.807\n",
      "    load_throughput: 43215.966\n",
      "    load_time_ms: 23.14\n",
      "    sample_throughput: 36.981\n",
      "    sample_time_ms: 27041.244\n",
      "    update_time_ms: 10.221\n",
      "  timestamp: 1635081976\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">          278.93</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">-5.04417</td><td style=\"text-align: right;\">               -3.87</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           416.292</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 11000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-26-36\n",
      "  done: false\n",
      "  episode_len_mean: 418.9230769230769\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8699999999999615\n",
      "  episode_reward_mean: -5.002692307692261\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 26\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7891933467653063\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008649930648380138\n",
      "          policy_loss: 0.06427860044770771\n",
      "          total_loss: 0.04022775474521849\n",
      "          vf_explained_var: 0.8645550012588501\n",
      "          vf_loss: 0.003408590362070956\n",
      "    num_agent_steps_sampled: 11000\n",
      "    num_agent_steps_trained: 11000\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 11000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.93333333333334\n",
      "    ram_util_percent: 50.33333333333332\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043026350177095586\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.63117641044602\n",
      "    mean_inference_ms: 2.2791659272556273\n",
      "    mean_raw_obs_processing_ms: 0.23362089885795592\n",
      "  time_since_restore: 298.33137369155884\n",
      "  time_this_iter_s: 19.400917768478394\n",
      "  time_total_s: 298.33137369155884\n",
      "  timers:\n",
      "    learn_throughput: 1230.764\n",
      "    learn_time_ms: 812.503\n",
      "    load_throughput: 45043.956\n",
      "    load_time_ms: 22.201\n",
      "    sample_throughput: 45.147\n",
      "    sample_time_ms: 22149.724\n",
      "    update_time_ms: 11.098\n",
      "  timestamp: 1635081996\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 11\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         298.331</td><td style=\"text-align: right;\">11000</td><td style=\"text-align: right;\">-5.00269</td><td style=\"text-align: right;\">               -3.87</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           418.923</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-26-55\n",
      "  done: false\n",
      "  episode_len_mean: 419.7857142857143\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8699999999999615\n",
      "  episode_reward_mean: -4.953214285714238\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 28\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7519490480422975\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008931262962184543\n",
      "          policy_loss: -0.03598511914412181\n",
      "          total_loss: -0.05621576234698296\n",
      "          vf_explained_var: 0.6135743856430054\n",
      "          vf_loss: 0.006842284664485811\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.31071428571431\n",
      "    ram_util_percent: 50.35714285714284\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043033460166915526\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.943545349598395\n",
      "    mean_inference_ms: 2.2761422444405635\n",
      "    mean_raw_obs_processing_ms: 0.23323137278203374\n",
      "  time_since_restore: 317.3034267425537\n",
      "  time_this_iter_s: 18.972053050994873\n",
      "  time_total_s: 317.3034267425537\n",
      "  timers:\n",
      "    learn_throughput: 1232.419\n",
      "    learn_time_ms: 811.412\n",
      "    load_throughput: 47110.655\n",
      "    load_time_ms: 21.227\n",
      "    sample_throughput: 46.945\n",
      "    sample_time_ms: 21301.64\n",
      "    update_time_ms: 11.415\n",
      "  timestamp: 1635082015\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         317.303</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-4.95321</td><td style=\"text-align: right;\">               -3.87</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           419.786</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 13000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-27-30\n",
      "  done: false\n",
      "  episode_len_mean: 421.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8699999999999615\n",
      "  episode_reward_mean: -4.9239999999999515\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 30\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7551762660344443\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010052299350094293\n",
      "          policy_loss: -0.11716279453701443\n",
      "          total_loss: -0.13601798208223448\n",
      "          vf_explained_var: 0.5371256470680237\n",
      "          vf_loss: 0.008193958817153341\n",
      "    num_agent_steps_sampled: 13000\n",
      "    num_agent_steps_trained: 13000\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 13000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.038\n",
      "    ram_util_percent: 49.982\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04303100295867937\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.30720963726248\n",
      "    mean_inference_ms: 2.273212768104422\n",
      "    mean_raw_obs_processing_ms: 0.31824761771908666\n",
      "  time_since_restore: 352.5407271385193\n",
      "  time_this_iter_s: 35.237300395965576\n",
      "  time_total_s: 352.5407271385193\n",
      "  timers:\n",
      "    learn_throughput: 1222.183\n",
      "    learn_time_ms: 818.208\n",
      "    load_throughput: 45193.345\n",
      "    load_time_ms: 22.127\n",
      "    sample_throughput: 44.792\n",
      "    sample_time_ms: 22325.267\n",
      "    update_time_ms: 11.528\n",
      "  timestamp: 1635082050\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 13\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         352.541</td><td style=\"text-align: right;\">13000</td><td style=\"text-align: right;\">  -4.924</td><td style=\"text-align: right;\">               -3.87</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">             421.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 14000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-27-54\n",
      "  done: false\n",
      "  episode_len_mean: 425.59375\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8699999999999615\n",
      "  episode_reward_mean: -4.916874999999951\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 32\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7047678258683945\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011030154584458766\n",
      "          policy_loss: -0.12582069900300769\n",
      "          total_loss: -0.1439033266570833\n",
      "          vf_explained_var: 0.3569977283477783\n",
      "          vf_loss: 0.008413546573299553\n",
      "    num_agent_steps_sampled: 14000\n",
      "    num_agent_steps_trained: 14000\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.16470588235293\n",
      "    ram_util_percent: 50.72352941176471\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043031073207550806\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.741649141645407\n",
      "    mean_inference_ms: 2.270817882733722\n",
      "    mean_raw_obs_processing_ms: 0.3868696378647752\n",
      "  time_since_restore: 376.4300456047058\n",
      "  time_this_iter_s: 23.889318466186523\n",
      "  time_total_s: 376.4300456047058\n",
      "  timers:\n",
      "    learn_throughput: 1221.874\n",
      "    learn_time_ms: 818.415\n",
      "    load_throughput: 47390.053\n",
      "    load_time_ms: 21.101\n",
      "    sample_throughput: 44.764\n",
      "    sample_time_ms: 22339.615\n",
      "    update_time_ms: 10.951\n",
      "  timestamp: 1635082074\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 14\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">          376.43</td><td style=\"text-align: right;\">14000</td><td style=\"text-align: right;\">-4.91687</td><td style=\"text-align: right;\">               -3.87</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           425.594</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-28-15\n",
      "  done: false\n",
      "  episode_len_mean: 429.1470588235294\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8699999999999615\n",
      "  episode_reward_mean: -4.913529411764657\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 34\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.690902331140306\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006897922081324411\n",
      "          policy_loss: -0.11341940859953563\n",
      "          total_loss: -0.13096090919441647\n",
      "          vf_explained_var: 0.3277705907821655\n",
      "          vf_loss: 0.009022625293194626\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_agent_steps_trained: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.37333333333335\n",
      "    ram_util_percent: 51.01\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04304285074656172\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.224021909167284\n",
      "    mean_inference_ms: 2.268967383855075\n",
      "    mean_raw_obs_processing_ms: 0.4427541081197645\n",
      "  time_since_restore: 397.5153694152832\n",
      "  time_this_iter_s: 21.085323810577393\n",
      "  time_total_s: 397.5153694152832\n",
      "  timers:\n",
      "    learn_throughput: 1210.334\n",
      "    learn_time_ms: 826.219\n",
      "    load_throughput: 49249.094\n",
      "    load_time_ms: 20.305\n",
      "    sample_throughput: 45.52\n",
      "    sample_time_ms: 21968.196\n",
      "    update_time_ms: 10.019\n",
      "  timestamp: 1635082095\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         397.515</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">-4.91353</td><td style=\"text-align: right;\">               -3.87</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           429.147</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-28-37\n",
      "  done: false\n",
      "  episode_len_mean: 429.4864864864865\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8699999999999615\n",
      "  episode_reward_mean: -4.866486486486437\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 37\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7218888176812066\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013307053525752366\n",
      "          policy_loss: 0.046900510208474265\n",
      "          total_loss: 0.0299309813314014\n",
      "          vf_explained_var: 0.2387276142835617\n",
      "          vf_loss: 0.009584005061899208\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.84193548387097\n",
      "    ram_util_percent: 50.89032258064517\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04305680297748185\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.534313064809616\n",
      "    mean_inference_ms: 2.2665885275269066\n",
      "    mean_raw_obs_processing_ms: 0.5098618326127691\n",
      "  time_since_restore: 419.4031295776367\n",
      "  time_this_iter_s: 21.887760162353516\n",
      "  time_total_s: 419.4031295776367\n",
      "  timers:\n",
      "    learn_throughput: 1202.929\n",
      "    learn_time_ms: 831.304\n",
      "    load_throughput: 47516.002\n",
      "    load_time_ms: 21.046\n",
      "    sample_throughput: 45.818\n",
      "    sample_time_ms: 21825.276\n",
      "    update_time_ms: 10.13\n",
      "  timestamp: 1635082117\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 16\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         419.403</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-4.86649</td><td style=\"text-align: right;\">               -3.87</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           429.486</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 17000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-28-59\n",
      "  done: false\n",
      "  episode_len_mean: 429.2564102564103\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8699999999999615\n",
      "  episode_reward_mean: -4.834871794871745\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 39\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.762646926773919\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009849403061103167\n",
      "          policy_loss: -0.051389231118890974\n",
      "          total_loss: -0.06848162727223503\n",
      "          vf_explained_var: 0.06664454191923141\n",
      "          vf_loss: 0.010041599159335925\n",
      "    num_agent_steps_sampled: 17000\n",
      "    num_agent_steps_trained: 17000\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 17000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.03225806451613\n",
      "    ram_util_percent: 50.85806451612901\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04306635782244329\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.123338826218497\n",
      "    mean_inference_ms: 2.265177034244266\n",
      "    mean_raw_obs_processing_ms: 0.545709526434697\n",
      "  time_since_restore: 441.17506217956543\n",
      "  time_this_iter_s: 21.77193260192871\n",
      "  time_total_s: 441.17506217956543\n",
      "  timers:\n",
      "    learn_throughput: 1200.76\n",
      "    learn_time_ms: 832.806\n",
      "    load_throughput: 49898.272\n",
      "    load_time_ms: 20.041\n",
      "    sample_throughput: 46.344\n",
      "    sample_time_ms: 21577.877\n",
      "    update_time_ms: 9.719\n",
      "  timestamp: 1635082139\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 17\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         441.175</td><td style=\"text-align: right;\">17000</td><td style=\"text-align: right;\">-4.83487</td><td style=\"text-align: right;\">               -3.87</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           429.256</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-29-21\n",
      "  done: false\n",
      "  episode_len_mean: 428.4761904761905\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8699999999999615\n",
      "  episode_reward_mean: -4.788333333333284\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 42\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.757556094063653\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007747989675264528\n",
      "          policy_loss: -0.0035424255662494237\n",
      "          total_loss: -0.021361231472757126\n",
      "          vf_explained_var: 0.20351792871952057\n",
      "          vf_loss: 0.009369354958309688\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.74848484848485\n",
      "    ram_util_percent: 51.05151515151516\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04308454495440842\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.570771854324736\n",
      "    mean_inference_ms: 2.263470153695119\n",
      "    mean_raw_obs_processing_ms: 0.5892782918584436\n",
      "  time_since_restore: 463.8489680290222\n",
      "  time_this_iter_s: 22.673905849456787\n",
      "  time_total_s: 463.8489680290222\n",
      "  timers:\n",
      "    learn_throughput: 1199.595\n",
      "    learn_time_ms: 833.615\n",
      "    load_throughput: 49529.41\n",
      "    load_time_ms: 20.19\n",
      "    sample_throughput: 46.29\n",
      "    sample_time_ms: 21602.832\n",
      "    update_time_ms: 8.987\n",
      "  timestamp: 1635082161\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         463.849</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">-4.78833</td><td style=\"text-align: right;\">               -3.87</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           428.476</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 19000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-29-45\n",
      "  done: false\n",
      "  episode_len_mean: 426.47727272727275\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8299999999999623\n",
      "  episode_reward_mean: -4.745454545454496\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 44\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7390783071517943\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007586226554122611\n",
      "          policy_loss: -0.04782730109161801\n",
      "          total_loss: -0.06543690727816688\n",
      "          vf_explained_var: 0.44524943828582764\n",
      "          vf_loss: 0.009401864207272107\n",
      "    num_agent_steps_sampled: 19000\n",
      "    num_agent_steps_trained: 19000\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 19000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.47058823529412\n",
      "    ram_util_percent: 51.21764705882353\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04309339186273501\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.241479150347633\n",
      "    mean_inference_ms: 2.2623662931876676\n",
      "    mean_raw_obs_processing_ms: 0.6127777535080176\n",
      "  time_since_restore: 487.4561505317688\n",
      "  time_this_iter_s: 23.607182502746582\n",
      "  time_total_s: 487.4561505317688\n",
      "  timers:\n",
      "    learn_throughput: 1197.584\n",
      "    learn_time_ms: 835.015\n",
      "    load_throughput: 46103.26\n",
      "    load_time_ms: 21.69\n",
      "    sample_throughput: 45.669\n",
      "    sample_time_ms: 21896.534\n",
      "    update_time_ms: 8.463\n",
      "  timestamp: 1635082185\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 19\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         487.456</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\">-4.74545</td><td style=\"text-align: right;\">               -3.83</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           426.477</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-30-08\n",
      "  done: false\n",
      "  episode_len_mean: 423.51063829787233\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.7799999999999634\n",
      "  episode_reward_mean: -4.685106382978676\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 47\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.64856805006663\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010768824265975536\n",
      "          policy_loss: 0.04829160703553094\n",
      "          total_loss: 0.032799794773260754\n",
      "          vf_explained_var: 0.20038259029388428\n",
      "          vf_loss: 0.010455425841630332\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.85454545454546\n",
      "    ram_util_percent: 50.97878787878787\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04310223436769619\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.795777675795517\n",
      "    mean_inference_ms: 2.2607635301794655\n",
      "    mean_raw_obs_processing_ms: 0.641559925055973\n",
      "  time_since_restore: 510.8827428817749\n",
      "  time_this_iter_s: 23.426592350006104\n",
      "  time_total_s: 510.8827428817749\n",
      "  timers:\n",
      "    learn_throughput: 1194.975\n",
      "    learn_time_ms: 836.838\n",
      "    load_throughput: 48242.719\n",
      "    load_time_ms: 20.729\n",
      "    sample_throughput: 44.796\n",
      "    sample_time_ms: 22323.546\n",
      "    update_time_ms: 8.689\n",
      "  timestamp: 1635082208\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 20\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         510.883</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-4.68511</td><td style=\"text-align: right;\">               -3.78</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           423.511</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-30-33\n",
      "  done: false\n",
      "  episode_len_mean: 421.6734693877551\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.699999999999965\n",
      "  episode_reward_mean: -4.648367346938729\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 49\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.660336396429274\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014142205701428523\n",
      "          policy_loss: -0.1246736056274838\n",
      "          total_loss: -0.13928459021780226\n",
      "          vf_explained_var: 0.15699106454849243\n",
      "          vf_loss: 0.011285273222407946\n",
      "    num_agent_steps_sampled: 21000\n",
      "    num_agent_steps_trained: 21000\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 21000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.27142857142857\n",
      "    ram_util_percent: 50.92000000000001\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04311010568081058\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.528742872404948\n",
      "    mean_inference_ms: 2.259831958367218\n",
      "    mean_raw_obs_processing_ms: 0.6571758474692327\n",
      "  time_since_restore: 535.5047316551208\n",
      "  time_this_iter_s: 24.621988773345947\n",
      "  time_total_s: 535.5047316551208\n",
      "  timers:\n",
      "    learn_throughput: 1189.197\n",
      "    learn_time_ms: 840.904\n",
      "    load_throughput: 47119.652\n",
      "    load_time_ms: 21.223\n",
      "    sample_throughput: 43.78\n",
      "    sample_time_ms: 22841.597\n",
      "    update_time_ms: 7.748\n",
      "  timestamp: 1635082233\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 21\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         535.505</td><td style=\"text-align: right;\">21000</td><td style=\"text-align: right;\">-4.64837</td><td style=\"text-align: right;\">                -3.7</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           421.673</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 22000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-30-56\n",
      "  done: false\n",
      "  episode_len_mean: 419.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.6699999999999657\n",
      "  episode_reward_mean: -4.601730769230723\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 52\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6243520471784803\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012735348652799771\n",
      "          policy_loss: 0.02514666650030348\n",
      "          total_loss: 0.009611735824081633\n",
      "          vf_explained_var: 0.20497895777225494\n",
      "          vf_loss: 0.010071825625426653\n",
      "    num_agent_steps_sampled: 22000\n",
      "    num_agent_steps_trained: 22000\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.74242424242425\n",
      "    ram_util_percent: 50.978787878787884\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04311937334318027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.16260513681455\n",
      "    mean_inference_ms: 2.258559905528259\n",
      "    mean_raw_obs_processing_ms: 0.6763392587293209\n",
      "  time_since_restore: 558.6550714969635\n",
      "  time_this_iter_s: 23.15033984184265\n",
      "  time_total_s: 558.6550714969635\n",
      "  timers:\n",
      "    learn_throughput: 1186.359\n",
      "    learn_time_ms: 842.915\n",
      "    load_throughput: 45019.347\n",
      "    load_time_ms: 22.213\n",
      "    sample_throughput: 42.997\n",
      "    sample_time_ms: 23257.641\n",
      "    update_time_ms: 6.718\n",
      "  timestamp: 1635082256\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 22\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         558.655</td><td style=\"text-align: right;\">22000</td><td style=\"text-align: right;\">-4.60173</td><td style=\"text-align: right;\">               -3.67</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">             419.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 23000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-31-19\n",
      "  done: false\n",
      "  episode_len_mean: 417.92727272727274\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.6699999999999657\n",
      "  episode_reward_mean: -4.563818181818136\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 55\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6514514499240454\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0113813852137485\n",
      "          policy_loss: -0.0029149037268426685\n",
      "          total_loss: -0.01758501728375753\n",
      "          vf_explained_var: 0.142608180642128\n",
      "          vf_loss: 0.011275332272958218\n",
      "    num_agent_steps_sampled: 23000\n",
      "    num_agent_steps_trained: 23000\n",
      "    num_steps_sampled: 23000\n",
      "    num_steps_trained: 23000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.409375\n",
      "    ram_util_percent: 51.08125\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04313073419874578\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.83128950018345\n",
      "    mean_inference_ms: 2.2573031588442065\n",
      "    mean_raw_obs_processing_ms: 0.6916954363340277\n",
      "  time_since_restore: 581.0860090255737\n",
      "  time_this_iter_s: 22.43093752861023\n",
      "  time_total_s: 581.0860090255737\n",
      "  timers:\n",
      "    learn_throughput: 1199.917\n",
      "    learn_time_ms: 833.391\n",
      "    load_throughput: 44760.25\n",
      "    load_time_ms: 22.341\n",
      "    sample_throughput: 45.484\n",
      "    sample_time_ms: 21985.791\n",
      "    update_time_ms: 7.18\n",
      "  timestamp: 1635082279\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 23\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         581.086</td><td style=\"text-align: right;\">23000</td><td style=\"text-align: right;\">-4.56382</td><td style=\"text-align: right;\">               -3.67</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           417.927</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-31-40\n",
      "  done: false\n",
      "  episode_len_mean: 417.5964912280702\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.6699999999999657\n",
      "  episode_reward_mean: -4.547017543859603\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 57\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.711485113037957\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011967323323325414\n",
      "          policy_loss: 0.02315625962283876\n",
      "          total_loss: 0.001988286276658376\n",
      "          vf_explained_var: 0.18542155623435974\n",
      "          vf_loss: 0.005348510390548553\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.58\n",
      "    ram_util_percent: 51.00666666666668\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043134960456688486\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.625229271318005\n",
      "    mean_inference_ms: 2.2564119831952065\n",
      "    mean_raw_obs_processing_ms: 0.6999344886061707\n",
      "  time_since_restore: 602.3039038181305\n",
      "  time_this_iter_s: 21.217894792556763\n",
      "  time_total_s: 602.3039038181305\n",
      "  timers:\n",
      "    learn_throughput: 1206.507\n",
      "    learn_time_ms: 828.839\n",
      "    load_throughput: 44911.223\n",
      "    load_time_ms: 22.266\n",
      "    sample_throughput: 46.032\n",
      "    sample_time_ms: 21723.821\n",
      "    update_time_ms: 6.829\n",
      "  timestamp: 1635082300\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 24\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         602.304</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">-4.54702</td><td style=\"text-align: right;\">               -3.67</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           417.596</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 25000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-32-17\n",
      "  done: false\n",
      "  episode_len_mean: 416.23333333333335\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.569999999999968\n",
      "  episode_reward_mean: -4.514833333333288\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 60\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6623299333784316\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011929440511884614\n",
      "          policy_loss: 0.09371734244955911\n",
      "          total_loss: 0.07371187491549386\n",
      "          vf_explained_var: 0.4078744351863861\n",
      "          vf_loss: 0.006021360308255276\n",
      "    num_agent_steps_sampled: 25000\n",
      "    num_agent_steps_trained: 25000\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 25000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.78867924528302\n",
      "    ram_util_percent: 51.015094339622635\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043135143387603464\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.334330530172526\n",
      "    mean_inference_ms: 2.2549815331124057\n",
      "    mean_raw_obs_processing_ms: 0.743020853964148\n",
      "  time_since_restore: 639.3421657085419\n",
      "  time_this_iter_s: 37.03826189041138\n",
      "  time_total_s: 639.3421657085419\n",
      "  timers:\n",
      "    learn_throughput: 1219.522\n",
      "    learn_time_ms: 819.994\n",
      "    load_throughput: 45343.925\n",
      "    load_time_ms: 22.054\n",
      "    sample_throughput: 42.867\n",
      "    sample_time_ms: 23327.824\n",
      "    update_time_ms: 7.128\n",
      "  timestamp: 1635082337\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 25\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         639.342</td><td style=\"text-align: right;\">25000</td><td style=\"text-align: right;\">-4.51483</td><td style=\"text-align: right;\">               -3.57</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           416.233</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 26000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-32-39\n",
      "  done: false\n",
      "  episode_len_mean: 415.9032258064516\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.569999999999968\n",
      "  episode_reward_mean: -4.500161290322535\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 62\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.650097046958076\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008321047345163812\n",
      "          policy_loss: 0.045744640131791435\n",
      "          total_loss: 0.026641113228268093\n",
      "          vf_explained_var: 0.21772433817386627\n",
      "          vf_loss: 0.006981389261879182\n",
      "    num_agent_steps_sampled: 26000\n",
      "    num_agent_steps_trained: 26000\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 26000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.51249999999999\n",
      "    ram_util_percent: 51.015625\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043134195832175294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.153491732355715\n",
      "    mean_inference_ms: 2.2540481840119435\n",
      "    mean_raw_obs_processing_ms: 0.7677611771700739\n",
      "  time_since_restore: 661.437150478363\n",
      "  time_this_iter_s: 22.094984769821167\n",
      "  time_total_s: 661.437150478363\n",
      "  timers:\n",
      "    learn_throughput: 1223.845\n",
      "    learn_time_ms: 817.097\n",
      "    load_throughput: 46858.444\n",
      "    load_time_ms: 21.341\n",
      "    sample_throughput: 42.821\n",
      "    sample_time_ms: 23353.113\n",
      "    update_time_ms: 6.433\n",
      "  timestamp: 1635082359\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 26\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         661.437</td><td style=\"text-align: right;\">26000</td><td style=\"text-align: right;\">-4.50016</td><td style=\"text-align: right;\">               -3.57</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           415.903</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 27000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-33-03\n",
      "  done: false\n",
      "  episode_len_mean: 414.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.569999999999968\n",
      "  episode_reward_mean: -4.465384615384571\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 65\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5693982601165772\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010688512164251858\n",
      "          policy_loss: 0.048386731412675646\n",
      "          total_loss: 0.03581677857372496\n",
      "          vf_explained_var: 0.23517434298992157\n",
      "          vf_loss: 0.012589600684198862\n",
      "    num_agent_steps_sampled: 27000\n",
      "    num_agent_steps_trained: 27000\n",
      "    num_steps_sampled: 27000\n",
      "    num_steps_trained: 27000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.2\n",
      "    ram_util_percent: 51.008823529411764\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04313117596269778\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.90270278184323\n",
      "    mean_inference_ms: 2.2526806055172566\n",
      "    mean_raw_obs_processing_ms: 0.7998739021415446\n",
      "  time_since_restore: 685.2846221923828\n",
      "  time_this_iter_s: 23.847471714019775\n",
      "  time_total_s: 685.2846221923828\n",
      "  timers:\n",
      "    learn_throughput: 1230.964\n",
      "    learn_time_ms: 812.371\n",
      "    load_throughput: 47037.798\n",
      "    load_time_ms: 21.259\n",
      "    sample_throughput: 42.436\n",
      "    sample_time_ms: 23564.759\n",
      "    update_time_ms: 7.189\n",
      "  timestamp: 1635082383\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 27\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         685.285</td><td style=\"text-align: right;\">27000</td><td style=\"text-align: right;\">-4.46538</td><td style=\"text-align: right;\">               -3.57</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">               414</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-33-28\n",
      "  done: false\n",
      "  episode_len_mean: 411.4852941176471\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.4999999999999694\n",
      "  episode_reward_mean: -4.425882352941132\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 68\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.605496581395467\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009836199851868333\n",
      "          policy_loss: 0.03624683262573348\n",
      "          total_loss: 0.02241532024410036\n",
      "          vf_explained_var: 0.10481560230255127\n",
      "          vf_loss: 0.01173164223631223\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.7\n",
      "    ram_util_percent: 51.02857142857143\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0431299238239667\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.674823895966227\n",
      "    mean_inference_ms: 2.2514211669956294\n",
      "    mean_raw_obs_processing_ms: 0.8272522026513296\n",
      "  time_since_restore: 709.918078660965\n",
      "  time_this_iter_s: 24.633456468582153\n",
      "  time_total_s: 709.918078660965\n",
      "  timers:\n",
      "    learn_throughput: 1238.226\n",
      "    learn_time_ms: 807.607\n",
      "    load_throughput: 47885.922\n",
      "    load_time_ms: 20.883\n",
      "    sample_throughput: 42.077\n",
      "    sample_time_ms: 23765.94\n",
      "    update_time_ms: 7.053\n",
      "  timestamp: 1635082408\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 28\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         709.918</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">-4.42588</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           411.485</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 29000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-33-51\n",
      "  done: false\n",
      "  episode_len_mean: 410.24285714285713\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.4999999999999694\n",
      "  episode_reward_mean: -4.404571428571384\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 70\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.622637814945645\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011814533246134474\n",
      "          policy_loss: -0.10158421380652322\n",
      "          total_loss: -0.11462485094865163\n",
      "          vf_explained_var: 0.28448349237442017\n",
      "          vf_loss: 0.012595017590663499\n",
      "    num_agent_steps_sampled: 29000\n",
      "    num_agent_steps_trained: 29000\n",
      "    num_steps_sampled: 29000\n",
      "    num_steps_trained: 29000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.63823529411765\n",
      "    ram_util_percent: 51.220588235294116\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04312939080689112\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.53323613701847\n",
      "    mean_inference_ms: 2.2506196371193417\n",
      "    mean_raw_obs_processing_ms: 0.8430395612884526\n",
      "  time_since_restore: 733.6051740646362\n",
      "  time_this_iter_s: 23.687095403671265\n",
      "  time_total_s: 733.6051740646362\n",
      "  timers:\n",
      "    learn_throughput: 1225.395\n",
      "    learn_time_ms: 816.063\n",
      "    load_throughput: 49135.839\n",
      "    load_time_ms: 20.352\n",
      "    sample_throughput: 42.075\n",
      "    sample_time_ms: 23766.944\n",
      "    update_time_ms: 6.245\n",
      "  timestamp: 1635082431\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29000\n",
      "  training_iteration: 29\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         733.605</td><td style=\"text-align: right;\">29000</td><td style=\"text-align: right;\">-4.40457</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           410.243</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-34-18\n",
      "  done: false\n",
      "  episode_len_mean: 409.06849315068496\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.4999999999999694\n",
      "  episode_reward_mean: -4.380410958904067\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 73\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.677878244717916\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010091685878115283\n",
      "          policy_loss: 0.04806926581594679\n",
      "          total_loss: 0.03356491459740533\n",
      "          vf_explained_var: 0.271481454372406\n",
      "          vf_loss: 0.01176984731767637\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.5972972972973\n",
      "    ram_util_percent: 51.47027027027027\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04312979310388412\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.337874745150444\n",
      "    mean_inference_ms: 2.249634773518076\n",
      "    mean_raw_obs_processing_ms: 0.8636652322012375\n",
      "  time_since_restore: 759.8568277359009\n",
      "  time_this_iter_s: 26.25165367126465\n",
      "  time_total_s: 759.8568277359009\n",
      "  timers:\n",
      "    learn_throughput: 1216.368\n",
      "    learn_time_ms: 822.12\n",
      "    load_throughput: 46991.212\n",
      "    load_time_ms: 21.281\n",
      "    sample_throughput: 41.594\n",
      "    sample_time_ms: 24042.036\n",
      "    update_time_ms: 6.547\n",
      "  timestamp: 1635082458\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 30\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         759.857</td><td style=\"text-align: right;\">30000</td><td style=\"text-align: right;\">-4.38041</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           409.068</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 31000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-34-42\n",
      "  done: false\n",
      "  episode_len_mean: 408.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.4999999999999694\n",
      "  episode_reward_mean: -4.369999999999957\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 75\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.671316901842753\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0113290529926269\n",
      "          policy_loss: -0.12211359474394057\n",
      "          total_loss: -0.1358913911713494\n",
      "          vf_explained_var: 0.3587067127227783\n",
      "          vf_loss: 0.012368921453081485\n",
      "    num_agent_steps_sampled: 31000\n",
      "    num_agent_steps_trained: 31000\n",
      "    num_steps_sampled: 31000\n",
      "    num_steps_trained: 31000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.84\n",
      "    ram_util_percent: 51.422857142857154\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043131486431269045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.2163447804191\n",
      "    mean_inference_ms: 2.2490792411494414\n",
      "    mean_raw_obs_processing_ms: 0.8755533543506672\n",
      "  time_since_restore: 784.209584236145\n",
      "  time_this_iter_s: 24.35275650024414\n",
      "  time_total_s: 784.209584236145\n",
      "  timers:\n",
      "    learn_throughput: 1214.626\n",
      "    learn_time_ms: 823.298\n",
      "    load_throughput: 47011.226\n",
      "    load_time_ms: 21.272\n",
      "    sample_throughput: 41.642\n",
      "    sample_time_ms: 24014.449\n",
      "    update_time_ms: 6.488\n",
      "  timestamp: 1635082482\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31000\n",
      "  training_iteration: 31\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">          784.21</td><td style=\"text-align: right;\">31000</td><td style=\"text-align: right;\">   -4.37</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">             408.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-35-06\n",
      "  done: false\n",
      "  episode_len_mean: 407.7564102564103\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.4999999999999694\n",
      "  episode_reward_mean: -4.348717948717906\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 78\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6616600354512534\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013075856874647782\n",
      "          policy_loss: -0.044789241751035054\n",
      "          total_loss: -0.058720867998070185\n",
      "          vf_explained_var: 0.15283510088920593\n",
      "          vf_loss: 0.01203118073948038\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.52571428571427\n",
      "    ram_util_percent: 51.40571428571429\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04313340345714889\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.045919247981956\n",
      "    mean_inference_ms: 2.2482720116823636\n",
      "    mean_raw_obs_processing_ms: 0.8909927093825479\n",
      "  time_since_restore: 808.5455961227417\n",
      "  time_this_iter_s: 24.33601188659668\n",
      "  time_total_s: 808.5455961227417\n",
      "  timers:\n",
      "    learn_throughput: 1218.241\n",
      "    learn_time_ms: 820.855\n",
      "    load_throughput: 47985.246\n",
      "    load_time_ms: 20.84\n",
      "    sample_throughput: 41.432\n",
      "    sample_time_ms: 24136.153\n",
      "    update_time_ms: 6.313\n",
      "  timestamp: 1635082506\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 32\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         808.546</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">-4.34872</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           407.756</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 33000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-35-29\n",
      "  done: false\n",
      "  episode_len_mean: 406.81481481481484\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.4999999999999694\n",
      "  episode_reward_mean: -4.329259259259215\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 81\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.642324619823032\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009879068393910017\n",
      "          policy_loss: 0.018496711055437723\n",
      "          total_loss: 0.005058300246795018\n",
      "          vf_explained_var: -0.0008851991733536124\n",
      "          vf_loss: 0.01249088120012958\n",
      "    num_agent_steps_sampled: 33000\n",
      "    num_agent_steps_trained: 33000\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 33000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.04242424242425\n",
      "    ram_util_percent: 51.375757575757575\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04313433758703589\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.887030426314844\n",
      "    mean_inference_ms: 2.2474752715565898\n",
      "    mean_raw_obs_processing_ms: 0.9041465221858099\n",
      "  time_since_restore: 831.7457418441772\n",
      "  time_this_iter_s: 23.200145721435547\n",
      "  time_total_s: 831.7457418441772\n",
      "  timers:\n",
      "    learn_throughput: 1216.532\n",
      "    learn_time_ms: 822.008\n",
      "    load_throughput: 50566.682\n",
      "    load_time_ms: 19.776\n",
      "    sample_throughput: 41.3\n",
      "    sample_time_ms: 24213.25\n",
      "    update_time_ms: 5.921\n",
      "  timestamp: 1635082529\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 33\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         831.746</td><td style=\"text-align: right;\">33000</td><td style=\"text-align: right;\">-4.32926</td><td style=\"text-align: right;\">                -3.5</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           406.815</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 34000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-35-54\n",
      "  done: false\n",
      "  episode_len_mean: 405.71084337349396\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.4899999999999696\n",
      "  episode_reward_mean: -4.31192771084333\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 83\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6442885716756184\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010795985133611907\n",
      "          policy_loss: -0.1081277416812049\n",
      "          total_loss: -0.12115695310963524\n",
      "          vf_explained_var: 0.3173306882381439\n",
      "          vf_loss: 0.012873876586349474\n",
      "    num_agent_steps_sampled: 34000\n",
      "    num_agent_steps_trained: 34000\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 34000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.12\n",
      "    ram_util_percent: 51.348571428571425\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043133727828315774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.787742423953308\n",
      "    mean_inference_ms: 2.2469588183167897\n",
      "    mean_raw_obs_processing_ms: 0.9116625796427141\n",
      "  time_since_restore: 856.2396743297577\n",
      "  time_this_iter_s: 24.493932485580444\n",
      "  time_total_s: 856.2396743297577\n",
      "  timers:\n",
      "    learn_throughput: 1212.563\n",
      "    learn_time_ms: 824.699\n",
      "    load_throughput: 48219.591\n",
      "    load_time_ms: 20.738\n",
      "    sample_throughput: 40.756\n",
      "    sample_time_ms: 24535.985\n",
      "    update_time_ms: 7.0\n",
      "  timestamp: 1635082554\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 34\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">          856.24</td><td style=\"text-align: right;\">34000</td><td style=\"text-align: right;\">-4.31193</td><td style=\"text-align: right;\">               -3.49</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           405.711</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 35000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-36-18\n",
      "  done: false\n",
      "  episode_len_mean: 404.2674418604651\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.4899999999999696\n",
      "  episode_reward_mean: -4.288604651162747\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 86\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.655133851369222\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011890041425621767\n",
      "          policy_loss: -0.10405504579345386\n",
      "          total_loss: -0.11197645515203476\n",
      "          vf_explained_var: 0.16498592495918274\n",
      "          vf_loss: 0.01803542369355758\n",
      "    num_agent_steps_sampled: 35000\n",
      "    num_agent_steps_trained: 35000\n",
      "    num_steps_sampled: 35000\n",
      "    num_steps_trained: 35000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.99428571428572\n",
      "    ram_util_percent: 51.28571428571429\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043131020920099274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.64742727076981\n",
      "    mean_inference_ms: 2.246189220139221\n",
      "    mean_raw_obs_processing_ms: 0.9213210245023032\n",
      "  time_since_restore: 880.3663294315338\n",
      "  time_this_iter_s: 24.126655101776123\n",
      "  time_total_s: 880.3663294315338\n",
      "  timers:\n",
      "    learn_throughput: 1211.508\n",
      "    learn_time_ms: 825.418\n",
      "    load_throughput: 46108.987\n",
      "    load_time_ms: 21.688\n",
      "    sample_throughput: 43.023\n",
      "    sample_time_ms: 23243.186\n",
      "    update_time_ms: 6.562\n",
      "  timestamp: 1635082578\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 35\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         880.366</td><td style=\"text-align: right;\">35000</td><td style=\"text-align: right;\"> -4.2886</td><td style=\"text-align: right;\">               -3.49</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           404.267</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-36-40\n",
      "  done: false\n",
      "  episode_len_mean: 403.13483146067415\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.46999999999997\n",
      "  episode_reward_mean: -4.268988764044901\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 89\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.598322113355001\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011083170390665921\n",
      "          policy_loss: 0.05041109836763806\n",
      "          total_loss: 0.035979920128981276\n",
      "          vf_explained_var: 0.3048502504825592\n",
      "          vf_loss: 0.01099788560726059\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.17419354838711\n",
      "    ram_util_percent: 51.032258064516135\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04312569527178099\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.514530533373694\n",
      "    mean_inference_ms: 2.245368353327753\n",
      "    mean_raw_obs_processing_ms: 0.9294450548346148\n",
      "  time_since_restore: 902.2244625091553\n",
      "  time_this_iter_s: 21.85813307762146\n",
      "  time_total_s: 902.2244625091553\n",
      "  timers:\n",
      "    learn_throughput: 1213.15\n",
      "    learn_time_ms: 824.3\n",
      "    load_throughput: 46189.316\n",
      "    load_time_ms: 21.65\n",
      "    sample_throughput: 43.066\n",
      "    sample_time_ms: 23220.104\n",
      "    update_time_ms: 7.053\n",
      "  timestamp: 1635082600\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 36\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         902.224</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">-4.26899</td><td style=\"text-align: right;\">               -3.47</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           403.135</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 37000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-37-22\n",
      "  done: false\n",
      "  episode_len_mean: 401.6521739130435\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.289999999999974\n",
      "  episode_reward_mean: -4.246413043478219\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 92\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5759582837422688\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010282551444605768\n",
      "          policy_loss: 0.056033299614985786\n",
      "          total_loss: 0.04016412819425265\n",
      "          vf_explained_var: 0.09309017658233643\n",
      "          vf_loss: 0.009376281021589723\n",
      "    num_agent_steps_sampled: 37000\n",
      "    num_agent_steps_trained: 37000\n",
      "    num_steps_sampled: 37000\n",
      "    num_steps_trained: 37000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.0101694915254\n",
      "    ram_util_percent: 50.74915254237288\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04312026815269429\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.391214550466277\n",
      "    mean_inference_ms: 2.244593068581863\n",
      "    mean_raw_obs_processing_ms: 0.9506246007440668\n",
      "  time_since_restore: 943.7142148017883\n",
      "  time_this_iter_s: 41.48975229263306\n",
      "  time_total_s: 943.7142148017883\n",
      "  timers:\n",
      "    learn_throughput: 1205.273\n",
      "    learn_time_ms: 829.688\n",
      "    load_throughput: 46078.695\n",
      "    load_time_ms: 21.702\n",
      "    sample_throughput: 40.033\n",
      "    sample_time_ms: 24979.581\n",
      "    update_time_ms: 6.378\n",
      "  timestamp: 1635082642\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37000\n",
      "  training_iteration: 37\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         943.714</td><td style=\"text-align: right;\">37000</td><td style=\"text-align: right;\">-4.24641</td><td style=\"text-align: right;\">               -3.29</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           401.652</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 38000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-37-45\n",
      "  done: false\n",
      "  episode_len_mean: 401.3723404255319\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.289999999999974\n",
      "  episode_reward_mean: -4.2387234042552775\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 94\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5277925199932523\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011795405507909489\n",
      "          policy_loss: -0.09179340286387337\n",
      "          total_loss: -0.10434600694311989\n",
      "          vf_explained_var: 0.03218136727809906\n",
      "          vf_loss: 0.012135552890443553\n",
      "    num_agent_steps_sampled: 38000\n",
      "    num_agent_steps_trained: 38000\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 38000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.26969696969697\n",
      "    ram_util_percent: 50.96969696969697\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04311734588516376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.312851636630302\n",
      "    mean_inference_ms: 2.244101610906408\n",
      "    mean_raw_obs_processing_ms: 0.9632291702169173\n",
      "  time_since_restore: 966.8909261226654\n",
      "  time_this_iter_s: 23.176711320877075\n",
      "  time_total_s: 966.8909261226654\n",
      "  timers:\n",
      "    learn_throughput: 1202.497\n",
      "    learn_time_ms: 831.603\n",
      "    load_throughput: 45200.943\n",
      "    load_time_ms: 22.123\n",
      "    sample_throughput: 40.272\n",
      "    sample_time_ms: 24831.038\n",
      "    update_time_ms: 6.808\n",
      "  timestamp: 1635082665\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 38\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         966.891</td><td style=\"text-align: right;\">38000</td><td style=\"text-align: right;\">-4.23872</td><td style=\"text-align: right;\">               -3.29</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           401.372</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 39000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-38-08\n",
      "  done: false\n",
      "  episode_len_mean: 400.63917525773195\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.289999999999974\n",
      "  episode_reward_mean: -4.22443298969068\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 97\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5245462947421604\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01018343463067577\n",
      "          policy_loss: 0.040793982479307385\n",
      "          total_loss: 0.028919815023740133\n",
      "          vf_explained_var: 0.11086351424455643\n",
      "          vf_loss: 0.012862121213563822\n",
      "    num_agent_steps_sampled: 39000\n",
      "    num_agent_steps_trained: 39000\n",
      "    num_steps_sampled: 39000\n",
      "    num_steps_trained: 39000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.44411764705882\n",
      "    ram_util_percent: 51.04117647058823\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043114245607352476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.200959520831844\n",
      "    mean_inference_ms: 2.2433823082824396\n",
      "    mean_raw_obs_processing_ms: 0.9801396035325354\n",
      "  time_since_restore: 990.5712747573853\n",
      "  time_this_iter_s: 23.68034863471985\n",
      "  time_total_s: 990.5712747573853\n",
      "  timers:\n",
      "    learn_throughput: 1202.626\n",
      "    learn_time_ms: 831.514\n",
      "    load_throughput: 45599.43\n",
      "    load_time_ms: 21.93\n",
      "    sample_throughput: 40.274\n",
      "    sample_time_ms: 24830.128\n",
      "    update_time_ms: 7.376\n",
      "  timestamp: 1635082688\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 39\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         990.571</td><td style=\"text-align: right;\">39000</td><td style=\"text-align: right;\">-4.22443</td><td style=\"text-align: right;\">               -3.29</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">           400.639</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-38-35\n",
      "  done: false\n",
      "  episode_len_mean: 399.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.289999999999974\n",
      "  episode_reward_mean: -4.202599999999959\n",
      "  episode_reward_min: -14.569999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 100\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4697987503475614\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010634254484794405\n",
      "          policy_loss: 0.08543101615375942\n",
      "          total_loss: 0.06884671085410649\n",
      "          vf_explained_var: 0.2760803997516632\n",
      "          vf_loss: 0.007581968137916798\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.36052631578947\n",
      "    ram_util_percent: 51.31052631578948\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043111726012748\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.097828093142322\n",
      "    mean_inference_ms: 2.2427366741568147\n",
      "    mean_raw_obs_processing_ms: 0.9951166544926322\n",
      "  time_since_restore: 1017.5164184570312\n",
      "  time_this_iter_s: 26.945143699645996\n",
      "  time_total_s: 1017.5164184570312\n",
      "  timers:\n",
      "    learn_throughput: 1214.117\n",
      "    learn_time_ms: 823.644\n",
      "    load_throughput: 46555.374\n",
      "    load_time_ms: 21.48\n",
      "    sample_throughput: 40.147\n",
      "    sample_time_ms: 24908.382\n",
      "    update_time_ms: 6.845\n",
      "  timestamp: 1635082715\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 40\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         1017.52</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\"> -4.2026</td><td style=\"text-align: right;\">               -3.29</td><td style=\"text-align: right;\">              -14.57</td><td style=\"text-align: right;\">            399.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 41000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-39-03\n",
      "  done: false\n",
      "  episode_len_mean: 397.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.289999999999974\n",
      "  episode_reward_mean: -4.078099999999958\n",
      "  episode_reward_min: -11.029999999999927\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 103\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3897101958592732\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01329011389265911\n",
      "          policy_loss: 0.02781195549501313\n",
      "          total_loss: 0.01568396844797664\n",
      "          vf_explained_var: 0.13525663316249847\n",
      "          vf_loss: 0.011104609398171306\n",
      "    num_agent_steps_sampled: 41000\n",
      "    num_agent_steps_trained: 41000\n",
      "    num_steps_sampled: 41000\n",
      "    num_steps_trained: 41000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.02307692307693\n",
      "    ram_util_percent: 51.415384615384625\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04313504097515346\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.98364356559065\n",
      "    mean_inference_ms: 2.2406120406878496\n",
      "    mean_raw_obs_processing_ms: 1.0318225268825438\n",
      "  time_since_restore: 1044.6740448474884\n",
      "  time_this_iter_s: 27.157626390457153\n",
      "  time_total_s: 1044.6740448474884\n",
      "  timers:\n",
      "    learn_throughput: 1209.935\n",
      "    learn_time_ms: 826.491\n",
      "    load_throughput: 47783.089\n",
      "    load_time_ms: 20.928\n",
      "    sample_throughput: 39.704\n",
      "    sample_time_ms: 25186.543\n",
      "    update_time_ms: 6.918\n",
      "  timestamp: 1635082743\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 41000\n",
      "  training_iteration: 41\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         1044.67</td><td style=\"text-align: right;\">41000</td><td style=\"text-align: right;\"> -4.0781</td><td style=\"text-align: right;\">               -3.29</td><td style=\"text-align: right;\">              -11.03</td><td style=\"text-align: right;\">            397.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 42000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-39-31\n",
      "  done: false\n",
      "  episode_len_mean: 394.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.1999999999999758\n",
      "  episode_reward_mean: -4.01509999999996\n",
      "  episode_reward_min: -11.029999999999927\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 106\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.341931695408291\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012903636297818603\n",
      "          policy_loss: 0.034918449405166835\n",
      "          total_loss: 0.024850120892127354\n",
      "          vf_explained_var: 0.14313368499279022\n",
      "          vf_loss: 0.012705801850340018\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.91707317073171\n",
      "    ram_util_percent: 51.404878048780475\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04313634116056392\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.434447580304788\n",
      "    mean_inference_ms: 2.237809561875005\n",
      "    mean_raw_obs_processing_ms: 1.0674267868637046\n",
      "  time_since_restore: 1073.4393038749695\n",
      "  time_this_iter_s: 28.76525902748108\n",
      "  time_total_s: 1073.4393038749695\n",
      "  timers:\n",
      "    learn_throughput: 1197.733\n",
      "    learn_time_ms: 834.911\n",
      "    load_throughput: 48491.418\n",
      "    load_time_ms: 20.622\n",
      "    sample_throughput: 39.031\n",
      "    sample_time_ms: 25620.981\n",
      "    update_time_ms: 7.255\n",
      "  timestamp: 1635082771\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 42\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         1073.44</td><td style=\"text-align: right;\">42000</td><td style=\"text-align: right;\"> -4.0151</td><td style=\"text-align: right;\">                -3.2</td><td style=\"text-align: right;\">              -11.03</td><td style=\"text-align: right;\">            394.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 43000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-40-01\n",
      "  done: false\n",
      "  episode_len_mean: 391.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.1099999999999777\n",
      "  episode_reward_mean: -3.98229999999996\n",
      "  episode_reward_min: -11.029999999999927\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 109\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.267160619629754\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010114387249010247\n",
      "          policy_loss: 0.03767357601059808\n",
      "          total_loss: 0.02765291018618478\n",
      "          vf_explained_var: 0.1800941824913025\n",
      "          vf_loss: 0.01214522431190643\n",
      "    num_agent_steps_sampled: 43000\n",
      "    num_agent_steps_trained: 43000\n",
      "    num_steps_sampled: 43000\n",
      "    num_steps_trained: 43000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.82142857142857\n",
      "    ram_util_percent: 51.726190476190474\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04313119321642429\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.046956624441265\n",
      "    mean_inference_ms: 2.2353296622503667\n",
      "    mean_raw_obs_processing_ms: 1.1022782420052712\n",
      "  time_since_restore: 1102.6868505477905\n",
      "  time_this_iter_s: 29.247546672821045\n",
      "  time_total_s: 1102.6868505477905\n",
      "  timers:\n",
      "    learn_throughput: 1174.19\n",
      "    learn_time_ms: 851.651\n",
      "    load_throughput: 45606.322\n",
      "    load_time_ms: 21.927\n",
      "    sample_throughput: 38.156\n",
      "    sample_time_ms: 26208.33\n",
      "    update_time_ms: 6.997\n",
      "  timestamp: 1635082801\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 43000\n",
      "  training_iteration: 43\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         1102.69</td><td style=\"text-align: right;\">43000</td><td style=\"text-align: right;\"> -3.9823</td><td style=\"text-align: right;\">               -3.11</td><td style=\"text-align: right;\">              -11.03</td><td style=\"text-align: right;\">            391.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-40-31\n",
      "  done: false\n",
      "  episode_len_mean: 388.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.039999999999979\n",
      "  episode_reward_mean: -3.8872999999999602\n",
      "  episode_reward_min: -4.9199999999999395\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 112\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1570054213205974\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0143216754685392\n",
      "          policy_loss: 0.019795795612865024\n",
      "          total_loss: 0.010805064108636644\n",
      "          vf_explained_var: 0.2280743271112442\n",
      "          vf_loss: 0.011863236289031596\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.42045454545455\n",
      "    ram_util_percent: 51.57954545454545\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04312891925170151\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.772776643989243\n",
      "    mean_inference_ms: 2.233347716254754\n",
      "    mean_raw_obs_processing_ms: 1.1361797441352088\n",
      "  time_since_restore: 1133.4235455989838\n",
      "  time_this_iter_s: 30.736695051193237\n",
      "  time_total_s: 1133.4235455989838\n",
      "  timers:\n",
      "    learn_throughput: 1166.439\n",
      "    learn_time_ms: 857.31\n",
      "    load_throughput: 45180.153\n",
      "    load_time_ms: 22.134\n",
      "    sample_throughput: 37.275\n",
      "    sample_time_ms: 26827.598\n",
      "    update_time_ms: 6.205\n",
      "  timestamp: 1635082831\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 44\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         1133.42</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\"> -3.8873</td><td style=\"text-align: right;\">               -3.04</td><td style=\"text-align: right;\">               -4.92</td><td style=\"text-align: right;\">            388.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 45000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-41-03\n",
      "  done: false\n",
      "  episode_len_mean: 385.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.809999999999984\n",
      "  episode_reward_mean: -3.8538999999999617\n",
      "  episode_reward_min: -4.9199999999999395\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 115\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.025714679559072\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01419542117871515\n",
      "          policy_loss: -0.10507083924280272\n",
      "          total_loss: -0.10985836444629563\n",
      "          vf_explained_var: 0.21830704808235168\n",
      "          vf_loss: 0.014759851743777593\n",
      "    num_agent_steps_sampled: 45000\n",
      "    num_agent_steps_trained: 45000\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 45000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.67111111111112\n",
      "    ram_util_percent: 51.65333333333334\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04312939900219748\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.56633955054545\n",
      "    mean_inference_ms: 2.2320052932558085\n",
      "    mean_raw_obs_processing_ms: 1.1694722168922487\n",
      "  time_since_restore: 1165.2309257984161\n",
      "  time_this_iter_s: 31.807380199432373\n",
      "  time_total_s: 1165.2309257984161\n",
      "  timers:\n",
      "    learn_throughput: 1158.676\n",
      "    learn_time_ms: 863.054\n",
      "    load_throughput: 45097.424\n",
      "    load_time_ms: 22.174\n",
      "    sample_throughput: 36.247\n",
      "    sample_time_ms: 27588.842\n",
      "    update_time_ms: 7.533\n",
      "  timestamp: 1635082863\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 45\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         1165.23</td><td style=\"text-align: right;\">45000</td><td style=\"text-align: right;\"> -3.8539</td><td style=\"text-align: right;\">               -2.81</td><td style=\"text-align: right;\">               -4.92</td><td style=\"text-align: right;\">            385.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 46000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-41-33\n",
      "  done: false\n",
      "  episode_len_mean: 381.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.809999999999984\n",
      "  episode_reward_mean: -3.810699999999962\n",
      "  episode_reward_min: -4.9199999999999395\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 119\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9205636395348442\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009890947444665857\n",
      "          policy_loss: 0.046419633759392634\n",
      "          total_loss: 0.04011409249570635\n",
      "          vf_explained_var: 0.2687522768974304\n",
      "          vf_loss: 0.01240554435385598\n",
      "    num_agent_steps_sampled: 46000\n",
      "    num_agent_steps_trained: 46000\n",
      "    num_steps_sampled: 46000\n",
      "    num_steps_trained: 46000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.39999999999999\n",
      "    ram_util_percent: 51.55238095238095\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04313420394465903\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.34580188281372\n",
      "    mean_inference_ms: 2.230486435129009\n",
      "    mean_raw_obs_processing_ms: 1.2129806647665031\n",
      "  time_since_restore: 1194.510267496109\n",
      "  time_this_iter_s: 29.27934169769287\n",
      "  time_total_s: 1194.510267496109\n",
      "  timers:\n",
      "    learn_throughput: 1157.807\n",
      "    learn_time_ms: 863.702\n",
      "    load_throughput: 43431.795\n",
      "    load_time_ms: 23.025\n",
      "    sample_throughput: 35.299\n",
      "    sample_time_ms: 28329.113\n",
      "    update_time_ms: 8.025\n",
      "  timestamp: 1635082893\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 46000\n",
      "  training_iteration: 46\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         1194.51</td><td style=\"text-align: right;\">46000</td><td style=\"text-align: right;\"> -3.8107</td><td style=\"text-align: right;\">               -2.81</td><td style=\"text-align: right;\">               -4.92</td><td style=\"text-align: right;\">            381.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 47000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-42-20\n",
      "  done: false\n",
      "  episode_len_mean: 374.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.7441999999999624\n",
      "  episode_reward_min: -4.9199999999999395\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 123\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9120742903815375\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007035836148026044\n",
      "          policy_loss: -0.013683400427301725\n",
      "          total_loss: -0.017488345669375526\n",
      "          vf_explained_var: 0.20107559859752655\n",
      "          vf_loss: 0.014964004585312472\n",
      "    num_agent_steps_sampled: 47000\n",
      "    num_agent_steps_trained: 47000\n",
      "    num_steps_sampled: 47000\n",
      "    num_steps_trained: 47000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.71470588235293\n",
      "    ram_util_percent: 51.46029411764705\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043134050850847545\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.211356983438296\n",
      "    mean_inference_ms: 2.229599907974885\n",
      "    mean_raw_obs_processing_ms: 1.27018604265286\n",
      "  time_since_restore: 1242.1771874427795\n",
      "  time_this_iter_s: 47.66691994667053\n",
      "  time_total_s: 1242.1771874427795\n",
      "  timers:\n",
      "    learn_throughput: 1169.271\n",
      "    learn_time_ms: 855.233\n",
      "    load_throughput: 41644.201\n",
      "    load_time_ms: 24.013\n",
      "    sample_throughput: 34.537\n",
      "    sample_time_ms: 28954.537\n",
      "    update_time_ms: 7.932\n",
      "  timestamp: 1635082940\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47000\n",
      "  training_iteration: 47\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         1242.18</td><td style=\"text-align: right;\">47000</td><td style=\"text-align: right;\"> -3.7442</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -4.92</td><td style=\"text-align: right;\">            374.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-42-51\n",
      "  done: false\n",
      "  episode_len_mean: 369.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.692199999999965\n",
      "  episode_reward_min: -4.9199999999999395\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 126\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9249416377809312\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006246397055851569\n",
      "          policy_loss: -0.013395390080081091\n",
      "          total_loss: -0.02182648041182094\n",
      "          vf_explained_var: 0.19258905947208405\n",
      "          vf_loss: 0.010506003986423214\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.01818181818182\n",
      "    ram_util_percent: 51.50909090909092\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04313190704486845\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.143730361977642\n",
      "    mean_inference_ms: 2.229128890623946\n",
      "    mean_raw_obs_processing_ms: 1.3122371591086763\n",
      "  time_since_restore: 1272.6719284057617\n",
      "  time_this_iter_s: 30.494740962982178\n",
      "  time_total_s: 1272.6719284057617\n",
      "  timers:\n",
      "    learn_throughput: 1168.275\n",
      "    learn_time_ms: 855.963\n",
      "    load_throughput: 43379.15\n",
      "    load_time_ms: 23.053\n",
      "    sample_throughput: 33.685\n",
      "    sample_time_ms: 29686.942\n",
      "    update_time_ms: 7.714\n",
      "  timestamp: 1635082971\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 48\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         1272.67</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> -3.6922</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -4.92</td><td style=\"text-align: right;\">            369.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 49000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-43-21\n",
      "  done: false\n",
      "  episode_len_mean: 362.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.6264999999999663\n",
      "  episode_reward_min: -4.9199999999999395\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 130\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8681173165639242\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006465584285203245\n",
      "          policy_loss: 0.010043538361787795\n",
      "          total_loss: 0.00621196652452151\n",
      "          vf_explained_var: 0.15400351583957672\n",
      "          vf_loss: 0.014526323529167308\n",
      "    num_agent_steps_sampled: 49000\n",
      "    num_agent_steps_trained: 49000\n",
      "    num_steps_sampled: 49000\n",
      "    num_steps_trained: 49000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.74418604651163\n",
      "    ram_util_percent: 51.78604651162791\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04313173081708789\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.107163679521612\n",
      "    mean_inference_ms: 2.228821287844434\n",
      "    mean_raw_obs_processing_ms: 1.3416809725469392\n",
      "  time_since_restore: 1303.2602887153625\n",
      "  time_this_iter_s: 30.58836030960083\n",
      "  time_total_s: 1303.2602887153625\n",
      "  timers:\n",
      "    learn_throughput: 1175.066\n",
      "    learn_time_ms: 851.016\n",
      "    load_throughput: 43252.108\n",
      "    load_time_ms: 23.12\n",
      "    sample_throughput: 32.914\n",
      "    sample_time_ms: 30382.303\n",
      "    update_time_ms: 7.951\n",
      "  timestamp: 1635083001\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49000\n",
      "  training_iteration: 49\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         1303.26</td><td style=\"text-align: right;\">49000</td><td style=\"text-align: right;\"> -3.6265</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -4.92</td><td style=\"text-align: right;\">            362.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 50000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-43-52\n",
      "  done: false\n",
      "  episode_len_mean: 356.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.566599999999968\n",
      "  episode_reward_min: -4.9199999999999395\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 133\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9435883495542738\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008688841623871232\n",
      "          policy_loss: -0.08754099541240268\n",
      "          total_loss: -0.09329243534141117\n",
      "          vf_explained_var: 0.11795086413621902\n",
      "          vf_loss: 0.013250001147389412\n",
      "    num_agent_steps_sampled: 50000\n",
      "    num_agent_steps_trained: 50000\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.55116279069767\n",
      "    ram_util_percent: 51.946511627906986\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04313032501308255\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.099192829026915\n",
      "    mean_inference_ms: 2.228513574578463\n",
      "    mean_raw_obs_processing_ms: 1.3473074985381934\n",
      "  time_since_restore: 1333.4250168800354\n",
      "  time_this_iter_s: 30.16472816467285\n",
      "  time_total_s: 1333.4250168800354\n",
      "  timers:\n",
      "    learn_throughput: 1177.659\n",
      "    learn_time_ms: 849.142\n",
      "    load_throughput: 44162.33\n",
      "    load_time_ms: 22.644\n",
      "    sample_throughput: 32.567\n",
      "    sample_time_ms: 30706.202\n",
      "    update_time_ms: 8.355\n",
      "  timestamp: 1635083032\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 50\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         1333.43</td><td style=\"text-align: right;\">50000</td><td style=\"text-align: right;\"> -3.5666</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -4.92</td><td style=\"text-align: right;\">            356.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 51000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-44-21\n",
      "  done: false\n",
      "  episode_len_mean: 350.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.5007999999999693\n",
      "  episode_reward_min: -4.41999999999995\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 137\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9121494478649563\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007818946592156649\n",
      "          policy_loss: 0.024478316555420557\n",
      "          total_loss: 0.02117800298664305\n",
      "          vf_explained_var: 0.12077346444129944\n",
      "          vf_loss: 0.015430233751734098\n",
      "    num_agent_steps_sampled: 51000\n",
      "    num_agent_steps_trained: 51000\n",
      "    num_steps_sampled: 51000\n",
      "    num_steps_trained: 51000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.00232558139534\n",
      "    ram_util_percent: 52.002325581395354\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04312254277193552\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.107003501077784\n",
      "    mean_inference_ms: 2.227949585994769\n",
      "    mean_raw_obs_processing_ms: 1.357930719349366\n",
      "  time_since_restore: 1362.9630086421967\n",
      "  time_this_iter_s: 29.537991762161255\n",
      "  time_total_s: 1362.9630086421967\n",
      "  timers:\n",
      "    learn_throughput: 1187.232\n",
      "    learn_time_ms: 842.295\n",
      "    load_throughput: 43229.373\n",
      "    load_time_ms: 23.132\n",
      "    sample_throughput: 32.31\n",
      "    sample_time_ms: 30950.134\n",
      "    update_time_ms: 8.35\n",
      "  timestamp: 1635083061\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 51000\n",
      "  training_iteration: 51\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         1362.96</td><td style=\"text-align: right;\">51000</td><td style=\"text-align: right;\"> -3.5008</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -4.42</td><td style=\"text-align: right;\">            350.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-44-52\n",
      "  done: false\n",
      "  episode_len_mean: 345.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.4584999999999697\n",
      "  episode_reward_min: -4.41999999999995\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 140\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9005720999505784\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007081227551718901\n",
      "          policy_loss: -0.11707980889413092\n",
      "          total_loss: -0.11758271753787994\n",
      "          vf_explained_var: 0.10957472771406174\n",
      "          vf_loss: 0.018148752922813097\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.4139534883721\n",
      "    ram_util_percent: 51.91162790697675\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043115789191156606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.124832333139796\n",
      "    mean_inference_ms: 2.227505397622509\n",
      "    mean_raw_obs_processing_ms: 1.3680251198512905\n",
      "  time_since_restore: 1393.5913009643555\n",
      "  time_this_iter_s: 30.628292322158813\n",
      "  time_total_s: 1393.5913009643555\n",
      "  timers:\n",
      "    learn_throughput: 1199.531\n",
      "    learn_time_ms: 833.659\n",
      "    load_throughput: 43609.081\n",
      "    load_time_ms: 22.931\n",
      "    sample_throughput: 32.107\n",
      "    sample_time_ms: 31145.497\n",
      "    update_time_ms: 8.118\n",
      "  timestamp: 1635083092\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 52\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         1393.59</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\"> -3.4585</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -4.42</td><td style=\"text-align: right;\">            345.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 53000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-45-22\n",
      "  done: false\n",
      "  episode_len_mean: 340.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.4077999999999706\n",
      "  episode_reward_min: -4.41999999999995\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 144\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.824673843383789\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006130309365384079\n",
      "          policy_loss: 0.01114232631193267\n",
      "          total_loss: 0.00977174590031306\n",
      "          vf_explained_var: 0.19500486552715302\n",
      "          vf_loss: 0.016569636751794153\n",
      "    num_agent_steps_sampled: 53000\n",
      "    num_agent_steps_trained: 53000\n",
      "    num_steps_sampled: 53000\n",
      "    num_steps_trained: 53000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.09069767441859\n",
      "    ram_util_percent: 51.82790697674418\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04310545946516771\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.156957441223053\n",
      "    mean_inference_ms: 2.226862332287609\n",
      "    mean_raw_obs_processing_ms: 1.3829619948788445\n",
      "  time_since_restore: 1423.812983751297\n",
      "  time_this_iter_s: 30.22168278694153\n",
      "  time_total_s: 1423.812983751297\n",
      "  timers:\n",
      "    learn_throughput: 1228.325\n",
      "    learn_time_ms: 814.117\n",
      "    load_throughput: 45967.141\n",
      "    load_time_ms: 21.755\n",
      "    sample_throughput: 31.987\n",
      "    sample_time_ms: 31262.747\n",
      "    update_time_ms: 8.561\n",
      "  timestamp: 1635083122\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 53000\n",
      "  training_iteration: 53\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         1423.81</td><td style=\"text-align: right;\">53000</td><td style=\"text-align: right;\"> -3.4078</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -4.42</td><td style=\"text-align: right;\">            340.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 54000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-45-54\n",
      "  done: false\n",
      "  episode_len_mean: 336.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.53999999999999\n",
      "  episode_reward_mean: -3.3679999999999723\n",
      "  episode_reward_min: -4.41999999999995\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 148\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8199499289194743\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008391462535709238\n",
      "          policy_loss: -0.006122508893410364\n",
      "          total_loss: -0.005747855661643876\n",
      "          vf_explained_var: 0.17876967787742615\n",
      "          vf_loss: 0.018154582981434134\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_steps_sampled: 54000\n",
      "    num_steps_trained: 54000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.16739130434782\n",
      "    ram_util_percent: 51.86521739130436\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043097243614201225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.197728249054578\n",
      "    mean_inference_ms: 2.2263738702079356\n",
      "    mean_raw_obs_processing_ms: 1.4001046184562893\n",
      "  time_since_restore: 1455.7950673103333\n",
      "  time_this_iter_s: 31.982083559036255\n",
      "  time_total_s: 1455.7950673103333\n",
      "  timers:\n",
      "    learn_throughput: 1241.194\n",
      "    learn_time_ms: 805.676\n",
      "    load_throughput: 48252.098\n",
      "    load_time_ms: 20.724\n",
      "    sample_throughput: 31.85\n",
      "    sample_time_ms: 31396.695\n",
      "    update_time_ms: 8.548\n",
      "  timestamp: 1635083154\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 54000\n",
      "  training_iteration: 54\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">          1455.8</td><td style=\"text-align: right;\">54000</td><td style=\"text-align: right;\">  -3.368</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -4.42</td><td style=\"text-align: right;\">             336.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 55000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-46-43\n",
      "  done: false\n",
      "  episode_len_mean: 331.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4999999999999907\n",
      "  episode_reward_mean: -3.3173999999999726\n",
      "  episode_reward_min: -4.41999999999995\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 152\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8175450947549607\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007575535715508231\n",
      "          policy_loss: 0.005364245590236452\n",
      "          total_loss: 0.0034049292819367514\n",
      "          vf_explained_var: 0.2224804162979126\n",
      "          vf_loss: 0.015837357565760612\n",
      "    num_agent_steps_sampled: 55000\n",
      "    num_agent_steps_trained: 55000\n",
      "    num_steps_sampled: 55000\n",
      "    num_steps_trained: 55000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.7830985915493\n",
      "    ram_util_percent: 51.65915492957745\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04308869948762715\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.246479044737253\n",
      "    mean_inference_ms: 2.2258724241130436\n",
      "    mean_raw_obs_processing_ms: 1.4301331530574677\n",
      "  time_since_restore: 1505.2099130153656\n",
      "  time_this_iter_s: 49.41484570503235\n",
      "  time_total_s: 1505.2099130153656\n",
      "  timers:\n",
      "    learn_throughput: 1251.478\n",
      "    learn_time_ms: 799.055\n",
      "    load_throughput: 48266.646\n",
      "    load_time_ms: 20.718\n",
      "    sample_throughput: 30.153\n",
      "    sample_time_ms: 33164.439\n",
      "    update_time_ms: 8.176\n",
      "  timestamp: 1635083203\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55000\n",
      "  training_iteration: 55\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         1505.21</td><td style=\"text-align: right;\">55000</td><td style=\"text-align: right;\"> -3.3174</td><td style=\"text-align: right;\">                -2.5</td><td style=\"text-align: right;\">               -4.42</td><td style=\"text-align: right;\">            331.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-47-16\n",
      "  done: false\n",
      "  episode_len_mean: 327.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4999999999999907\n",
      "  episode_reward_mean: -3.2793999999999737\n",
      "  episode_reward_min: -4.41999999999995\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 155\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.867372957865397\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0077518738533963\n",
      "          policy_loss: -0.047169538877076575\n",
      "          total_loss: -0.05249385672310988\n",
      "          vf_explained_var: 0.20508338510990143\n",
      "          vf_loss: 0.012961817025724385\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.17999999999999\n",
      "    ram_util_percent: 51.59111111111111\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04308024609915312\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.28970618421425\n",
      "    mean_inference_ms: 2.225555354040105\n",
      "    mean_raw_obs_processing_ms: 1.4530552448131846\n",
      "  time_since_restore: 1537.2015633583069\n",
      "  time_this_iter_s: 31.991650342941284\n",
      "  time_total_s: 1537.2015633583069\n",
      "  timers:\n",
      "    learn_throughput: 1252.483\n",
      "    learn_time_ms: 798.414\n",
      "    load_throughput: 48481.273\n",
      "    load_time_ms: 20.627\n",
      "    sample_throughput: 29.908\n",
      "    sample_time_ms: 33436.252\n",
      "    update_time_ms: 8.158\n",
      "  timestamp: 1635083236\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 56\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">          1537.2</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\"> -3.2794</td><td style=\"text-align: right;\">                -2.5</td><td style=\"text-align: right;\">               -4.42</td><td style=\"text-align: right;\">            327.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 57000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-47-45\n",
      "  done: false\n",
      "  episode_len_mean: 322.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4999999999999907\n",
      "  episode_reward_mean: -3.2202999999999746\n",
      "  episode_reward_min: -4.159999999999956\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 159\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.808551503552331\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0076771912231715965\n",
      "          policy_loss: -0.008203245202700297\n",
      "          total_loss: -0.011554401616255443\n",
      "          vf_explained_var: 0.2818618416786194\n",
      "          vf_loss: 0.014350496377382014\n",
      "    num_agent_steps_sampled: 57000\n",
      "    num_agent_steps_trained: 57000\n",
      "    num_steps_sampled: 57000\n",
      "    num_steps_trained: 57000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.53488372093024\n",
      "    ram_util_percent: 51.595348837209286\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0430727976565924\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.35819512115775\n",
      "    mean_inference_ms: 2.2253114734419985\n",
      "    mean_raw_obs_processing_ms: 1.4711500360243386\n",
      "  time_since_restore: 1566.8260779380798\n",
      "  time_this_iter_s: 29.62451457977295\n",
      "  time_total_s: 1566.8260779380798\n",
      "  timers:\n",
      "    learn_throughput: 1244.554\n",
      "    learn_time_ms: 803.5\n",
      "    load_throughput: 51113.334\n",
      "    load_time_ms: 19.564\n",
      "    sample_throughput: 31.618\n",
      "    sample_time_ms: 31627.58\n",
      "    update_time_ms: 8.537\n",
      "  timestamp: 1635083265\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 57000\n",
      "  training_iteration: 57\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         1566.83</td><td style=\"text-align: right;\">57000</td><td style=\"text-align: right;\"> -3.2203</td><td style=\"text-align: right;\">                -2.5</td><td style=\"text-align: right;\">               -4.16</td><td style=\"text-align: right;\">            322.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 58000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-48-16\n",
      "  done: false\n",
      "  episode_len_mean: 317.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4999999999999907\n",
      "  episode_reward_mean: -3.1756999999999764\n",
      "  episode_reward_min: -4.159999999999956\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 163\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9112569040722318\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008444485181352155\n",
      "          policy_loss: -0.002540884498092863\n",
      "          total_loss: -0.003869716243611442\n",
      "          vf_explained_var: 0.120509572327137\n",
      "          vf_loss: 0.01736151058640745\n",
      "    num_agent_steps_sampled: 58000\n",
      "    num_agent_steps_trained: 58000\n",
      "    num_steps_sampled: 58000\n",
      "    num_steps_trained: 58000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.06363636363636\n",
      "    ram_util_percent: 51.93181818181818\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0430690200589903\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.43546778421278\n",
      "    mean_inference_ms: 2.225223440302746\n",
      "    mean_raw_obs_processing_ms: 1.4775869472232726\n",
      "  time_since_restore: 1597.8685734272003\n",
      "  time_this_iter_s: 31.042495489120483\n",
      "  time_total_s: 1597.8685734272003\n",
      "  timers:\n",
      "    learn_throughput: 1220.079\n",
      "    learn_time_ms: 819.619\n",
      "    load_throughput: 49265.465\n",
      "    load_time_ms: 20.298\n",
      "    sample_throughput: 31.58\n",
      "    sample_time_ms: 31665.701\n",
      "    update_time_ms: 8.33\n",
      "  timestamp: 1635083296\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 58000\n",
      "  training_iteration: 58\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         1597.87</td><td style=\"text-align: right;\">58000</td><td style=\"text-align: right;\"> -3.1757</td><td style=\"text-align: right;\">                -2.5</td><td style=\"text-align: right;\">               -4.16</td><td style=\"text-align: right;\">            317.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 59000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-48-48\n",
      "  done: false\n",
      "  episode_len_mean: 314.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4999999999999907\n",
      "  episode_reward_mean: -3.149499999999977\n",
      "  episode_reward_min: -4.159999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 166\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8792390373018053\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007205016840471896\n",
      "          policy_loss: -0.09949131136139235\n",
      "          total_loss: -0.10429538703627056\n",
      "          vf_explained_var: 0.18535976111888885\n",
      "          vf_loss: 0.013628059056484038\n",
      "    num_agent_steps_sampled: 59000\n",
      "    num_agent_steps_trained: 59000\n",
      "    num_steps_sampled: 59000\n",
      "    num_steps_trained: 59000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.67608695652174\n",
      "    ram_util_percent: 52.12826086956521\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04306809959543905\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.497007238741855\n",
      "    mean_inference_ms: 2.225209750094286\n",
      "    mean_raw_obs_processing_ms: 1.4835800121233698\n",
      "  time_since_restore: 1630.1206266880035\n",
      "  time_this_iter_s: 32.25205326080322\n",
      "  time_total_s: 1630.1206266880035\n",
      "  timers:\n",
      "    learn_throughput: 1189.909\n",
      "    learn_time_ms: 840.4\n",
      "    load_throughput: 51041.12\n",
      "    load_time_ms: 19.592\n",
      "    sample_throughput: 31.435\n",
      "    sample_time_ms: 31811.975\n",
      "    update_time_ms: 8.238\n",
      "  timestamp: 1635083328\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 59000\n",
      "  training_iteration: 59\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         1630.12</td><td style=\"text-align: right;\">59000</td><td style=\"text-align: right;\"> -3.1495</td><td style=\"text-align: right;\">                -2.5</td><td style=\"text-align: right;\">               -4.16</td><td style=\"text-align: right;\">            314.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-49-20\n",
      "  done: false\n",
      "  episode_len_mean: 311.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4999999999999907\n",
      "  episode_reward_mean: -3.114499999999977\n",
      "  episode_reward_min: -4.159999999999956\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 170\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.855470531516605\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006752913134271113\n",
      "          policy_loss: 0.030178257491853502\n",
      "          total_loss: 0.025029530914293396\n",
      "          vf_explained_var: 0.26784804463386536\n",
      "          vf_loss: 0.013068334396100707\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.24347826086957\n",
      "    ram_util_percent: 52.18478260869564\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0430659781179684\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.582643272433934\n",
      "    mean_inference_ms: 2.225243957299443\n",
      "    mean_raw_obs_processing_ms: 1.4926331981036207\n",
      "  time_since_restore: 1662.030957698822\n",
      "  time_this_iter_s: 31.91033101081848\n",
      "  time_total_s: 1662.030957698822\n",
      "  timers:\n",
      "    learn_throughput: 1182.191\n",
      "    learn_time_ms: 845.887\n",
      "    load_throughput: 49403.281\n",
      "    load_time_ms: 20.242\n",
      "    sample_throughput: 31.27\n",
      "    sample_time_ms: 31979.031\n",
      "    update_time_ms: 9.391\n",
      "  timestamp: 1635083360\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 60\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         1662.03</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\"> -3.1145</td><td style=\"text-align: right;\">                -2.5</td><td style=\"text-align: right;\">               -4.16</td><td style=\"text-align: right;\">            311.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 61000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-49-54\n",
      "  done: false\n",
      "  episode_len_mean: 306.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4999999999999907\n",
      "  episode_reward_mean: -3.067199999999979\n",
      "  episode_reward_min: -4.159999999999956\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 174\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8147993723551432\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00948778133449903\n",
      "          policy_loss: 0.012578862574365404\n",
      "          total_loss: 0.009015148257215817\n",
      "          vf_explained_var: 0.21534617245197296\n",
      "          vf_loss: 0.014109890845914682\n",
      "    num_agent_steps_sampled: 61000\n",
      "    num_agent_steps_trained: 61000\n",
      "    num_steps_sampled: 61000\n",
      "    num_steps_trained: 61000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.0808510638298\n",
      "    ram_util_percent: 52.51702127659575\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0430624925454055\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.67067184279101\n",
      "    mean_inference_ms: 2.2251571239929717\n",
      "    mean_raw_obs_processing_ms: 1.503377033296793\n",
      "  time_since_restore: 1695.235520362854\n",
      "  time_this_iter_s: 33.20456266403198\n",
      "  time_total_s: 1695.235520362854\n",
      "  timers:\n",
      "    learn_throughput: 1177.589\n",
      "    learn_time_ms: 849.192\n",
      "    load_throughput: 47987.936\n",
      "    load_time_ms: 20.839\n",
      "    sample_throughput: 30.92\n",
      "    sample_time_ms: 32341.917\n",
      "    update_time_ms: 9.429\n",
      "  timestamp: 1635083394\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 61000\n",
      "  training_iteration: 61\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         1695.24</td><td style=\"text-align: right;\">61000</td><td style=\"text-align: right;\"> -3.0672</td><td style=\"text-align: right;\">                -2.5</td><td style=\"text-align: right;\">               -4.16</td><td style=\"text-align: right;\">            306.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 62000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-50-26\n",
      "  done: false\n",
      "  episode_len_mean: 301.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4999999999999907\n",
      "  episode_reward_mean: -3.017999999999979\n",
      "  episode_reward_min: -4.049999999999958\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 178\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7776668614811368\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014041781665708047\n",
      "          policy_loss: -0.01745737368861834\n",
      "          total_loss: -0.019657660606834625\n",
      "          vf_explained_var: 0.15750481188297272\n",
      "          vf_loss: 0.014874292403045627\n",
      "    num_agent_steps_sampled: 62000\n",
      "    num_agent_steps_trained: 62000\n",
      "    num_steps_sampled: 62000\n",
      "    num_steps_trained: 62000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.60652173913043\n",
      "    ram_util_percent: 52.37826086956523\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04305851341276688\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.762425270418607\n",
      "    mean_inference_ms: 2.2250583675740927\n",
      "    mean_raw_obs_processing_ms: 1.5153026463474302\n",
      "  time_since_restore: 1727.7627170085907\n",
      "  time_this_iter_s: 32.527196645736694\n",
      "  time_total_s: 1727.7627170085907\n",
      "  timers:\n",
      "    learn_throughput: 1176.15\n",
      "    learn_time_ms: 850.231\n",
      "    load_throughput: 45891.85\n",
      "    load_time_ms: 21.79\n",
      "    sample_throughput: 30.741\n",
      "    sample_time_ms: 32529.738\n",
      "    update_time_ms: 9.469\n",
      "  timestamp: 1635083426\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 62000\n",
      "  training_iteration: 62\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         1727.76</td><td style=\"text-align: right;\">62000</td><td style=\"text-align: right;\">  -3.018</td><td style=\"text-align: right;\">                -2.5</td><td style=\"text-align: right;\">               -4.05</td><td style=\"text-align: right;\">             301.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 63000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-51-16\n",
      "  done: false\n",
      "  episode_len_mean: 298.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3599999999999937\n",
      "  episode_reward_mean: -2.9806999999999806\n",
      "  episode_reward_min: -4.049999999999958\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 181\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.662956209977468\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008347620993515333\n",
      "          policy_loss: -0.10997213878565365\n",
      "          total_loss: -0.11157005147801505\n",
      "          vf_explained_var: 0.16436432301998138\n",
      "          vf_loss: 0.014614268795897563\n",
      "    num_agent_steps_sampled: 63000\n",
      "    num_agent_steps_trained: 63000\n",
      "    num_steps_sampled: 63000\n",
      "    num_steps_trained: 63000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.78309859154929\n",
      "    ram_util_percent: 52.269014084507035\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0430558135582595\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.83499294373403\n",
      "    mean_inference_ms: 2.225012633847906\n",
      "    mean_raw_obs_processing_ms: 1.5327751340922378\n",
      "  time_since_restore: 1777.1176710128784\n",
      "  time_this_iter_s: 49.35495400428772\n",
      "  time_total_s: 1777.1176710128784\n",
      "  timers:\n",
      "    learn_throughput: 1172.055\n",
      "    learn_time_ms: 853.203\n",
      "    load_throughput: 46102.044\n",
      "    load_time_ms: 21.691\n",
      "    sample_throughput: 29.036\n",
      "    sample_time_ms: 34440.523\n",
      "    update_time_ms: 9.442\n",
      "  timestamp: 1635083476\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 63000\n",
      "  training_iteration: 63\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         1777.12</td><td style=\"text-align: right;\">63000</td><td style=\"text-align: right;\"> -2.9807</td><td style=\"text-align: right;\">               -2.36</td><td style=\"text-align: right;\">               -4.05</td><td style=\"text-align: right;\">            298.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-51-47\n",
      "  done: false\n",
      "  episode_len_mean: 294.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3599999999999937\n",
      "  episode_reward_mean: -2.9444999999999806\n",
      "  episode_reward_min: -4.049999999999958\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 185\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6441487630208333\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006657905462346747\n",
      "          policy_loss: -0.009086063007513683\n",
      "          total_loss: -0.01085330926709705\n",
      "          vf_explained_var: 0.09369421005249023\n",
      "          vf_loss: 0.014341347198933364\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.89090909090909\n",
      "    ram_util_percent: 52.22045454545453\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043054571114296704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.93415218189445\n",
      "    mean_inference_ms: 2.2249834013722762\n",
      "    mean_raw_obs_processing_ms: 1.5568661773799437\n",
      "  time_since_restore: 1808.203155040741\n",
      "  time_this_iter_s: 31.08548402786255\n",
      "  time_total_s: 1808.203155040741\n",
      "  timers:\n",
      "    learn_throughput: 1169.45\n",
      "    learn_time_ms: 855.103\n",
      "    load_throughput: 46104.223\n",
      "    load_time_ms: 21.69\n",
      "    sample_throughput: 29.114\n",
      "    sample_time_ms: 34348.213\n",
      "    update_time_ms: 10.209\n",
      "  timestamp: 1635083507\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 64\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">          1808.2</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\"> -2.9445</td><td style=\"text-align: right;\">               -2.36</td><td style=\"text-align: right;\">               -4.05</td><td style=\"text-align: right;\">            294.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 65000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-52-19\n",
      "  done: false\n",
      "  episode_len_mean: 289.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3599999999999937\n",
      "  episode_reward_mean: -2.897899999999982\n",
      "  episode_reward_min: -4.049999999999958\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 189\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6169296860694886\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006055288375641194\n",
      "          policy_loss: 0.014575867851575216\n",
      "          total_loss: 0.011750376390086279\n",
      "          vf_explained_var: 0.06582662463188171\n",
      "          vf_loss: 0.013041041356821855\n",
      "    num_agent_steps_sampled: 65000\n",
      "    num_agent_steps_trained: 65000\n",
      "    num_steps_sampled: 65000\n",
      "    num_steps_trained: 65000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.83260869565218\n",
      "    ram_util_percent: 52.20652173913044\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04305623086704836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.038525185445277\n",
      "    mean_inference_ms: 2.225043771364461\n",
      "    mean_raw_obs_processing_ms: 1.5813560252842287\n",
      "  time_since_restore: 1840.32204079628\n",
      "  time_this_iter_s: 32.11888575553894\n",
      "  time_total_s: 1840.32204079628\n",
      "  timers:\n",
      "    learn_throughput: 1170.713\n",
      "    learn_time_ms: 854.18\n",
      "    load_throughput: 48136.195\n",
      "    load_time_ms: 20.774\n",
      "    sample_throughput: 30.655\n",
      "    sample_time_ms: 32621.605\n",
      "    update_time_ms: 9.142\n",
      "  timestamp: 1635083539\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 65000\n",
      "  training_iteration: 65\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         1840.32</td><td style=\"text-align: right;\">65000</td><td style=\"text-align: right;\"> -2.8979</td><td style=\"text-align: right;\">               -2.36</td><td style=\"text-align: right;\">               -4.05</td><td style=\"text-align: right;\">            289.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 66000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-52-49\n",
      "  done: false\n",
      "  episode_len_mean: 285.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3599999999999937\n",
      "  episode_reward_mean: -2.8567999999999825\n",
      "  episode_reward_min: -4.049999999999958\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 193\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5212063683403863\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006432129509238078\n",
      "          policy_loss: 0.01163641901479827\n",
      "          total_loss: 0.00984994661476877\n",
      "          vf_explained_var: 0.09864071011543274\n",
      "          vf_loss: 0.013103986303839419\n",
      "    num_agent_steps_sampled: 66000\n",
      "    num_agent_steps_trained: 66000\n",
      "    num_steps_sampled: 66000\n",
      "    num_steps_trained: 66000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.05116279069767\n",
      "    ram_util_percent: 52.13720930232558\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04305845673464031\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.144811317488994\n",
      "    mean_inference_ms: 2.2251128464657044\n",
      "    mean_raw_obs_processing_ms: 1.58888308685705\n",
      "  time_since_restore: 1870.6021463871002\n",
      "  time_this_iter_s: 30.280105590820312\n",
      "  time_total_s: 1870.6021463871002\n",
      "  timers:\n",
      "    learn_throughput: 1174.282\n",
      "    learn_time_ms: 851.584\n",
      "    load_throughput: 48193.828\n",
      "    load_time_ms: 20.75\n",
      "    sample_throughput: 30.813\n",
      "    sample_time_ms: 32454.049\n",
      "    update_time_ms: 8.307\n",
      "  timestamp: 1635083569\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 66000\n",
      "  training_iteration: 66\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">          1870.6</td><td style=\"text-align: right;\">66000</td><td style=\"text-align: right;\"> -2.8568</td><td style=\"text-align: right;\">               -2.36</td><td style=\"text-align: right;\">               -4.05</td><td style=\"text-align: right;\">            285.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 67000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-53-21\n",
      "  done: false\n",
      "  episode_len_mean: 280.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3599999999999937\n",
      "  episode_reward_mean: -2.8030999999999846\n",
      "  episode_reward_min: -3.5499999999999683\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 197\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5092832605044046\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0068425690639251965\n",
      "          policy_loss: 0.006868941419654422\n",
      "          total_loss: 0.006154790230923229\n",
      "          vf_explained_var: 0.06040404736995697\n",
      "          vf_loss: 0.01403655292880204\n",
      "    num_agent_steps_sampled: 67000\n",
      "    num_agent_steps_trained: 67000\n",
      "    num_steps_sampled: 67000\n",
      "    num_steps_trained: 67000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.10434782608695\n",
      "    ram_util_percent: 52.18043478260871\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04305836803140064\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.255404370042278\n",
      "    mean_inference_ms: 2.225183401895157\n",
      "    mean_raw_obs_processing_ms: 1.5975878128772922\n",
      "  time_since_restore: 1902.7014997005463\n",
      "  time_this_iter_s: 32.099353313446045\n",
      "  time_total_s: 1902.7014997005463\n",
      "  timers:\n",
      "    learn_throughput: 1174.442\n",
      "    learn_time_ms: 851.468\n",
      "    load_throughput: 45793.594\n",
      "    load_time_ms: 21.837\n",
      "    sample_throughput: 30.581\n",
      "    sample_time_ms: 32700.172\n",
      "    update_time_ms: 8.437\n",
      "  timestamp: 1635083601\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 67000\n",
      "  training_iteration: 67\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">          1902.7</td><td style=\"text-align: right;\">67000</td><td style=\"text-align: right;\"> -2.8031</td><td style=\"text-align: right;\">               -2.36</td><td style=\"text-align: right;\">               -3.55</td><td style=\"text-align: right;\">            280.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-53-56\n",
      "  done: false\n",
      "  episode_len_mean: 276.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3599999999999937\n",
      "  episode_reward_mean: -2.7623999999999853\n",
      "  episode_reward_min: -3.47999999999997\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 201\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5647515773773193\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007285638196774357\n",
      "          policy_loss: 0.017278763982984755\n",
      "          total_loss: 0.015546701062056753\n",
      "          vf_explained_var: 0.1080409437417984\n",
      "          vf_loss: 0.013551171734515163\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.68800000000002\n",
      "    ram_util_percent: 52.52799999999999\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043057061351442376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.367329661145686\n",
      "    mean_inference_ms: 2.2252186569393952\n",
      "    mean_raw_obs_processing_ms: 1.6072909094547192\n",
      "  time_since_restore: 1937.5604219436646\n",
      "  time_this_iter_s: 34.858922243118286\n",
      "  time_total_s: 1937.5604219436646\n",
      "  timers:\n",
      "    learn_throughput: 1191.804\n",
      "    learn_time_ms: 839.064\n",
      "    load_throughput: 45358.93\n",
      "    load_time_ms: 22.046\n",
      "    sample_throughput: 30.218\n",
      "    sample_time_ms: 33093.322\n",
      "    update_time_ms: 8.909\n",
      "  timestamp: 1635083636\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 68\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         1937.56</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\"> -2.7624</td><td style=\"text-align: right;\">               -2.36</td><td style=\"text-align: right;\">               -3.48</td><td style=\"text-align: right;\">            276.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 69000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-54-30\n",
      "  done: false\n",
      "  episode_len_mean: 272.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3599999999999937\n",
      "  episode_reward_mean: -2.7273999999999865\n",
      "  episode_reward_min: -3.229999999999975\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 205\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5491428467962476\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006484706241645534\n",
      "          policy_loss: 0.01639820080664423\n",
      "          total_loss: 0.015379443267981211\n",
      "          vf_explained_var: 0.0523436963558197\n",
      "          vf_loss: 0.014148435576094522\n",
      "    num_agent_steps_sampled: 69000\n",
      "    num_agent_steps_trained: 69000\n",
      "    num_steps_sampled: 69000\n",
      "    num_steps_trained: 69000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.8061224489796\n",
      "    ram_util_percent: 52.62040816326531\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04305471384670132\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.479166729310794\n",
      "    mean_inference_ms: 2.2252132078437157\n",
      "    mean_raw_obs_processing_ms: 1.6176033837002064\n",
      "  time_since_restore: 1971.6447968482971\n",
      "  time_this_iter_s: 34.08437490463257\n",
      "  time_total_s: 1971.6447968482971\n",
      "  timers:\n",
      "    learn_throughput: 1220.616\n",
      "    learn_time_ms: 819.258\n",
      "    load_throughput: 45459.466\n",
      "    load_time_ms: 21.998\n",
      "    sample_throughput: 30.033\n",
      "    sample_time_ms: 33296.978\n",
      "    update_time_ms: 8.463\n",
      "  timestamp: 1635083670\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 69000\n",
      "  training_iteration: 69\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         1971.64</td><td style=\"text-align: right;\">69000</td><td style=\"text-align: right;\"> -2.7274</td><td style=\"text-align: right;\">               -2.36</td><td style=\"text-align: right;\">               -3.23</td><td style=\"text-align: right;\">            272.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 70000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-55-03\n",
      "  done: false\n",
      "  episode_len_mean: 270.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3599999999999937\n",
      "  episode_reward_mean: -2.702499999999985\n",
      "  episode_reward_min: -3.2099999999999755\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 209\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7117195818159314\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013230702139081372\n",
      "          policy_loss: 0.004006277355882857\n",
      "          total_loss: 0.0029204951806200874\n",
      "          vf_explained_var: 0.02281450666487217\n",
      "          vf_loss: 0.015369876939803361\n",
      "    num_agent_steps_sampled: 70000\n",
      "    num_agent_steps_trained: 70000\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.30434782608695\n",
      "    ram_util_percent: 52.42391304347827\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04305164655348391\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.58868195920007\n",
      "    mean_inference_ms: 2.2251303673860905\n",
      "    mean_raw_obs_processing_ms: 1.6284786089289995\n",
      "  time_since_restore: 2003.9449167251587\n",
      "  time_this_iter_s: 32.30011987686157\n",
      "  time_total_s: 2003.9449167251587\n",
      "  timers:\n",
      "    learn_throughput: 1228.497\n",
      "    learn_time_ms: 814.003\n",
      "    load_throughput: 47005.905\n",
      "    load_time_ms: 21.274\n",
      "    sample_throughput: 29.991\n",
      "    sample_time_ms: 33343.774\n",
      "    update_time_ms: 6.89\n",
      "  timestamp: 1635083703\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 70\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         2003.94</td><td style=\"text-align: right;\">70000</td><td style=\"text-align: right;\"> -2.7025</td><td style=\"text-align: right;\">               -2.36</td><td style=\"text-align: right;\">               -3.21</td><td style=\"text-align: right;\">            270.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 71000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-55-52\n",
      "  done: false\n",
      "  episode_len_mean: 267.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -2.678999999999986\n",
      "  episode_reward_min: -3.1699999999999764\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 213\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7139503399531046\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008260322019374182\n",
      "          policy_loss: 0.035200661586390604\n",
      "          total_loss: 0.03212252747681406\n",
      "          vf_explained_var: 0.07477491348981857\n",
      "          vf_loss: 0.013648352130419678\n",
      "    num_agent_steps_sampled: 71000\n",
      "    num_agent_steps_trained: 71000\n",
      "    num_steps_sampled: 71000\n",
      "    num_steps_trained: 71000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.23239436619718\n",
      "    ram_util_percent: 52.23802816901407\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04304750815671213\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.69309876687383\n",
      "    mean_inference_ms: 2.2249014647177567\n",
      "    mean_raw_obs_processing_ms: 1.6498148438552078\n",
      "  time_since_restore: 2053.799724817276\n",
      "  time_this_iter_s: 49.85480809211731\n",
      "  time_total_s: 2053.799724817276\n",
      "  timers:\n",
      "    learn_throughput: 1237.211\n",
      "    learn_time_ms: 808.27\n",
      "    load_throughput: 49551.529\n",
      "    load_time_ms: 20.181\n",
      "    sample_throughput: 28.558\n",
      "    sample_time_ms: 35016.045\n",
      "    update_time_ms: 6.435\n",
      "  timestamp: 1635083752\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 71000\n",
      "  training_iteration: 71\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">          2053.8</td><td style=\"text-align: right;\">71000</td><td style=\"text-align: right;\">  -2.679</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -3.17</td><td style=\"text-align: right;\">             267.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-56-25\n",
      "  done: false\n",
      "  episode_len_mean: 266.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -2.6643999999999877\n",
      "  episode_reward_min: -3.049999999999979\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 217\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5592495878537496\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008669924377312555\n",
      "          policy_loss: 0.02498373645875189\n",
      "          total_loss: 0.020599088817834853\n",
      "          vf_explained_var: 0.0848105326294899\n",
      "          vf_loss: 0.010774353873502049\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.85\n",
      "    ram_util_percent: 52.2304347826087\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04304250530747069\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.793586604552228\n",
      "    mean_inference_ms: 2.224599791092822\n",
      "    mean_raw_obs_processing_ms: 1.6714254470509382\n",
      "  time_since_restore: 2085.84068274498\n",
      "  time_this_iter_s: 32.04095792770386\n",
      "  time_total_s: 2085.84068274498\n",
      "  timers:\n",
      "    learn_throughput: 1238.512\n",
      "    learn_time_ms: 807.42\n",
      "    load_throughput: 52302.163\n",
      "    load_time_ms: 19.12\n",
      "    sample_throughput: 28.596\n",
      "    sample_time_ms: 34969.459\n",
      "    update_time_ms: 6.498\n",
      "  timestamp: 1635083785\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 72\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         2085.84</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\"> -2.6644</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -3.05</td><td style=\"text-align: right;\">            266.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 73000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-56-58\n",
      "  done: false\n",
      "  episode_len_mean: 265.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -2.6567999999999863\n",
      "  episode_reward_min: -2.9199999999999817\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 221\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6019215914938185\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014771231813939936\n",
      "          policy_loss: -0.01717242201169332\n",
      "          total_loss: -0.016350777116086748\n",
      "          vf_explained_var: 0.04576577618718147\n",
      "          vf_loss: 0.016102296403712697\n",
      "    num_agent_steps_sampled: 73000\n",
      "    num_agent_steps_trained: 73000\n",
      "    num_steps_sampled: 73000\n",
      "    num_steps_trained: 73000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.78125\n",
      "    ram_util_percent: 52.27291666666667\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04303803838239783\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.893129736090085\n",
      "    mean_inference_ms: 2.2243792149571098\n",
      "    mean_raw_obs_processing_ms: 1.6858744329421074\n",
      "  time_since_restore: 2119.7578229904175\n",
      "  time_this_iter_s: 33.91714024543762\n",
      "  time_total_s: 2119.7578229904175\n",
      "  timers:\n",
      "    learn_throughput: 1240.747\n",
      "    learn_time_ms: 805.966\n",
      "    load_throughput: 52345.505\n",
      "    load_time_ms: 19.104\n",
      "    sample_throughput: 29.916\n",
      "    sample_time_ms: 33427.077\n",
      "    update_time_ms: 6.551\n",
      "  timestamp: 1635083818\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 73000\n",
      "  training_iteration: 73\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         2119.76</td><td style=\"text-align: right;\">73000</td><td style=\"text-align: right;\"> -2.6568</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -2.92</td><td style=\"text-align: right;\">            265.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 74000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-57-31\n",
      "  done: false\n",
      "  episode_len_mean: 264.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2799999999999954\n",
      "  episode_reward_mean: -2.644599999999987\n",
      "  episode_reward_min: -2.9199999999999817\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 225\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4391031132804022\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005679183074808552\n",
      "          policy_loss: 0.006883026079999076\n",
      "          total_loss: 0.007012276848157247\n",
      "          vf_explained_var: 0.18955251574516296\n",
      "          vf_loss: 0.014236320782866742\n",
      "    num_agent_steps_sampled: 74000\n",
      "    num_agent_steps_trained: 74000\n",
      "    num_steps_sampled: 74000\n",
      "    num_steps_trained: 74000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.1723404255319\n",
      "    ram_util_percent: 52.121276595744675\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04303460136906317\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.990807850576257\n",
      "    mean_inference_ms: 2.2241946394688727\n",
      "    mean_raw_obs_processing_ms: 1.6933011724727935\n",
      "  time_since_restore: 2152.6932258605957\n",
      "  time_this_iter_s: 32.93540287017822\n",
      "  time_total_s: 2152.6932258605957\n",
      "  timers:\n",
      "    learn_throughput: 1240.015\n",
      "    learn_time_ms: 806.442\n",
      "    load_throughput: 52446.566\n",
      "    load_time_ms: 19.067\n",
      "    sample_throughput: 29.751\n",
      "    sample_time_ms: 33612.748\n",
      "    update_time_ms: 5.52\n",
      "  timestamp: 1635083851\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 74000\n",
      "  training_iteration: 74\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         2152.69</td><td style=\"text-align: right;\">74000</td><td style=\"text-align: right;\"> -2.6446</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">               -2.92</td><td style=\"text-align: right;\">            264.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 75000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-58-03\n",
      "  done: false\n",
      "  episode_len_mean: 262.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.249999999999996\n",
      "  episode_reward_mean: -2.6291999999999884\n",
      "  episode_reward_min: -2.9199999999999817\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 229\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3156792004903157\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007611965527962323\n",
      "          policy_loss: 0.006761506034268274\n",
      "          total_loss: 0.007925645013650258\n",
      "          vf_explained_var: 0.1466636210680008\n",
      "          vf_loss: 0.013940328794221083\n",
      "    num_agent_steps_sampled: 75000\n",
      "    num_agent_steps_trained: 75000\n",
      "    num_steps_sampled: 75000\n",
      "    num_steps_trained: 75000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.33555555555554\n",
      "    ram_util_percent: 52.095555555555556\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04303070440418742\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.085052619589405\n",
      "    mean_inference_ms: 2.223968870552483\n",
      "    mean_raw_obs_processing_ms: 1.7013102325551959\n",
      "  time_since_restore: 2184.1784789562225\n",
      "  time_this_iter_s: 31.48525309562683\n",
      "  time_total_s: 2184.1784789562225\n",
      "  timers:\n",
      "    learn_throughput: 1239.71\n",
      "    learn_time_ms: 806.64\n",
      "    load_throughput: 50241.114\n",
      "    load_time_ms: 19.904\n",
      "    sample_throughput: 29.809\n",
      "    sample_time_ms: 33547.455\n",
      "    update_time_ms: 6.499\n",
      "  timestamp: 1635083883\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 75000\n",
      "  training_iteration: 75\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         2184.18</td><td style=\"text-align: right;\">75000</td><td style=\"text-align: right;\"> -2.6292</td><td style=\"text-align: right;\">               -2.25</td><td style=\"text-align: right;\">               -2.92</td><td style=\"text-align: right;\">            262.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-58-36\n",
      "  done: false\n",
      "  episode_len_mean: 261.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.249999999999996\n",
      "  episode_reward_mean: -2.6154999999999875\n",
      "  episode_reward_min: -2.9199999999999817\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 233\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3086388972070482\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005593588202193919\n",
      "          policy_loss: 0.021053226623270246\n",
      "          total_loss: 0.022666201574934854\n",
      "          vf_explained_var: 0.12492784857749939\n",
      "          vf_loss: 0.014419685107552344\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.38125000000001\n",
      "    ram_util_percent: 52.166666666666664\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04302625203578632\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.178289721998272\n",
      "    mean_inference_ms: 2.2237420021294825\n",
      "    mean_raw_obs_processing_ms: 1.709606875688702\n",
      "  time_since_restore: 2217.60889005661\n",
      "  time_this_iter_s: 33.43041110038757\n",
      "  time_total_s: 2217.60889005661\n",
      "  timers:\n",
      "    learn_throughput: 1233.308\n",
      "    learn_time_ms: 810.827\n",
      "    load_throughput: 50445.291\n",
      "    load_time_ms: 19.823\n",
      "    sample_throughput: 29.535\n",
      "    sample_time_ms: 33858.223\n",
      "    update_time_ms: 6.574\n",
      "  timestamp: 1635083916\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 76\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         2217.61</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\"> -2.6155</td><td style=\"text-align: right;\">               -2.25</td><td style=\"text-align: right;\">               -2.92</td><td style=\"text-align: right;\">            261.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 77000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_13-59-10\n",
      "  done: false\n",
      "  episode_len_mean: 260.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.249999999999996\n",
      "  episode_reward_mean: -2.600899999999988\n",
      "  episode_reward_min: -2.9199999999999817\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 237\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3708100928200615\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004303925289010143\n",
      "          policy_loss: 0.014611925764216317\n",
      "          total_loss: 0.01511111284295718\n",
      "          vf_explained_var: 0.12921778857707977\n",
      "          vf_loss: 0.013992092758417129\n",
      "    num_agent_steps_sampled: 77000\n",
      "    num_agent_steps_trained: 77000\n",
      "    num_steps_sampled: 77000\n",
      "    num_steps_trained: 77000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.31489361702128\n",
      "    ram_util_percent: 52.30212765957448\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04302215951205302\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.270093835177548\n",
      "    mean_inference_ms: 2.223540481983075\n",
      "    mean_raw_obs_processing_ms: 1.718385142676031\n",
      "  time_since_restore: 2250.725687980652\n",
      "  time_this_iter_s: 33.11679792404175\n",
      "  time_total_s: 2250.725687980652\n",
      "  timers:\n",
      "    learn_throughput: 1231.082\n",
      "    learn_time_ms: 812.293\n",
      "    load_throughput: 50642.634\n",
      "    load_time_ms: 19.746\n",
      "    sample_throughput: 29.448\n",
      "    sample_time_ms: 33958.278\n",
      "    update_time_ms: 6.786\n",
      "  timestamp: 1635083950\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 77000\n",
      "  training_iteration: 77\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         2250.73</td><td style=\"text-align: right;\">77000</td><td style=\"text-align: right;\"> -2.6009</td><td style=\"text-align: right;\">               -2.25</td><td style=\"text-align: right;\">               -2.92</td><td style=\"text-align: right;\">            260.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 78000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-00-02\n",
      "  done: false\n",
      "  episode_len_mean: 258.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.5867999999999887\n",
      "  episode_reward_min: -2.9199999999999817\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 241\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.024999999999999994\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2725055509143406\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006586731271201214\n",
      "          policy_loss: 0.016471090912818908\n",
      "          total_loss: 0.018288878599802653\n",
      "          vf_explained_var: 0.09500110149383545\n",
      "          vf_loss: 0.014378175894833273\n",
      "    num_agent_steps_sampled: 78000\n",
      "    num_agent_steps_trained: 78000\n",
      "    num_steps_sampled: 78000\n",
      "    num_steps_trained: 78000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.51891891891891\n",
      "    ram_util_percent: 52.37972972972973\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04301679592328614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.360673371681155\n",
      "    mean_inference_ms: 2.2233608682734443\n",
      "    mean_raw_obs_processing_ms: 1.7364162304185433\n",
      "  time_since_restore: 2302.825339078903\n",
      "  time_this_iter_s: 52.09965109825134\n",
      "  time_total_s: 2302.825339078903\n",
      "  timers:\n",
      "    learn_throughput: 1238.15\n",
      "    learn_time_ms: 807.657\n",
      "    load_throughput: 53115.988\n",
      "    load_time_ms: 18.827\n",
      "    sample_throughput: 28.02\n",
      "    sample_time_ms: 35688.432\n",
      "    update_time_ms: 6.45\n",
      "  timestamp: 1635084002\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 78000\n",
      "  training_iteration: 78\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         2302.83</td><td style=\"text-align: right;\">78000</td><td style=\"text-align: right;\"> -2.5868</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -2.92</td><td style=\"text-align: right;\">            258.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 79000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-00-37\n",
      "  done: false\n",
      "  episode_len_mean: 257.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.574599999999989\n",
      "  episode_reward_min: -2.9199999999999817\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 245\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.024999999999999994\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2972793738047281\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00557703354378659\n",
      "          policy_loss: 0.018231557640764447\n",
      "          total_loss: 0.019380651497178606\n",
      "          vf_explained_var: 0.18454040586948395\n",
      "          vf_loss: 0.013982462510466576\n",
      "    num_agent_steps_sampled: 79000\n",
      "    num_agent_steps_trained: 79000\n",
      "    num_steps_sampled: 79000\n",
      "    num_steps_trained: 79000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.09411764705881\n",
      "    ram_util_percent: 52.34901960784314\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04301124186668969\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.45112432578256\n",
      "    mean_inference_ms: 2.2232102271901235\n",
      "    mean_raw_obs_processing_ms: 1.7545295451293064\n",
      "  time_since_restore: 2338.078900575638\n",
      "  time_this_iter_s: 35.25356149673462\n",
      "  time_total_s: 2338.078900575638\n",
      "  timers:\n",
      "    learn_throughput: 1243.691\n",
      "    learn_time_ms: 804.058\n",
      "    load_throughput: 52974.37\n",
      "    load_time_ms: 18.877\n",
      "    sample_throughput: 27.926\n",
      "    sample_time_ms: 35808.932\n",
      "    update_time_ms: 6.361\n",
      "  timestamp: 1635084037\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79000\n",
      "  training_iteration: 79\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         2338.08</td><td style=\"text-align: right;\">79000</td><td style=\"text-align: right;\"> -2.5746</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -2.92</td><td style=\"text-align: right;\">            257.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-01-12\n",
      "  done: false\n",
      "  episode_len_mean: 255.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.5592999999999893\n",
      "  episode_reward_min: -2.9199999999999817\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 249\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.024999999999999994\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2028002169397143\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003743473168825062\n",
      "          policy_loss: -0.041593517280287214\n",
      "          total_loss: -0.039764325155152214\n",
      "          vf_explained_var: 0.11949621886014938\n",
      "          vf_loss: 0.013763611091093884\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.50399999999998\n",
      "    ram_util_percent: 52.224000000000004\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043005330872114896\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.540327514007146\n",
      "    mean_inference_ms: 2.2230278232050718\n",
      "    mean_raw_obs_processing_ms: 1.7698046181683649\n",
      "  time_since_restore: 2373.3461363315582\n",
      "  time_this_iter_s: 35.26723575592041\n",
      "  time_total_s: 2373.3461363315582\n",
      "  timers:\n",
      "    learn_throughput: 1243.27\n",
      "    learn_time_ms: 804.331\n",
      "    load_throughput: 51467.894\n",
      "    load_time_ms: 19.43\n",
      "    sample_throughput: 27.697\n",
      "    sample_time_ms: 36104.775\n",
      "    update_time_ms: 6.435\n",
      "  timestamp: 1635084072\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 80\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         2373.35</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> -2.5593</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -2.92</td><td style=\"text-align: right;\">            255.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 81000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-01-47\n",
      "  done: false\n",
      "  episode_len_mean: 254.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.547699999999989\n",
      "  episode_reward_min: -2.9199999999999817\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 254\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012499999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.194903349876404\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008132817542400122\n",
      "          policy_loss: -0.02941139779157109\n",
      "          total_loss: -0.02334882699780994\n",
      "          vf_explained_var: 0.12114426493644714\n",
      "          vf_loss: 0.017909942184471422\n",
      "    num_agent_steps_sampled: 81000\n",
      "    num_agent_steps_trained: 81000\n",
      "    num_steps_sampled: 81000\n",
      "    num_steps_trained: 81000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.77999999999999\n",
      "    ram_util_percent: 52.376\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042997148214843725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.648672041217345\n",
      "    mean_inference_ms: 2.2227941147240724\n",
      "    mean_raw_obs_processing_ms: 1.7784657836082216\n",
      "  time_since_restore: 2408.470604658127\n",
      "  time_this_iter_s: 35.1244683265686\n",
      "  time_total_s: 2408.470604658127\n",
      "  timers:\n",
      "    learn_throughput: 1240.448\n",
      "    learn_time_ms: 806.16\n",
      "    load_throughput: 51368.301\n",
      "    load_time_ms: 19.467\n",
      "    sample_throughput: 28.877\n",
      "    sample_time_ms: 34629.354\n",
      "    update_time_ms: 7.202\n",
      "  timestamp: 1635084107\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 81000\n",
      "  training_iteration: 81\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         2408.47</td><td style=\"text-align: right;\">81000</td><td style=\"text-align: right;\"> -2.5477</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -2.92</td><td style=\"text-align: right;\">            254.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 82000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-02-22\n",
      "  done: false\n",
      "  episode_len_mean: 253.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.5369999999999893\n",
      "  episode_reward_min: -2.9199999999999817\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 258\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012499999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.109177621205648\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011064990746449944\n",
      "          policy_loss: 0.0019756917738252217\n",
      "          total_loss: 0.004783243354823854\n",
      "          vf_explained_var: 0.18018405139446259\n",
      "          vf_loss: 0.013761016353964806\n",
      "    num_agent_steps_sampled: 82000\n",
      "    num_agent_steps_trained: 82000\n",
      "    num_steps_sampled: 82000\n",
      "    num_steps_trained: 82000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.53200000000001\n",
      "    ram_util_percent: 52.37\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04299241229964957\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.734119305452495\n",
      "    mean_inference_ms: 2.2226501649739774\n",
      "    mean_raw_obs_processing_ms: 1.7860069550211108\n",
      "  time_since_restore: 2443.4940140247345\n",
      "  time_this_iter_s: 35.023409366607666\n",
      "  time_total_s: 2443.4940140247345\n",
      "  timers:\n",
      "    learn_throughput: 1240.576\n",
      "    learn_time_ms: 806.077\n",
      "    load_throughput: 51113.21\n",
      "    load_time_ms: 19.564\n",
      "    sample_throughput: 28.631\n",
      "    sample_time_ms: 34927.415\n",
      "    update_time_ms: 7.156\n",
      "  timestamp: 1635084142\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 82000\n",
      "  training_iteration: 82\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         2443.49</td><td style=\"text-align: right;\">82000</td><td style=\"text-align: right;\">  -2.537</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -2.92</td><td style=\"text-align: right;\">             253.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 83000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-02-58\n",
      "  done: false\n",
      "  episode_len_mean: 252.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.52129999999999\n",
      "  episode_reward_min: -2.9199999999999817\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 262\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012499999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0152728080749511\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00854548165631807\n",
      "          policy_loss: 0.043381691889630425\n",
      "          total_loss: 0.043492080519596736\n",
      "          vf_explained_var: 0.16184931993484497\n",
      "          vf_loss: 0.010156299489446812\n",
      "    num_agent_steps_sampled: 83000\n",
      "    num_agent_steps_trained: 83000\n",
      "    num_steps_sampled: 83000\n",
      "    num_steps_trained: 83000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.14117647058823\n",
      "    ram_util_percent: 52.366666666666674\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04299260630413685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.82000039930078\n",
      "    mean_inference_ms: 2.2225011867091147\n",
      "    mean_raw_obs_processing_ms: 1.7937633720672432\n",
      "  time_since_restore: 2479.029674768448\n",
      "  time_this_iter_s: 35.53566074371338\n",
      "  time_total_s: 2479.029674768448\n",
      "  timers:\n",
      "    learn_throughput: 1243.909\n",
      "    learn_time_ms: 803.917\n",
      "    load_throughput: 51378.495\n",
      "    load_time_ms: 19.463\n",
      "    sample_throughput: 28.496\n",
      "    sample_time_ms: 35092.189\n",
      "    update_time_ms: 6.68\n",
      "  timestamp: 1635084178\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 83000\n",
      "  training_iteration: 83\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         2479.03</td><td style=\"text-align: right;\">83000</td><td style=\"text-align: right;\"> -2.5213</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -2.92</td><td style=\"text-align: right;\">            252.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-03-31\n",
      "  done: false\n",
      "  episode_len_mean: 250.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.1899999999999973\n",
      "  episode_reward_mean: -2.50289999999999\n",
      "  episode_reward_min: -2.909999999999982\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 266\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012499999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9396879984272851\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005177083332431249\n",
      "          policy_loss: -0.03103602727254232\n",
      "          total_loss: -0.02655711786614524\n",
      "          vf_explained_var: 0.17864227294921875\n",
      "          vf_loss: 0.013811076608382994\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.72340425531915\n",
      "    ram_util_percent: 52.19148936170211\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04299055542866777\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.904360110307433\n",
      "    mean_inference_ms: 2.2223137462191964\n",
      "    mean_raw_obs_processing_ms: 1.8017031233890932\n",
      "  time_since_restore: 2512.005047559738\n",
      "  time_this_iter_s: 32.97537279129028\n",
      "  time_total_s: 2512.005047559738\n",
      "  timers:\n",
      "    learn_throughput: 1241.438\n",
      "    learn_time_ms: 805.518\n",
      "    load_throughput: 48736.748\n",
      "    load_time_ms: 20.518\n",
      "    sample_throughput: 28.496\n",
      "    sample_time_ms: 35092.291\n",
      "    update_time_ms: 7.806\n",
      "  timestamp: 1635084211\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 84\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         2512.01</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\"> -2.5029</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">               -2.91</td><td style=\"text-align: right;\">            250.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 85000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-04-24\n",
      "  done: false\n",
      "  episode_len_mean: 247.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0300000000000007\n",
      "  episode_reward_mean: -2.478299999999991\n",
      "  episode_reward_min: -2.8299999999999836\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 271\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012499999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9374240868621402\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004984585201702569\n",
      "          policy_loss: -0.012237703551848729\n",
      "          total_loss: -0.0044098546935452355\n",
      "          vf_explained_var: 0.14866098761558533\n",
      "          vf_loss: 0.017139784753736523\n",
      "    num_agent_steps_sampled: 85000\n",
      "    num_agent_steps_trained: 85000\n",
      "    num_steps_sampled: 85000\n",
      "    num_steps_trained: 85000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.71447368421052\n",
      "    ram_util_percent: 52.33684210526315\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042986229047856685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.007620151116193\n",
      "    mean_inference_ms: 2.2220267161763574\n",
      "    mean_raw_obs_processing_ms: 1.8222255063971458\n",
      "  time_since_restore: 2565.232909679413\n",
      "  time_this_iter_s: 53.22786211967468\n",
      "  time_total_s: 2565.232909679413\n",
      "  timers:\n",
      "    learn_throughput: 1239.156\n",
      "    learn_time_ms: 807.001\n",
      "    load_throughput: 50225.11\n",
      "    load_time_ms: 19.91\n",
      "    sample_throughput: 26.834\n",
      "    sample_time_ms: 37265.78\n",
      "    update_time_ms: 7.521\n",
      "  timestamp: 1635084264\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 85000\n",
      "  training_iteration: 85\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         2565.23</td><td style=\"text-align: right;\">85000</td><td style=\"text-align: right;\"> -2.4783</td><td style=\"text-align: right;\">               -2.03</td><td style=\"text-align: right;\">               -2.83</td><td style=\"text-align: right;\">            247.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 86000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-04-59\n",
      "  done: false\n",
      "  episode_len_mean: 246.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0300000000000007\n",
      "  episode_reward_mean: -2.4672999999999914\n",
      "  episode_reward_min: -2.8299999999999836\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 275\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006249999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8806563516457876\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004749406619489744\n",
      "          policy_loss: 0.038010713458061215\n",
      "          total_loss: 0.04260693167646726\n",
      "          vf_explained_var: 0.1140073761343956\n",
      "          vf_loss: 0.013373098149895669\n",
      "    num_agent_steps_sampled: 86000\n",
      "    num_agent_steps_trained: 86000\n",
      "    num_steps_sampled: 86000\n",
      "    num_steps_trained: 86000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.43061224489794\n",
      "    ram_util_percent: 52.27346938775511\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042982075490239\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.088597562826543\n",
      "    mean_inference_ms: 2.2217649224235227\n",
      "    mean_raw_obs_processing_ms: 1.8387389827267742\n",
      "  time_since_restore: 2599.852367401123\n",
      "  time_this_iter_s: 34.619457721710205\n",
      "  time_total_s: 2599.852367401123\n",
      "  timers:\n",
      "    learn_throughput: 1239.653\n",
      "    learn_time_ms: 806.677\n",
      "    load_throughput: 49780.419\n",
      "    load_time_ms: 20.088\n",
      "    sample_throughput: 26.749\n",
      "    sample_time_ms: 37385.096\n",
      "    update_time_ms: 7.173\n",
      "  timestamp: 1635084299\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 86000\n",
      "  training_iteration: 86\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         2599.85</td><td style=\"text-align: right;\">86000</td><td style=\"text-align: right;\"> -2.4673</td><td style=\"text-align: right;\">               -2.03</td><td style=\"text-align: right;\">               -2.83</td><td style=\"text-align: right;\">            246.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 87000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-05-34\n",
      "  done: false\n",
      "  episode_len_mean: 245.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0300000000000007\n",
      "  episode_reward_mean: -2.4527999999999914\n",
      "  episode_reward_min: -2.759999999999985\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 279\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8528572565979428\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004191921375099778\n",
      "          policy_loss: -0.04614872758587201\n",
      "          total_loss: -0.03935868905650245\n",
      "          vf_explained_var: 0.023441558703780174\n",
      "          vf_loss: 0.015305509945998589\n",
      "    num_agent_steps_sampled: 87000\n",
      "    num_agent_steps_trained: 87000\n",
      "    num_steps_sampled: 87000\n",
      "    num_steps_trained: 87000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.9156862745098\n",
      "    ram_util_percent: 52.36470588235295\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042977205763628915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.168803050012148\n",
      "    mean_inference_ms: 2.2214704021880705\n",
      "    mean_raw_obs_processing_ms: 1.8526451913632862\n",
      "  time_since_restore: 2635.0923109054565\n",
      "  time_this_iter_s: 35.239943504333496\n",
      "  time_total_s: 2635.0923109054565\n",
      "  timers:\n",
      "    learn_throughput: 1239.572\n",
      "    learn_time_ms: 806.73\n",
      "    load_throughput: 49544.797\n",
      "    load_time_ms: 20.184\n",
      "    sample_throughput: 26.597\n",
      "    sample_time_ms: 37598.241\n",
      "    update_time_ms: 6.538\n",
      "  timestamp: 1635084334\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 87000\n",
      "  training_iteration: 87\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         2635.09</td><td style=\"text-align: right;\">87000</td><td style=\"text-align: right;\"> -2.4528</td><td style=\"text-align: right;\">               -2.03</td><td style=\"text-align: right;\">               -2.76</td><td style=\"text-align: right;\">            245.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-06-09\n",
      "  done: false\n",
      "  episode_len_mean: 243.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0300000000000007\n",
      "  episode_reward_mean: -2.438499999999992\n",
      "  episode_reward_min: -2.759999999999985\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 284\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0015624999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0001492785082924\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003887963692452148\n",
      "          policy_loss: -0.001894808808962504\n",
      "          total_loss: 0.0035319018695089553\n",
      "          vf_explained_var: 0.21359889209270477\n",
      "          vf_loss: 0.015422126578374041\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.1\n",
      "    ram_util_percent: 52.357142857142854\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042971851971426514\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.266704691407977\n",
      "    mean_inference_ms: 2.2211226147344743\n",
      "    mean_raw_obs_processing_ms: 1.8605923446142167\n",
      "  time_since_restore: 2669.599676132202\n",
      "  time_this_iter_s: 34.507365226745605\n",
      "  time_total_s: 2669.599676132202\n",
      "  timers:\n",
      "    learn_throughput: 1241.919\n",
      "    learn_time_ms: 805.206\n",
      "    load_throughput: 49482.372\n",
      "    load_time_ms: 20.209\n",
      "    sample_throughput: 27.902\n",
      "    sample_time_ms: 35839.951\n",
      "    update_time_ms: 7.012\n",
      "  timestamp: 1635084369\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 88\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">          2669.6</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\"> -2.4385</td><td style=\"text-align: right;\">               -2.03</td><td style=\"text-align: right;\">               -2.76</td><td style=\"text-align: right;\">            243.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 89000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-06-43\n",
      "  done: false\n",
      "  episode_len_mean: 242.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0300000000000007\n",
      "  episode_reward_mean: -2.427099999999992\n",
      "  episode_reward_min: -2.759999999999985\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 288\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0007812499999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8071586377090878\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007467644785560168\n",
      "          policy_loss: 0.015973518209324942\n",
      "          total_loss: 0.021207677904102537\n",
      "          vf_explained_var: 0.10487408936023712\n",
      "          vf_loss: 0.013299916001657645\n",
      "    num_agent_steps_sampled: 89000\n",
      "    num_agent_steps_trained: 89000\n",
      "    num_steps_sampled: 89000\n",
      "    num_steps_trained: 89000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.27551020408163\n",
      "    ram_util_percent: 52.38571428571428\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0429685362390469\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.344104789552503\n",
      "    mean_inference_ms: 2.2208636972971987\n",
      "    mean_raw_obs_processing_ms: 1.8672765151955286\n",
      "  time_since_restore: 2704.255361557007\n",
      "  time_this_iter_s: 34.65568542480469\n",
      "  time_total_s: 2704.255361557007\n",
      "  timers:\n",
      "    learn_throughput: 1244.68\n",
      "    learn_time_ms: 803.419\n",
      "    load_throughput: 49452.676\n",
      "    load_time_ms: 20.221\n",
      "    sample_throughput: 27.947\n",
      "    sample_time_ms: 35782.016\n",
      "    update_time_ms: 7.04\n",
      "  timestamp: 1635084403\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 89000\n",
      "  training_iteration: 89\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         2704.26</td><td style=\"text-align: right;\">89000</td><td style=\"text-align: right;\"> -2.4271</td><td style=\"text-align: right;\">               -2.03</td><td style=\"text-align: right;\">               -2.76</td><td style=\"text-align: right;\">            242.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 90000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-07-19\n",
      "  done: false\n",
      "  episode_len_mean: 241.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0300000000000007\n",
      "  episode_reward_mean: -2.417299999999992\n",
      "  episode_reward_min: -2.759999999999985\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 292\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0007812499999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7129866712623172\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0029825534614786875\n",
      "          policy_loss: -0.025014345058136517\n",
      "          total_loss: -0.021234332356188032\n",
      "          vf_explained_var: 0.2834034264087677\n",
      "          vf_loss: 0.010907548924701081\n",
      "    num_agent_steps_sampled: 90000\n",
      "    num_agent_steps_trained: 90000\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.22549019607845\n",
      "    ram_util_percent: 52.39607843137255\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04296574277400029\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.42199883875298\n",
      "    mean_inference_ms: 2.2206255728039372\n",
      "    mean_raw_obs_processing_ms: 1.8741404337641203\n",
      "  time_since_restore: 2739.6216185092926\n",
      "  time_this_iter_s: 35.36625695228577\n",
      "  time_total_s: 2739.6216185092926\n",
      "  timers:\n",
      "    learn_throughput: 1243.04\n",
      "    learn_time_ms: 804.479\n",
      "    load_throughput: 50477.346\n",
      "    load_time_ms: 19.811\n",
      "    sample_throughput: 27.94\n",
      "    sample_time_ms: 35791.069\n",
      "    update_time_ms: 7.115\n",
      "  timestamp: 1635084439\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 90\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         2739.62</td><td style=\"text-align: right;\">90000</td><td style=\"text-align: right;\"> -2.4173</td><td style=\"text-align: right;\">               -2.03</td><td style=\"text-align: right;\">               -2.76</td><td style=\"text-align: right;\">            241.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 91000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-07-55\n",
      "  done: false\n",
      "  episode_len_mean: 240.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.0300000000000007\n",
      "  episode_reward_mean: -2.403999999999993\n",
      "  episode_reward_min: -2.759999999999985\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 297\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003906249999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6756083879205915\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.002695597678833571\n",
      "          policy_loss: -0.005620761919352743\n",
      "          total_loss: 0.0035907172080543305\n",
      "          vf_explained_var: 0.10129614919424057\n",
      "          vf_loss: 0.015966512604306142\n",
      "    num_agent_steps_sampled: 91000\n",
      "    num_agent_steps_trained: 91000\n",
      "    num_steps_sampled: 91000\n",
      "    num_steps_trained: 91000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.10588235294117\n",
      "    ram_util_percent: 52.37058823529412\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04296763573464325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.519380238654993\n",
      "    mean_inference_ms: 2.2203392881702233\n",
      "    mean_raw_obs_processing_ms: 1.8830161029640533\n",
      "  time_since_restore: 2775.4494421482086\n",
      "  time_this_iter_s: 35.827823638916016\n",
      "  time_total_s: 2775.4494421482086\n",
      "  timers:\n",
      "    learn_throughput: 1243.366\n",
      "    learn_time_ms: 804.268\n",
      "    load_throughput: 46829.931\n",
      "    load_time_ms: 21.354\n",
      "    sample_throughput: 27.886\n",
      "    sample_time_ms: 35860.607\n",
      "    update_time_ms: 6.516\n",
      "  timestamp: 1635084475\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 91000\n",
      "  training_iteration: 91\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         2775.45</td><td style=\"text-align: right;\">91000</td><td style=\"text-align: right;\">  -2.404</td><td style=\"text-align: right;\">               -2.03</td><td style=\"text-align: right;\">               -2.76</td><td style=\"text-align: right;\">             240.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-08-46\n",
      "  done: false\n",
      "  episode_len_mean: 239.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.010000000000001\n",
      "  episode_reward_mean: -2.3911999999999924\n",
      "  episode_reward_min: -2.759999999999985\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 301\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00019531249999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6503707746664683\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0035408857120272173\n",
      "          policy_loss: -0.07380921931730376\n",
      "          total_loss: -0.06772244738207923\n",
      "          vf_explained_var: 0.08067691326141357\n",
      "          vf_loss: 0.012589786325891812\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.49864864864864\n",
      "    ram_util_percent: 52.32027027027027\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04296846338140682\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.59446263589409\n",
      "    mean_inference_ms: 2.2200841733979333\n",
      "    mean_raw_obs_processing_ms: 1.8980121300568573\n",
      "  time_since_restore: 2827.276373386383\n",
      "  time_this_iter_s: 51.82693123817444\n",
      "  time_total_s: 2827.276373386383\n",
      "  timers:\n",
      "    learn_throughput: 1242.108\n",
      "    learn_time_ms: 805.083\n",
      "    load_throughput: 44788.546\n",
      "    load_time_ms: 22.327\n",
      "    sample_throughput: 26.639\n",
      "    sample_time_ms: 37538.95\n",
      "    update_time_ms: 6.963\n",
      "  timestamp: 1635084526\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 92\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         2827.28</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\"> -2.3912</td><td style=\"text-align: right;\">               -2.01</td><td style=\"text-align: right;\">               -2.76</td><td style=\"text-align: right;\">            239.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 93000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-09-22\n",
      "  done: false\n",
      "  episode_len_mean: 237.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.010000000000001\n",
      "  episode_reward_mean: -2.3791999999999933\n",
      "  episode_reward_min: -2.759999999999985\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 306\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.765624999999998e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7352458165751563\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006941747074102977\n",
      "          policy_loss: -0.043642450124025345\n",
      "          total_loss: -0.03459905766778522\n",
      "          vf_explained_var: 0.1115916445851326\n",
      "          vf_loss: 0.016395170593427287\n",
      "    num_agent_steps_sampled: 93000\n",
      "    num_agent_steps_trained: 93000\n",
      "    num_steps_sampled: 93000\n",
      "    num_steps_trained: 93000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.96799999999999\n",
      "    ram_util_percent: 52.361999999999995\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04296828780621262\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.685923438015706\n",
      "    mean_inference_ms: 2.2197285556292887\n",
      "    mean_raw_obs_processing_ms: 1.917035602149091\n",
      "  time_since_restore: 2862.5206937789917\n",
      "  time_this_iter_s: 35.24432039260864\n",
      "  time_total_s: 2862.5206937789917\n",
      "  timers:\n",
      "    learn_throughput: 1236.476\n",
      "    learn_time_ms: 808.75\n",
      "    load_throughput: 42302.783\n",
      "    load_time_ms: 23.639\n",
      "    sample_throughput: 26.663\n",
      "    sample_time_ms: 37504.691\n",
      "    update_time_ms: 6.978\n",
      "  timestamp: 1635084562\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 93000\n",
      "  training_iteration: 93\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         2862.52</td><td style=\"text-align: right;\">93000</td><td style=\"text-align: right;\"> -2.3792</td><td style=\"text-align: right;\">               -2.01</td><td style=\"text-align: right;\">               -2.76</td><td style=\"text-align: right;\">            237.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 94000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-09-58\n",
      "  done: false\n",
      "  episode_len_mean: 236.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.010000000000001\n",
      "  episode_reward_mean: -2.368599999999993\n",
      "  episode_reward_min: -2.759999999999985\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 310\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.765624999999998e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6501571271154616\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005067001579162265\n",
      "          policy_loss: 0.00752199747496181\n",
      "          total_loss: 0.012611712846491072\n",
      "          vf_explained_var: 0.2024020254611969\n",
      "          vf_loss: 0.011590795105116234\n",
      "    num_agent_steps_sampled: 94000\n",
      "    num_agent_steps_trained: 94000\n",
      "    num_steps_sampled: 94000\n",
      "    num_steps_trained: 94000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.24716981132076\n",
      "    ram_util_percent: 52.466037735849056\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04296887753998057\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.75923684882453\n",
      "    mean_inference_ms: 2.2194651745868\n",
      "    mean_raw_obs_processing_ms: 1.9299015053460016\n",
      "  time_since_restore: 2899.246435403824\n",
      "  time_this_iter_s: 36.72574162483215\n",
      "  time_total_s: 2899.246435403824\n",
      "  timers:\n",
      "    learn_throughput: 1238.447\n",
      "    learn_time_ms: 807.463\n",
      "    load_throughput: 42274.003\n",
      "    load_time_ms: 23.655\n",
      "    sample_throughput: 26.398\n",
      "    sample_time_ms: 37881.401\n",
      "    update_time_ms: 6.54\n",
      "  timestamp: 1635084598\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 94000\n",
      "  training_iteration: 94\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         2899.25</td><td style=\"text-align: right;\">94000</td><td style=\"text-align: right;\"> -2.3686</td><td style=\"text-align: right;\">               -2.01</td><td style=\"text-align: right;\">               -2.76</td><td style=\"text-align: right;\">            236.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 95000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-10-34\n",
      "  done: false\n",
      "  episode_len_mean: 234.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.010000000000001\n",
      "  episode_reward_mean: -2.348299999999994\n",
      "  episode_reward_min: -2.719999999999986\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 315\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.765624999999998e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5255890574720171\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003033218959904755\n",
      "          policy_loss: 0.007795814507537418\n",
      "          total_loss: 0.016725203312105603\n",
      "          vf_explained_var: 0.15644387900829315\n",
      "          vf_loss: 0.014184985775500536\n",
      "    num_agent_steps_sampled: 95000\n",
      "    num_agent_steps_trained: 95000\n",
      "    num_steps_sampled: 95000\n",
      "    num_steps_trained: 95000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.25490196078431\n",
      "    ram_util_percent: 52.51960784313726\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04296942329879306\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.850295251644887\n",
      "    mean_inference_ms: 2.2191493848460473\n",
      "    mean_raw_obs_processing_ms: 1.9371483411086619\n",
      "  time_since_restore: 2935.194729566574\n",
      "  time_this_iter_s: 35.948294162750244\n",
      "  time_total_s: 2935.194729566574\n",
      "  timers:\n",
      "    learn_throughput: 1239.776\n",
      "    learn_time_ms: 806.597\n",
      "    load_throughput: 43007.299\n",
      "    load_time_ms: 23.252\n",
      "    sample_throughput: 27.658\n",
      "    sample_time_ms: 36155.367\n",
      "    update_time_ms: 5.954\n",
      "  timestamp: 1635084634\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 95000\n",
      "  training_iteration: 95\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         2935.19</td><td style=\"text-align: right;\">95000</td><td style=\"text-align: right;\"> -2.3483</td><td style=\"text-align: right;\">               -2.01</td><td style=\"text-align: right;\">               -2.72</td><td style=\"text-align: right;\">            234.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-11-11\n",
      "  done: false\n",
      "  episode_len_mean: 233.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.010000000000001\n",
      "  episode_reward_mean: -2.3321999999999945\n",
      "  episode_reward_min: -2.579999999999989\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 319\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.882812499999999e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3999578419658873\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0041667050031701965\n",
      "          policy_loss: 0.006040275428030226\n",
      "          total_loss: 0.012379464589887194\n",
      "          vf_explained_var: 0.17526651918888092\n",
      "          vf_loss: 0.010338567776812448\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.13653846153845\n",
      "    ram_util_percent: 52.50769230769231\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04297024040730328\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.922932101812773\n",
      "    mean_inference_ms: 2.218883404483085\n",
      "    mean_raw_obs_processing_ms: 1.9431702650243354\n",
      "  time_since_restore: 2971.870411634445\n",
      "  time_this_iter_s: 36.675682067871094\n",
      "  time_total_s: 2971.870411634445\n",
      "  timers:\n",
      "    learn_throughput: 1239.137\n",
      "    learn_time_ms: 807.013\n",
      "    load_throughput: 45181.126\n",
      "    load_time_ms: 22.133\n",
      "    sample_throughput: 27.502\n",
      "    sample_time_ms: 36361.327\n",
      "    update_time_ms: 6.356\n",
      "  timestamp: 1635084671\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 96\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         2971.87</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\"> -2.3322</td><td style=\"text-align: right;\">               -2.01</td><td style=\"text-align: right;\">               -2.58</td><td style=\"text-align: right;\">            233.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 97000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-11-47\n",
      "  done: false\n",
      "  episode_len_mean: 232.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.010000000000001\n",
      "  episode_reward_mean: -2.3215999999999943\n",
      "  episode_reward_min: -2.579999999999989\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 324\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.4414062499999995e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6230243583520253\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.07356970500118515\n",
      "          policy_loss: -0.008097355647219553\n",
      "          total_loss: 0.003571563959121704\n",
      "          vf_explained_var: 0.055365897715091705\n",
      "          vf_loss: 0.01789736787064208\n",
      "    num_agent_steps_sampled: 97000\n",
      "    num_agent_steps_trained: 97000\n",
      "    num_steps_sampled: 97000\n",
      "    num_steps_trained: 97000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.08235294117647\n",
      "    ram_util_percent: 52.45882352941177\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04297147519004709\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.012519786391113\n",
      "    mean_inference_ms: 2.2185397196020578\n",
      "    mean_raw_obs_processing_ms: 1.9509912956321989\n",
      "  time_since_restore: 3007.429951906204\n",
      "  time_this_iter_s: 35.55954027175903\n",
      "  time_total_s: 3007.429951906204\n",
      "  timers:\n",
      "    learn_throughput: 1238.919\n",
      "    learn_time_ms: 807.155\n",
      "    load_throughput: 47307.15\n",
      "    load_time_ms: 21.138\n",
      "    sample_throughput: 27.477\n",
      "    sample_time_ms: 36394.166\n",
      "    update_time_ms: 6.291\n",
      "  timestamp: 1635084707\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 97000\n",
      "  training_iteration: 97\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         3007.43</td><td style=\"text-align: right;\">97000</td><td style=\"text-align: right;\"> -2.3216</td><td style=\"text-align: right;\">               -2.01</td><td style=\"text-align: right;\">               -2.58</td><td style=\"text-align: right;\">            232.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 98000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-12-23\n",
      "  done: false\n",
      "  episode_len_mean: 231.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.010000000000001\n",
      "  episode_reward_mean: -2.3151999999999946\n",
      "  episode_reward_min: -2.579999999999989\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 328\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.662109375e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5386399855216344\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006972965296538192\n",
      "          policy_loss: 0.023830982132090464\n",
      "          total_loss: 0.031007749173376294\n",
      "          vf_explained_var: 0.13894133269786835\n",
      "          vf_loss: 0.012562912278291252\n",
      "    num_agent_steps_sampled: 98000\n",
      "    num_agent_steps_trained: 98000\n",
      "    num_steps_sampled: 98000\n",
      "    num_steps_trained: 98000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.2326923076923\n",
      "    ram_util_percent: 52.41346153846154\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04297311781982458\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.084110075015943\n",
      "    mean_inference_ms: 2.218304534509128\n",
      "    mean_raw_obs_processing_ms: 1.957524282662282\n",
      "  time_since_restore: 3043.460404396057\n",
      "  time_this_iter_s: 36.030452489852905\n",
      "  time_total_s: 3043.460404396057\n",
      "  timers:\n",
      "    learn_throughput: 1237.632\n",
      "    learn_time_ms: 807.995\n",
      "    load_throughput: 45419.346\n",
      "    load_time_ms: 22.017\n",
      "    sample_throughput: 27.363\n",
      "    sample_time_ms: 36545.226\n",
      "    update_time_ms: 5.874\n",
      "  timestamp: 1635084743\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 98000\n",
      "  training_iteration: 98\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         3043.46</td><td style=\"text-align: right;\">98000</td><td style=\"text-align: right;\"> -2.3152</td><td style=\"text-align: right;\">               -2.01</td><td style=\"text-align: right;\">               -2.58</td><td style=\"text-align: right;\">            231.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 99000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-13-14\n",
      "  done: false\n",
      "  episode_len_mean: 230.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.3057999999999947\n",
      "  episode_reward_min: -2.579999999999989\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 333\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.662109375e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6529193096690707\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01852685574078434\n",
      "          policy_loss: -0.034465215769078995\n",
      "          total_loss: -0.024184731642405193\n",
      "          vf_explained_var: 0.15444035828113556\n",
      "          vf_loss: 0.016809003427624702\n",
      "    num_agent_steps_sampled: 99000\n",
      "    num_agent_steps_trained: 99000\n",
      "    num_steps_sampled: 99000\n",
      "    num_steps_trained: 99000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.29452054794521\n",
      "    ram_util_percent: 52.28630136986302\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04297543480478309\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.172506803027428\n",
      "    mean_inference_ms: 2.218005074657828\n",
      "    mean_raw_obs_processing_ms: 1.974327827148044\n",
      "  time_since_restore: 3094.724416255951\n",
      "  time_this_iter_s: 51.2640118598938\n",
      "  time_total_s: 3094.724416255951\n",
      "  timers:\n",
      "    learn_throughput: 1234.013\n",
      "    learn_time_ms: 810.364\n",
      "    load_throughput: 43539.503\n",
      "    load_time_ms: 22.968\n",
      "    sample_throughput: 26.176\n",
      "    sample_time_ms: 38202.86\n",
      "    update_time_ms: 5.75\n",
      "  timestamp: 1635084794\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 99000\n",
      "  training_iteration: 99\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         3094.72</td><td style=\"text-align: right;\">99000</td><td style=\"text-align: right;\"> -2.3058</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.58</td><td style=\"text-align: right;\">            230.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-13-51\n",
      "  done: false\n",
      "  episode_len_mean: 229.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.2975999999999948\n",
      "  episode_reward_min: -2.579999999999989\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 337\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.662109375e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5602583017614152\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0032486958469500377\n",
      "          policy_loss: 0.03156962738268905\n",
      "          total_loss: 0.03843191812435786\n",
      "          vf_explained_var: 0.15803495049476624\n",
      "          vf_loss: 0.012464754210991993\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.6\n",
      "    ram_util_percent: 52.40576923076923\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042978283254908815\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.24274995891337\n",
      "    mean_inference_ms: 2.217780981396914\n",
      "    mean_raw_obs_processing_ms: 1.987971019543013\n",
      "  time_since_restore: 3131.5995378494263\n",
      "  time_this_iter_s: 36.87512159347534\n",
      "  time_total_s: 3131.5995378494263\n",
      "  timers:\n",
      "    learn_throughput: 1236.752\n",
      "    learn_time_ms: 808.57\n",
      "    load_throughput: 42032.696\n",
      "    load_time_ms: 23.791\n",
      "    sample_throughput: 26.073\n",
      "    sample_time_ms: 38353.551\n",
      "    update_time_ms: 6.808\n",
      "  timestamp: 1635084831\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 100\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">          3131.6</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\"> -2.2976</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.58</td><td style=\"text-align: right;\">            229.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 101000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-14-28\n",
      "  done: false\n",
      "  episode_len_mean: 229.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.290599999999995\n",
      "  episode_reward_min: -2.53999999999999\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 341\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.8310546875e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5495842927032046\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003935811847607182\n",
      "          policy_loss: -0.032523603902922735\n",
      "          total_loss: -0.025541627324289745\n",
      "          vf_explained_var: 0.14634329080581665\n",
      "          vf_loss: 0.012477751014133294\n",
      "    num_agent_steps_sampled: 101000\n",
      "    num_agent_steps_trained: 101000\n",
      "    num_steps_sampled: 101000\n",
      "    num_steps_trained: 101000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.84444444444445\n",
      "    ram_util_percent: 52.75925925925927\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04298163946697638\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.312428117704748\n",
      "    mean_inference_ms: 2.21758130042323\n",
      "    mean_raw_obs_processing_ms: 1.992877061106039\n",
      "  time_since_restore: 3169.126264810562\n",
      "  time_this_iter_s: 37.526726961135864\n",
      "  time_total_s: 3169.126264810562\n",
      "  timers:\n",
      "    learn_throughput: 1232.825\n",
      "    learn_time_ms: 811.145\n",
      "    load_throughput: 44423.563\n",
      "    load_time_ms: 22.511\n",
      "    sample_throughput: 25.959\n",
      "    sample_time_ms: 38522.142\n",
      "    update_time_ms: 6.897\n",
      "  timestamp: 1635084868\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 101000\n",
      "  training_iteration: 101\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         3169.13</td><td style=\"text-align: right;\">101000</td><td style=\"text-align: right;\"> -2.2906</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">            229.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 102000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-15-05\n",
      "  done: false\n",
      "  episode_len_mean: 228.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.281199999999995\n",
      "  episode_reward_min: -2.53999999999999\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 346\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.1552734375e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5153044525119993\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004605702757578298\n",
      "          policy_loss: -0.0023487175504366556\n",
      "          total_loss: 0.0055538120369116465\n",
      "          vf_explained_var: 0.25401973724365234\n",
      "          vf_loss: 0.013055535550746653\n",
      "    num_agent_steps_sampled: 102000\n",
      "    num_agent_steps_trained: 102000\n",
      "    num_steps_sampled: 102000\n",
      "    num_steps_trained: 102000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.79607843137254\n",
      "    ram_util_percent: 52.80588235294118\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042985858979015745\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.397101920555652\n",
      "    mean_inference_ms: 2.217306233341645\n",
      "    mean_raw_obs_processing_ms: 1.9993931974496832\n",
      "  time_since_restore: 3205.139718770981\n",
      "  time_this_iter_s: 36.0134539604187\n",
      "  time_total_s: 3205.139718770981\n",
      "  timers:\n",
      "    learn_throughput: 1230.909\n",
      "    learn_time_ms: 812.408\n",
      "    load_throughput: 45044.391\n",
      "    load_time_ms: 22.2\n",
      "    sample_throughput: 27.071\n",
      "    sample_time_ms: 36940.201\n",
      "    update_time_ms: 6.49\n",
      "  timestamp: 1635084905\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 102000\n",
      "  training_iteration: 102\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         3205.14</td><td style=\"text-align: right;\">102000</td><td style=\"text-align: right;\"> -2.2812</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">            228.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 103000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-15-42\n",
      "  done: false\n",
      "  episode_len_mean: 227.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.2750999999999952\n",
      "  episode_reward_min: -2.53999999999999\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 350\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.57763671875e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.41337089737256366\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003396988532157942\n",
      "          policy_loss: 0.007935110645161735\n",
      "          total_loss: 0.014442344547973739\n",
      "          vf_explained_var: 0.18229785561561584\n",
      "          vf_loss: 0.010640930084304678\n",
      "    num_agent_steps_sampled: 103000\n",
      "    num_agent_steps_trained: 103000\n",
      "    num_steps_sampled: 103000\n",
      "    num_steps_trained: 103000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.63333333333333\n",
      "    ram_util_percent: 52.76666666666666\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04298886341852561\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.46406450506887\n",
      "    mean_inference_ms: 2.2170834446488263\n",
      "    mean_raw_obs_processing_ms: 2.0047747824490054\n",
      "  time_since_restore: 3242.562128305435\n",
      "  time_this_iter_s: 37.422409534454346\n",
      "  time_total_s: 3242.562128305435\n",
      "  timers:\n",
      "    learn_throughput: 1233.014\n",
      "    learn_time_ms: 811.021\n",
      "    load_throughput: 47517.401\n",
      "    load_time_ms: 21.045\n",
      "    sample_throughput: 26.911\n",
      "    sample_time_ms: 37159.877\n",
      "    update_time_ms: 7.267\n",
      "  timestamp: 1635084942\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 103000\n",
      "  training_iteration: 103\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         3242.56</td><td style=\"text-align: right;\">103000</td><td style=\"text-align: right;\"> -2.2751</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">            227.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-16-19\n",
      "  done: false\n",
      "  episode_len_mean: 227.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.2707999999999955\n",
      "  episode_reward_min: -2.53999999999999\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 355\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.288818359375e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6151476717657514\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004541725390428929\n",
      "          policy_loss: -0.012216410040855408\n",
      "          total_loss: -0.0027110187543763053\n",
      "          vf_explained_var: 0.15242302417755127\n",
      "          vf_loss: 0.015656862325138517\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.38490566037733\n",
      "    ram_util_percent: 52.73396226415095\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04299339723900698\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.547314655719873\n",
      "    mean_inference_ms: 2.216839658521616\n",
      "    mean_raw_obs_processing_ms: 2.0116026668366094\n",
      "  time_since_restore: 3280.0263068675995\n",
      "  time_this_iter_s: 37.46417856216431\n",
      "  time_total_s: 3280.0263068675995\n",
      "  timers:\n",
      "    learn_throughput: 1231.559\n",
      "    learn_time_ms: 811.979\n",
      "    load_throughput: 48891.671\n",
      "    load_time_ms: 20.453\n",
      "    sample_throughput: 26.857\n",
      "    sample_time_ms: 37233.663\n",
      "    update_time_ms: 6.963\n",
      "  timestamp: 1635084979\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 104\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         3280.03</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\"> -2.2708</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">            227.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 105000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-16-56\n",
      "  done: false\n",
      "  episode_len_mean: 226.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.267299999999995\n",
      "  episode_reward_min: -2.53999999999999\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 359\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1444091796875e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4582119915220473\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008836527197748802\n",
      "          policy_loss: 0.017805405457814536\n",
      "          total_loss: 0.025616566671265495\n",
      "          vf_explained_var: 0.0648215264081955\n",
      "          vf_loss: 0.012393271302183468\n",
      "    num_agent_steps_sampled: 105000\n",
      "    num_agent_steps_trained: 105000\n",
      "    num_steps_sampled: 105000\n",
      "    num_steps_trained: 105000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.3566037735849\n",
      "    ram_util_percent: 52.577358490566034\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042995449936793974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.61293940579901\n",
      "    mean_inference_ms: 2.2166406943637535\n",
      "    mean_raw_obs_processing_ms: 2.017233160859166\n",
      "  time_since_restore: 3316.7085704803467\n",
      "  time_this_iter_s: 36.68226361274719\n",
      "  time_total_s: 3316.7085704803467\n",
      "  timers:\n",
      "    learn_throughput: 1230.159\n",
      "    learn_time_ms: 812.903\n",
      "    load_throughput: 46514.38\n",
      "    load_time_ms: 21.499\n",
      "    sample_throughput: 26.806\n",
      "    sample_time_ms: 37305.065\n",
      "    update_time_ms: 6.928\n",
      "  timestamp: 1635085016\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 105000\n",
      "  training_iteration: 105\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         3316.71</td><td style=\"text-align: right;\">105000</td><td style=\"text-align: right;\"> -2.2673</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">            226.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 106000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-17-50\n",
      "  done: false\n",
      "  episode_len_mean: 226.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.2611999999999957\n",
      "  episode_reward_min: -2.53999999999999\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 364\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1444091796875e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6365479177898831\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007877317884222634\n",
      "          policy_loss: -0.006320915950669183\n",
      "          total_loss: 0.004606870727406608\n",
      "          vf_explained_var: 0.07436185330152512\n",
      "          vf_loss: 0.01729325961528553\n",
      "    num_agent_steps_sampled: 106000\n",
      "    num_agent_steps_trained: 106000\n",
      "    num_steps_sampled: 106000\n",
      "    num_steps_trained: 106000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.27105263157894\n",
      "    ram_util_percent: 52.4921052631579\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04299464145942558\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.693302634271554\n",
      "    mean_inference_ms: 2.2164224964738186\n",
      "    mean_raw_obs_processing_ms: 2.0330126057466806\n",
      "  time_since_restore: 3370.4716930389404\n",
      "  time_this_iter_s: 53.76312255859375\n",
      "  time_total_s: 3370.4716930389404\n",
      "  timers:\n",
      "    learn_throughput: 1230.285\n",
      "    learn_time_ms: 812.82\n",
      "    load_throughput: 44550.538\n",
      "    load_time_ms: 22.446\n",
      "    sample_throughput: 25.632\n",
      "    sample_time_ms: 39013.417\n",
      "    update_time_ms: 6.552\n",
      "  timestamp: 1635085070\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 106000\n",
      "  training_iteration: 106\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         3370.47</td><td style=\"text-align: right;\">106000</td><td style=\"text-align: right;\"> -2.2612</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">            226.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 107000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-18-24\n",
      "  done: false\n",
      "  episode_len_mean: 225.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.2580999999999953\n",
      "  episode_reward_min: -2.53999999999999\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 368\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1444091796875e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.43379965358310274\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0024357337421389187\n",
      "          policy_loss: 0.026795685787995658\n",
      "          total_loss: 0.03491421358452903\n",
      "          vf_explained_var: 0.053023580461740494\n",
      "          vf_loss: 0.012456523192425569\n",
      "    num_agent_steps_sampled: 107000\n",
      "    num_agent_steps_trained: 107000\n",
      "    num_steps_sampled: 107000\n",
      "    num_steps_trained: 107000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.71799999999999\n",
      "    ram_util_percent: 52.41\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04299430068227493\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.755987440132348\n",
      "    mean_inference_ms: 2.2162546096641558\n",
      "    mean_raw_obs_processing_ms: 2.041787081163873\n",
      "  time_since_restore: 3404.8799183368683\n",
      "  time_this_iter_s: 34.408225297927856\n",
      "  time_total_s: 3404.8799183368683\n",
      "  timers:\n",
      "    learn_throughput: 1209.353\n",
      "    learn_time_ms: 826.888\n",
      "    load_throughput: 42774.054\n",
      "    load_time_ms: 23.379\n",
      "    sample_throughput: 25.718\n",
      "    sample_time_ms: 38883.299\n",
      "    update_time_ms: 6.582\n",
      "  timestamp: 1635085104\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 107000\n",
      "  training_iteration: 107\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         3404.88</td><td style=\"text-align: right;\">107000</td><td style=\"text-align: right;\"> -2.2581</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">            225.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-19-00\n",
      "  done: false\n",
      "  episode_len_mean: 225.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.255999999999996\n",
      "  episode_reward_min: -2.53999999999999\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 373\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.7220458984375e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.49564829071362815\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0030646130619105608\n",
      "          policy_loss: -0.01878499537706375\n",
      "          total_loss: -0.007087736576795578\n",
      "          vf_explained_var: 0.0735543891787529\n",
      "          vf_loss: 0.01665374135805501\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.7235294117647\n",
      "    ram_util_percent: 52.45882352941177\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042994310435234676\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.83331966982384\n",
      "    mean_inference_ms: 2.2160535802448944\n",
      "    mean_raw_obs_processing_ms: 2.047832771431734\n",
      "  time_since_restore: 3440.9167342185974\n",
      "  time_this_iter_s: 36.036815881729126\n",
      "  time_total_s: 3440.9167342185974\n",
      "  timers:\n",
      "    learn_throughput: 1209.738\n",
      "    learn_time_ms: 826.625\n",
      "    load_throughput: 44422.34\n",
      "    load_time_ms: 22.511\n",
      "    sample_throughput: 25.717\n",
      "    sample_time_ms: 38885.158\n",
      "    update_time_ms: 6.454\n",
      "  timestamp: 1635085140\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 108\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         3440.92</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">  -2.256</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">             225.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 109000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-19-37\n",
      "  done: false\n",
      "  episode_len_mean: 225.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.2525999999999957\n",
      "  episode_reward_min: -2.53999999999999\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 377\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.86102294921875e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5081882764895757\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003426184891495391\n",
      "          policy_loss: 0.02621337870756785\n",
      "          total_loss: 0.033719621267583634\n",
      "          vf_explained_var: 0.0949847623705864\n",
      "          vf_loss: 0.012588125922613673\n",
      "    num_agent_steps_sampled: 109000\n",
      "    num_agent_steps_trained: 109000\n",
      "    num_steps_sampled: 109000\n",
      "    num_steps_trained: 109000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.4403846153846\n",
      "    ram_util_percent: 52.60961538461538\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0429945755751692\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.89423866879686\n",
      "    mean_inference_ms: 2.215907615763003\n",
      "    mean_raw_obs_processing_ms: 2.0528712037010504\n",
      "  time_since_restore: 3477.2944860458374\n",
      "  time_this_iter_s: 36.37775182723999\n",
      "  time_total_s: 3477.2944860458374\n",
      "  timers:\n",
      "    learn_throughput: 1213.792\n",
      "    learn_time_ms: 823.864\n",
      "    load_throughput: 45791.545\n",
      "    load_time_ms: 21.838\n",
      "    sample_throughput: 26.738\n",
      "    sample_time_ms: 37399.666\n",
      "    update_time_ms: 6.689\n",
      "  timestamp: 1635085177\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 109000\n",
      "  training_iteration: 109\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         3477.29</td><td style=\"text-align: right;\">109000</td><td style=\"text-align: right;\"> -2.2526</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">            225.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 110000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-20-13\n",
      "  done: false\n",
      "  episode_len_mean: 224.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.2498999999999962\n",
      "  episode_reward_min: -2.53999999999999\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 382\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.430511474609375e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5957576387458378\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004352790328791735\n",
      "          policy_loss: -0.007871108750502268\n",
      "          total_loss: 0.003484443575143814\n",
      "          vf_explained_var: 0.05126935988664627\n",
      "          vf_loss: 0.017313126598795255\n",
      "    num_agent_steps_sampled: 110000\n",
      "    num_agent_steps_trained: 110000\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.5\n",
      "    ram_util_percent: 52.699999999999996\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04299512259372801\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.969026183533416\n",
      "    mean_inference_ms: 2.215741091463725\n",
      "    mean_raw_obs_processing_ms: 2.05941493905945\n",
      "  time_since_restore: 3513.1167340278625\n",
      "  time_this_iter_s: 35.82224798202515\n",
      "  time_total_s: 3513.1167340278625\n",
      "  timers:\n",
      "    learn_throughput: 1212.563\n",
      "    learn_time_ms: 824.7\n",
      "    load_throughput: 46815.818\n",
      "    load_time_ms: 21.36\n",
      "    sample_throughput: 26.813\n",
      "    sample_time_ms: 37294.929\n",
      "    update_time_ms: 5.959\n",
      "  timestamp: 1635085213\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 110\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         3513.12</td><td style=\"text-align: right;\">110000</td><td style=\"text-align: right;\"> -2.2499</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">            224.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 111000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-20-48\n",
      "  done: false\n",
      "  episode_len_mean: 224.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.2485999999999966\n",
      "  episode_reward_min: -2.53999999999999\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 386\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557373046875e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7212026993433635\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005156121253396072\n",
      "          policy_loss: 0.015578163746330474\n",
      "          total_loss: 0.021808903084860908\n",
      "          vf_explained_var: 0.09067834913730621\n",
      "          vf_loss: 0.013442763975924915\n",
      "    num_agent_steps_sampled: 111000\n",
      "    num_agent_steps_trained: 111000\n",
      "    num_steps_sampled: 111000\n",
      "    num_steps_trained: 111000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.736\n",
      "    ram_util_percent: 52.632000000000005\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04299541091364054\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.02819661303465\n",
      "    mean_inference_ms: 2.215623051990726\n",
      "    mean_raw_obs_processing_ms: 2.0646749693560706\n",
      "  time_since_restore: 3548.204413175583\n",
      "  time_this_iter_s: 35.08767914772034\n",
      "  time_total_s: 3548.204413175583\n",
      "  timers:\n",
      "    learn_throughput: 1214.862\n",
      "    learn_time_ms: 823.139\n",
      "    load_throughput: 46750.903\n",
      "    load_time_ms: 21.39\n",
      "    sample_throughput: 26.988\n",
      "    sample_time_ms: 37052.849\n",
      "    update_time_ms: 5.691\n",
      "  timestamp: 1635085248\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 111000\n",
      "  training_iteration: 111\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">          3548.2</td><td style=\"text-align: right;\">111000</td><td style=\"text-align: right;\"> -2.2486</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">            224.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-21-39\n",
      "  done: false\n",
      "  episode_len_mean: 224.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.246299999999996\n",
      "  episode_reward_min: -2.579999999999989\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 390\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557373046875e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8581100516849094\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005584556892321283\n",
      "          policy_loss: -0.020282677312692007\n",
      "          total_loss: -0.015145190722412534\n",
      "          vf_explained_var: 0.09455060213804245\n",
      "          vf_loss: 0.013718587170458502\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.28082191780823\n",
      "    ram_util_percent: 52.54657534246576\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04299565170730002\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.08628107169102\n",
      "    mean_inference_ms: 2.2155156772359033\n",
      "    mean_raw_obs_processing_ms: 2.0758226445374204\n",
      "  time_since_restore: 3599.462255716324\n",
      "  time_this_iter_s: 51.25784254074097\n",
      "  time_total_s: 3599.462255716324\n",
      "  timers:\n",
      "    learn_throughput: 1219.152\n",
      "    learn_time_ms: 820.243\n",
      "    load_throughput: 48299.217\n",
      "    load_time_ms: 20.704\n",
      "    sample_throughput: 25.92\n",
      "    sample_time_ms: 38580.805\n",
      "    update_time_ms: 5.577\n",
      "  timestamp: 1635085299\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 112\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         3599.46</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\"> -2.2463</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.58</td><td style=\"text-align: right;\">            224.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 113000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-22-15\n",
      "  done: false\n",
      "  episode_len_mean: 225.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.2508999999999957\n",
      "  episode_reward_min: -2.5899999999999888\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 395\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557373046875e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8979679233498044\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013799669739704474\n",
      "          policy_loss: -0.01547656249668863\n",
      "          total_loss: -0.007259251756800545\n",
      "          vf_explained_var: 0.15145809948444366\n",
      "          vf_loss: 0.017196989887290532\n",
      "    num_agent_steps_sampled: 113000\n",
      "    num_agent_steps_trained: 113000\n",
      "    num_steps_sampled: 113000\n",
      "    num_steps_trained: 113000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.4096153846154\n",
      "    ram_util_percent: 52.41538461538461\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04299320480936505\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.157222972023302\n",
      "    mean_inference_ms: 2.2154006165543776\n",
      "    mean_raw_obs_processing_ms: 2.0899072171487267\n",
      "  time_since_restore: 3635.332422733307\n",
      "  time_this_iter_s: 35.87016701698303\n",
      "  time_total_s: 3635.332422733307\n",
      "  timers:\n",
      "    learn_throughput: 1220.498\n",
      "    learn_time_ms: 819.338\n",
      "    load_throughput: 48400.544\n",
      "    load_time_ms: 20.661\n",
      "    sample_throughput: 26.023\n",
      "    sample_time_ms: 38427.188\n",
      "    update_time_ms: 4.825\n",
      "  timestamp: 1635085335\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 113000\n",
      "  training_iteration: 113\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         3635.33</td><td style=\"text-align: right;\">113000</td><td style=\"text-align: right;\"> -2.2509</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">            225.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 114000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-22-50\n",
      "  done: false\n",
      "  episode_len_mean: 225.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.2548999999999957\n",
      "  episode_reward_min: -2.5899999999999888\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 399\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557373046875e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8169161286618974\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004150703684132553\n",
      "          policy_loss: 0.015615962673392561\n",
      "          total_loss: 0.0201341077271435\n",
      "          vf_explained_var: 0.2009023278951645\n",
      "          vf_loss: 0.012687306250962945\n",
      "    num_agent_steps_sampled: 114000\n",
      "    num_agent_steps_trained: 114000\n",
      "    num_steps_sampled: 114000\n",
      "    num_steps_trained: 114000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.49183673469389\n",
      "    ram_util_percent: 52.41224489795919\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04298980738488705\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.213119590572642\n",
      "    mean_inference_ms: 2.2153265096607693\n",
      "    mean_raw_obs_processing_ms: 2.0973481638102482\n",
      "  time_since_restore: 3670.0983436107635\n",
      "  time_this_iter_s: 34.765920877456665\n",
      "  time_total_s: 3670.0983436107635\n",
      "  timers:\n",
      "    learn_throughput: 1222.345\n",
      "    learn_time_ms: 818.1\n",
      "    load_throughput: 48426.082\n",
      "    load_time_ms: 20.65\n",
      "    sample_throughput: 26.206\n",
      "    sample_time_ms: 38158.728\n",
      "    update_time_ms: 4.851\n",
      "  timestamp: 1635085370\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 114000\n",
      "  training_iteration: 114\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">          3670.1</td><td style=\"text-align: right;\">114000</td><td style=\"text-align: right;\"> -2.2549</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">            225.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 115000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-23-23\n",
      "  done: false\n",
      "  episode_len_mean: 226.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.2609999999999952\n",
      "  episode_reward_min: -2.5899999999999888\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 403\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5762786865234374e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7450522091653612\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005662928244764921\n",
      "          policy_loss: 0.009028987255361345\n",
      "          total_loss: 0.015129440401991209\n",
      "          vf_explained_var: 0.09296313673257828\n",
      "          vf_loss: 0.013550976208514637\n",
      "    num_agent_steps_sampled: 115000\n",
      "    num_agent_steps_trained: 115000\n",
      "    num_steps_sampled: 115000\n",
      "    num_steps_trained: 115000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.94166666666666\n",
      "    ram_util_percent: 52.383333333333326\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042986251130785365\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.267559167135698\n",
      "    mean_inference_ms: 2.2152492977029046\n",
      "    mean_raw_obs_processing_ms: 2.1010677242240576\n",
      "  time_since_restore: 3703.498541355133\n",
      "  time_this_iter_s: 33.40019774436951\n",
      "  time_total_s: 3703.498541355133\n",
      "  timers:\n",
      "    learn_throughput: 1225.822\n",
      "    learn_time_ms: 815.779\n",
      "    load_throughput: 48470.628\n",
      "    load_time_ms: 20.631\n",
      "    sample_throughput: 26.432\n",
      "    sample_time_ms: 37832.407\n",
      "    update_time_ms: 5.212\n",
      "  timestamp: 1635085403\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 115000\n",
      "  training_iteration: 115\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">          3703.5</td><td style=\"text-align: right;\">115000</td><td style=\"text-align: right;\">  -2.261</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">             226.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-23-59\n",
      "  done: false\n",
      "  episode_len_mean: 226.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.2626999999999953\n",
      "  episode_reward_min: -2.5899999999999888\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 407\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5762786865234374e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6312297191884783\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004031243974887467\n",
      "          policy_loss: -0.04250357043411997\n",
      "          total_loss: -0.03517894256446097\n",
      "          vf_explained_var: 0.11245667934417725\n",
      "          vf_loss: 0.013636927575700813\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.31\n",
      "    ram_util_percent: 52.540000000000006\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04298233156029012\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.321548583441356\n",
      "    mean_inference_ms: 2.215170622550937\n",
      "    mean_raw_obs_processing_ms: 2.1047110607389437\n",
      "  time_since_restore: 3738.8294727802277\n",
      "  time_this_iter_s: 35.330931425094604\n",
      "  time_total_s: 3738.8294727802277\n",
      "  timers:\n",
      "    learn_throughput: 1223.751\n",
      "    learn_time_ms: 817.16\n",
      "    load_throughput: 48622.678\n",
      "    load_time_ms: 20.567\n",
      "    sample_throughput: 27.787\n",
      "    sample_time_ms: 35987.57\n",
      "    update_time_ms: 5.508\n",
      "  timestamp: 1635085439\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 116\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         3738.83</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\"> -2.2627</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">            226.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 117000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-24-34\n",
      "  done: false\n",
      "  episode_len_mean: 226.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.265699999999995\n",
      "  episode_reward_min: -2.5899999999999888\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 412\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7881393432617187e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6707199652989705\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003085861725271223\n",
      "          policy_loss: -0.01470608553952641\n",
      "          total_loss: -0.004117316835456424\n",
      "          vf_explained_var: 0.09271086752414703\n",
      "          vf_loss: 0.017295969298316373\n",
      "    num_agent_steps_sampled: 117000\n",
      "    num_agent_steps_trained: 117000\n",
      "    num_steps_sampled: 117000\n",
      "    num_steps_trained: 117000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.492\n",
      "    ram_util_percent: 52.64000000000001\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04297667931318819\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.386501813908225\n",
      "    mean_inference_ms: 2.215065032641005\n",
      "    mean_raw_obs_processing_ms: 2.109556447018704\n",
      "  time_since_restore: 3773.86483335495\n",
      "  time_this_iter_s: 35.03536057472229\n",
      "  time_total_s: 3773.86483335495\n",
      "  timers:\n",
      "    learn_throughput: 1247.695\n",
      "    learn_time_ms: 801.478\n",
      "    load_throughput: 47963.736\n",
      "    load_time_ms: 20.849\n",
      "    sample_throughput: 27.728\n",
      "    sample_time_ms: 36065.075\n",
      "    update_time_ms: 6.037\n",
      "  timestamp: 1635085474\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 117000\n",
      "  training_iteration: 117\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         3773.86</td><td style=\"text-align: right;\">117000</td><td style=\"text-align: right;\"> -2.2657</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">            226.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 118000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-25-09\n",
      "  done: false\n",
      "  episode_len_mean: 226.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.2672999999999957\n",
      "  episode_reward_min: -2.5899999999999888\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 416\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.940696716308593e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5386604819032881\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003567569382182809\n",
      "          policy_loss: 0.028861620194382136\n",
      "          total_loss: 0.034323945807086094\n",
      "          vf_explained_var: 0.11112948507070541\n",
      "          vf_loss: 0.010848927130508754\n",
      "    num_agent_steps_sampled: 118000\n",
      "    num_agent_steps_trained: 118000\n",
      "    num_steps_sampled: 118000\n",
      "    num_steps_trained: 118000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.10384615384615\n",
      "    ram_util_percent: 52.72115384615385\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04297221818951706\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.437780424350112\n",
      "    mean_inference_ms: 2.2149802742186013\n",
      "    mean_raw_obs_processing_ms: 2.113434181557648\n",
      "  time_since_restore: 3809.7068457603455\n",
      "  time_this_iter_s: 35.84201240539551\n",
      "  time_total_s: 3809.7068457603455\n",
      "  timers:\n",
      "    learn_throughput: 1247.496\n",
      "    learn_time_ms: 801.606\n",
      "    load_throughput: 47231.238\n",
      "    load_time_ms: 21.172\n",
      "    sample_throughput: 27.743\n",
      "    sample_time_ms: 36045.078\n",
      "    update_time_ms: 6.134\n",
      "  timestamp: 1635085509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 118000\n",
      "  training_iteration: 118\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">         3809.71</td><td style=\"text-align: right;\">118000</td><td style=\"text-align: right;\"> -2.2673</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">            226.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 119000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-26-03\n",
      "  done: false\n",
      "  episode_len_mean: 226.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.2689999999999957\n",
      "  episode_reward_min: -2.5899999999999888\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 421\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.470348358154297e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6521365529961056\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005265877956954136\n",
      "          policy_loss: -0.022585116244024702\n",
      "          total_loss: -0.010123679372999404\n",
      "          vf_explained_var: 0.06916885823011398\n",
      "          vf_loss: 0.01898280143116911\n",
      "    num_agent_steps_sampled: 119000\n",
      "    num_agent_steps_trained: 119000\n",
      "    num_steps_sampled: 119000\n",
      "    num_steps_trained: 119000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.20129870129871\n",
      "    ram_util_percent: 52.6948051948052\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042966804218602536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.500386946777372\n",
      "    mean_inference_ms: 2.2148829275647017\n",
      "    mean_raw_obs_processing_ms: 2.125666106174699\n",
      "  time_since_restore: 3863.6842033863068\n",
      "  time_this_iter_s: 53.977357625961304\n",
      "  time_total_s: 3863.6842033863068\n",
      "  timers:\n",
      "    learn_throughput: 1244.829\n",
      "    learn_time_ms: 803.323\n",
      "    load_throughput: 46861.637\n",
      "    load_time_ms: 21.339\n",
      "    sample_throughput: 26.453\n",
      "    sample_time_ms: 37802.902\n",
      "    update_time_ms: 6.467\n",
      "  timestamp: 1635085563\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 119000\n",
      "  training_iteration: 119\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         3863.68</td><td style=\"text-align: right;\">119000</td><td style=\"text-align: right;\">  -2.269</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">             226.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-26-37\n",
      "  done: false\n",
      "  episode_len_mean: 227.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.2757999999999954\n",
      "  episode_reward_min: -2.849999999999983\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 425\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.470348358154297e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9586391501956516\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009692963810608843\n",
      "          policy_loss: 0.01288099918100569\n",
      "          total_loss: 0.01725533397661315\n",
      "          vf_explained_var: 0.08545073866844177\n",
      "          vf_loss: 0.01396072506904602\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.38541666666667\n",
      "    ram_util_percent: 52.55416666666667\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0429626527884776\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.549369748384844\n",
      "    mean_inference_ms: 2.214801022434077\n",
      "    mean_raw_obs_processing_ms: 2.1353911558491627\n",
      "  time_since_restore: 3897.584071159363\n",
      "  time_this_iter_s: 33.89986777305603\n",
      "  time_total_s: 3897.584071159363\n",
      "  timers:\n",
      "    learn_throughput: 1247.659\n",
      "    learn_time_ms: 801.501\n",
      "    load_throughput: 47965.711\n",
      "    load_time_ms: 20.848\n",
      "    sample_throughput: 26.586\n",
      "    sample_time_ms: 37613.382\n",
      "    update_time_ms: 6.054\n",
      "  timestamp: 1635085597\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 120\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">         3897.58</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\"> -2.2758</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.85</td><td style=\"text-align: right;\">            227.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 121000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-27-12\n",
      "  done: false\n",
      "  episode_len_mean: 227.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.2780999999999954\n",
      "  episode_reward_min: -2.849999999999983\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 429\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.470348358154297e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6427232417795393\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0042301514468\n",
      "          policy_loss: 0.024696239166789584\n",
      "          total_loss: 0.03196198874049717\n",
      "          vf_explained_var: 0.1254882514476776\n",
      "          vf_loss: 0.013692979286942217\n",
      "    num_agent_steps_sampled: 121000\n",
      "    num_agent_steps_trained: 121000\n",
      "    num_steps_sampled: 121000\n",
      "    num_steps_trained: 121000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.71199999999999\n",
      "    ram_util_percent: 52.632\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04295912684413444\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.597208265321832\n",
      "    mean_inference_ms: 2.2147298160356796\n",
      "    mean_raw_obs_processing_ms: 2.143473808785054\n",
      "  time_since_restore: 3932.4846329689026\n",
      "  time_this_iter_s: 34.900561809539795\n",
      "  time_total_s: 3932.4846329689026\n",
      "  timers:\n",
      "    learn_throughput: 1249.191\n",
      "    learn_time_ms: 800.518\n",
      "    load_throughput: 48723.217\n",
      "    load_time_ms: 20.524\n",
      "    sample_throughput: 26.599\n",
      "    sample_time_ms: 37595.931\n",
      "    update_time_ms: 6.081\n",
      "  timestamp: 1635085632\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 121000\n",
      "  training_iteration: 121\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         3932.48</td><td style=\"text-align: right;\">121000</td><td style=\"text-align: right;\"> -2.2781</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.85</td><td style=\"text-align: right;\">            227.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 122000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-27-47\n",
      "  done: false\n",
      "  episode_len_mean: 228.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.2850999999999955\n",
      "  episode_reward_min: -2.849999999999983\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 433\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.2351741790771484e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7348436103926764\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0063005875488492\n",
      "          policy_loss: -0.0012662548157903882\n",
      "          total_loss: 0.005855242411295573\n",
      "          vf_explained_var: 0.06923781335353851\n",
      "          vf_loss: 0.014469935124119123\n",
      "    num_agent_steps_sampled: 122000\n",
      "    num_agent_steps_trained: 122000\n",
      "    num_steps_sampled: 122000\n",
      "    num_steps_trained: 122000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.9857142857143\n",
      "    ram_util_percent: 52.53265306122448\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04295571765880718\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.64467605884496\n",
      "    mean_inference_ms: 2.214682033305772\n",
      "    mean_raw_obs_processing_ms: 2.1463761259532554\n",
      "  time_since_restore: 3966.788218975067\n",
      "  time_this_iter_s: 34.30358600616455\n",
      "  time_total_s: 3966.788218975067\n",
      "  timers:\n",
      "    learn_throughput: 1246.758\n",
      "    learn_time_ms: 802.08\n",
      "    load_throughput: 48630.345\n",
      "    load_time_ms: 20.563\n",
      "    sample_throughput: 27.856\n",
      "    sample_time_ms: 35899.197\n",
      "    update_time_ms: 5.952\n",
      "  timestamp: 1635085667\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 122000\n",
      "  training_iteration: 122\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         3966.79</td><td style=\"text-align: right;\">122000</td><td style=\"text-align: right;\"> -2.2851</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.85</td><td style=\"text-align: right;\">            228.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 123000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-28-19\n",
      "  done: false\n",
      "  episode_len_mean: 228.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.289899999999995\n",
      "  episode_reward_min: -2.849999999999983\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 437\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.2351741790771484e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7426974885993534\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0048835016481360485\n",
      "          policy_loss: -0.04355158996250894\n",
      "          total_loss: -0.036369036303626166\n",
      "          vf_explained_var: 0.10556825995445251\n",
      "          vf_loss: 0.014609524483482043\n",
      "    num_agent_steps_sampled: 123000\n",
      "    num_agent_steps_trained: 123000\n",
      "    num_steps_sampled: 123000\n",
      "    num_steps_trained: 123000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.4608695652174\n",
      "    ram_util_percent: 52.467391304347835\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042951691230915134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.689851594986138\n",
      "    mean_inference_ms: 2.2146113036269104\n",
      "    mean_raw_obs_processing_ms: 2.1493923376844792\n",
      "  time_since_restore: 3999.414801597595\n",
      "  time_this_iter_s: 32.626582622528076\n",
      "  time_total_s: 3999.414801597595\n",
      "  timers:\n",
      "    learn_throughput: 1248.141\n",
      "    learn_time_ms: 801.191\n",
      "    load_throughput: 46605.915\n",
      "    load_time_ms: 21.457\n",
      "    sample_throughput: 28.11\n",
      "    sample_time_ms: 35574.549\n",
      "    update_time_ms: 6.058\n",
      "  timestamp: 1635085699\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 123000\n",
      "  training_iteration: 123\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         3999.41</td><td style=\"text-align: right;\">123000</td><td style=\"text-align: right;\"> -2.2899</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.85</td><td style=\"text-align: right;\">            228.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-28-54\n",
      "  done: false\n",
      "  episode_len_mean: 229.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.292099999999995\n",
      "  episode_reward_min: -2.849999999999983\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 442\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1175870895385742e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6124577826923794\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0023363238699609214\n",
      "          policy_loss: -0.004861614770359463\n",
      "          total_loss: 0.006208226250277625\n",
      "          vf_explained_var: 0.12751153111457825\n",
      "          vf_loss: 0.017194417708863815\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.44285714285714\n",
      "    ram_util_percent: 52.42653061224491\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04294603206067215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.743227962981834\n",
      "    mean_inference_ms: 2.214478775716547\n",
      "    mean_raw_obs_processing_ms: 2.153477095428774\n",
      "  time_since_restore: 4033.680983543396\n",
      "  time_this_iter_s: 34.26618194580078\n",
      "  time_total_s: 4033.680983543396\n",
      "  timers:\n",
      "    learn_throughput: 1249.336\n",
      "    learn_time_ms: 800.425\n",
      "    load_throughput: 46576.984\n",
      "    load_time_ms: 21.47\n",
      "    sample_throughput: 28.149\n",
      "    sample_time_ms: 35525.313\n",
      "    update_time_ms: 5.961\n",
      "  timestamp: 1635085734\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 124\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         4033.68</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\"> -2.2921</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.85</td><td style=\"text-align: right;\">            229.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 125000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-29-31\n",
      "  done: false\n",
      "  episode_len_mean: 229.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.295399999999995\n",
      "  episode_reward_min: -2.849999999999983\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 446\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.587935447692871e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5525793800751369\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0026193519137526285\n",
      "          policy_loss: 0.018967998110585742\n",
      "          total_loss: 0.02700096012817489\n",
      "          vf_explained_var: 0.10669311136007309\n",
      "          vf_loss: 0.013558752990017334\n",
      "    num_agent_steps_sampled: 125000\n",
      "    num_agent_steps_trained: 125000\n",
      "    num_steps_sampled: 125000\n",
      "    num_steps_trained: 125000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.97547169811322\n",
      "    ram_util_percent: 52.633962264150945\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042944656806661846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.785951278725506\n",
      "    mean_inference_ms: 2.214379708636346\n",
      "    mean_raw_obs_processing_ms: 2.1567031291054857\n",
      "  time_since_restore: 4070.755858898163\n",
      "  time_this_iter_s: 37.074875354766846\n",
      "  time_total_s: 4070.755858898163\n",
      "  timers:\n",
      "    learn_throughput: 1244.224\n",
      "    learn_time_ms: 803.714\n",
      "    load_throughput: 48503.531\n",
      "    load_time_ms: 20.617\n",
      "    sample_throughput: 27.863\n",
      "    sample_time_ms: 35890.001\n",
      "    update_time_ms: 6.31\n",
      "  timestamp: 1635085771\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 125000\n",
      "  training_iteration: 125\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         4070.76</td><td style=\"text-align: right;\">125000</td><td style=\"text-align: right;\"> -2.2954</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.85</td><td style=\"text-align: right;\">            229.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 126000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-30-28\n",
      "  done: false\n",
      "  episode_len_mean: 229.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.293899999999995\n",
      "  episode_reward_min: -2.849999999999983\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 451\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.7939677238464354e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5039425472418467\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0029796943016980907\n",
      "          policy_loss: -0.021350785262054868\n",
      "          total_loss: -0.008979235920641157\n",
      "          vf_explained_var: 0.12019248306751251\n",
      "          vf_loss: 0.017410974421848854\n",
      "    num_agent_steps_sampled: 126000\n",
      "    num_agent_steps_trained: 126000\n",
      "    num_steps_sampled: 126000\n",
      "    num_steps_trained: 126000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.9219512195122\n",
      "    ram_util_percent: 52.87317073170731\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04294396551097636\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.838948180212405\n",
      "    mean_inference_ms: 2.2142864257576007\n",
      "    mean_raw_obs_processing_ms: 2.167459176841011\n",
      "  time_since_restore: 4128.24232006073\n",
      "  time_this_iter_s: 57.48646116256714\n",
      "  time_total_s: 4128.24232006073\n",
      "  timers:\n",
      "    learn_throughput: 1233.203\n",
      "    learn_time_ms: 810.896\n",
      "    load_throughput: 46783.338\n",
      "    load_time_ms: 21.375\n",
      "    sample_throughput: 26.249\n",
      "    sample_time_ms: 38096.332\n",
      "    update_time_ms: 7.199\n",
      "  timestamp: 1635085828\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 126000\n",
      "  training_iteration: 126\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">         4128.24</td><td style=\"text-align: right;\">126000</td><td style=\"text-align: right;\"> -2.2939</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.85</td><td style=\"text-align: right;\">            229.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 127000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-31-07\n",
      "  done: false\n",
      "  episode_len_mean: 229.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.298399999999995\n",
      "  episode_reward_min: -2.849999999999983\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 455\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3969838619232177e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7926800711287393\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005048125683541116\n",
      "          policy_loss: 0.021090817948182423\n",
      "          total_loss: 0.026956383056110807\n",
      "          vf_explained_var: 0.10323823988437653\n",
      "          vf_loss: 0.013792368645469347\n",
      "    num_agent_steps_sampled: 127000\n",
      "    num_agent_steps_trained: 127000\n",
      "    num_steps_sampled: 127000\n",
      "    num_steps_trained: 127000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.05714285714286\n",
      "    ram_util_percent: 52.776785714285715\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04294367590233186\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.88156865774173\n",
      "    mean_inference_ms: 2.2142133753585056\n",
      "    mean_raw_obs_processing_ms: 2.175986657755441\n",
      "  time_since_restore: 4167.343930721283\n",
      "  time_this_iter_s: 39.10161066055298\n",
      "  time_total_s: 4167.343930721283\n",
      "  timers:\n",
      "    learn_throughput: 1226.886\n",
      "    learn_time_ms: 815.072\n",
      "    load_throughput: 48199.089\n",
      "    load_time_ms: 20.747\n",
      "    sample_throughput: 25.975\n",
      "    sample_time_ms: 38499.184\n",
      "    update_time_ms: 7.393\n",
      "  timestamp: 1635085867\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 127000\n",
      "  training_iteration: 127\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         4167.34</td><td style=\"text-align: right;\">127000</td><td style=\"text-align: right;\"> -2.2984</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.85</td><td style=\"text-align: right;\">            229.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-31-48\n",
      "  done: false\n",
      "  episode_len_mean: 230.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.3004999999999947\n",
      "  episode_reward_min: -2.849999999999983\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 459\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3969838619232177e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.668900438480907\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.002763907532524949\n",
      "          policy_loss: 0.026127682998776436\n",
      "          total_loss: 0.032627915632393624\n",
      "          vf_explained_var: 0.0674629956483841\n",
      "          vf_loss: 0.013189236478259167\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.17068965517242\n",
      "    ram_util_percent: 52.8551724137931\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042943916125770025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.924500180158176\n",
      "    mean_inference_ms: 2.214188449205365\n",
      "    mean_raw_obs_processing_ms: 2.1845754693315604\n",
      "  time_since_restore: 4207.8042278289795\n",
      "  time_this_iter_s: 40.46029710769653\n",
      "  time_total_s: 4207.8042278289795\n",
      "  timers:\n",
      "    learn_throughput: 1224.368\n",
      "    learn_time_ms: 816.748\n",
      "    load_throughput: 47618.606\n",
      "    load_time_ms: 21.0\n",
      "    sample_throughput: 25.668\n",
      "    sample_time_ms: 38959.164\n",
      "    update_time_ms: 7.251\n",
      "  timestamp: 1635085908\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 128\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">          4207.8</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\"> -2.3005</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.85</td><td style=\"text-align: right;\">            230.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 129000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-32-24\n",
      "  done: false\n",
      "  episode_len_mean: 230.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.3054999999999946\n",
      "  episode_reward_min: -2.849999999999983\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 464\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.984919309616089e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5414771907859378\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00397567713296654\n",
      "          policy_loss: -0.03489278223779466\n",
      "          total_loss: -0.022726591842042076\n",
      "          vf_explained_var: 0.12443017959594727\n",
      "          vf_loss: 0.017580961135940418\n",
      "    num_agent_steps_sampled: 129000\n",
      "    num_agent_steps_trained: 129000\n",
      "    num_steps_sampled: 129000\n",
      "    num_steps_trained: 129000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.32156862745099\n",
      "    ram_util_percent: 52.69019607843137\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04294424988653661\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.97733684038797\n",
      "    mean_inference_ms: 2.214146413274679\n",
      "    mean_raw_obs_processing_ms: 2.186929257036912\n",
      "  time_since_restore: 4243.88220500946\n",
      "  time_this_iter_s: 36.07797718048096\n",
      "  time_total_s: 4243.88220500946\n",
      "  timers:\n",
      "    learn_throughput: 1223.898\n",
      "    learn_time_ms: 817.061\n",
      "    load_throughput: 46687.623\n",
      "    load_time_ms: 21.419\n",
      "    sample_throughput: 26.905\n",
      "    sample_time_ms: 37167.919\n",
      "    update_time_ms: 7.846\n",
      "  timestamp: 1635085944\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 129000\n",
      "  training_iteration: 129\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         4243.88</td><td style=\"text-align: right;\">129000</td><td style=\"text-align: right;\"> -2.3055</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.85</td><td style=\"text-align: right;\">            230.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 130000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-33-02\n",
      "  done: false\n",
      "  episode_len_mean: 230.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.3086999999999946\n",
      "  episode_reward_min: -2.849999999999983\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 468\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.492459654808044e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6642578197850122\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008748047371221269\n",
      "          policy_loss: 0.028383769177728228\n",
      "          total_loss: 0.03589715626504686\n",
      "          vf_explained_var: 0.10622832924127579\n",
      "          vf_loss: 0.014155967067927122\n",
      "    num_agent_steps_sampled: 130000\n",
      "    num_agent_steps_trained: 130000\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.2781818181818\n",
      "    ram_util_percent: 52.99818181818183\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04294580118893366\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.020170171861693\n",
      "    mean_inference_ms: 2.2141591113177053\n",
      "    mean_raw_obs_processing_ms: 2.18893873638571\n",
      "  time_since_restore: 4282.199474573135\n",
      "  time_this_iter_s: 38.31726956367493\n",
      "  time_total_s: 4282.199474573135\n",
      "  timers:\n",
      "    learn_throughput: 1223.464\n",
      "    learn_time_ms: 817.351\n",
      "    load_throughput: 44535.969\n",
      "    load_time_ms: 22.454\n",
      "    sample_throughput: 26.59\n",
      "    sample_time_ms: 37608.399\n",
      "    update_time_ms: 7.842\n",
      "  timestamp: 1635085982\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 130\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">          4282.2</td><td style=\"text-align: right;\">130000</td><td style=\"text-align: right;\"> -2.3087</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.85</td><td style=\"text-align: right;\">            230.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 131000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-33-38\n",
      "  done: false\n",
      "  episode_len_mean: 231.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.314599999999994\n",
      "  episode_reward_min: -2.849999999999983\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 472\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.492459654808044e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6012796954976187\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006520568633334594\n",
      "          policy_loss: 0.024470579541391795\n",
      "          total_loss: 0.03183350082900789\n",
      "          vf_explained_var: 0.1390974223613739\n",
      "          vf_loss: 0.013375717680901289\n",
      "    num_agent_steps_sampled: 131000\n",
      "    num_agent_steps_trained: 131000\n",
      "    num_steps_sampled: 131000\n",
      "    num_steps_trained: 131000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.27450980392157\n",
      "    ram_util_percent: 52.87254901960785\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04294867270487985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.062207823953596\n",
      "    mean_inference_ms: 2.2141671450451663\n",
      "    mean_raw_obs_processing_ms: 2.191041942669207\n",
      "  time_since_restore: 4317.998946428299\n",
      "  time_this_iter_s: 35.799471855163574\n",
      "  time_total_s: 4317.998946428299\n",
      "  timers:\n",
      "    learn_throughput: 1217.773\n",
      "    learn_time_ms: 821.171\n",
      "    load_throughput: 42491.262\n",
      "    load_time_ms: 23.534\n",
      "    sample_throughput: 26.53\n",
      "    sample_time_ms: 37693.42\n",
      "    update_time_ms: 7.822\n",
      "  timestamp: 1635086018\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 131000\n",
      "  training_iteration: 131\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">            4318</td><td style=\"text-align: right;\">131000</td><td style=\"text-align: right;\"> -2.3146</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.85</td><td style=\"text-align: right;\">            231.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-34-12\n",
      "  done: false\n",
      "  episode_len_mean: 231.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.313499999999995\n",
      "  episode_reward_min: -2.849999999999983\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 477\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.492459654808044e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3881260573863983\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0014302081797085166\n",
      "          policy_loss: -0.010243340830008189\n",
      "          total_loss: 0.003332324243254132\n",
      "          vf_explained_var: 0.0906200110912323\n",
      "          vf_loss: 0.017456925339582895\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.18571428571427\n",
      "    ram_util_percent: 52.80612244897959\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0429513958958505\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.113617854755052\n",
      "    mean_inference_ms: 2.2141539912391943\n",
      "    mean_raw_obs_processing_ms: 2.1936507358086303\n",
      "  time_since_restore: 4352.234864473343\n",
      "  time_this_iter_s: 34.235918045043945\n",
      "  time_total_s: 4352.234864473343\n",
      "  timers:\n",
      "    learn_throughput: 1216.564\n",
      "    learn_time_ms: 821.987\n",
      "    load_throughput: 40747.033\n",
      "    load_time_ms: 24.542\n",
      "    sample_throughput: 26.536\n",
      "    sample_time_ms: 37684.328\n",
      "    update_time_ms: 8.203\n",
      "  timestamp: 1635086052\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 132\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         4352.23</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\"> -2.3135</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.85</td><td style=\"text-align: right;\">            231.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 133000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-35-07\n",
      "  done: false\n",
      "  episode_len_mean: 231.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.3101999999999943\n",
      "  episode_reward_min: -2.849999999999983\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 481\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.746229827404022e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3912694540288713\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0025197066569439307\n",
      "          policy_loss: 0.03938919934961531\n",
      "          total_loss: 0.04906545811229282\n",
      "          vf_explained_var: 0.05556079000234604\n",
      "          vf_loss: 0.013588954860137569\n",
      "    num_agent_steps_sampled: 133000\n",
      "    num_agent_steps_trained: 133000\n",
      "    num_steps_sampled: 133000\n",
      "    num_steps_trained: 133000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.13205128205128\n",
      "    ram_util_percent: 52.91025641025642\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042953500042676375\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.154225296427448\n",
      "    mean_inference_ms: 2.2141455553605622\n",
      "    mean_raw_obs_processing_ms: 2.201061563666434\n",
      "  time_since_restore: 4407.032700300217\n",
      "  time_this_iter_s: 54.79783582687378\n",
      "  time_total_s: 4407.032700300217\n",
      "  timers:\n",
      "    learn_throughput: 1212.261\n",
      "    learn_time_ms: 824.905\n",
      "    load_throughput: 41986.832\n",
      "    load_time_ms: 23.817\n",
      "    sample_throughput: 25.063\n",
      "    sample_time_ms: 39899.177\n",
      "    update_time_ms: 8.512\n",
      "  timestamp: 1635086107\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 133000\n",
      "  training_iteration: 133\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">         4407.03</td><td style=\"text-align: right;\">133000</td><td style=\"text-align: right;\"> -2.3102</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.85</td><td style=\"text-align: right;\">            231.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 134000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-35-44\n",
      "  done: false\n",
      "  episode_len_mean: 231.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.3120999999999943\n",
      "  episode_reward_min: -2.849999999999983\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 485\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.73114913702011e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.43531002120839224\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018737291256191252\n",
      "          policy_loss: -0.047482193923658794\n",
      "          total_loss: -0.034203535980648465\n",
      "          vf_explained_var: 0.018661221489310265\n",
      "          vf_loss: 0.0176317579837309\n",
      "    num_agent_steps_sampled: 134000\n",
      "    num_agent_steps_trained: 134000\n",
      "    num_steps_sampled: 134000\n",
      "    num_steps_trained: 134000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.37115384615383\n",
      "    ram_util_percent: 53.073076923076925\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04295529084511593\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.195099614975494\n",
      "    mean_inference_ms: 2.2141256055135954\n",
      "    mean_raw_obs_processing_ms: 2.2083732387432833\n",
      "  time_since_restore: 4443.490510702133\n",
      "  time_this_iter_s: 36.457810401916504\n",
      "  time_total_s: 4443.490510702133\n",
      "  timers:\n",
      "    learn_throughput: 1212.614\n",
      "    learn_time_ms: 824.665\n",
      "    load_throughput: 42494.663\n",
      "    load_time_ms: 23.532\n",
      "    sample_throughput: 24.926\n",
      "    sample_time_ms: 40118.924\n",
      "    update_time_ms: 8.325\n",
      "  timestamp: 1635086144\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 134000\n",
      "  training_iteration: 134\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         4443.49</td><td style=\"text-align: right;\">134000</td><td style=\"text-align: right;\"> -2.3121</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.85</td><td style=\"text-align: right;\">            231.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 135000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-36-16\n",
      "  done: false\n",
      "  episode_len_mean: 230.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9300000000000015\n",
      "  episode_reward_mean: -2.308699999999994\n",
      "  episode_reward_min: -2.849999999999983\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 489\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.73114913702011e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6541353589958615\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012117441700077178\n",
      "          policy_loss: -0.11389175785912407\n",
      "          total_loss: -0.10334316632813878\n",
      "          vf_explained_var: 0.0860644057393074\n",
      "          vf_loss: 0.017089945264160633\n",
      "    num_agent_steps_sampled: 135000\n",
      "    num_agent_steps_trained: 135000\n",
      "    num_steps_sampled: 135000\n",
      "    num_steps_trained: 135000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.28936170212766\n",
      "    ram_util_percent: 53.1595744680851\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04295668157436591\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.23469324513414\n",
      "    mean_inference_ms: 2.214101036230097\n",
      "    mean_raw_obs_processing_ms: 2.2114100529727057\n",
      "  time_since_restore: 4476.315136909485\n",
      "  time_this_iter_s: 32.824626207351685\n",
      "  time_total_s: 4476.315136909485\n",
      "  timers:\n",
      "    learn_throughput: 1211.336\n",
      "    learn_time_ms: 825.535\n",
      "    load_throughput: 42497.204\n",
      "    load_time_ms: 23.531\n",
      "    sample_throughput: 25.193\n",
      "    sample_time_ms: 39693.513\n",
      "    update_time_ms: 8.029\n",
      "  timestamp: 1635086176\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 135000\n",
      "  training_iteration: 135\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         4476.32</td><td style=\"text-align: right;\">135000</td><td style=\"text-align: right;\"> -2.3087</td><td style=\"text-align: right;\">               -1.93</td><td style=\"text-align: right;\">               -2.85</td><td style=\"text-align: right;\">            230.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-36-44\n",
      "  done: false\n",
      "  episode_len_mean: 234.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9600000000000015\n",
      "  episode_reward_mean: -2.3415999999999944\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 493\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.73114913702011e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.565693978468577\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018272887623878256\n",
      "          policy_loss: 0.0018837882412804498\n",
      "          total_loss: 0.0017498183581564162\n",
      "          vf_explained_var: 0.11554254591464996\n",
      "          vf_loss: 0.015522970052229034\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.655\n",
      "    ram_util_percent: 53.24249999999999\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04295808005465368\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.271313135367883\n",
      "    mean_inference_ms: 2.214075420404071\n",
      "    mean_raw_obs_processing_ms: 2.213089126090347\n",
      "  time_since_restore: 4503.9902312755585\n",
      "  time_this_iter_s: 27.67509436607361\n",
      "  time_total_s: 4503.9902312755585\n",
      "  timers:\n",
      "    learn_throughput: 1226.6\n",
      "    learn_time_ms: 815.262\n",
      "    load_throughput: 43079.477\n",
      "    load_time_ms: 23.213\n",
      "    sample_throughput: 27.23\n",
      "    sample_time_ms: 36724.721\n",
      "    update_time_ms: 6.733\n",
      "  timestamp: 1635086204\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 136\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         4503.99</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\"> -2.3416</td><td style=\"text-align: right;\">               -1.96</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            234.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 137000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-37-17\n",
      "  done: false\n",
      "  episode_len_mean: 235.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9600000000000015\n",
      "  episode_reward_mean: -2.350899999999994\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 497\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.73114913702011e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9773135046164195\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006798189185501732\n",
      "          policy_loss: 7.62972566816542e-05\n",
      "          total_loss: 0.004925903264019225\n",
      "          vf_explained_var: 0.1934642344713211\n",
      "          vf_loss: 0.01462274023021261\n",
      "    num_agent_steps_sampled: 137000\n",
      "    num_agent_steps_trained: 137000\n",
      "    num_steps_sampled: 137000\n",
      "    num_steps_trained: 137000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.92340425531916\n",
      "    ram_util_percent: 53.21914893617021\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04295919165567666\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.307146044238408\n",
      "    mean_inference_ms: 2.214040774733492\n",
      "    mean_raw_obs_processing_ms: 2.21469752151585\n",
      "  time_since_restore: 4536.833678007126\n",
      "  time_this_iter_s: 32.84344673156738\n",
      "  time_total_s: 4536.833678007126\n",
      "  timers:\n",
      "    learn_throughput: 1232.087\n",
      "    learn_time_ms: 811.631\n",
      "    load_throughput: 44090.281\n",
      "    load_time_ms: 22.681\n",
      "    sample_throughput: 27.698\n",
      "    sample_time_ms: 36103.399\n",
      "    update_time_ms: 6.46\n",
      "  timestamp: 1635086237\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 137000\n",
      "  training_iteration: 137\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         4536.83</td><td style=\"text-align: right;\">137000</td><td style=\"text-align: right;\"> -2.3509</td><td style=\"text-align: right;\">               -1.96</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            235.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 138000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-37-49\n",
      "  done: false\n",
      "  episode_len_mean: 236.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9600000000000015\n",
      "  episode_reward_mean: -2.360099999999994\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 500\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.73114913702011e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0349299165937635\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006816661002614586\n",
      "          policy_loss: -0.08825313730372322\n",
      "          total_loss: -0.08472214879261122\n",
      "          vf_explained_var: 0.2242291122674942\n",
      "          vf_loss: 0.01388028697628114\n",
      "    num_agent_steps_sampled: 138000\n",
      "    num_agent_steps_trained: 138000\n",
      "    num_steps_sampled: 138000\n",
      "    num_steps_trained: 138000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.8777777777778\n",
      "    ram_util_percent: 53.1911111111111\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042960404752762665\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.333252829188744\n",
      "    mean_inference_ms: 2.2140125140443634\n",
      "    mean_raw_obs_processing_ms: 2.2158924907227107\n",
      "  time_since_restore: 4568.417385101318\n",
      "  time_this_iter_s: 31.583707094192505\n",
      "  time_total_s: 4568.417385101318\n",
      "  timers:\n",
      "    learn_throughput: 1234.621\n",
      "    learn_time_ms: 809.965\n",
      "    load_throughput: 45570.745\n",
      "    load_time_ms: 21.944\n",
      "    sample_throughput: 28.395\n",
      "    sample_time_ms: 35217.488\n",
      "    update_time_ms: 7.227\n",
      "  timestamp: 1635086269\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 138000\n",
      "  training_iteration: 138\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         4568.42</td><td style=\"text-align: right;\">138000</td><td style=\"text-align: right;\"> -2.3601</td><td style=\"text-align: right;\">               -1.96</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            236.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 139000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-38-20\n",
      "  done: false\n",
      "  episode_len_mean: 237.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9600000000000015\n",
      "  episode_reward_mean: -2.3730999999999933\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 504\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.73114913702011e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.008568126625485\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004463800990215776\n",
      "          policy_loss: -0.02059173492921723\n",
      "          total_loss: -0.01743556418352657\n",
      "          vf_explained_var: 0.3152037262916565\n",
      "          vf_loss: 0.013241853182100588\n",
      "    num_agent_steps_sampled: 139000\n",
      "    num_agent_steps_trained: 139000\n",
      "    num_steps_sampled: 139000\n",
      "    num_steps_trained: 139000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.56888888888889\n",
      "    ram_util_percent: 53.19555555555556\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042962531945335926\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.36700756373775\n",
      "    mean_inference_ms: 2.21398987183228\n",
      "    mean_raw_obs_processing_ms: 2.2175326996820304\n",
      "  time_since_restore: 4600.006260633469\n",
      "  time_this_iter_s: 31.58887553215027\n",
      "  time_total_s: 4600.006260633469\n",
      "  timers:\n",
      "    learn_throughput: 1233.268\n",
      "    learn_time_ms: 810.854\n",
      "    load_throughput: 45701.733\n",
      "    load_time_ms: 21.881\n",
      "    sample_throughput: 28.761\n",
      "    sample_time_ms: 34768.79\n",
      "    update_time_ms: 6.07\n",
      "  timestamp: 1635086300\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 139000\n",
      "  training_iteration: 139\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         4600.01</td><td style=\"text-align: right;\">139000</td><td style=\"text-align: right;\"> -2.3731</td><td style=\"text-align: right;\">               -1.96</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            237.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-38-53\n",
      "  done: false\n",
      "  episode_len_mean: 238.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9600000000000015\n",
      "  episode_reward_mean: -2.3876999999999926\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 508\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.365574568510055e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0216674970255957\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013336360826512106\n",
      "          policy_loss: 0.026704002958205013\n",
      "          total_loss: 0.030315399997764163\n",
      "          vf_explained_var: 0.23518376052379608\n",
      "          vf_loss: 0.013828069996088743\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.79782608695653\n",
      "    ram_util_percent: 53.24565217391304\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04296595978852118\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.39940637580855\n",
      "    mean_inference_ms: 2.213986313039795\n",
      "    mean_raw_obs_processing_ms: 2.219265393679824\n",
      "  time_since_restore: 4632.397166252136\n",
      "  time_this_iter_s: 32.3909056186676\n",
      "  time_total_s: 4632.397166252136\n",
      "  timers:\n",
      "    learn_throughput: 1231.98\n",
      "    learn_time_ms: 811.701\n",
      "    load_throughput: 47851.941\n",
      "    load_time_ms: 20.898\n",
      "    sample_throughput: 29.26\n",
      "    sample_time_ms: 34175.887\n",
      "    update_time_ms: 6.407\n",
      "  timestamp: 1635086333\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 140\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">          4632.4</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\"> -2.3877</td><td style=\"text-align: right;\">               -1.96</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            238.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 141000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-39-45\n",
      "  done: false\n",
      "  episode_len_mean: 239.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9600000000000015\n",
      "  episode_reward_mean: -2.3946999999999927\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 512\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.365574568510055e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8462322003311581\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004276585184977301\n",
      "          policy_loss: 0.030944175438748465\n",
      "          total_loss: 0.03648729572693507\n",
      "          vf_explained_var: 0.20154988765716553\n",
      "          vf_loss: 0.014005441084090206\n",
      "    num_agent_steps_sampled: 141000\n",
      "    num_agent_steps_trained: 141000\n",
      "    num_steps_sampled: 141000\n",
      "    num_steps_trained: 141000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.45066666666666\n",
      "    ram_util_percent: 53.156\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04296946701363719\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.43167205579475\n",
      "    mean_inference_ms: 2.213989329150783\n",
      "    mean_raw_obs_processing_ms: 2.225820653985185\n",
      "  time_since_restore: 4684.606943845749\n",
      "  time_this_iter_s: 52.20977759361267\n",
      "  time_total_s: 4684.606943845749\n",
      "  timers:\n",
      "    learn_throughput: 1236.925\n",
      "    learn_time_ms: 808.456\n",
      "    load_throughput: 47975.476\n",
      "    load_time_ms: 20.844\n",
      "    sample_throughput: 27.918\n",
      "    sample_time_ms: 35819.668\n",
      "    update_time_ms: 6.87\n",
      "  timestamp: 1635086385\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 141000\n",
      "  training_iteration: 141\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         4684.61</td><td style=\"text-align: right;\">141000</td><td style=\"text-align: right;\"> -2.3947</td><td style=\"text-align: right;\">               -1.96</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            239.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 142000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-40-20\n",
      "  done: false\n",
      "  episode_len_mean: 240.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9600000000000015\n",
      "  episode_reward_mean: -2.4021999999999926\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 516\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.1827872842550277e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6732130249341329\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005322478179883285\n",
      "          policy_loss: 0.028300877577728697\n",
      "          total_loss: 0.035081624570820064\n",
      "          vf_explained_var: 0.11457231640815735\n",
      "          vf_loss: 0.01351287824412187\n",
      "    num_agent_steps_sampled: 142000\n",
      "    num_agent_steps_trained: 142000\n",
      "    num_steps_sampled: 142000\n",
      "    num_steps_trained: 142000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.70612244897958\n",
      "    ram_util_percent: 53.526530612244905\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04297336178532303\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.463061912816556\n",
      "    mean_inference_ms: 2.2140098342313035\n",
      "    mean_raw_obs_processing_ms: 2.2324292485930006\n",
      "  time_since_restore: 4719.279718637466\n",
      "  time_this_iter_s: 34.67277479171753\n",
      "  time_total_s: 4719.279718637466\n",
      "  timers:\n",
      "    learn_throughput: 1239.564\n",
      "    learn_time_ms: 806.736\n",
      "    load_throughput: 47950.082\n",
      "    load_time_ms: 20.855\n",
      "    sample_throughput: 27.882\n",
      "    sample_time_ms: 35865.606\n",
      "    update_time_ms: 6.498\n",
      "  timestamp: 1635086420\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 142000\n",
      "  training_iteration: 142\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         4719.28</td><td style=\"text-align: right;\">142000</td><td style=\"text-align: right;\"> -2.4022</td><td style=\"text-align: right;\">               -1.96</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            240.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 143000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-40-56\n",
      "  done: false\n",
      "  episode_len_mean: 240.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9600000000000015\n",
      "  episode_reward_mean: -2.4087999999999927\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 520\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.1827872842550277e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5591182129250633\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0033434058757916\n",
      "          policy_loss: -0.0067255212201012505\n",
      "          total_loss: 0.001283343177702692\n",
      "          vf_explained_var: 0.12106984108686447\n",
      "          vf_loss: 0.013600045546061463\n",
      "    num_agent_steps_sampled: 143000\n",
      "    num_agent_steps_trained: 143000\n",
      "    num_steps_sampled: 143000\n",
      "    num_steps_trained: 143000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.80576923076923\n",
      "    ram_util_percent: 53.59230769230768\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04297743175117933\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.4935762295071\n",
      "    mean_inference_ms: 2.2140393259322373\n",
      "    mean_raw_obs_processing_ms: 2.233401528432095\n",
      "  time_since_restore: 4755.34296131134\n",
      "  time_this_iter_s: 36.0632426738739\n",
      "  time_total_s: 4755.34296131134\n",
      "  timers:\n",
      "    learn_throughput: 1239.273\n",
      "    learn_time_ms: 806.925\n",
      "    load_throughput: 46108.075\n",
      "    load_time_ms: 21.688\n",
      "    sample_throughput: 29.42\n",
      "    sample_time_ms: 33990.784\n",
      "    update_time_ms: 6.798\n",
      "  timestamp: 1635086456\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 143000\n",
      "  training_iteration: 143\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">         4755.34</td><td style=\"text-align: right;\">143000</td><td style=\"text-align: right;\"> -2.4088</td><td style=\"text-align: right;\">               -1.96</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            240.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-41-28\n",
      "  done: false\n",
      "  episode_len_mean: 241.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9600000000000015\n",
      "  episode_reward_mean: -2.4118999999999926\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 524\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0913936421275138e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8338230351607004\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005182702114390104\n",
      "          policy_loss: -0.0035704692204793294\n",
      "          total_loss: 0.002188854126466645\n",
      "          vf_explained_var: 0.14845393598079681\n",
      "          vf_loss: 0.014097551359898514\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.94222222222223\n",
      "    ram_util_percent: 53.40444444444445\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04298157839280121\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.52343702359347\n",
      "    mean_inference_ms: 2.214065798750676\n",
      "    mean_raw_obs_processing_ms: 2.234308612974015\n",
      "  time_since_restore: 4787.3793823719025\n",
      "  time_this_iter_s: 32.036421060562134\n",
      "  time_total_s: 4787.3793823719025\n",
      "  timers:\n",
      "    learn_throughput: 1223.13\n",
      "    learn_time_ms: 817.575\n",
      "    load_throughput: 44602.841\n",
      "    load_time_ms: 22.42\n",
      "    sample_throughput: 29.818\n",
      "    sample_time_ms: 33537.337\n",
      "    update_time_ms: 6.98\n",
      "  timestamp: 1635086488\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 144\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         4787.38</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\"> -2.4119</td><td style=\"text-align: right;\">               -1.96</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            241.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 145000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-42-02\n",
      "  done: false\n",
      "  episode_len_mean: 241.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9600000000000015\n",
      "  episode_reward_mean: -2.413399999999992\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 529\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0913936421275138e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5172797958056132\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.001616553089491567\n",
      "          policy_loss: -0.019092321230305565\n",
      "          total_loss: -0.0066483815511067705\n",
      "          vf_explained_var: 0.1119462251663208\n",
      "          vf_loss: 0.01761673592651884\n",
      "    num_agent_steps_sampled: 145000\n",
      "    num_agent_steps_trained: 145000\n",
      "    num_steps_sampled: 145000\n",
      "    num_steps_trained: 145000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.926\n",
      "    ram_util_percent: 53.38599999999999\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04298638202021201\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.56010345651595\n",
      "    mean_inference_ms: 2.214090346791297\n",
      "    mean_raw_obs_processing_ms: 2.235626030127994\n",
      "  time_since_restore: 4822.015118122101\n",
      "  time_this_iter_s: 34.635735750198364\n",
      "  time_total_s: 4822.015118122101\n",
      "  timers:\n",
      "    learn_throughput: 1227.232\n",
      "    learn_time_ms: 814.842\n",
      "    load_throughput: 43343.154\n",
      "    load_time_ms: 23.072\n",
      "    sample_throughput: 29.655\n",
      "    sample_time_ms: 33720.791\n",
      "    update_time_ms: 6.78\n",
      "  timestamp: 1635086522\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 145000\n",
      "  training_iteration: 145\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   145</td><td style=\"text-align: right;\">         4822.02</td><td style=\"text-align: right;\">145000</td><td style=\"text-align: right;\"> -2.4134</td><td style=\"text-align: right;\">               -1.96</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            241.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 146000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-42-38\n",
      "  done: false\n",
      "  episode_len_mean: 241.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9600000000000015\n",
      "  episode_reward_mean: -2.4123999999999923\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 533\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.456968210637569e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7898201353020138\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0030677765639997765\n",
      "          policy_loss: 0.009584531767500772\n",
      "          total_loss: 0.014577784968747034\n",
      "          vf_explained_var: 0.1495211124420166\n",
      "          vf_loss: 0.012891455356859498\n",
      "    num_agent_steps_sampled: 146000\n",
      "    num_agent_steps_trained: 146000\n",
      "    num_steps_sampled: 146000\n",
      "    num_steps_trained: 146000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.154\n",
      "    ram_util_percent: 53.413999999999994\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04299095995597293\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.58897118484008\n",
      "    mean_inference_ms: 2.214114997689828\n",
      "    mean_raw_obs_processing_ms: 2.236911948518285\n",
      "  time_since_restore: 4857.214435100555\n",
      "  time_this_iter_s: 35.19931697845459\n",
      "  time_total_s: 4857.214435100555\n",
      "  timers:\n",
      "    learn_throughput: 1224.368\n",
      "    learn_time_ms: 816.748\n",
      "    load_throughput: 43867.37\n",
      "    load_time_ms: 22.796\n",
      "    sample_throughput: 29.01\n",
      "    sample_time_ms: 34471.149\n",
      "    update_time_ms: 7.145\n",
      "  timestamp: 1635086558\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 146000\n",
      "  training_iteration: 146\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         4857.21</td><td style=\"text-align: right;\">146000</td><td style=\"text-align: right;\"> -2.4124</td><td style=\"text-align: right;\">               -1.96</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            241.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 147000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-43-12\n",
      "  done: false\n",
      "  episode_len_mean: 241.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9600000000000015\n",
      "  episode_reward_mean: -2.412099999999992\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 537\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.7284841053187846e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6795257601473067\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0030212543664962115\n",
      "          policy_loss: 0.014749289386802249\n",
      "          total_loss: 0.021024352974361844\n",
      "          vf_explained_var: 0.13725654780864716\n",
      "          vf_loss: 0.013070320203486417\n",
      "    num_agent_steps_sampled: 147000\n",
      "    num_agent_steps_trained: 147000\n",
      "    num_steps_sampled: 147000\n",
      "    num_steps_trained: 147000\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.24489795918367\n",
      "    ram_util_percent: 53.30612244897959\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042998384058654186\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.618067796772642\n",
      "    mean_inference_ms: 2.2141539736694966\n",
      "    mean_raw_obs_processing_ms: 2.238291194153731\n",
      "  time_since_restore: 4891.883464574814\n",
      "  time_this_iter_s: 34.66902947425842\n",
      "  time_total_s: 4891.883464574814\n",
      "  timers:\n",
      "    learn_throughput: 1223.882\n",
      "    learn_time_ms: 817.073\n",
      "    load_throughput: 42287.386\n",
      "    load_time_ms: 23.648\n",
      "    sample_throughput: 28.858\n",
      "    sample_time_ms: 34652.075\n",
      "    update_time_ms: 7.496\n",
      "  timestamp: 1635086592\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 147000\n",
      "  training_iteration: 147\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         4891.88</td><td style=\"text-align: right;\">147000</td><td style=\"text-align: right;\"> -2.4121</td><td style=\"text-align: right;\">               -1.96</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            241.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-44-04\n",
      "  done: false\n",
      "  episode_len_mean: 241.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9600000000000015\n",
      "  episode_reward_mean: -2.4131999999999927\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 541\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7390493750572205\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005982513124436492\n",
      "          policy_loss: 0.012603153785069784\n",
      "          total_loss: 0.01830065536002318\n",
      "          vf_explained_var: 0.1769164353609085\n",
      "          vf_loss: 0.013087995412449042\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.964\n",
      "    ram_util_percent: 53.31466666666666\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04300605775109906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.64681467757985\n",
      "    mean_inference_ms: 2.214213744062428\n",
      "    mean_raw_obs_processing_ms: 2.244455819328013\n",
      "  time_since_restore: 4943.752230882645\n",
      "  time_this_iter_s: 51.86876630783081\n",
      "  time_total_s: 4943.752230882645\n",
      "  timers:\n",
      "    learn_throughput: 1223.587\n",
      "    learn_time_ms: 817.269\n",
      "    load_throughput: 40276.944\n",
      "    load_time_ms: 24.828\n",
      "    sample_throughput: 27.263\n",
      "    sample_time_ms: 36679.81\n",
      "    update_time_ms: 6.74\n",
      "  timestamp: 1635086644\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 148\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">         4943.75</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\"> -2.4132</td><td style=\"text-align: right;\">               -1.96</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            241.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 149000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-44-36\n",
      "  done: false\n",
      "  episode_len_mean: 241.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9600000000000015\n",
      "  episode_reward_mean: -2.4156999999999926\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 545\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6073363025983175\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0018336379311176326\n",
      "          policy_loss: -0.025709973441229925\n",
      "          total_loss: -0.01934330008096165\n",
      "          vf_explained_var: 0.17595909535884857\n",
      "          vf_loss: 0.012440038824246989\n",
      "    num_agent_steps_sampled: 149000\n",
      "    num_agent_steps_trained: 149000\n",
      "    num_steps_sampled: 149000\n",
      "    num_steps_trained: 149000\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.82826086956521\n",
      "    ram_util_percent: 53.334782608695654\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04301085493087876\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.674286600888525\n",
      "    mean_inference_ms: 2.214255834471844\n",
      "    mean_raw_obs_processing_ms: 2.250515446831555\n",
      "  time_since_restore: 4975.971142292023\n",
      "  time_this_iter_s: 32.21891140937805\n",
      "  time_total_s: 4975.971142292023\n",
      "  timers:\n",
      "    learn_throughput: 1225.79\n",
      "    learn_time_ms: 815.8\n",
      "    load_throughput: 41841.032\n",
      "    load_time_ms: 23.9\n",
      "    sample_throughput: 27.215\n",
      "    sample_time_ms: 36744.899\n",
      "    update_time_ms: 7.16\n",
      "  timestamp: 1635086676\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 149000\n",
      "  training_iteration: 149\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         4975.97</td><td style=\"text-align: right;\">149000</td><td style=\"text-align: right;\"> -2.4157</td><td style=\"text-align: right;\">               -1.96</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            241.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 150000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-45-11\n",
      "  done: false\n",
      "  episode_len_mean: 241.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9600000000000015\n",
      "  episode_reward_mean: -2.419499999999992\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 550\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.821210263296962e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6835529989666409\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0030416299324991136\n",
      "          policy_loss: 0.0008695341646671295\n",
      "          total_loss: 0.010097421043448978\n",
      "          vf_explained_var: 0.14701497554779053\n",
      "          vf_loss: 0.016063414834853677\n",
      "    num_agent_steps_sampled: 150000\n",
      "    num_agent_steps_trained: 150000\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.17142857142856\n",
      "    ram_util_percent: 53.35714285714285\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04301573623366878\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.705736283307743\n",
      "    mean_inference_ms: 2.214278597933563\n",
      "    mean_raw_obs_processing_ms: 2.2530620261048164\n",
      "  time_since_restore: 5010.494785070419\n",
      "  time_this_iter_s: 34.523642778396606\n",
      "  time_total_s: 5010.494785070419\n",
      "  timers:\n",
      "    learn_throughput: 1226.555\n",
      "    learn_time_ms: 815.292\n",
      "    load_throughput: 42002.389\n",
      "    load_time_ms: 23.808\n",
      "    sample_throughput: 27.057\n",
      "    sample_time_ms: 36958.925\n",
      "    update_time_ms: 6.893\n",
      "  timestamp: 1635086711\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 150\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         5010.49</td><td style=\"text-align: right;\">150000</td><td style=\"text-align: right;\"> -2.4195</td><td style=\"text-align: right;\">               -1.96</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            241.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 151000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-45-46\n",
      "  done: false\n",
      "  episode_len_mean: 241.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9600000000000015\n",
      "  episode_reward_mean: -2.417799999999992\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 554\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.410605131648481e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.637831738922331\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003003049307680941\n",
      "          policy_loss: 0.0529499802324507\n",
      "          total_loss: 0.05802251216438081\n",
      "          vf_explained_var: 0.11312811821699142\n",
      "          vf_loss: 0.011450850285796656\n",
      "    num_agent_steps_sampled: 151000\n",
      "    num_agent_steps_trained: 151000\n",
      "    num_steps_sampled: 151000\n",
      "    num_steps_trained: 151000\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.93999999999998\n",
      "    ram_util_percent: 53.36999999999999\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043018644340505806\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.72926142992957\n",
      "    mean_inference_ms: 2.214264659752827\n",
      "    mean_raw_obs_processing_ms: 2.2541125918491307\n",
      "  time_since_restore: 5045.457070112228\n",
      "  time_this_iter_s: 34.96228504180908\n",
      "  time_total_s: 5045.457070112228\n",
      "  timers:\n",
      "    learn_throughput: 1228.216\n",
      "    learn_time_ms: 814.189\n",
      "    load_throughput: 42128.532\n",
      "    load_time_ms: 23.737\n",
      "    sample_throughput: 28.381\n",
      "    sample_time_ms: 35234.836\n",
      "    update_time_ms: 7.34\n",
      "  timestamp: 1635086746\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 151000\n",
      "  training_iteration: 151\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         5045.46</td><td style=\"text-align: right;\">151000</td><td style=\"text-align: right;\"> -2.4178</td><td style=\"text-align: right;\">               -1.96</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            241.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-46-21\n",
      "  done: false\n",
      "  episode_len_mean: 242.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9600000000000015\n",
      "  episode_reward_mean: -2.420199999999993\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 558\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7053025658242404e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6336153321795993\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003363290944544575\n",
      "          policy_loss: -0.028065955059395895\n",
      "          total_loss: -0.021494544711377885\n",
      "          vf_explained_var: 0.12964622676372528\n",
      "          vf_loss: 0.012907565664499998\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.022\n",
      "    ram_util_percent: 53.45799999999999\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04302092586709188\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.750878849407385\n",
      "    mean_inference_ms: 2.214208379244233\n",
      "    mean_raw_obs_processing_ms: 2.2552381970930355\n",
      "  time_since_restore: 5080.416213512421\n",
      "  time_this_iter_s: 34.95914340019226\n",
      "  time_total_s: 5080.416213512421\n",
      "  timers:\n",
      "    learn_throughput: 1227.163\n",
      "    learn_time_ms: 814.888\n",
      "    load_throughput: 42328.697\n",
      "    load_time_ms: 23.625\n",
      "    sample_throughput: 28.358\n",
      "    sample_time_ms: 35262.812\n",
      "    update_time_ms: 7.358\n",
      "  timestamp: 1635086781\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 152\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         5080.42</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\"> -2.4202</td><td style=\"text-align: right;\">               -1.96</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            242.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 153000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-46-53\n",
      "  done: false\n",
      "  episode_len_mean: 242.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9600000000000015\n",
      "  episode_reward_mean: -2.4260999999999924\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 562\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.526512829121202e-15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8368986421161227\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00926599759395717\n",
      "          policy_loss: -0.019705679515997568\n",
      "          total_loss: -0.01269449761344327\n",
      "          vf_explained_var: 0.048487111926078796\n",
      "          vf_loss: 0.015380167412675089\n",
      "    num_agent_steps_sampled: 153000\n",
      "    num_agent_steps_trained: 153000\n",
      "    num_steps_sampled: 153000\n",
      "    num_steps_trained: 153000\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.03777777777779\n",
      "    ram_util_percent: 53.473333333333336\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04302506669734678\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.770666155795084\n",
      "    mean_inference_ms: 2.214136951914769\n",
      "    mean_raw_obs_processing_ms: 2.2564333755611985\n",
      "  time_since_restore: 5112.364357471466\n",
      "  time_this_iter_s: 31.94814395904541\n",
      "  time_total_s: 5112.364357471466\n",
      "  timers:\n",
      "    learn_throughput: 1200.989\n",
      "    learn_time_ms: 832.647\n",
      "    load_throughput: 43546.464\n",
      "    load_time_ms: 22.964\n",
      "    sample_throughput: 28.707\n",
      "    sample_time_ms: 34834.658\n",
      "    update_time_ms: 6.942\n",
      "  timestamp: 1635086813\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 153000\n",
      "  training_iteration: 153\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">         5112.36</td><td style=\"text-align: right;\">153000</td><td style=\"text-align: right;\"> -2.4261</td><td style=\"text-align: right;\">               -1.96</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            242.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 154000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-47-28\n",
      "  done: false\n",
      "  episode_len_mean: 243.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9600000000000015\n",
      "  episode_reward_mean: -2.432899999999992\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 566\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.526512829121202e-15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5404697789086236\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02496759911421991\n",
      "          policy_loss: -0.07738174912002352\n",
      "          total_loss: -0.06796436049044133\n",
      "          vf_explained_var: 0.06194563955068588\n",
      "          vf_loss: 0.014822086836728785\n",
      "    num_agent_steps_sampled: 154000\n",
      "    num_agent_steps_trained: 154000\n",
      "    num_steps_sampled: 154000\n",
      "    num_steps_trained: 154000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.40588235294118\n",
      "    ram_util_percent: 53.480392156862756\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04302855883492771\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.789916349325946\n",
      "    mean_inference_ms: 2.214050289998484\n",
      "    mean_raw_obs_processing_ms: 2.2575611498138244\n",
      "  time_since_restore: 5147.620077848434\n",
      "  time_this_iter_s: 35.255720376968384\n",
      "  time_total_s: 5147.620077848434\n",
      "  timers:\n",
      "    learn_throughput: 1206.797\n",
      "    learn_time_ms: 828.639\n",
      "    load_throughput: 44432.222\n",
      "    load_time_ms: 22.506\n",
      "    sample_throughput: 28.441\n",
      "    sample_time_ms: 35160.44\n",
      "    update_time_ms: 7.21\n",
      "  timestamp: 1635086848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 154000\n",
      "  training_iteration: 154\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         5147.62</td><td style=\"text-align: right;\">154000</td><td style=\"text-align: right;\"> -2.4329</td><td style=\"text-align: right;\">               -1.96</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            243.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 155000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-48-22\n",
      "  done: false\n",
      "  episode_len_mean: 242.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9600000000000015\n",
      "  episode_reward_mean: -2.428899999999992\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 571\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2789769243681808e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6234940654701657\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0025901594492874756\n",
      "          policy_loss: 0.0038064224024613696\n",
      "          total_loss: 0.013462632728947533\n",
      "          vf_explained_var: 0.13391032814979553\n",
      "          vf_loss: 0.015891149464166827\n",
      "    num_agent_steps_sampled: 155000\n",
      "    num_agent_steps_trained: 155000\n",
      "    num_steps_sampled: 155000\n",
      "    num_steps_trained: 155000\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.83026315789475\n",
      "    ram_util_percent: 53.44736842105263\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043031298720079804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.813187661567905\n",
      "    mean_inference_ms: 2.2139355671672787\n",
      "    mean_raw_obs_processing_ms: 2.2646496321039105\n",
      "  time_since_restore: 5201.396404981613\n",
      "  time_this_iter_s: 53.77632713317871\n",
      "  time_total_s: 5201.396404981613\n",
      "  timers:\n",
      "    learn_throughput: 1210.0\n",
      "    learn_time_ms: 826.446\n",
      "    load_throughput: 45542.392\n",
      "    load_time_ms: 21.958\n",
      "    sample_throughput: 26.971\n",
      "    sample_time_ms: 37076.767\n",
      "    update_time_ms: 7.343\n",
      "  timestamp: 1635086902\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 155000\n",
      "  training_iteration: 155\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">          5201.4</td><td style=\"text-align: right;\">155000</td><td style=\"text-align: right;\"> -2.4289</td><td style=\"text-align: right;\">               -1.96</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            242.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-48-58\n",
      "  done: false\n",
      "  episode_len_mean: 242.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9600000000000015\n",
      "  episode_reward_mean: -2.429799999999992\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 575\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.394884621840904e-15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7133997814522849\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.04741345521347063\n",
      "          policy_loss: 0.02013859653638469\n",
      "          total_loss: 0.023851570735375086\n",
      "          vf_explained_var: 0.1415913701057434\n",
      "          vf_loss: 0.010846972243032521\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.52884615384616\n",
      "    ram_util_percent: 53.35769230769232\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04303343351289937\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.831691175713896\n",
      "    mean_inference_ms: 2.213857881813673\n",
      "    mean_raw_obs_processing_ms: 2.2704531140954547\n",
      "  time_since_restore: 5237.534822463989\n",
      "  time_this_iter_s: 36.1384174823761\n",
      "  time_total_s: 5237.534822463989\n",
      "  timers:\n",
      "    learn_throughput: 1213.924\n",
      "    learn_time_ms: 823.775\n",
      "    load_throughput: 48078.148\n",
      "    load_time_ms: 20.799\n",
      "    sample_throughput: 26.9\n",
      "    sample_time_ms: 37174.337\n",
      "    update_time_ms: 7.362\n",
      "  timestamp: 1635086938\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 156\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   156</td><td style=\"text-align: right;\">         5237.53</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\"> -2.4298</td><td style=\"text-align: right;\">               -1.96</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            242.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 157000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-49-31\n",
      "  done: false\n",
      "  episode_len_mean: 243.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.434299999999992\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 580\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.59232693276135e-15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5490999397304323\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.002371281126289304\n",
      "          policy_loss: -0.029687040133608712\n",
      "          total_loss: -0.018415496912267473\n",
      "          vf_explained_var: 0.09942352771759033\n",
      "          vf_loss: 0.01676254292122192\n",
      "    num_agent_steps_sampled: 157000\n",
      "    num_agent_steps_trained: 157000\n",
      "    num_steps_sampled: 157000\n",
      "    num_steps_trained: 157000\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.11304347826088\n",
      "    ram_util_percent: 53.36304347826088\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04303567377033923\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.853259270717654\n",
      "    mean_inference_ms: 2.2137414542177805\n",
      "    mean_raw_obs_processing_ms: 2.273844265241391\n",
      "  time_since_restore: 5270.001095056534\n",
      "  time_this_iter_s: 32.466272592544556\n",
      "  time_total_s: 5270.001095056534\n",
      "  timers:\n",
      "    learn_throughput: 1189.989\n",
      "    learn_time_ms: 840.344\n",
      "    load_throughput: 48065.091\n",
      "    load_time_ms: 20.805\n",
      "    sample_throughput: 27.072\n",
      "    sample_time_ms: 36938.424\n",
      "    update_time_ms: 6.52\n",
      "  timestamp: 1635086971\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 157000\n",
      "  training_iteration: 157\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">            5270</td><td style=\"text-align: right;\">157000</td><td style=\"text-align: right;\"> -2.4343</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            243.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 158000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-50-07\n",
      "  done: false\n",
      "  episode_len_mean: 243.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.432199999999992\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 584\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.796163466380675e-15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.525060929523574\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004076071978959008\n",
      "          policy_loss: 0.05260199076599545\n",
      "          total_loss: 0.05716368117266231\n",
      "          vf_explained_var: 0.1345278024673462\n",
      "          vf_loss: 0.009812300943303853\n",
      "    num_agent_steps_sampled: 158000\n",
      "    num_agent_steps_trained: 158000\n",
      "    num_steps_sampled: 158000\n",
      "    num_steps_trained: 158000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.4846153846154\n",
      "    ram_util_percent: 53.36730769230768\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04303766080150776\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.869755401136985\n",
      "    mean_inference_ms: 2.213659639689959\n",
      "    mean_raw_obs_processing_ms: 2.2746853397318847\n",
      "  time_since_restore: 5306.567662715912\n",
      "  time_this_iter_s: 36.56656765937805\n",
      "  time_total_s: 5306.567662715912\n",
      "  timers:\n",
      "    learn_throughput: 1190.853\n",
      "    learn_time_ms: 839.735\n",
      "    load_throughput: 49473.617\n",
      "    load_time_ms: 20.213\n",
      "    sample_throughput: 28.241\n",
      "    sample_time_ms: 35409.251\n",
      "    update_time_ms: 6.822\n",
      "  timestamp: 1635087007\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 158000\n",
      "  training_iteration: 158\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">         5306.57</td><td style=\"text-align: right;\">158000</td><td style=\"text-align: right;\"> -2.4322</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            243.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 159000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-50-45\n",
      "  done: false\n",
      "  episode_len_mean: 243.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.431699999999992\n",
      "  episode_reward_min: -3.3999999999999715\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 588\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.3980817331903375e-15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.44729905790752833\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0034441308560055005\n",
      "          policy_loss: -0.03742951816982693\n",
      "          total_loss: -0.02839421522286203\n",
      "          vf_explained_var: 0.0751173198223114\n",
      "          vf_loss: 0.013508291573574145\n",
      "    num_agent_steps_sampled: 159000\n",
      "    num_agent_steps_trained: 159000\n",
      "    num_steps_sampled: 159000\n",
      "    num_steps_trained: 159000\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.08333333333336\n",
      "    ram_util_percent: 53.820370370370384\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043039887147997474\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.886981958128473\n",
      "    mean_inference_ms: 2.2135796537926935\n",
      "    mean_raw_obs_processing_ms: 2.2756122182168657\n",
      "  time_since_restore: 5344.096335887909\n",
      "  time_this_iter_s: 37.52867317199707\n",
      "  time_total_s: 5344.096335887909\n",
      "  timers:\n",
      "    learn_throughput: 1191.057\n",
      "    learn_time_ms: 839.59\n",
      "    load_throughput: 49186.661\n",
      "    load_time_ms: 20.331\n",
      "    sample_throughput: 27.825\n",
      "    sample_time_ms: 35939.477\n",
      "    update_time_ms: 7.52\n",
      "  timestamp: 1635087045\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 159000\n",
      "  training_iteration: 159\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">          5344.1</td><td style=\"text-align: right;\">159000</td><td style=\"text-align: right;\"> -2.4317</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">                -3.4</td><td style=\"text-align: right;\">            243.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-51-21\n",
      "  done: false\n",
      "  episode_len_mean: 239.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.395399999999993\n",
      "  episode_reward_min: -3.2599999999999745\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 593\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1990408665951687e-15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4187665906217363\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.001488856633033701\n",
      "          policy_loss: 0.011997706608639823\n",
      "          total_loss: 0.023626134710179436\n",
      "          vf_explained_var: 0.09007091820240021\n",
      "          vf_loss: 0.015816092842982875\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.54423076923075\n",
      "    ram_util_percent: 53.848076923076924\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04304375838164685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.910784199426942\n",
      "    mean_inference_ms: 2.2134864487249324\n",
      "    mean_raw_obs_processing_ms: 2.2769194602818823\n",
      "  time_since_restore: 5380.229269981384\n",
      "  time_this_iter_s: 36.13293409347534\n",
      "  time_total_s: 5380.229269981384\n",
      "  timers:\n",
      "    learn_throughput: 1190.363\n",
      "    learn_time_ms: 840.08\n",
      "    load_throughput: 46516.289\n",
      "    load_time_ms: 21.498\n",
      "    sample_throughput: 27.702\n",
      "    sample_time_ms: 36098.321\n",
      "    update_time_ms: 8.035\n",
      "  timestamp: 1635087081\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 160\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         5380.23</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\"> -2.3954</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -3.26</td><td style=\"text-align: right;\">            239.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 161000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-51-57\n",
      "  done: false\n",
      "  episode_len_mean: 238.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9800000000000015\n",
      "  episode_reward_mean: -2.3866999999999927\n",
      "  episode_reward_min: -3.2599999999999745\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 597\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.995204332975844e-16\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4790285461478763\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0029850101564080312\n",
      "          policy_loss: 0.006420391384098265\n",
      "          total_loss: 0.01482117043601142\n",
      "          vf_explained_var: 0.06938512623310089\n",
      "          vf_loss: 0.013191063339925474\n",
      "    num_agent_steps_sampled: 161000\n",
      "    num_agent_steps_trained: 161000\n",
      "    num_steps_sampled: 161000\n",
      "    num_steps_trained: 161000\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.56666666666666\n",
      "    ram_util_percent: 53.90196078431372\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043047210564796316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.930541158269545\n",
      "    mean_inference_ms: 2.2134224228124078\n",
      "    mean_raw_obs_processing_ms: 2.2781662881313083\n",
      "  time_since_restore: 5415.9878742694855\n",
      "  time_this_iter_s: 35.758604288101196\n",
      "  time_total_s: 5415.9878742694855\n",
      "  timers:\n",
      "    learn_throughput: 1189.738\n",
      "    learn_time_ms: 840.521\n",
      "    load_throughput: 47390.213\n",
      "    load_time_ms: 21.101\n",
      "    sample_throughput: 27.641\n",
      "    sample_time_ms: 36178.707\n",
      "    update_time_ms: 7.37\n",
      "  timestamp: 1635087117\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 161000\n",
      "  training_iteration: 161\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">         5415.99</td><td style=\"text-align: right;\">161000</td><td style=\"text-align: right;\"> -2.3867</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">               -3.26</td><td style=\"text-align: right;\">            238.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 162000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-52-52\n",
      "  done: false\n",
      "  episode_len_mean: 236.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9700000000000015\n",
      "  episode_reward_mean: -2.3632999999999935\n",
      "  episode_reward_min: -3.2599999999999745\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 602\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.997602166487922e-16\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.39881921377446916\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.001953511273054313\n",
      "          policy_loss: -0.016591212898492812\n",
      "          total_loss: -0.0037389232880539365\n",
      "          vf_explained_var: 0.13449157774448395\n",
      "          vf_loss: 0.016840482296215164\n",
      "    num_agent_steps_sampled: 162000\n",
      "    num_agent_steps_trained: 162000\n",
      "    num_steps_sampled: 162000\n",
      "    num_steps_trained: 162000\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.8367088607595\n",
      "    ram_util_percent: 53.7873417721519\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04305114246797288\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.957093057493797\n",
      "    mean_inference_ms: 2.2133646908443056\n",
      "    mean_raw_obs_processing_ms: 2.2853902152753847\n",
      "  time_since_restore: 5471.637178897858\n",
      "  time_this_iter_s: 55.64930462837219\n",
      "  time_total_s: 5471.637178897858\n",
      "  timers:\n",
      "    learn_throughput: 1183.575\n",
      "    learn_time_ms: 844.898\n",
      "    load_throughput: 46485.305\n",
      "    load_time_ms: 21.512\n",
      "    sample_throughput: 26.149\n",
      "    sample_time_ms: 38242.419\n",
      "    update_time_ms: 7.747\n",
      "  timestamp: 1635087172\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 162000\n",
      "  training_iteration: 162\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         5471.64</td><td style=\"text-align: right;\">162000</td><td style=\"text-align: right;\"> -2.3633</td><td style=\"text-align: right;\">               -1.97</td><td style=\"text-align: right;\">               -3.26</td><td style=\"text-align: right;\">            236.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 163000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-53-29\n",
      "  done: false\n",
      "  episode_len_mean: 234.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9700000000000015\n",
      "  episode_reward_mean: -2.343599999999994\n",
      "  episode_reward_min: -3.0699999999999785\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 606\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.498801083243961e-16\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.39423719644546507\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0015869850506030206\n",
      "          policy_loss: -0.019353175991111332\n",
      "          total_loss: -0.009530979891618092\n",
      "          vf_explained_var: 0.01618017815053463\n",
      "          vf_loss: 0.01376457101561957\n",
      "    num_agent_steps_sampled: 163000\n",
      "    num_agent_steps_trained: 163000\n",
      "    num_steps_sampled: 163000\n",
      "    num_steps_trained: 163000\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.45660377358492\n",
      "    ram_util_percent: 53.81886792452831\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04305365309785847\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.979361008802726\n",
      "    mean_inference_ms: 2.213316599388312\n",
      "    mean_raw_obs_processing_ms: 2.291273100726672\n",
      "  time_since_restore: 5508.5618624687195\n",
      "  time_this_iter_s: 36.924683570861816\n",
      "  time_total_s: 5508.5618624687195\n",
      "  timers:\n",
      "    learn_throughput: 1207.626\n",
      "    learn_time_ms: 828.071\n",
      "    load_throughput: 44728.746\n",
      "    load_time_ms: 22.357\n",
      "    sample_throughput: 25.802\n",
      "    sample_time_ms: 38756.118\n",
      "    update_time_ms: 7.595\n",
      "  timestamp: 1635087209\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 163000\n",
      "  training_iteration: 163\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   163</td><td style=\"text-align: right;\">         5508.56</td><td style=\"text-align: right;\">163000</td><td style=\"text-align: right;\"> -2.3436</td><td style=\"text-align: right;\">               -1.97</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">            234.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-54-04\n",
      "  done: false\n",
      "  episode_len_mean: 233.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9700000000000015\n",
      "  episode_reward_mean: -2.335199999999994\n",
      "  episode_reward_min: -3.0699999999999785\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 611\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.494005416219805e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6166693886121114\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005963750944783107\n",
      "          policy_loss: -0.028580114907688563\n",
      "          total_loss: -0.017269080215030246\n",
      "          vf_explained_var: 0.08314958214759827\n",
      "          vf_loss: 0.017477726625899474\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.48599999999999\n",
      "    ram_util_percent: 53.81399999999999\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04305644098955625\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.007137028954638\n",
      "    mean_inference_ms: 2.213256337041326\n",
      "    mean_raw_obs_processing_ms: 2.2950942870342534\n",
      "  time_since_restore: 5543.530653476715\n",
      "  time_this_iter_s: 34.968791007995605\n",
      "  time_total_s: 5543.530653476715\n",
      "  timers:\n",
      "    learn_throughput: 1210.523\n",
      "    learn_time_ms: 826.089\n",
      "    load_throughput: 45825.715\n",
      "    load_time_ms: 21.822\n",
      "    sample_throughput: 25.82\n",
      "    sample_time_ms: 38730.199\n",
      "    update_time_ms: 7.562\n",
      "  timestamp: 1635087244\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 164\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         5543.53</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\"> -2.3352</td><td style=\"text-align: right;\">               -1.97</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">            233.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 165000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-54-40\n",
      "  done: false\n",
      "  episode_len_mean: 232.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9700000000000015\n",
      "  episode_reward_mean: -2.3253999999999944\n",
      "  episode_reward_min: -3.0699999999999785\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 615\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.494005416219805e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.43176334897677104\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004423788125873705\n",
      "          policy_loss: 0.057907785144117145\n",
      "          total_loss: 0.06334247580832905\n",
      "          vf_explained_var: 0.09106312692165375\n",
      "          vf_loss: 0.009752322465647011\n",
      "    num_agent_steps_sampled: 165000\n",
      "    num_agent_steps_trained: 165000\n",
      "    num_steps_sampled: 165000\n",
      "    num_steps_trained: 165000\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.52\n",
      "    ram_util_percent: 53.94\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04305844594366233\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.02904623052645\n",
      "    mean_inference_ms: 2.2131891680575433\n",
      "    mean_raw_obs_processing_ms: 2.2963364681868974\n",
      "  time_since_restore: 5578.699812173843\n",
      "  time_this_iter_s: 35.169158697128296\n",
      "  time_total_s: 5578.699812173843\n",
      "  timers:\n",
      "    learn_throughput: 1206.352\n",
      "    learn_time_ms: 828.946\n",
      "    load_throughput: 45051.31\n",
      "    load_time_ms: 22.197\n",
      "    sample_throughput: 27.125\n",
      "    sample_time_ms: 36866.54\n",
      "    update_time_ms: 7.509\n",
      "  timestamp: 1635087280\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 165000\n",
      "  training_iteration: 165\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">          5578.7</td><td style=\"text-align: right;\">165000</td><td style=\"text-align: right;\"> -2.3254</td><td style=\"text-align: right;\">               -1.97</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">            232.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 166000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-55-16\n",
      "  done: false\n",
      "  episode_len_mean: 231.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9700000000000015\n",
      "  episode_reward_mean: -2.3184999999999945\n",
      "  episode_reward_min: -3.0699999999999785\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 620\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.7470027081099023e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4514637400706609\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0014119191836660624\n",
      "          policy_loss: -0.009604430033100976\n",
      "          total_loss: 0.0030806416438685525\n",
      "          vf_explained_var: 0.12928254902362823\n",
      "          vf_loss: 0.017199708128141034\n",
      "    num_agent_steps_sampled: 166000\n",
      "    num_agent_steps_trained: 166000\n",
      "    num_steps_sampled: 166000\n",
      "    num_steps_trained: 166000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.47547169811321\n",
      "    ram_util_percent: 53.97924528301886\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04306064039105566\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.056267484421177\n",
      "    mean_inference_ms: 2.213100433633841\n",
      "    mean_raw_obs_processing_ms: 2.298024201660206\n",
      "  time_since_restore: 5615.537999153137\n",
      "  time_this_iter_s: 36.83818697929382\n",
      "  time_total_s: 5615.537999153137\n",
      "  timers:\n",
      "    learn_throughput: 1189.4\n",
      "    learn_time_ms: 840.76\n",
      "    load_throughput: 42912.344\n",
      "    load_time_ms: 23.303\n",
      "    sample_throughput: 27.083\n",
      "    sample_time_ms: 36924.091\n",
      "    update_time_ms: 7.209\n",
      "  timestamp: 1635087316\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 166000\n",
      "  training_iteration: 166\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   166</td><td style=\"text-align: right;\">         5615.54</td><td style=\"text-align: right;\">166000</td><td style=\"text-align: right;\"> -2.3185</td><td style=\"text-align: right;\">               -1.97</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">            231.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 167000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-55-53\n",
      "  done: false\n",
      "  episode_len_mean: 230.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9700000000000015\n",
      "  episode_reward_mean: -2.3053999999999952\n",
      "  episode_reward_min: -2.859999999999983\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 624\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.8735013540549512e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4128494107060962\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0016543934365010277\n",
      "          policy_loss: 0.039713896397087306\n",
      "          total_loss: 0.0485853161662817\n",
      "          vf_explained_var: 0.06524360924959183\n",
      "          vf_loss: 0.01299990965053439\n",
      "    num_agent_steps_sampled: 167000\n",
      "    num_agent_steps_trained: 167000\n",
      "    num_steps_sampled: 167000\n",
      "    num_steps_trained: 167000\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.6096153846154\n",
      "    ram_util_percent: 54.03846153846154\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04306220858069336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.078758353286783\n",
      "    mean_inference_ms: 2.2130383580308477\n",
      "    mean_raw_obs_processing_ms: 2.299554314760471\n",
      "  time_since_restore: 5652.319695234299\n",
      "  time_this_iter_s: 36.7816960811615\n",
      "  time_total_s: 5652.319695234299\n",
      "  timers:\n",
      "    learn_throughput: 1212.461\n",
      "    learn_time_ms: 824.769\n",
      "    load_throughput: 43669.287\n",
      "    load_time_ms: 22.899\n",
      "    sample_throughput: 26.758\n",
      "    sample_time_ms: 37372.004\n",
      "    update_time_ms: 7.246\n",
      "  timestamp: 1635087353\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 167000\n",
      "  training_iteration: 167\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         5652.32</td><td style=\"text-align: right;\">167000</td><td style=\"text-align: right;\"> -2.3054</td><td style=\"text-align: right;\">               -1.97</td><td style=\"text-align: right;\">               -2.86</td><td style=\"text-align: right;\">            230.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-56-29\n",
      "  done: false\n",
      "  episode_len_mean: 230.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9700000000000015\n",
      "  episode_reward_mean: -2.306099999999995\n",
      "  episode_reward_min: -2.859999999999983\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 628\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.367506770274756e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5956302987204658\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011106099948812679\n",
      "          policy_loss: -0.019848362356424332\n",
      "          total_loss: -0.010830404278304841\n",
      "          vf_explained_var: 0.061132822185754776\n",
      "          vf_loss: 0.014974260857949654\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.37\n",
      "    ram_util_percent: 54.065999999999995\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043063887139814946\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.1011556733083\n",
      "    mean_inference_ms: 2.2129899106562543\n",
      "    mean_raw_obs_processing_ms: 2.301133413119223\n",
      "  time_since_restore: 5687.610402584076\n",
      "  time_this_iter_s: 35.29070734977722\n",
      "  time_total_s: 5687.610402584076\n",
      "  timers:\n",
      "    learn_throughput: 1212.693\n",
      "    learn_time_ms: 824.611\n",
      "    load_throughput: 44752.179\n",
      "    load_time_ms: 22.345\n",
      "    sample_throughput: 26.849\n",
      "    sample_time_ms: 37245.42\n",
      "    update_time_ms: 6.929\n",
      "  timestamp: 1635087389\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 168\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">         5687.61</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\"> -2.3061</td><td style=\"text-align: right;\">               -1.97</td><td style=\"text-align: right;\">               -2.86</td><td style=\"text-align: right;\">            230.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 169000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-57-25\n",
      "  done: false\n",
      "  episode_len_mean: 229.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.298799999999994\n",
      "  episode_reward_min: -2.859999999999983\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 633\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.367506770274756e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5731973707675934\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004886184576435287\n",
      "          policy_loss: -0.02221787323554357\n",
      "          total_loss: -0.009468668947617213\n",
      "          vf_explained_var: 0.12080489099025726\n",
      "          vf_loss: 0.018481178126401373\n",
      "    num_agent_steps_sampled: 169000\n",
      "    num_agent_steps_trained: 169000\n",
      "    num_steps_sampled: 169000\n",
      "    num_steps_trained: 169000\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.29625\n",
      "    ram_util_percent: 54.03125\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04306586561181915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.130194037521413\n",
      "    mean_inference_ms: 2.212936554367555\n",
      "    mean_raw_obs_processing_ms: 2.3080809045661788\n",
      "  time_since_restore: 5743.668239593506\n",
      "  time_this_iter_s: 56.05783700942993\n",
      "  time_total_s: 5743.668239593506\n",
      "  timers:\n",
      "    learn_throughput: 1208.317\n",
      "    learn_time_ms: 827.597\n",
      "    load_throughput: 42611.876\n",
      "    load_time_ms: 23.468\n",
      "    sample_throughput: 25.579\n",
      "    sample_time_ms: 39094.496\n",
      "    update_time_ms: 6.201\n",
      "  timestamp: 1635087445\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 169000\n",
      "  training_iteration: 169\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         5743.67</td><td style=\"text-align: right;\">169000</td><td style=\"text-align: right;\"> -2.2988</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -2.86</td><td style=\"text-align: right;\">            229.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 170000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-58-01\n",
      "  done: false\n",
      "  episode_len_mean: 229.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.2952999999999943\n",
      "  episode_reward_min: -2.859999999999983\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 637\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.683753385137378e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5548675772216585\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.028157844054013264\n",
      "          policy_loss: 0.04039472606447008\n",
      "          total_loss: 0.04614023110932774\n",
      "          vf_explained_var: 0.09956896305084229\n",
      "          vf_loss: 0.011294180175496472\n",
      "    num_agent_steps_sampled: 170000\n",
      "    num_agent_steps_trained: 170000\n",
      "    num_steps_sampled: 170000\n",
      "    num_steps_trained: 170000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.35471698113206\n",
      "    ram_util_percent: 53.94905660377359\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04306491943382744\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.153410713381152\n",
      "    mean_inference_ms: 2.2129041754530268\n",
      "    mean_raw_obs_processing_ms: 2.313780895736492\n",
      "  time_since_restore: 5780.3038647174835\n",
      "  time_this_iter_s: 36.63562512397766\n",
      "  time_total_s: 5780.3038647174835\n",
      "  timers:\n",
      "    learn_throughput: 1208.533\n",
      "    learn_time_ms: 827.45\n",
      "    load_throughput: 44755.713\n",
      "    load_time_ms: 22.344\n",
      "    sample_throughput: 25.545\n",
      "    sample_time_ms: 39146.629\n",
      "    update_time_ms: 5.592\n",
      "  timestamp: 1635087481\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 170000\n",
      "  training_iteration: 170\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">          5780.3</td><td style=\"text-align: right;\">170000</td><td style=\"text-align: right;\"> -2.2953</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -2.86</td><td style=\"text-align: right;\">            229.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 171000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-58-34\n",
      "  done: false\n",
      "  episode_len_mean: 230.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.3040999999999947\n",
      "  episode_reward_min: -2.859999999999983\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 641\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.025630077706068e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8340502076678806\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0053441980481781885\n",
      "          policy_loss: 0.023260335955354903\n",
      "          total_loss: 0.02918536571992768\n",
      "          vf_explained_var: 0.07544578611850739\n",
      "          vf_loss: 0.014265528331614203\n",
      "    num_agent_steps_sampled: 171000\n",
      "    num_agent_steps_trained: 171000\n",
      "    num_steps_sampled: 171000\n",
      "    num_steps_trained: 171000\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.84130434782608\n",
      "    ram_util_percent: 53.8913043478261\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043064152876032324\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.175842379715714\n",
      "    mean_inference_ms: 2.2128688544229447\n",
      "    mean_raw_obs_processing_ms: 2.314815706855188\n",
      "  time_since_restore: 5812.486263990402\n",
      "  time_this_iter_s: 32.1823992729187\n",
      "  time_total_s: 5812.486263990402\n",
      "  timers:\n",
      "    learn_throughput: 1207.575\n",
      "    learn_time_ms: 828.106\n",
      "    load_throughput: 45975.909\n",
      "    load_time_ms: 21.751\n",
      "    sample_throughput: 25.781\n",
      "    sample_time_ms: 38788.655\n",
      "    update_time_ms: 5.921\n",
      "  timestamp: 1635087514\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 171000\n",
      "  training_iteration: 171\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         5812.49</td><td style=\"text-align: right;\">171000</td><td style=\"text-align: right;\"> -2.3041</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -2.86</td><td style=\"text-align: right;\">            230.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-59-00\n",
      "  done: false\n",
      "  episode_len_mean: 233.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.332599999999994\n",
      "  episode_reward_min: -4.479999999999949\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 644\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.025630077706068e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0298634257581498\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012423661266738703\n",
      "          policy_loss: 0.057312012877729204\n",
      "          total_loss: 0.05745740450090832\n",
      "          vf_explained_var: 0.019467923790216446\n",
      "          vf_loss: 0.010444023914250365\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.61891891891894\n",
      "    ram_util_percent: 53.883783783783784\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04306431709849029\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.19144365552381\n",
      "    mean_inference_ms: 2.212857352786896\n",
      "    mean_raw_obs_processing_ms: 2.315639541661489\n",
      "  time_since_restore: 5838.474995613098\n",
      "  time_this_iter_s: 25.988731622695923\n",
      "  time_total_s: 5838.474995613098\n",
      "  timers:\n",
      "    learn_throughput: 1212.726\n",
      "    learn_time_ms: 824.589\n",
      "    load_throughput: 46934.002\n",
      "    load_time_ms: 21.307\n",
      "    sample_throughput: 27.912\n",
      "    sample_time_ms: 35826.508\n",
      "    update_time_ms: 5.89\n",
      "  timestamp: 1635087540\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 172\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">         5838.47</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\"> -2.3326</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -4.48</td><td style=\"text-align: right;\">            233.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 173000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-59-23\n",
      "  done: false\n",
      "  episode_len_mean: 236.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.3690999999999933\n",
      "  episode_reward_min: -4.989999999999938\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 647\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.025630077706068e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2084165652592977\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03989609102431883\n",
      "          policy_loss: 0.05705099950234095\n",
      "          total_loss: 0.055312181429730524\n",
      "          vf_explained_var: 0.053940825164318085\n",
      "          vf_loss: 0.010345348561693552\n",
      "    num_agent_steps_sampled: 173000\n",
      "    num_agent_steps_trained: 173000\n",
      "    num_steps_sampled: 173000\n",
      "    num_steps_trained: 173000\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.33636363636363\n",
      "    ram_util_percent: 53.89090909090908\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04306432879959404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.205143418606305\n",
      "    mean_inference_ms: 2.2128524344950793\n",
      "    mean_raw_obs_processing_ms: 2.3163685748210483\n",
      "  time_since_restore: 5861.873416900635\n",
      "  time_this_iter_s: 23.39842128753662\n",
      "  time_total_s: 5861.873416900635\n",
      "  timers:\n",
      "    learn_throughput: 1207.455\n",
      "    learn_time_ms: 828.188\n",
      "    load_throughput: 47613.309\n",
      "    load_time_ms: 21.003\n",
      "    sample_throughput: 29.011\n",
      "    sample_time_ms: 34469.557\n",
      "    update_time_ms: 6.914\n",
      "  timestamp: 1635087563\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 173000\n",
      "  training_iteration: 173\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         5861.87</td><td style=\"text-align: right;\">173000</td><td style=\"text-align: right;\"> -2.3691</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -4.99</td><td style=\"text-align: right;\">            236.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 174000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_14-59-46\n",
      "  done: false\n",
      "  episode_len_mean: 240.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.403599999999993\n",
      "  episode_reward_min: -4.989999999999938\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 650\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0538445116559108e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2931940151585473\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008207870649876112\n",
      "          policy_loss: 0.06230967856115765\n",
      "          total_loss: 0.0589980750448174\n",
      "          vf_explained_var: 0.12491179257631302\n",
      "          vf_loss: 0.00962033637592362\n",
      "    num_agent_steps_sampled: 174000\n",
      "    num_agent_steps_trained: 174000\n",
      "    num_steps_sampled: 174000\n",
      "    num_steps_trained: 174000\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.29393939393938\n",
      "    ram_util_percent: 54.081818181818186\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043063844499236906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.21716790016267\n",
      "    mean_inference_ms: 2.2128317296450937\n",
      "    mean_raw_obs_processing_ms: 2.3168698369008998\n",
      "  time_since_restore: 5884.990199327469\n",
      "  time_this_iter_s: 23.116782426834106\n",
      "  time_total_s: 5884.990199327469\n",
      "  timers:\n",
      "    learn_throughput: 1219.1\n",
      "    learn_time_ms: 820.277\n",
      "    load_throughput: 45637.088\n",
      "    load_time_ms: 21.912\n",
      "    sample_throughput: 30.037\n",
      "    sample_time_ms: 33291.908\n",
      "    update_time_ms: 6.448\n",
      "  timestamp: 1635087586\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 174000\n",
      "  training_iteration: 174\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   174</td><td style=\"text-align: right;\">         5884.99</td><td style=\"text-align: right;\">174000</td><td style=\"text-align: right;\"> -2.4036</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -4.99</td><td style=\"text-align: right;\">            240.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 175000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-00-11\n",
      "  done: false\n",
      "  episode_len_mean: 243.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.4336999999999924\n",
      "  episode_reward_min: -4.989999999999938\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 652\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0538445116559108e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6487786418861814\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.025261608314693578\n",
      "          policy_loss: -0.10307237075434791\n",
      "          total_loss: -0.10895534157752991\n",
      "          vf_explained_var: -0.061349883675575256\n",
      "          vf_loss: 0.0106048118976307\n",
      "    num_agent_steps_sampled: 175000\n",
      "    num_agent_steps_trained: 175000\n",
      "    num_steps_sampled: 175000\n",
      "    num_steps_trained: 175000\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.94166666666666\n",
      "    ram_util_percent: 53.88333333333335\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04306359311377865\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.223952574884564\n",
      "    mean_inference_ms: 2.21282756162052\n",
      "    mean_raw_obs_processing_ms: 2.3172314896570554\n",
      "  time_since_restore: 5910.156340837479\n",
      "  time_this_iter_s: 25.166141510009766\n",
      "  time_total_s: 5910.156340837479\n",
      "  timers:\n",
      "    learn_throughput: 1219.724\n",
      "    learn_time_ms: 819.858\n",
      "    load_throughput: 44854.357\n",
      "    load_time_ms: 22.294\n",
      "    sample_throughput: 30.967\n",
      "    sample_time_ms: 32292.15\n",
      "    update_time_ms: 6.012\n",
      "  timestamp: 1635087611\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 175000\n",
      "  training_iteration: 175\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         5910.16</td><td style=\"text-align: right;\">175000</td><td style=\"text-align: right;\"> -2.4337</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -4.99</td><td style=\"text-align: right;\">            243.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-00-41\n",
      "  done: false\n",
      "  episode_len_mean: 245.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.453599999999992\n",
      "  episode_reward_min: -4.989999999999938\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 656\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.580766767483865e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9863228473398421\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006422297283000174\n",
      "          policy_loss: 0.038264256260461274\n",
      "          total_loss: 0.040565128748615585\n",
      "          vf_explained_var: 0.31334421038627625\n",
      "          vf_loss: 0.012164098117500544\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.55714285714286\n",
      "    ram_util_percent: 53.94047619047619\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04306287780071268\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.23651089812228\n",
      "    mean_inference_ms: 2.2128208911010825\n",
      "    mean_raw_obs_processing_ms: 2.3177599908745825\n",
      "  time_since_restore: 5939.507390260696\n",
      "  time_this_iter_s: 29.351049423217773\n",
      "  time_total_s: 5939.507390260696\n",
      "  timers:\n",
      "    learn_throughput: 1235.228\n",
      "    learn_time_ms: 809.567\n",
      "    load_throughput: 46925.233\n",
      "    load_time_ms: 21.31\n",
      "    sample_throughput: 31.692\n",
      "    sample_time_ms: 31553.856\n",
      "    update_time_ms: 6.718\n",
      "  timestamp: 1635087641\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 176\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   176</td><td style=\"text-align: right;\">         5939.51</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\"> -2.4536</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -4.99</td><td style=\"text-align: right;\">            245.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 177000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-01-05\n",
      "  done: false\n",
      "  episode_len_mean: 248.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.488999999999991\n",
      "  episode_reward_min: -4.989999999999938\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 659\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.580766767483865e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.156460577911801\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009529428067979968\n",
      "          policy_loss: 0.06248801483048333\n",
      "          total_loss: 0.059719882822699015\n",
      "          vf_explained_var: 0.45721766352653503\n",
      "          vf_loss: 0.008796473743212926\n",
      "    num_agent_steps_sampled: 177000\n",
      "    num_agent_steps_trained: 177000\n",
      "    num_steps_sampled: 177000\n",
      "    num_steps_trained: 177000\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.76857142857142\n",
      "    ram_util_percent: 53.951428571428565\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0430617249971036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.24429002632496\n",
      "    mean_inference_ms: 2.212815615976407\n",
      "    mean_raw_obs_processing_ms: 2.3181386773740256\n",
      "  time_since_restore: 5963.730582952499\n",
      "  time_this_iter_s: 24.22319269180298\n",
      "  time_total_s: 5963.730582952499\n",
      "  timers:\n",
      "    learn_throughput: 1237.737\n",
      "    learn_time_ms: 807.926\n",
      "    load_throughput: 46898.526\n",
      "    load_time_ms: 21.323\n",
      "    sample_throughput: 33.005\n",
      "    sample_time_ms: 30298.787\n",
      "    update_time_ms: 7.496\n",
      "  timestamp: 1635087665\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 177000\n",
      "  training_iteration: 177\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         5963.73</td><td style=\"text-align: right;\">177000</td><td style=\"text-align: right;\">  -2.489</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -4.99</td><td style=\"text-align: right;\">             248.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 178000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-01-50\n",
      "  done: false\n",
      "  episode_len_mean: 250.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.5078999999999905\n",
      "  episode_reward_min: -4.989999999999938\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 662\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.580766767483865e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1004619518915812\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00911628600601454\n",
      "          policy_loss: 0.04809270285897785\n",
      "          total_loss: 0.048303976986143325\n",
      "          vf_explained_var: 0.1109803095459938\n",
      "          vf_loss: 0.011215894038064613\n",
      "    num_agent_steps_sampled: 178000\n",
      "    num_agent_steps_trained: 178000\n",
      "    num_steps_sampled: 178000\n",
      "    num_steps_trained: 178000\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.01875\n",
      "    ram_util_percent: 53.9765625\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04306013079308482\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.25149975760416\n",
      "    mean_inference_ms: 2.2128204522095767\n",
      "    mean_raw_obs_processing_ms: 2.3212313741760386\n",
      "  time_since_restore: 6008.937345027924\n",
      "  time_this_iter_s: 45.206762075424194\n",
      "  time_total_s: 6008.937345027924\n",
      "  timers:\n",
      "    learn_throughput: 1237.25\n",
      "    learn_time_ms: 808.244\n",
      "    load_throughput: 44473.351\n",
      "    load_time_ms: 22.485\n",
      "    sample_throughput: 31.961\n",
      "    sample_time_ms: 31287.812\n",
      "    update_time_ms: 8.148\n",
      "  timestamp: 1635087710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 178000\n",
      "  training_iteration: 178\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   178</td><td style=\"text-align: right;\">         6008.94</td><td style=\"text-align: right;\">178000</td><td style=\"text-align: right;\"> -2.5079</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -4.99</td><td style=\"text-align: right;\">            250.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 179000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-02-19\n",
      "  done: false\n",
      "  episode_len_mean: 252.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.52619999999999\n",
      "  episode_reward_min: -4.989999999999938\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 665\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.580766767483865e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9468972272343106\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009228495346941814\n",
      "          policy_loss: -0.034942463371488784\n",
      "          total_loss: -0.03342751496367984\n",
      "          vf_explained_var: 0.04455925524234772\n",
      "          vf_loss: 0.010983920346997264\n",
      "    num_agent_steps_sampled: 179000\n",
      "    num_agent_steps_trained: 179000\n",
      "    num_steps_sampled: 179000\n",
      "    num_steps_trained: 179000\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.2904761904762\n",
      "    ram_util_percent: 54.12857142857144\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04305866497051\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.25752156060379\n",
      "    mean_inference_ms: 2.212834254913081\n",
      "    mean_raw_obs_processing_ms: 2.3243601982758295\n",
      "  time_since_restore: 6037.943200111389\n",
      "  time_this_iter_s: 29.005855083465576\n",
      "  time_total_s: 6037.943200111389\n",
      "  timers:\n",
      "    learn_throughput: 1241.518\n",
      "    learn_time_ms: 805.466\n",
      "    load_throughput: 46998.952\n",
      "    load_time_ms: 21.277\n",
      "    sample_throughput: 34.982\n",
      "    sample_time_ms: 28585.973\n",
      "    update_time_ms: 8.274\n",
      "  timestamp: 1635087739\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 179000\n",
      "  training_iteration: 179\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         6037.94</td><td style=\"text-align: right;\">179000</td><td style=\"text-align: right;\"> -2.5262</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -4.99</td><td style=\"text-align: right;\">            252.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-02-53\n",
      "  done: false\n",
      "  episode_len_mean: 253.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.5367999999999897\n",
      "  episode_reward_min: -4.989999999999938\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 669\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.580766767483865e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7998042106628418\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005696324754767327\n",
      "          policy_loss: 0.004230546289020114\n",
      "          total_loss: 0.010732752250300513\n",
      "          vf_explained_var: 0.1574195772409439\n",
      "          vf_loss: 0.014500248639119997\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.99583333333334\n",
      "    ram_util_percent: 53.97291666666666\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04305701058395476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.26501439791798\n",
      "    mean_inference_ms: 2.212854199850639\n",
      "    mean_raw_obs_processing_ms: 2.3251490051039467\n",
      "  time_since_restore: 6071.783150672913\n",
      "  time_this_iter_s: 33.83995056152344\n",
      "  time_total_s: 6071.783150672913\n",
      "  timers:\n",
      "    learn_throughput: 1241.76\n",
      "    learn_time_ms: 805.309\n",
      "    load_throughput: 46296.126\n",
      "    load_time_ms: 21.6\n",
      "    sample_throughput: 35.328\n",
      "    sample_time_ms: 28305.9\n",
      "    update_time_ms: 8.683\n",
      "  timestamp: 1635087773\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 180\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         6071.78</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\"> -2.5368</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -4.99</td><td style=\"text-align: right;\">            253.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 181000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-03-19\n",
      "  done: false\n",
      "  episode_len_mean: 256.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.567799999999989\n",
      "  episode_reward_min: -4.989999999999938\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 672\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.580766767483865e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.073092422220442\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006481282386308212\n",
      "          policy_loss: 0.015570561918947432\n",
      "          total_loss: 0.01319786583383878\n",
      "          vf_explained_var: 0.2794027626514435\n",
      "          vf_loss: 0.008358229917171734\n",
      "    num_agent_steps_sampled: 181000\n",
      "    num_agent_steps_trained: 181000\n",
      "    num_steps_sampled: 181000\n",
      "    num_steps_trained: 181000\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.06756756756756\n",
      "    ram_util_percent: 53.932432432432435\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04305568133195296\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.269168377612182\n",
      "    mean_inference_ms: 2.2128628554105734\n",
      "    mean_raw_obs_processing_ms: 2.3247936436483014\n",
      "  time_since_restore: 6097.878154039383\n",
      "  time_this_iter_s: 26.095003366470337\n",
      "  time_total_s: 6097.878154039383\n",
      "  timers:\n",
      "    learn_throughput: 1242.84\n",
      "    learn_time_ms: 804.609\n",
      "    load_throughput: 45793.394\n",
      "    load_time_ms: 21.837\n",
      "    sample_throughput: 36.107\n",
      "    sample_time_ms: 27695.552\n",
      "    update_time_ms: 10.155\n",
      "  timestamp: 1635087799\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 181000\n",
      "  training_iteration: 181\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   181</td><td style=\"text-align: right;\">         6097.88</td><td style=\"text-align: right;\">181000</td><td style=\"text-align: right;\"> -2.5678</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -4.99</td><td style=\"text-align: right;\">            256.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 182000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-03-43\n",
      "  done: false\n",
      "  episode_len_mean: 260.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.602299999999988\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 675\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.580766767483865e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.19745411740409\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016703600900403615\n",
      "          policy_loss: 0.026434444470538034\n",
      "          total_loss: 0.027273283898830415\n",
      "          vf_explained_var: 0.044270604848861694\n",
      "          vf_loss: 0.012813380102994012\n",
      "    num_agent_steps_sampled: 182000\n",
      "    num_agent_steps_trained: 182000\n",
      "    num_steps_sampled: 182000\n",
      "    num_steps_trained: 182000\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.93529411764706\n",
      "    ram_util_percent: 53.838235294117645\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04305434266939535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.271384337826035\n",
      "    mean_inference_ms: 2.2128720129058475\n",
      "    mean_raw_obs_processing_ms: 2.3243597964751372\n",
      "  time_since_restore: 6121.356429815292\n",
      "  time_this_iter_s: 23.478275775909424\n",
      "  time_total_s: 6121.356429815292\n",
      "  timers:\n",
      "    learn_throughput: 1245.349\n",
      "    learn_time_ms: 802.988\n",
      "    load_throughput: 46529.448\n",
      "    load_time_ms: 21.492\n",
      "    sample_throughput: 36.434\n",
      "    sample_time_ms: 27446.548\n",
      "    update_time_ms: 10.074\n",
      "  timestamp: 1635087823\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 182000\n",
      "  training_iteration: 182\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">         6121.36</td><td style=\"text-align: right;\">182000</td><td style=\"text-align: right;\"> -2.6023</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            260.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 183000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-04-12\n",
      "  done: false\n",
      "  episode_len_mean: 262.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.6234999999999875\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 679\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.580766767483865e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.011239007446501\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006971406984847701\n",
      "          policy_loss: 0.01137792795068688\n",
      "          total_loss: 0.017780493034256828\n",
      "          vf_explained_var: 0.1599670946598053\n",
      "          vf_loss: 0.016514954674575064\n",
      "    num_agent_steps_sampled: 183000\n",
      "    num_agent_steps_trained: 183000\n",
      "    num_steps_sampled: 183000\n",
      "    num_steps_trained: 183000\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.9690476190476\n",
      "    ram_util_percent: 53.790476190476184\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04305292889048915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.27370446274529\n",
      "    mean_inference_ms: 2.2128992441382396\n",
      "    mean_raw_obs_processing_ms: 2.323854234846043\n",
      "  time_since_restore: 6151.017253398895\n",
      "  time_this_iter_s: 29.660823583602905\n",
      "  time_total_s: 6151.017253398895\n",
      "  timers:\n",
      "    learn_throughput: 1254.353\n",
      "    learn_time_ms: 797.224\n",
      "    load_throughput: 48500.221\n",
      "    load_time_ms: 20.618\n",
      "    sample_throughput: 35.612\n",
      "    sample_time_ms: 28080.354\n",
      "    update_time_ms: 9.074\n",
      "  timestamp: 1635087852\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 183000\n",
      "  training_iteration: 183\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">         6151.02</td><td style=\"text-align: right;\">183000</td><td style=\"text-align: right;\"> -2.6235</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            262.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-04-41\n",
      "  done: false\n",
      "  episode_len_mean: 264.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.6414999999999873\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 682\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.580766767483865e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0311132722430758\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008103045732553306\n",
      "          policy_loss: 0.018074104769362345\n",
      "          total_loss: 0.01819708388712671\n",
      "          vf_explained_var: 0.2057335525751114\n",
      "          vf_loss: 0.010434110488535629\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.86341463414634\n",
      "    ram_util_percent: 53.80731707317074\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04305171018717818\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.27430456145636\n",
      "    mean_inference_ms: 2.212912216202217\n",
      "    mean_raw_obs_processing_ms: 2.3233928274632683\n",
      "  time_since_restore: 6179.401013851166\n",
      "  time_this_iter_s: 28.383760452270508\n",
      "  time_total_s: 6179.401013851166\n",
      "  timers:\n",
      "    learn_throughput: 1245.691\n",
      "    learn_time_ms: 802.767\n",
      "    load_throughput: 49791.411\n",
      "    load_time_ms: 20.084\n",
      "    sample_throughput: 34.962\n",
      "    sample_time_ms: 28602.213\n",
      "    update_time_ms: 8.953\n",
      "  timestamp: 1635087881\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 184\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">          6179.4</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\"> -2.6415</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            264.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 185000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-05-10\n",
      "  done: false\n",
      "  episode_len_mean: 266.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.660299999999987\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 685\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.580766767483865e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0028484827942319\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007069269274547531\n",
      "          policy_loss: -0.10021314041482078\n",
      "          total_loss: -0.09550734787351556\n",
      "          vf_explained_var: 0.23436233401298523\n",
      "          vf_loss: 0.014734274759474728\n",
      "    num_agent_steps_sampled: 185000\n",
      "    num_agent_steps_trained: 185000\n",
      "    num_steps_sampled: 185000\n",
      "    num_steps_trained: 185000\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.43170731707318\n",
      "    ram_util_percent: 53.76829268292683\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04305040593377495\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.27381055754146\n",
      "    mean_inference_ms: 2.21291474665305\n",
      "    mean_raw_obs_processing_ms: 2.322853510134797\n",
      "  time_since_restore: 6208.390108823776\n",
      "  time_this_iter_s: 28.989094972610474\n",
      "  time_total_s: 6208.390108823776\n",
      "  timers:\n",
      "    learn_throughput: 1246.294\n",
      "    learn_time_ms: 802.379\n",
      "    load_throughput: 52819.333\n",
      "    load_time_ms: 18.932\n",
      "    sample_throughput: 34.501\n",
      "    sample_time_ms: 28984.662\n",
      "    update_time_ms: 9.394\n",
      "  timestamp: 1635087910\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 185000\n",
      "  training_iteration: 185\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   185</td><td style=\"text-align: right;\">         6208.39</td><td style=\"text-align: right;\">185000</td><td style=\"text-align: right;\"> -2.6603</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            266.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 186000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-05-41\n",
      "  done: false\n",
      "  episode_len_mean: 267.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.678399999999986\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 689\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.580766767483865e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7722541318999396\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005213172922696272\n",
      "          policy_loss: 0.001293000289135509\n",
      "          total_loss: 0.005000645625922415\n",
      "          vf_explained_var: 0.23144526779651642\n",
      "          vf_loss: 0.011430185132970413\n",
      "    num_agent_steps_sampled: 186000\n",
      "    num_agent_steps_trained: 186000\n",
      "    num_steps_sampled: 186000\n",
      "    num_steps_trained: 186000\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.98\n",
      "    ram_util_percent: 53.724444444444444\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04304842152376911\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.271869213095243\n",
      "    mean_inference_ms: 2.212910390379832\n",
      "    mean_raw_obs_processing_ms: 2.3221660772191517\n",
      "  time_since_restore: 6239.877172231674\n",
      "  time_this_iter_s: 31.48706340789795\n",
      "  time_total_s: 6239.877172231674\n",
      "  timers:\n",
      "    learn_throughput: 1246.229\n",
      "    learn_time_ms: 802.421\n",
      "    load_throughput: 52914.222\n",
      "    load_time_ms: 18.899\n",
      "    sample_throughput: 34.248\n",
      "    sample_time_ms: 29199.182\n",
      "    update_time_ms: 8.59\n",
      "  timestamp: 1635087941\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 186000\n",
      "  training_iteration: 186\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   186</td><td style=\"text-align: right;\">         6239.88</td><td style=\"text-align: right;\">186000</td><td style=\"text-align: right;\"> -2.6784</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            267.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=380549)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 187000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-06-28\n",
      "  done: false\n",
      "  episode_len_mean: 270.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.708299999999986\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 693\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.580766767483865e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1835541798008813\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011988522111465411\n",
      "          policy_loss: -0.0008499574330117967\n",
      "          total_loss: 0.0014804030458132427\n",
      "          vf_explained_var: 0.30632245540618896\n",
      "          vf_loss: 0.014165900813208687\n",
      "    num_agent_steps_sampled: 187000\n",
      "    num_agent_steps_trained: 187000\n",
      "    num_steps_sampled: 187000\n",
      "    num_steps_trained: 187000\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.36060606060607\n",
      "    ram_util_percent: 53.63636363636364\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04304608489615086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.268622038756284\n",
      "    mean_inference_ms: 2.212901017946003\n",
      "    mean_raw_obs_processing_ms: 2.325105439977396\n",
      "  time_since_restore: 6286.495095729828\n",
      "  time_this_iter_s: 46.61792349815369\n",
      "  time_total_s: 6286.495095729828\n",
      "  timers:\n",
      "    learn_throughput: 1239.004\n",
      "    learn_time_ms: 807.1\n",
      "    load_throughput: 51500.63\n",
      "    load_time_ms: 19.417\n",
      "    sample_throughput: 31.814\n",
      "    sample_time_ms: 31432.846\n",
      "    update_time_ms: 8.348\n",
      "  timestamp: 1635087988\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 187000\n",
      "  training_iteration: 187\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">          6286.5</td><td style=\"text-align: right;\">187000</td><td style=\"text-align: right;\"> -2.7083</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            270.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-06-59\n",
      "  done: false\n",
      "  episode_len_mean: 272.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.7214999999999865\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 696\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.580766767483865e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9122961421807607\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006699995721525593\n",
      "          policy_loss: 0.03034875334964858\n",
      "          total_loss: 0.03019070534242524\n",
      "          vf_explained_var: 0.2895183265209198\n",
      "          vf_loss: 0.00896491601338817\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.50909090909092\n",
      "    ram_util_percent: 53.67272727272728\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0430443305221097\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.265285142810068\n",
      "    mean_inference_ms: 2.212889209970885\n",
      "    mean_raw_obs_processing_ms: 2.327343502647874\n",
      "  time_since_restore: 6317.226484537125\n",
      "  time_this_iter_s: 30.731388807296753\n",
      "  time_total_s: 6317.226484537125\n",
      "  timers:\n",
      "    learn_throughput: 1227.799\n",
      "    learn_time_ms: 814.465\n",
      "    load_throughput: 53495.91\n",
      "    load_time_ms: 18.693\n",
      "    sample_throughput: 33.356\n",
      "    sample_time_ms: 29979.237\n",
      "    update_time_ms: 8.131\n",
      "  timestamp: 1635088019\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 188\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   188</td><td style=\"text-align: right;\">         6317.23</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\"> -2.7215</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            272.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 189000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-07-29\n",
      "  done: false\n",
      "  episode_len_mean: 274.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.745799999999986\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 700\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.580766767483865e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9062743888960945\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006438529541372123\n",
      "          policy_loss: 0.017561052905188668\n",
      "          total_loss: 0.021695766184065077\n",
      "          vf_explained_var: 0.21062467992305756\n",
      "          vf_loss: 0.013197456217474408\n",
      "    num_agent_steps_sampled: 189000\n",
      "    num_agent_steps_trained: 189000\n",
      "    num_steps_sampled: 189000\n",
      "    num_steps_trained: 189000\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.2139534883721\n",
      "    ram_util_percent: 53.78837209302325\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043042291712536186\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.259260203172747\n",
      "    mean_inference_ms: 2.2128778744315123\n",
      "    mean_raw_obs_processing_ms: 2.3270921328100864\n",
      "  time_since_restore: 6347.326187610626\n",
      "  time_this_iter_s: 30.099703073501587\n",
      "  time_total_s: 6347.326187610626\n",
      "  timers:\n",
      "    learn_throughput: 1228.921\n",
      "    learn_time_ms: 813.722\n",
      "    load_throughput: 50765.162\n",
      "    load_time_ms: 19.699\n",
      "    sample_throughput: 33.234\n",
      "    sample_time_ms: 30089.367\n",
      "    update_time_ms: 8.083\n",
      "  timestamp: 1635088049\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 189000\n",
      "  training_iteration: 189\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">         6347.33</td><td style=\"text-align: right;\">189000</td><td style=\"text-align: right;\"> -2.7458</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            274.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 190000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-08-00\n",
      "  done: false\n",
      "  episode_len_mean: 275.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.758499999999985\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 703\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.580766767483865e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8505037976635828\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004321634074443952\n",
      "          policy_loss: -0.09724817822376887\n",
      "          total_loss: -0.09291625883844164\n",
      "          vf_explained_var: 0.1196347251534462\n",
      "          vf_loss: 0.012836957991951042\n",
      "    num_agent_steps_sampled: 190000\n",
      "    num_agent_steps_trained: 190000\n",
      "    num_steps_sampled: 190000\n",
      "    num_steps_trained: 190000\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.09777777777776\n",
      "    ram_util_percent: 53.72\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04304119886107606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.254120658654223\n",
      "    mean_inference_ms: 2.2128690722323316\n",
      "    mean_raw_obs_processing_ms: 2.326003076252756\n",
      "  time_since_restore: 6378.913667917252\n",
      "  time_this_iter_s: 31.587480306625366\n",
      "  time_total_s: 6378.913667917252\n",
      "  timers:\n",
      "    learn_throughput: 1228.597\n",
      "    learn_time_ms: 813.936\n",
      "    load_throughput: 49431.811\n",
      "    load_time_ms: 20.23\n",
      "    sample_throughput: 33.486\n",
      "    sample_time_ms: 29863.557\n",
      "    update_time_ms: 7.812\n",
      "  timestamp: 1635088080\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 190000\n",
      "  training_iteration: 190\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   190</td><td style=\"text-align: right;\">         6378.91</td><td style=\"text-align: right;\">190000</td><td style=\"text-align: right;\"> -2.7585</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            275.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 191000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-08-30\n",
      "  done: false\n",
      "  episode_len_mean: 277.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.7785999999999844\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 707\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.903833837419325e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9347180055247413\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010795824569188678\n",
      "          policy_loss: 0.013954985638459523\n",
      "          total_loss: 0.018269987859659723\n",
      "          vf_explained_var: 0.10139656811952591\n",
      "          vf_loss: 0.013662186016639074\n",
      "    num_agent_steps_sampled: 191000\n",
      "    num_agent_steps_trained: 191000\n",
      "    num_steps_sampled: 191000\n",
      "    num_steps_trained: 191000\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.95348837209302\n",
      "    ram_util_percent: 53.66976744186047\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04303952751480223\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.245856772750404\n",
      "    mean_inference_ms: 2.2128493064014636\n",
      "    mean_raw_obs_processing_ms: 2.324583949257266\n",
      "  time_since_restore: 6408.644435167313\n",
      "  time_this_iter_s: 29.730767250061035\n",
      "  time_total_s: 6408.644435167313\n",
      "  timers:\n",
      "    learn_throughput: 1228.332\n",
      "    learn_time_ms: 814.112\n",
      "    load_throughput: 48617.492\n",
      "    load_time_ms: 20.569\n",
      "    sample_throughput: 33.081\n",
      "    sample_time_ms: 30229.018\n",
      "    update_time_ms: 5.861\n",
      "  timestamp: 1635088110\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 191000\n",
      "  training_iteration: 191\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">         6408.64</td><td style=\"text-align: right;\">191000</td><td style=\"text-align: right;\"> -2.7786</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            277.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-09-00\n",
      "  done: false\n",
      "  episode_len_mean: 279.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.7988999999999833\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 711\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.903833837419325e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9894795795281728\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00810265156171014\n",
      "          policy_loss: 0.016501261666417123\n",
      "          total_loss: 0.022351809798015487\n",
      "          vf_explained_var: 0.06857814639806747\n",
      "          vf_loss: 0.01574534263668789\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.57857142857144\n",
      "    ram_util_percent: 53.62142857142857\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043037835775917126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.23662562108986\n",
      "    mean_inference_ms: 2.2128269565026915\n",
      "    mean_raw_obs_processing_ms: 2.323115919098309\n",
      "  time_since_restore: 6438.314498186111\n",
      "  time_this_iter_s: 29.670063018798828\n",
      "  time_total_s: 6438.314498186111\n",
      "  timers:\n",
      "    learn_throughput: 1225.357\n",
      "    learn_time_ms: 816.089\n",
      "    load_throughput: 49946.818\n",
      "    load_time_ms: 20.021\n",
      "    sample_throughput: 32.418\n",
      "    sample_time_ms: 30846.821\n",
      "    update_time_ms: 5.912\n",
      "  timestamp: 1635088140\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 192\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">         6438.31</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\"> -2.7989</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            279.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4b4a2_00000:\n",
      "  agent_timesteps_total: 193000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_15-09-31\n",
      "  done: false\n",
      "  episode_len_mean: 281.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.9200000000000015\n",
      "  episode_reward_mean: -2.813199999999984\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 714\n",
      "  experiment_id: f0df27c57fd44ab2b393235f81519793\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.903833837419325e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6880873872174157\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01242304261419586\n",
      "          policy_loss: -0.030038443124956554\n",
      "          total_loss: -0.02522884060939153\n",
      "          vf_explained_var: 0.051288966089487076\n",
      "          vf_loss: 0.01169047321503361\n",
      "    num_agent_steps_sampled: 193000\n",
      "    num_agent_steps_trained: 193000\n",
      "    num_steps_sampled: 193000\n",
      "    num_steps_trained: 193000\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.5909090909091\n",
      "    ram_util_percent: 53.64772727272728\n",
      "  pid: 380543\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04303666249320497\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.228976909359243\n",
      "    mean_inference_ms: 2.212825391259341\n",
      "    mean_raw_obs_processing_ms: 2.322072644914353\n",
      "  time_since_restore: 6469.24723815918\n",
      "  time_this_iter_s: 30.932739973068237\n",
      "  time_total_s: 6469.24723815918\n",
      "  timers:\n",
      "    learn_throughput: 1224.907\n",
      "    learn_time_ms: 816.389\n",
      "    load_throughput: 49850.471\n",
      "    load_time_ms: 20.06\n",
      "    sample_throughput: 32.286\n",
      "    sample_time_ms: 30973.238\n",
      "    update_time_ms: 6.455\n",
      "  timestamp: 1635088171\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 193000\n",
      "  training_iteration: 193\n",
      "  trial_id: 4b4a2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.37 GiB heap, 0.0/11.68 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-24_13-21-27<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4b4a2_00000</td><td>RUNNING </td><td>192.168.3.5:380543</td><td style=\"text-align: right;\">   193</td><td style=\"text-align: right;\">         6469.25</td><td style=\"text-align: right;\">193000</td><td style=\"text-align: right;\"> -2.8132</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            281.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "analysis = tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 1,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 1000,\n",
    "             #\"gamma\": 0.99,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO MultiTask <=10 pretrained (AngelaCNN + changed policy) (3 noops after placement) r: -0.01\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
